[{"url": "https://github.com/ANYbotics/elevation_mapping/issues/4", "issue_title": "Add example dataset to test installation", "issue_status": "Open", "posted_on": "May 13, 2014", "issue_contents": ["Title \"Test Installation on Basic Dataset\" in README"]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/13", "issue_title": "Verify/revise frame id handling in GetGridMap request", "issue_status": "Open", "posted_on": "Oct 22, 2014", "issue_contents": ["Currently z-component is ignored when determining the map positions with respect to the requested frame, might want to transform maps"]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/25", "issue_title": "Underlying map error", "issue_status": "Open", "posted_on": "Sep 14, 2015", "issue_contents": ["Fix issue when current map is bigger than underlying map."]}
,{"url": "https://github.com/AutonomyLab/bebop_autonomy/issues/40", "issue_title": "Migrate all package.xml files to format 2", "issue_status": "Open", "posted_on": "Mar 30, 2016", "issue_contents": ["Resources:"]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/1024", "issue_title": "Update drive-px2 dependencies", "issue_status": "Open", "posted_on": "Jan 11, 2018", "issue_contents": ["For now, the versions of SSL package was updated from 4.9 to 4.10.", "\n", "Finally, how do you think to deal with continuous updates of dependencies like this?", " you need to have an Autoware branch and CI pipeline that builds for ARM64 and their version of Ubuntu. CI has to always start building from a clean and latest Ubuntu version.", "I suggest to have 1h workshop with Apex and we can show you how CI is done correctly.", "First of all we will add docker. "], "issue_code": ["sudo apt-get install -y libssl1.0.0=1.0.2g-1ubuntu4.10\nsudo apt-get install -y libssl-dev=1.0.2g-1ubuntu4.10\n"], "contents_details_more": ["Ubuntu version: 4.9.38-rt25-tegra", "ROS Version: kinetic", "Autoware branch: develop"]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/1618", "issue_title": "[Feature]System monitor plugin for rviz", "issue_status": "Open", "posted_on": "Oct 18, 2018", "issue_contents": ["Now, Autoware displays CPU/Memory usage only on run time manager.", "\nHowever, users would most likely be looking at rviz when driving a car.", "\nIt be safe and useful if CPU/Memory usage can be monitored through rviz panel plugin.", "I want to add a simple system monitor plugin for rviz."]}
,{"url": "https://github.com/AutonomyLab/bebop_autonomy/issues/129", "issue_title": "looks like some bugs about moving the virtual camera (TF and joint_states)", "issue_status": "Open", "posted_on": "Aug 3, 2017", "issue_contents": ["After start the bebop_nodelet.launch, use rqt_tf_tree tool, seems work fine.", "\nuse rostopic echo /bebop/joint_states seems get the correct camera Pan&Tilt states.", "\nand I try to move the virtual camera, for example,", "The camera view is moving, but use rostopic echo /bebop/joint_states can't get any messages any more, and rqt_tf_tree tools shows bellow,", "It seems that the connection is interrupted.", "\nHow to fix it?", "Thanks!", "I have tried indigo-devel and sdk-3-12 two branches of this package, it seems that this problem still exist.", "I have the same problem. Would you fix it? Is any other topic where I can get the camera angles?", "\nThank you.", "I forked the repository and fixed the bug for the beobp2: ", ", great!! Why you didn't open a pull request? It would be nice to share your results with others.", " Because I am not sure if the package would still work with the bebop 1. I only have a bebop2 to test :/", ", don't worry. I think the maintainer or contributors have a Bebop 1 to test the code.", " Ok, I made the pull request: ", " ", "  why it is called virtual since there is a camera there!!! I don't get it", "Hi ", " , I really don't know. You should ask the authors of the ROS package. However, please, ask your questions in the relevant sections. If there are no issues on this feel free to open a new one, otherwise follow the existing discussions."], "issue_code": ["rostopic pub /bebop/camera_control geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: -10.0\n  z: 0.0\"\n"]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/2003", "issue_title": "[Feature] Error Analyzer Tool for Autoware Health Checker.", "issue_status": "Open", "posted_on": "Feb 19, 2019", "issue_contents": ["Autoware Health Checker publish a lot of diagnostics data, but it is very difficult for us to know the cause of the problem.", "We add graph-based analyzer for Autoware nodes and publish filtered data.", "\n", "\nAnalyzer is watching the topic connection between Autoware Nodes and try to find the cause of the diagnostics data."]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/52", "issue_title": "Modifying Elevation_mapping to work with wheeled robots", "issue_status": "Open", "posted_on": "Dec 8, 2017", "issue_contents": ["I was wondering what files/methods I need to look at to modify Elevation Mapping to work with a wheeled robot? In your video, it shows red for any height change such as walking from floor to a plank. As well it shows blue for any flat surface. Is there a way to modify this so that the robot can be blue for minor slopes that it can climb?"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/26", "issue_title": "Person recognition", "issue_status": "Open", "posted_on": "Apr 9, 2018", "issue_contents": ["A good place to start is to take a look at the ", " and ", " actions, the task would be to create an action just like these.", "We don't necessarily need to use this in the end, but ", " obviously has things for this. Any other interesting approach is also welcome.", "This is for recognizing i.e. members of the team and say their names or follow?", "Yeah, the action after recognizing the person will vary from task to task, but that is indeed the first part of the problem.", "\nThe second thing to consider is that we might want to recognize people without associating them with a name, i.e. tracking persons inside the appartment and being able to distinguish between them", "If someone wants to try out CNNs for this, a Siamese network might be useful. ", " is a paper in which the model is described; ", "'s a useful blog post as well.", "I was thinking of bag of features may be simpler", "Yeah, it's definitely simpler; I don't know how the accuracies compare though (as the set of people grows).", "I'd like to add a few comments here that we might need to consider:", "\nIt's important to be able to recognize the ", " as well, e.g. post-man, delivery man, police, etc. so we might want to do something that not only relies on the face of the person but also on their clothing.", "How's the progress on this? With Single Shot Multibox, detecting people seems to be quite reliable. As for classifying roles, genders, and emotions, I think we can use the same ", " state currently used in ", ". The image recognition service that the action interacts with takes a model name in its request. We can specify which model to use here.", "Detection is not the only thing here though; it's important to recognise the actual person as well (so learning from one/only a few examples is needed).", " can't we do a binary classifier trained on a single pic of one person?", "\n", " yeah I'll start putting something together for this.", "I suppose we could use something like the examplar SVMs described ", " (in this case, they're used for detection, but we could use the same idea for recognition) - or the Siamese networks I suggested above.", "The model described ", " seems to be very appropriate; trying that one out is the most reasonable thing to do (there is an existing implementation ", ").", "I believe there was a person at HBRS computer science working on this subject. As far as I can remember he simply used a pre-train network (e.g. VGG16) took the output of a resized image of the face and compared it  against each sample of a set of saved face features using cosine similarity. You can extend this by using data augmentation of the sampled image to obtain a more robust description.", "I recently noticed in the PR list of ", " 's ", " repo a recommendation to use ", " instead of OpenCV for face detection which sounds interesting: ", ".", "Additionally, I believe the classification models trained by Octavio can be loaded into ", " like with object recognition. Then the will be no extra code needed for gender and emotion classification. All you'll need in the repo are the model files. The action then can simply request the service with the appropriate model name, and a classification result will be returned.", "Should this be closed in favour of ", "?", "A unique discussion here is the mention of Siamese networks for learning new faces, which is a feature for the person identity model. The other issue deals with refactoring and architecture."], "issue_code": ["detect_person", "gender_recognition", "ImageRecognition", "perceive_plane_action", "face_classification", "dlib", "ImageRecognitionServer"]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/1726", "issue_title": "safety check function for other lanes", "issue_status": "Open", "posted_on": "Nov 21, 2018", "issue_contents": ["Safety confirmation is necessary under the following circumstances, but the function is not yet implemented.", "Is thit easy to add-on?", "Thank you for pointing out, I updated it."], "contents_details_more": ["Lane change", "Turn right at an intersection (in left-hand traffic country)", "Turn left at an intersection (in right-hand traffic country)", "Turn left at an intersection (in right-hand traffic country)"]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/56", "issue_title": "Question: What is the argument for choosing the pose interface?", "issue_status": "Open", "posted_on": "Jan 5, 2018", "issue_contents": ["Elevation mapping now listens to a pose topic with message type geometry_msgs/PoseWithCovarianceStamped. What is the argument for choosing this? Another option would be to enable the user to provide the pose using a nav_msgs/Odometry (maybe as an alternative), as this also contains the pose and covariance. A nice perk of an Odometry interface is easy integration with ", ".", "I haven't found a way to solve this, so I use a node to export PoseWithCovarianceStamped from Odometry. Feel free to use/modify: ", "Sorry ", " do you know any translator to get that PoseWithCovarianceStamped from tf because gmapping puts odometry in tf", "Hi ", ", at the moment we don't support nav_msgs/Odometry. There is no reason to not use it. If anyone wants to write a feature to support odometry messages feel free to submit a pull request. In the meantime, ", "'s node should do the job.", " ", " will listen to a tf transform and publish it as a PoseWithCovarianceStamped message."]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/59", "issue_title": "Parameter to reduce/regulate verbosity level", "issue_status": "Open", "posted_on": "Jan 30, 2018", "issue_contents": ["Is there a parameter to reduce the verbosity level of the elevation mapping node (e.g. removing all the ROS_INFO \"elevation map has been fused ...\")?", "Thanks,", "\nMarco.", "You can set the logger level either externally or in the launch file with", "and the file ", " with", "Does this help?", "Yes! thank you!", "Too much output slows down the execution."], "issue_code": ["<env name=\"ROSCONSOLE_CONFIG_FILE\" value=\"$(find your_package)/config/rosconsole.conf\"/>\n", "rosconsole.conf", "log4j.logger.ros.elevation_mapping=WARN\n"]}
,{"url": "https://github.com/AutonomyLab/ardrone_autonomy/issues/175", "issue_title": "Question about slow yaw and a takeoff issue", "issue_status": "Open", "posted_on": "Dec 30, 2015", "issue_contents": ["I'm currently working with the ardrone autonomy library, and I've run into two problems that I was hoping I might get some input on.", "The first problem being adjusting the drones yaw at small values. If I use the max value of 1 and -1, it will yaw left and right fine. But, as I get to smaller values, such as 0.1 and -0.1, it will yaw slowly right, but when it it supposed to yaw slowly left, the motors seem to struggle and it fails to yaw left.", "This brings me to the second issue, which may not be directly related to the library, but I was hoping for some input on it. I attempted to run this same code on a brand new never used drone to test if it was a physical issue causing the yaw problems. I performed the flat trim command using the autonomy library before it launched. But, as soon as it launched it appeared to try to do a flip and crashed. I loaded up the android app to test it and performed a flat trim there. It successfully launched using the app without flipping, but something was off because it was stuck moving backwards slightly. When I tried flying it using the autonomy library again, it didn't flip, but it had the same backwards movement.", "Any ideas or suggestions on what the problems might be?", "Similar problem. Having any suggestions?"]}
,{"url": "https://github.com/AutonomyLab/bebop_autonomy/issues/173", "issue_title": "Anafi Support", "issue_status": "Open", "posted_on": "Sep 4, 2018", "issue_contents": ["Searched for \"Anafi\" but did not find any existing issues. Are there any plans for adding support for the new Parrot Anafi? Before that can happen, it appears the SDK first has to be made available by Parrot, see ", ".", "Hey there! The ", " has been officially updated this week. Now, existing modules for Bebop drones should be usable in Anafi.", "Has anyone used this software with the Anafi?", "I didn't need it. The Parrot SDK does the work as of now. If you are willing to use ROS/2, you can interface with the SDK quite easily", " , thanks for the quick reply. And what about ROS 1 ? Also, could you point me to the code you used for interfacing ROS2 with the Anafi  ?"]}
,{"url": "https://github.com/AutonomyLab/bebop_autonomy/issues/174", "issue_title": "bebop_autonomy + opencv", "issue_status": "Open", "posted_on": "Oct 1, 2018", "issue_contents": ["hello!", "i'm trying to import the video stream from my drone bebop2 (using ros kinetic and bebop_autonomy) to opencv", "i'm connecting to the drone using the command", "\n$ roslaunch bebop_tools bebop_nodelet_iv.launch", "this command allows me to connect to the drone and to get the video stream in a new windows, but i can't import that stream in opencv.", "I've installed opencv_bridge ( $ sudo apt-get install ros-kinetic-cv-bridge ) but I can't use it.", "\nDoes anyone know how to get the video stream in opencv?", "Thanks", "Tutorials on using the cv_bridge package for coding in ", " and ", ". For one of my projects I use a modified version of the Python example.", "I doesn't understand how the cv bridge work", "\nDo I have to create a new nodlet and suscribe to the video stream?"]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/57", "issue_title": "Stereo camera parameter descriptions missing/unclear", "issue_status": "Open", "posted_on": "Jan 25, 2018", "issue_contents": ["The stereo camera sensor model includes the following parameters:", "However, there is no explanation of what these parameters mean. Can anyone help me with this?", "Any updates on this? I am trying to use Zed camera for elevation mapping and am stuck at this step as well.", "See ", " .", "Hi ", ",", "\nthis ", " describes the parameters in more detail compared to the paper.", "In the future, we will add a detailed description of the parameters.", "Regarding new camera sensors, the users have to measure the corresponding parameters by themselves.", "Thank you for this additional source, ", ".", "Unfortunately, I don't fully understand the parameters of the sensor model, yet. I've \"reverse-engineered\" (in other words, looked it  ", " up) this particular formula:", "\n", "However, I asume, that the ", "-factor is given by the focal length (", ") times the baseline (", "). Further, I denoted the disparity by ", ". ", " is the column and ", " the row, as it seems to me.", "Because I didn't know what to do further, I plotted everthing with the provided parameters in ", ". For the disparity, I asumend a range from 30 to 180 (just for a first shot).", "\n", "So, at first glance, it seems plausible so far. The devation increases with larger distances, as expected.", "The questions that remain:", "Okay, here are some news! At first, forget what I've written in the post above.", "So, let's have a look at new theory. The first mistake in the above post was not to respect the spatiality of an image. The variance is not constant over the whole image. Why am I thinking that? Have a look at the following image. The surface in front of the camera is perfectly perpendicular the to optical axis. So, the two points have an equal distance to the camera in z-direction. But, the euclidean distance is unequal (D1<D2). For my understanding, this means a point that is farther from the camera is more prone to errors.", "This theory fits pretty well to the formula above. I used ", " for testing and generated an image with the dimension 640x480 and a constant disparity of 100 (just a first-shot).", "Here is the result:", "\n", "Looks good at first glace. However, it doesn't tell us what are the parameters stand for. And last but not leat where this magic ", " comes from.", "The magic ", " was my biggest question. I'm pretty sure that this is half the resolution in one direction (2*240=480 --> a common resolution). But why is it fixed in the source code?", "Next, the parameters. At first ", " seems to be just a common offset for the whole variance all over the image. ", " looks like a gain without any (direct) relation to the disparity and ", " is the same with respect to the disparity?", " can be a vertical shift or, in other words, the principal point? I adjusted it in to my image resoltion and it seems to represent the principal point. So, just ", " is left. This seems to be bit tricky. It shifts the image horizontally with respect to the disparity.  Have a look at the image below.", "But now, two questions are left. How to setup the measurement and select the parameters ", ", ", ", ", " and ", "?", "Hi ", " thesis you have referenced in this post referred to a student project report for more details about the sensor error model for the RealSense ZR300 on page 35, as follows:", "\nTakahiro, M., Fankhauser, P., and Bjelonic, M. (2017). \u201cError Analysis of the RealSense", "\nZR300, Obstacle Removal for Elevation Mapping, and Swing Trajectory Planning for", "\nANYmal\u201d. Student\u2019s Project. ETH Zurich, Switzerland.", "\nUnfortunately, this report is not accessible; I think should be accessible from ETH library. Not sure if ", "  has found an answer about what these parameters mean, looking for more details too. If possible please help. Thanks.", "I found an interesting repo, submitted by ", ". ", " contains MATLAB code to generate the sensor model for a stereo camera, based on these mystic parameters. Actually, I don't have that much time to analyze the code in depth...But at first glance, it should be possible to work this out.", "And maybe Hannes will read this and share his work, mentioned in the code ", " thank you for your input and the link to the repo. ", " ", " the original author of that sensor processor.", "And maybe Hannes will read this and share his work, mentioned in the code smiley.", "Let's see. ", " does ", " work for you?", "Thanks ", ". Yes it does. Although, I end up adding another sensor processor with in the elevation mapping framework using different parameters from my own tests, but it is interesting to review the work done by Nguyen et.al and then extended by ", ". Thanks.", " good to know. In what setup are you using elevation mapping?", "For terrain mapping in robotics applications and only for research purpose.", " thanks, I was just curious on what cool projects elevation map gets used."], "issue_quotes": ["And maybe Hannes will read this and share his work, mentioned in the code smiley."], "contents_details_more": ["p_1", "p_2", "p_3", "p_4", "p_5", "lateral_factor", "depth_to_disparity_factor", "How to solve for ", " to ", "? I could imagine an iterative solution, started with an educated guess...", "Where does the ", " comes from? (It seems a bit magic, or does it refer to a specific camera resolution?)", "In the example ", " a related project work is referenced. I searched quite a long time, but couldn't find anything. It would be great if someone could send me this work."]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/1354", "issue_title": "Docker Startup Error - autoware-docker: line 138: kill: (4109) - No such process", "issue_status": "Open", "posted_on": "Jun 26, 2018", "issue_contents": ["Fill-out only one section depending on whether you are reporting a bug or a new feature.", "The following is the error that is repeatedly produced when I try to run the autoware-docker after following all the steps in sequence as listed in the page:", "nvidia@nvidia:~$ autoware-docker", "\n[INFO]:Autoware-Driveworks module is found. Autoware can use driveworks.", "\n[WARN]:Execute script with sudoers.", "\n[INFO]:The autoware-docker image [tier4cr.io/dpx2/autoware:1.6.2] was ready.", "\n[INFO]:GMSL Camera initialization sequence is start.", "\n.....autoware-docker: line 138: kill: (4109) - No such process", "[INFO]:GMSL Camera initialization sequence was end.", "\nnon-network local connections being added to access control list", "\naccess control disabled, clients can connect from any host", "\ndocker: Error response from daemon: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:359: container init caused \\\"rootfs_linux.go:54: mounting \\\\\\\"/usr/lib/pkgconfig/driveworks.pc\\\\\\\" to rootfs \\\\\\\"/var/lib/docker/overlay2/a68cd442350b924721909e322bb30ce898db759a4548a1a39488e50341936e1f/merged\\\\\\\" at \\\\\\\"/var/lib/docker/overlay2/a68cd442350b924721909e322bb30ce898db759a4548a1a39488e50341936e1f/merged/usr/lib/pkgconfig/driveworks.pc\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\"\\n\": Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type.", "The command is supposed to open the Autoware interface.", "An error is produced as the one shown in the attachment."]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/1368", "issue_title": "Autoware Source build is not working with Drive PX2", "issue_status": "Open", "posted_on": "Jul 11, 2018", "issue_contents": ["Fill-out only one section depending on whether you are reporting a bug or a new feature.", "Despite meeting all the prerequisites for the source build specified in this website:", "At the command line:", "I encountered the following error:", "The error report is as follows:", "The command was just supposed to run the code and setup the catkin work-space. Any help in this regard is appreciated.", " The error report attached does not contain any errors.  Are you sure it's the complete file?", " & ", ", I am also getting build errors, while trying to install autoware from the source. Could you please help me to fix.", "Please check PR(", " ) (source build error without log information ", ")", "Kind regards,", "\nAjay", "Here's how we managed to compile Autoware 1.8", "\nInstructions based on these two links with some modifications and additions", "\n", "\n", "Ubuntu 16.04 with proposed packages 2018/10/22:", "\nsudo apt update", "\nsudo apt upgrade", "\nto fix gnome-terminal, set: LC_ALL=en_US.UTF-8", "Preparations for ros/autoware installation Replace graphics libraries supplied by nvidia with default ones.", "mkdir -p backup/usr/lib", "\nsudo cp -a /usr/lib/libdrm* backup/usr/lib", "\nsudo cp -a /usr/lib/libwayland-* backup/usr/lib", "mkdir -p backup/etc/nvidia", "\nsudo cp -a /etc/nvidia/nvidia_gl.conf backup/etc/nvidia", "\nsudo cp -a /etc/nvidia/nvidia_egl.conf backup/etc/nvidia", "sudo apt-get install --reinstall -y libdrm2 libdrm-dev libwayland-client0 libwayland-cursor0 libwayland-egl1-mesa libwayland-server0 libwayland-dev", "\nsudo ldconfig", "REBOOT", "ROS INSTALLATION", "\nsudo sh -c 'echo \"deb ", " $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list'", "\nsudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116", "\nsudo apt-get update", "sudo apt-get install -y build-essential cmake python-pip", "\nsudo apt-get install -y checkinstall", "\nsudo apt-get install -y python-rosdep python-rosinstall-generator python-wstool python-rosinstall build-essential", "\nsudo apt-get install -y libavutil-ffmpeg54", "\nsudo apt-get install -y libswresample-ffmpeg1", "\nsudo apt-get install -y libavformat-ffmpeg56", "\nsudo apt-get install -y libswscale-ffmpeg3", "\nsudo apt-get install -y libssl1.0.0=1.0.2g-1ubuntu4.13", "\nsudo apt-get install -y libssl-dev=1.0.2g-1ubuntu4.13", "\nsudo apt-get install -y ros-kinetic-desktop-full", "\nsudo apt-get install -y ros-kinetic-nmea-msgs ros-kinetic-nmea-navsat-driver ros-kinetic-sound-play ros-kinetic-jsk-visualization ros-kinetic-grid-map ros-kinetic-gps-common", "\nsudo apt-get install -y ros-kinetic-controller-manager ros-kinetic-ros-control ros-kinetic-ros-controllers ros-kinetic-gazebo-ros-control ros-kinetic-joystick-drivers", "\nsudo apt-get install -y libnlopt-dev freeglut3-dev qtbase5-dev libqt5opengl5-dev libssh2-1-dev libarmadillo-dev libpcap-dev gksu libgl1-mesa-dev libglew-dev", "\nsudo apt-get install -y ros-kinetic-camera-info-manager-py ros-kinetic-camera-info-manager", "AUTOWARE", "\nsource /opt/ros/kinetic/setup.bash", "sudo apt-get install -y openssh-server libnlopt-dev freeglut3-dev qtbase5-dev libqt5opengl5-dev libssh2-1-dev libarmadillo-dev libpcap-dev git", "\nsudo apt-get install -y libnlopt-dev freeglut3-dev qt5-default libqt5opengl5-dev libssh2-1-dev libarmadillo-dev libpcap-dev libglew-dev gksu", "\nsudo apt-get install -y libxmu-dev python-wxgtk3.0 python-wxgtk3.0-dev", "sudo ln -s /usr/include/aarch64-linux-gnu/qt5 /usr/include/qt5", "\nsudo ln -s /usr/local/cuda/lib64/libcudart.so /usr/lib/libcudart.so", "cd", "\ngit clone ", "\ncd ~/Autoware", "\ngit submodule update --init --recursive", "cd ~/Autoware/ros/src", "\ncatkin_init_workspace", "\ncd ..", "\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO", "\n./catkin_make_release -j 1", "To run demo follow this video:", "\n"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/44", "issue_title": "Create a find (object) action", "issue_status": "Open", "posted_on": "May 1, 2018", "issue_contents": ["The current action is embedded in the perceive plane action and should be refactored and separated to its own action server.", "\nThis should also apply to other objects, for instance, shelves, and counters. It can potentially be even more generalizable, i.e. people.", "I agree that we need this action, but I don't think we can write it before having a decent knowledge base; without that, finding things will be a shot in the dark.", "I'm talking about two types of knowledge here: encyclopedic knowledge (e.g. cups are stored in the kitchen), but also user-specific knowledge (e.g. Alex's cup is usually on the kitchen counter and not in the cupboard).", "I agree first we need to at least fix ", ", then solve this one. I guess for the world model we need to detect and recognize this objects first anyways, which was a bit what I was going for.", "So recently the object detection code is rewritten as an action server in this ", ". Does that more or less solve this issue, or do you have some other thing in mind for the action?", "This is not an action about pure perception, but also requires a knowledge base (where things are likely to be stored so that we don't search blindly) and a recovery/exploration strategy in case the item is not found immediately.", "PR ", " is an early-stage WIP for creating this knowledge base for the record.", "Now that the basic implementation of the ontology and the knowledge base interface is set up (", "), I finally have an initial draft of the action: ", "The definition of the action allows finding either a specific object or an object of a given category, though for now, I'm only focused on specific objects.", "The basic workflow of the action (in case of finding a specific object) is as follows:", "There's obviously still quite a lot to be done here, but it's a start.", "possibly related paper and the project it belongs to"], "contents_details_more": ["we first check if the object is already in the knowledge base (i.e. if we already know where it is); if yes, that is the result of the action (although we should actually verify that the object is still there)", "if the knowledge base doesn't have information about where the object is, we look for the default storing location in the ontology and we take that information as the result (although, just as above, we should verify that the object is actually there)"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/57", "issue_title": "Sound localization without manyears", "issue_status": "Open", "posted_on": "Jul 6, 2018", "issue_contents": ["Possible alternatives: ", ":'( is this for DSPL only?", "We are not allowed to add additional hardware in DSPL, so yeah, we need to look for alternatives.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/70", "issue_title": "Travis build doesn't consider new dependencies added to rosinstall file", "issue_status": "Open", "posted_on": "Sep 11, 2018", "issue_contents": ["\n            ", "\n          ", "Probably related to ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/73", "issue_title": "Continuous scene perception for accurate world modelling", "issue_status": "Open", "posted_on": "Oct 12, 2018", "issue_contents": ["In the current implementation of our ", " action, we store the poses of the detected objects with respect to the ", " frame in the world model, but we don't update them afterwards. This leads to a problem when the robot moves since the objects' poses are not valid anymore - unless we also keep track of the robot's motion. We could alternatively store the poses with respect to a fixed frame, but then we would be relying too much on the localisation being accurate.", "An obvious solution here would be reperceiving the scene so that we can correct for any localisation errors when the robot moves. This will take care of some dynamic aspects of the environment (e.g. objects being moved around), but if we want our world model to be accurate, we also need to be able to figure out that we might be looking at the same objects so that we can update their poses accordingly (looking into ", " may be worthwhile here).", "cc ", "This might be interesting to explore: "], "issue_code": ["perceive_plane", "base_link"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/74", "issue_title": "Store speech relevant information in an ontology.", "issue_status": "Open", "posted_on": "Oct 16, 2018", "issue_contents": ["Store questions, answers, commands etc. in a database rather than in text files.", "A database would provide a more central and structured environment for storing questions and answers.", "The speech packages (nodes) and actions would both refer to the same database rather than load the questions, answers, commands etc. from text files. Further, a database would allow to store additional information. For example the context of a question.", "Instead of querying for the information we need we are loading text files, render the content and then look for the needed information.", "Set up a database for speech.", "I'm refactoring the speech actions and improve the speech packages. Both need the same information to function.", "In my opinion, these are things that should be either stored in the knowledge base or the ontology (see ", "). I don't think we should be storing this in a different database and wouldn't really devote time to this until the KB + ontology is done, and while we have the text files.", " may have something to add to this", "As long as it is a database where we can maintain this information I'm fine with it. I did not know that there is something already. I would still leave this issue open until we have integrated a database usage into our speech packages.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/95", "issue_title": "Document the knowledge base changes in the action READMEs", "issue_status": "Open", "posted_on": "Feb 8, 2019", "issue_contents": ["Right now, the documentation of the actions doesn't include any information about what changes are made to the knowledge base before/after the execution (if any). This should be included in the README files so that it's easier to synchronise the planning domains with the action implementations."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/100", "issue_title": "DMPs for top-down motions", "issue_status": "Open", "posted_on": "Feb 22, 2019", "issue_contents": ["\n            ", "\n          ", " We're having various reliability issues when we execute top-down DMPs (in some cases, joint limits are reached and the arm simply starts moving before reaching the goal, but oscillations are a more common problem, such that the arm tries goes up and down and often misses the goal).", "We should look at this next week.", "As a reminder, we had queries about the choice of the forward gain and the equation of your controller for calculating velocities."]}
,{"url": "https://github.com/bluesat/numbat_software/issues/30", "issue_title": "Find solution for getting wifi to work at 1km, no line of sight", "issue_status": "Open", "posted_on": "Aug 6, 2014", "issue_contents": ["For many of the tasks we need wifi communication that works at least 1km with no line of sight (hills and trees in the way).", "Need a design for a solution + costs. Approximate are ok, we need them soon to apply for funding.", "Is there an over-whelming requirement for omnidirectional performance?", "Only on the robot side. The ground station side can be directional since we can move it around during the run if necessary.", "Looking at ~250 per loco M9 station, plus potential costs for antenna", "OR, ~350 per rocketM900, plus potential costs for antenna", "Antenna can be anywhere from 50-300 dollars each, the rocket can use two antennas.", "Min cost: $500, two loco M9s without any antennas,", "Max cost: $2000, two rocket M900s, + 4 antennas. <-Might be massively over-provisioned, but will probably get us what we need. Consider this our upper limit on comms costing.", "ALTERNATIVE: Because these are all ubiquiti products, We can spend $500, get the bare minimum (two locos), and set up a dropper to drop our pre-existing picostation as relay stations."]}
,{"url": "https://github.com/bluesat/numbat_software/issues/31", "issue_title": "Decide hardware requirements for new ground station", "issue_status": "Open", "posted_on": "Aug 8, 2014", "issue_contents": ["Need a design we can use to estimate costs for the new ground station. Ground station requirements so far:"], "contents_details_more": ["Use outside", "Battery powered", "Good controllers (joystick, gamepads etc)", "Easy to transport & setup", "Allows drivers to easily see camera and diagnostic feeds"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/32", "issue_title": "Find camera gimbal that can hold a generic webcam", "issue_status": "Open", "posted_on": "Aug 8, 2014", "issue_contents": ["The robot will vibrate a lot when driving so it would help the drivers to have a steady video feed. Gimbals are often used to quadcopters for this reason so there should be commercially available ones which aren't too expensive.", "Please post below possible gimbal options with pros, cons and price for each."]}
,{"url": "https://github.com/bluesat/numbat_software/issues/33", "issue_title": "Find cheap, small, low res camera", "issue_status": "Open", "posted_on": "Aug 8, 2014", "issue_contents": ["The robot will need many camera feeds to see all the different parts of the machine whilst operating. Most of the feeds could be low res so we could save a lot of money by choosing the cheapest camera for our needs.", "One option is to choose a cheap webcam but it would be good to look at other options as well. Please consider the following issues:"], "contents_details_more": ["Needs to work outside (try and find camera reviews to see if other people have tested this for a particular camera)", "Can be low res (e.g. 240x320)", "Need an affordable way to attach many of these cameras to a computer", "Smaller size is good for fitting it on robot & adds less weight"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/34", "issue_title": "Decide on initial designs to prototype for drive base", "issue_status": "Open", "posted_on": "Aug 8, 2014", "issue_contents": ["Pick 2 designs to flesh out more & do rough prototypes if necessary", "Hey everyone mech and design related i have finished a rough design in CAD just so we can see what it looks like a bit easier, catch you all tomorrow", "I've done the motor calculations spreadsheet ", ". Its based on ", ". It'd be good if someone with a mechanical background could look over this.", "The current spreadsheet has the motor values for a bag motor (", ") with a 100:1 gearbox (", "), one for each wheel. From the calculations it seems this could work with plenty of head room if we need more torque. This assumes we use the 42cm beach wheels.", "Heads up for ", " ", ". Also for Daniel and Chris if I could find their github accounts...", "More motor specs (", ")  - todo: try using these values in caluclations spreadsheet since I think these are taken experimentally and may be better numbers than on the manufactorers website.", "Mech feels it is better to use 24cm wheels, does this impact motor selection?", "You can see the change it will make by editing the motor calc spreadsheet. I've just modified it so the wheels are 24cm. We can easily change the gearbox ratio to compensate for the speed drop (so use 50:1 instead of 100:1).", "\nAnother option is to add a dual intake to the gear box (so it takes 2 of the bag motors). Not sure if you guys think this is needed but it will double the torque if we want it. (", ")", "\nAngle gearbox ", "\ngearbox + motor "]}
,{"url": "https://github.com/bluesat/numbat_software/issues/35", "issue_title": "Rough drive base estimate", "issue_status": "Open", "posted_on": "Aug 8, 2014", "issue_contents": ["We need a rough estimate on the drive base cost including manufacturing. To do this ", " needs to be done asap."]}
,{"url": "https://github.com/AutonomyLab/bebop_autonomy/issues/175", "issue_title": "Problem while running 'roslaunch bebop_tools joy_teleop.launch'", "issue_status": "Open", "posted_on": "Oct 6, 2018", "issue_contents": ["process[bebop/robot_state_publisher-3]: started with pid [25681]", "\n[ INFO] [1538823198.803695714]: Initializing nodelet with 4 worker threads.", "\n[ INFO] [1538823198.824396477]: [BebopSDK] 06:53:18:824 | Bebop:225 - Bebop Cnstr()", "\n[ INFO] [1538823198.824488178]: Nodelet Cstr", "\n[ INFO] [1538823198.835769368]: Connecting to Bebop ...", "\n[ INFO] [1538823198.839339577]: [CB] 06:53:18:839 | Ardrone3PilotingStateFlatTrimChanged:388 - [STATES] Enabling states/ardrone3/PilotingState/FlatTrimChanged", "\n[ INFO] [1538823198.841026381]: [CB] 06:53:18:840 | Ardrone3PilotingStateFlyingStateChanged:432 - [STATES] Enabling states/ardrone3/PilotingState/FlyingStateChanged", "\n[ INFO] [1538823198.843168907]: [CB] 06:53:18:843 | Ardrone3PilotingStateNavigateHomeStateChanged:534 - [STATES] Enabling states/ardrone3/PilotingState/NavigateHomeStateChanged", "\n[ INFO] [1538823198.844744114]: [CB] 06:53:18:844 | Ardrone3PilotingStatePositionChanged:592 - [STATES] Enabling states/ardrone3/PilotingState/PositionChanged", "\n[ INFO] [1538823198.846297198]: [CB] 06:53:18:846 | Ardrone3PilotingStateSpeedChanged:657 - [STATES] Enabling states/ardrone3/PilotingState/SpeedChanged", "\n[ INFO] [1538823198.847975587]: [CB] 06:53:18:847 | Ardrone3PilotingStateAttitudeChanged:722 - [STATES] Enabling states/ardrone3/PilotingState/AttitudeChanged", "\n[ INFO] [1538823198.850291920]: [CB] 06:53:18:850 | Ardrone3PilotingStateAltitudeChanged:838 - [STATES] Enabling states/ardrone3/PilotingState/AltitudeChanged", "\n[ INFO] [1538823198.857008883]: [CB] 06:53:18:856 | Ardrone3MediaStreamingStateVideoEnableChanged:1388 - [STATES] Enabling states/ardrone3/MediaStreamingState/VideoEnableChanged", "\n[ INFO] [1538823198.860702512]: [CB] 06:53:18:860 | Ardrone3CameraStateOrientation:1490 - [STATES] Enabling states/ardrone3/CameraState/Orientation", "\n[ INFO] [1538823198.868364668]: [CB] 06:53:18:868 | Ardrone3GPSStateNumberOfSatelliteChanged:1882 - [STATES] Enabling states/ardrone3/GPSState/NumberOfSatelliteChanged", "\n[ INFO] [1538823198.873329170]: [CB] 06:53:18:873 | CommonCommonStateBatteryStateChanged:148 - [STATES] Enabling states/common/CommonState/BatteryStateChanged", "\n[ INFO] [1538823198.878183000]: [CB] 06:53:18:878 | CommonCommonStateWifiSignalChanged:510 - [STATES] Enabling states/common/CommonState/WifiSignalChanged", "\n[ INFO] [1538823198.884725042]: [CB] 06:53:18:884 | CommonOverHeatStateOverHeatChanged:1023 - [STATES] Enabling states/common/OverHeatState/OverHeatChanged", "\n[ INFO] [1538823198.887108733]: [CB] 06:53:18:887 | CommonMavlinkStateMavlinkFilePlayingStateChanged:1118 - [STATES] Enabling states/common/MavlinkState/MavlinkFilePlayingStateChanged", "\n[ INFO] [1538823198.891094260]: [CB] 06:53:18:891 | CommonMavlinkStateMavlinkPlayErrorStateChanged:1183 - [STATES] Enabling states/common/MavlinkState/MavlinkPlayErrorStateChanged", "\n[ INFO] [1538823198.898683973]: [CB] 06:53:18:898 | CommonFlightPlanStateAvailabilityStateChanged:1568 - [STATES] Enabling states/common/FlightPlanState/AvailabilityStateChanged", "\n[ INFO] [1538823198.900389483]: [CB] 06:53:18:900 | CommonFlightPlanStateComponentStateListChanged:1619 - [STATES] Enabling states/common/FlightPlanState/ComponentStateListChanged", "\n[ INFO] [1538823198.913339449]: [ARCONTROLLER_Network] 06:53:18:913 | ARCONTROLLER_Network_GetAvailableSocketPort:128 - d2c_port port: 43210", "\n[ INFO] [1538823198.913591260]: [ARCONTROLLER_Stream2] 06:53:18:913 | ARCONTROLLER_Stream2_Open_Socket:143 - udp local port stream: 55004", "\n[ INFO] [1538823198.913633846]: [ARCONTROLLER_Stream2] 06:53:18:913 | ARCONTROLLER_Stream2_Open_Socket:143 - udp local port control: 55005", "\n[ INFO] [1538823198.941004607]: [BebopSDK] 06:53:18:940 | CommandReceivedCallback:113 - Command Received Callback LWP id is: 25839", "\n[ INFO] [1538823199.186035417]: [BebopSDK] 06:53:19:185 | Connect:321 - BebopSDK inited, lwp_id: 25677", "\n[ WARN] [1538823199.186144569]: Resetting all settings ...", "\n[ INFO] [1538823199.186252511]: [BebopSDK] 06:53:19:186 | ResetAllSettings:417 - All settings of the drone have been reset to default values.", "\n[ INFO] [1538823202.186481342]: Fetching all settings from the Drone ...", "\n[ INFO] [1538823202.217245118]: Value for PilotingSettingsMaxAltitudeCurrent recved: 30", "\n[ INFO] [1538823202.217351656]: [CB] 06:53:22:217 | Update:111 - Checking if PilotingSettingsMaxAltitudeCurrent exists in params ...", "\n[ INFO] [1538823202.219138711]: [CB] 06:53:22:219 | Update:114 -   No", "\n[ INFO] [1538823202.222523673]: Value for PilotingSettingsMaxDistanceValue recved: 2000", "\n[ INFO] [1538823202.222615768]: [CB] 06:53:22:222 | Update:351 - Checking if PilotingSettingsMaxDistanceValue exists in params ...", "\n[ INFO] [1538823202.224107422]: [CB] 06:53:22:224 | Update:354 -   No", "\n[ INFO] [1538823202.225210088]: Value for PilotingSettingsNoFlyOverMaxDistanceShouldnotflyover recved: 0", "\n[ INFO] [1538823202.225265211]: [CB] 06:53:22:225 | Update:431 - Checking if PilotingSettingsNoFlyOverMaxDistanceShouldnotflyover exists in params ...", "\n[ INFO] [1538823202.226098429]: [CB] 06:53:22:226 | Update:434 -   No", "\n[ INFO] [1538823202.227077026]: Value for PilotingSettingsMaxTiltCurrent recved: 20", "\n[ INFO] [1538823202.227122022]: [CB] 06:53:22:227 | Update:191 - Checking if PilotingSettingsMaxTiltCurrent exists in params ...", "\n[ INFO] [1538823202.227913597]: [CB] 06:53:22:227 | Update:194 -   No", "\n[ INFO] [1538823202.228954156]: Value for PilotingSettingsAbsolutControlOn recved: 0", "\n[ INFO] [1538823202.229008676]: [CB] 06:53:22:228 | Update:271 - Checking if PilotingSettingsAbsolutControlOn exists in params ...", "\n[ INFO] [1538823202.229946824]: [CB] 06:53:22:229 | Update:274 -   No", "\n[ INFO] [1538823202.237474164]: Value for PilotingSettingsBankedTurnValue recved: 0", "\n[ INFO] [1538823202.237670878]: [CB] 06:53:22:237 | Update:511 - Checking if PilotingSettingsBankedTurnValue exists in params ...", "\n[ INFO] [1538823202.239268522]: [CB] 06:53:22:239 | Update:514 -   No", "\n[ INFO] [1538823202.241079290]: Value for SpeedSettingsMaxVerticalSpeedCurrent recved: 1", "\n[ INFO] [1538823202.241167742]: [CB] 06:53:22:241 | Update:991 - Checking if SpeedSettingsMaxVerticalSpeedCurrent exists in params ...", "\n[ INFO] [1538823202.242310517]: [CB] 06:53:22:242 | Update:994 -   No", "\n[ INFO] [1538823202.243583428]: Value for SpeedSettingsMaxRotationSpeedCurrent recved: 100", "\n[ INFO] [1538823202.243632206]: [CB] 06:53:22:243 | Update:1071 - Checking if SpeedSettingsMaxRotationSpeedCurrent exists in params ...", "\n[ INFO] [1538823202.244803523]: [CB] 06:53:22:244 | Update:1074 -   No", "\n[ INFO] [1538823202.246441410]: Value for SpeedSettingsHullProtectionPresent recved: 0", "\n[ INFO] [1538823202.246537558]: [CB] 06:53:22:246 | Update:1151 - Checking if SpeedSettingsHullProtectionPresent exists in params ...", "\n[ INFO] [1538823202.248715002]: [CB] 06:53:22:248 | Update:1154 -   No", "\n[ INFO] [1538823202.250503785]: Value for SpeedSettingsOutdoorOutdoor recved: 1", "\n[ INFO] [1538823202.250574318]: [CB] 06:53:22:250 | Update:1231 - Checking if SpeedSettingsOutdoorOutdoor exists in params ...", "\n[ INFO] [1538823202.251693093]: [CB] 06:53:22:251 | Update:1234 -   No", "\n[ INFO] [1538823202.253528837]: Value for SpeedSettingsMaxPitchRollRotationSpeedCurrent recved: 300", "\n[ INFO] [1538823202.253718241]: [CB] 06:53:22:253 | Update:1311 - Checking if SpeedSettingsMaxPitchRollRotationSpeedCurrent exists in params ...", "\n[ INFO] [1538823202.256454836]: [CB] 06:53:22:256 | Update:1314 -   No", "\n[ INFO] [1538823202.257763846]: Value for NetworkSettingsWifiSelectionType recved: 1", "\n[ INFO] [1538823202.257822619]: [CB] 06:53:22:257 | Update:1417 - Checking if NetworkSettingsWifiSelectionType exists in params ...", "\n[ INFO] [1538823202.258750473]: [CB] 06:53:22:258 | Update:1420 -   No", "\n[ INFO] [1538823202.259892516]: Value for NetworkSettingsWifiSelectionBand recved: 0", "\n[ INFO] [1538823202.259947723]: [CB] 06:53:22:259 | Update:1438 - Checking if NetworkSettingsWifiSelectionBand exists in params ...", "\n[ INFO] [1538823202.260668879]: [CB] 06:53:22:260 | Update:1441 -   No", "\n[ INFO] [1538823202.261572969]: Value for NetworkSettingsWifiSelectionChannel recved: 6", "\n[ INFO] [1538823202.261626729]: [CB] 06:53:22:261 | Update:1459 - Checking if NetworkSettingsWifiSelectionChannel exists in params ...", "\n[ INFO] [1538823202.262472451]: [CB] 06:53:22:262 | Update:1462 -   No", "\n[ INFO] [1538823202.275887951]: Value for PictureSettingsVideoStabilizationModeMode recved: 0", "\n[ INFO] [1538823202.275995942]: [CB] 06:53:22:275 | Update:1539 - Checking if PictureSettingsVideoStabilizationModeMode exists in params ...", "\n[ INFO] [1538823202.277649515]: [CB] 06:53:22:277 | Update:1542 -   No", "\n[ INFO] [1538823202.279249754]: Value for PictureSettingsVideoRecordingModeMode recved: 1", "\n[ INFO] [1538823202.279329183]: [CB] 06:53:22:279 | Update:1619 - Checking if PictureSettingsVideoRecordingModeMode exists in params ...", "\n[ INFO] [1538823202.280732051]: [CB] 06:53:22:280 | Update:1622 -   No", "\n[ INFO] [1538823202.282783921]: Value for PictureSettingsVideoFramerateFramerate recved: 2", "\n[ INFO] [1538823202.282853789]: [CB] 06:53:22:282 | Update:1699 - Checking if PictureSettingsVideoFramerateFramerate exists in params ...", "\n[ INFO] [1538823202.284228552]: [CB] 06:53:22:284 | Update:1702 -   No", "\n[ INFO] [1538823202.285929433]: Value for PictureSettingsVideoResolutionsType recved: 0", "\n[ INFO] [1538823202.286113219]: [CB] 06:53:22:286 | Update:1779 - Checking if PictureSettingsVideoResolutionsType exists in params ...", "\n[ INFO] [1538823202.288534270]: [CB] 06:53:22:288 | Update:1782 -   No", "\n[ INFO] [1538823202.291349539]: Value for GPSSettingsReturnHomeDelayDelay recved: 30", "\n[ INFO] [1538823202.291392095]: [CB] 06:53:22:291 | Update:1939 - Checking if GPSSettingsReturnHomeDelayDelay exists in params ...", "\n[ INFO] [1538823202.292387832]: [CB] 06:53:22:292 | Update:1942 -   No", "\n[ INFO] [1538823202.295640896]: Value for GPSSettingsHomeTypeType recved: 0", "\n[ INFO] [1538823202.295697317]: [CB] 06:53:22:295 | Update:1859 - Checking if GPSSettingsHomeTypeType exists in params ...", "\n[ INFO] [1538823202.296792623]: [CB] 06:53:22:296 | Update:1862 -   No", "\n[ INFO] [1538823205.378791389]: Dynamic reconfigure callback with level: -1", "\n[ INFO] [1538823205.378957504]: [CB] 06:53:25:378 | UpdateBebopFromROS:549 - PilotingSettingsMinAltitudeCurrent changed!", "\n[ERROR] [1538823205.379009443]: [CB] 06:53:25:378 | UpdateBebopFromROS:557 - Value of PilotingSettingsMinAltitude was not initialized either by Bebop or Params.", "\n[ INFO] [1538823205.379046742]: [CB] 06:53:25:379 | UpdateBebopFromROS:629 - PilotingSettingsCirclingDirectionValue changed!", "\n[ERROR] [1538823205.379083353]: [CB] 06:53:25:379 | UpdateBebopFromROS:637 - Value of PilotingSettingsCirclingDirection was not initialized either by Bebop or Params.", "\n[ INFO] [1538823205.379122876]: [CB] 06:53:25:379 | UpdateBebopFromROS:789 - PilotingSettingsCirclingAltitudeValue changed!", "\n[ERROR] [1538823205.379158593]: [CB] 06:53:25:379 | UpdateBebopFromROS:797 - Value of PilotingSettingsCirclingAltitude was not initialized either by Bebop or Params.", "\n[ INFO] [1538823205.379196893]: [CB] 06:53:25:379 | UpdateBebopFromROS:869 - PilotingSettingsPitchModeValue changed!", "\n[ERROR] [1538823205.379234146]: [CB] 06:53:25:379 | UpdateBebopFromROS:877 - Value of PilotingSettingsPitchMode was not initialized either by Bebop or Params.", "\n[ INFO] [1538823205.404326010]: Enabling video stream ...", "\n[ WARN] [1538823205.404401209]: [BebopSDK] 06:53:25:404 | StartStreaming:359 - Video streaming started ...", "\n[ INFO] [1538823205.404606712]: Nodelet lwp_id: 25677", "\n[ INFO] [1538823205.404648044]: [CameraThread] thread lwp_id: 26095", "\n[ INFO] [1538823205.404690620]: bebop_driver nodelet loaded.", "\n[ INFO] [1538823205.404769563]: [AuxThread] thread lwp_id: 26096", "\n[ INFO] [1538823205.415929764]: [ARSTREAM2_StreamReceiver] 06:53:25:415 | ARSTREAM2_StreamReceiver_RunNetworkThread:1666 - Receiver thread running", "\n[ INFO] [1538823205.416025196]: [ARSTREAM2_StreamReceiver] 06:53:25:415 | ARSTREAM2_StreamReceiver_StartAppOutput:1911 - App output is running", "\n[ INFO] [1538823205.416269523]: [ARSTREAM2_StreamReceiver] 06:53:25:416 | ARSTREAM2_StreamReceiver_RunAppOutputThread:1336 - App output thread running", "\n[ WARN] [1538823205.446037062]: [ARSTREAM2_Rtcp] 06:53:25:446 | ARSTREAM2_RTCP_Receiver_ProcessSenderReport:290 - Unexpected sender SSRC", "\n[ERROR] [1538823205.446070339]: [ARSTREAM2_Rtcp] 06:53:25:446 | ARSTREAM2_RTCP_Receiver_ProcessCompoundPacket:2099 - Failed to process sender report (-1)", "\n[ WARN] [1538823205.446103811]: [ARSTREAM2_Rtcp] 06:53:25:446 | ARSTREAM2_RTCP_ProcessApplicationClockDelta:1405 - Unexpected peer SSRC", "\n[ERROR] [1538823205.446121690]: [ARSTREAM2_Rtcp] 06:53:25:446 | ARSTREAM2_RTCP_Receiver_ProcessCompoundPacket:2126 - Failed to process application clock delta (-1)", "\n[ INFO] [1538823205.449117037]: [ARSTREAM2_H264Filter] 06:53:25:449 | ARSTREAM2_H264Filter_ProcessAu:593 - AU output cancelled (waitForSync)", "\n[ INFO] [1538823205.482426905]: [ARSTREAM2_H264Filter] 06:53:25:482 | ARSTREAM2_H264Filter_Sync:65 - SPS/PPS sync OK", "\n[ INFO] [1538823205.482459049]: [BebopSDK] 06:53:25:482 | DecoderConfigCallback:147 - H264 configuration packet received: #SPS: 27 #PPS: 9 (MP4? 0)", "\n[ INFO] [1538823205.482590698]: [ARSTREAM2_H264FilterError] 06:53:25:482 | ARSTREAM2_H264FilterError_GenerateGrayIdrFrame:101 - Gray I slice NALU output size: 1629", "\n[ INFO] [1538823205.482696948]: [BebopSDK] 06:53:25:482 | FrameReceivedCallback:174 - Frame Recv & Decode LWP id: 26099", "\n[ INFO] [1538823205.483387196]: [Decoder] 06:53:25:483 | InitCodec:117 - H264 Codec is partially initialized!", "\n[ INFO] [1538823205.483412900]: [Decoder] 06:53:25:483 | Decode:263 - Updating H264 codec parameters (Buffer Size: 36) ...", "\n[ERROR] [1538823205.484634998]: [Decoder] 06:53:25:484 | Decode:302 - Frame size changed to 856 x 480", "\n[ INFO] [1538823205.484662164]: [Decoder] 06:53:25:484 | ReallocateBuffers:123 - Buffer reallocation request", "\n[ INFO] [1538823205.486036905]: camera calibration URL: package://bebop_driver/data/bebop2_camera_calib.yaml", "\n[ WARN] [1538823222.812228046]: [BebopSDK] 06:53:42:812 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.238141173]: [BebopSDK] 06:53:43:238 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.241169004]: [BebopSDK] 06:53:43:241 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.243932188]: [BebopSDK] 06:53:43:243 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.604985309]: [BebopSDK] 06:53:43:604 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.612021005]: [BebopSDK] 06:53:43:611 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.615234343]: [BebopSDK] 06:53:43:615 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.971556935]: [BebopSDK] 06:53:43:971 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.976344352]: [BebopSDK] 06:53:43:976 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.982322599]: [BebopSDK] 06:53:43:982 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823223.983807792]: [BebopSDK] 06:53:43:983 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.342386580]: [BebopSDK] 06:53:44:342 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.346271738]: [BebopSDK] 06:53:44:346 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.350725041]: [BebopSDK] 06:53:44:350 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.703864415]: [BebopSDK] 06:53:44:703 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.708806003]: [BebopSDK] 06:53:44:708 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.712381489]: [BebopSDK] 06:53:44:712 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823224.714087472]: [BebopSDK] 06:53:44:714 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.112084185]: [BebopSDK] 06:53:45:112 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.116326821]: [BebopSDK] 06:53:45:116 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.120322659]: [BebopSDK] 06:53:45:120 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.121788502]: [BebopSDK] 06:53:45:121 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.470753777]: [BebopSDK] 06:53:45:470 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.474798004]: [BebopSDK] 06:53:45:474 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.479043749]: [BebopSDK] 06:53:45:479 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.486147217]: [BebopSDK] 06:53:45:486 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.834201615]: [BebopSDK] 06:53:45:834 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.839890038]: [BebopSDK] 06:53:45:839 | FrameReceivedCallback:191 - Previous frame might have been missed.", "\n[ WARN] [1538823225.842718609]: [BebopSDK] 06:53:45:842 | FrameReceivedCallback:191 - Previous frame might have been missed.", "Hi, did you fixed this error?"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/38", "issue_title": "Version control for CAD", "issue_status": "Open", "posted_on": "Aug 9, 2014", "issue_contents": ["\n            ", "\n          "]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/144", "issue_title": "\u201cWhat else do you know?\u201d Scenario - Analysis mode", "issue_status": "Open", "posted_on": "May 9, 2019", "issue_contents": ["Inspired by the \u201cwhere is this?\u201d task, I think it would be really interesting to stress test our NLP by asking the robot questions about ", " the knowledge we have modeled. (Think of analysis mode in Westworld).", "This can be built in stages and members can add the different things they\u2019re working on to this \u201cAnalysis mode\u201d.", "Based on the last few days, things I can think of:", " ", " this might be a nice SDP project for WS19 if we don\u2019t want to tackle this ourselves", "I suppose it should be doable in a semester-long project - at least a basic version of the system. We should however possibly try to decompose it into clear steps that students should follow; otherwise, they might easily get lost in details.", "In general, I like the idea a lot though.", "Yeah, definitely! If the team doesn\u2019t want to, I\u2019ll take care of phrasing and sending it to "], "contents_details_more": []}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/111", "issue_title": "Create a new LookAtAction", "issue_status": "Open", "posted_on": "Mar 5, 2019", "issue_contents": ["We need a new skill to move/turn the head towards a specific point in order to perceive.", "This component is needed for the ", " [see ", "] task in order to find the trash bins located on the arena.", "I would rename this to a skill which configures the whole robot to look at a point of interest, i.e. a ", ". This skill may include:", "After the above operations, the skill can be included in a larger skill or extended to handle ", ", which would also involve:", " ", " this may need to be in a separated, bigger issue/project", "The first 3 bullets sound to me like a perfect use for FTSM. I agree the second part belongs on a different PR (and skill).", "Here's a ", " on getting the distances from a 3D point to each plane of the camera's frustum. I believe the distances would be then in world coordinates (e.g. meters)."], "issue_code": ["LookAtAction"], "contents_details_more": ["turn the head to an appropriate angle", "raise or lower the head to an appropriate height", "move the arm out of the way of the cameras", "navigate to a general area which may have a subject of interest", "look around the area to find the subject", "find an ideal location to observe the subject"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/36", "issue_title": "Control board design: 1st prototype", "issue_status": "Open", "posted_on": "Aug 8, 2014", "issue_contents": ["The robot will need some sort of microcontroller to handle low level I/O for robot. This includes readings sensors and controlling motors.", "The first step is to design a pcb with pic microcontroller and a few simple input/outputs such as leds or servo outputs. You can design it in Eagle PCB then we'll get it manufactured so it can be tested."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/153", "issue_title": "Review CI in domestic repositories", "issue_status": "Open", "posted_on": "May 23, 2019", "issue_contents": ["Travis is failing to catch some of the failing builds. After the refactoring, it makes sense to take a look at how the Travis and Docker setups were made, and improve the workflow.", "The refactoring helped to reduce the build time for about 10 minutes, but there are still improvements that can be made:", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions."], "contents_details_more": ["on deployment, the docker image is being built from scratch", "publishing images for long-term branches and building against them"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/155", "issue_title": "Integration of mbot NLU", "issue_status": "Open", "posted_on": "May 23, 2019", "issue_contents": ["From ", ":", "It's missing everything that it's inside the Mbot folder (physically on Lucy) and to add my dataset to the dataset repository you mentioned last time.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", " what is missing here?"], "issue_quotes": ["It's missing everything that it's inside the Mbot folder (physically on Lucy) and to add my dataset to the dataset repository you mentioned last time."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/176", "issue_title": "Setting up kinetic CI with Python 3", "issue_status": "Open", "posted_on": "May 25, 2019", "issue_contents": ["\n            ", "\n          ", "Based on my tests with docker this doesn't seem to be an issue anymore. Will close for now.", "Reopened to address the migration to Python 3 for ", "."], "issue_code": ["kinetic"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/175", "issue_title": "Migration of packages to Python 3", "issue_status": "Open", "posted_on": "May 25, 2019", "issue_contents": ["\n            ", "\n          ", "I can't recall if there was a discussion about this, but I think we should treat this issue independent of the Melodic migration for the time being. Because of ROPOD, we cannot (fully) migrate to Melodic now (i.e. we need our development machines to have Ubuntu 16.04/ROS kinetic), but most of the code can be migrated to Python 3 anyway since ROS kinetic works fine with Python 3 (with the exception of a few packages that need some fixing, notably ", " and ", ").", "I\u2019m just not sure what the current issues are of setting up kinetic with python 3", "As I said, I've only had problems with a few packages (I forgot to mention OpenCV in the above list, which is also problematic); everything else is working fine with Python 3.", "What about using docker?", "You mean docker for melodic, right?", "I\u2019m just not sure what the current issues are of setting up kinetic with python 3", "As I said, I've only had problems with a few packages (I forgot to mention OpenCV in the above list, which is also problematic); everything else is working fine with Python 3.", "Alright, based on that we would need:", "What about using docker?", "You mean docker for melodic, right?", "Yes, ", " (the staff) could use docker for melodic. The rest can upgrade or have a bionic partition.", "In general, OpenCV shouldn't be an issue. The big change was ", "->", " since they went from OpenCV 2 to 3. ", "->", " is just OpenCV 3. I haven't got a chance to add tests for ", ", however.", "I reopened ", " and updated it to ", ", and I can take care of it once someone creates the script.", " I was referring to Python 3 on ", " when I said there are problems with OpenCV; ", " to ", " might not be a problem.", "FWIW, on an unrelated test, I ended up having to test something with opencv and python3. ", " needs to be built with python3, e.g. see "], "issue_code": ["mongodb_store", "tf2", "indigo", "kinetic", "kinetic", "melodic", "cv_bridge", "kinetic", "kinetic", "kinetic", "melodic", "cv_bridge"], "issue_quotes": ["I\u2019m just not sure what the current issues are of setting up kinetic with python 3", "What about using docker?", "As I said, I've only had problems with a few packages (I forgot to mention OpenCV in the above list, which is also problematic); everything else is working fine with Python 3.", "You mean docker for melodic, right?"], "contents_details_more": ["someone to setup the CI to use Kinetic with Python3 (using the script above?)"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/41", "issue_title": "(Mech) Bearing and Bushings", "issue_status": "Open", "posted_on": "Aug 23, 2014", "issue_contents": ["Just a site for finding various kinds of bushings and bearing."]}
,{"url": "https://github.com/bluesat/numbat_software/issues/42", "issue_title": "Information about components", "issue_status": "Open", "posted_on": "Aug 23, 2014", "issue_contents": ["Hey,", "I am going to get started on designing the main body compartment of the rover.", "Could someone from elec give me a run down on:", "No hurry on this, but it would be good to draw something up to work from.", "Ok, I'll have a rough schematic for you next weekend based off the electronics used for buttercup. It won't be exact but hopefully give you a rough idea of sizes and weights.", "Battery info:", "\n", "\nWe'll need space for 4 on the robot for november. From testing we'll figure out how much we'll need on the final robot.", "Update: For the systems going inside the chassis; Is it possible to get model numbers of the parts we have purchased and a rough guideline to other components that are made in bluesat."], "contents_details_more": ["Components to be used, their size and weight.", "The locations of ports and an idea as to how they're to be interconnected/mounted."]}
,{"url": "https://github.com/bluesat/numbat_software/issues/43", "issue_title": "Mech Differential System", "issue_status": "Open", "posted_on": "Aug 30, 2014", "issue_contents": ["This is just a basic explanation of how the differential is going to work using lego. just check out the .gif on the site. (I reckon the bar rather than the gears)", "\n", "Added center unit with start to differential bar in the mech folder on google drive, can someone work on producing a full cad assembly?"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/45", "issue_title": "How to sense pH and soil humidity from sample?", "issue_status": "Open", "posted_on": "Sep 20, 2014", "issue_contents": ["\n            ", "\n          ", "soil-botics requirements", "soil get, then what. for competition:", "will get samples from \"sites\", to see if sites are geologically significant, chance for biological life", "teams get tasks, to do at site, teams then picks sites for potential with 0.5km", "then collect, return desired sub-surface samples from 5cm or deeper", "\nsamples at least 5g, may consist of single rock, loose soil, anything in between.", "\nreturn full depth including topsoil but teams must be able to distinguish soil depth for any sample. portion of sample below 5cm determins sample mass (so you can have just a large scoop as long as its smooth)", "ONBOARD EQUIPMENT at a MINIMUM should test for pH and soil humidity", "\nadditional gear up to our discretion, to meet any other objectives", "onboard chemicals need approval if theyre hazardous. all chemicals must stay contained on rover.", "thoughts", "well, we can just get commercial soil sensors for humidity and pH, any complications in using soil stabby things?", "once digging soil isnt compact anymore... will sensor still work", "i dunno if we need to deal with medium size rocks etc.. how easily do commercial sensors break, when stabbing soil", "how quickly will humidity/ph change as soil goes deeper...", "range of ph, humidity on \"mars\" is within range of commercial sensors?", "mixing a bunch of rain-like water with soil for ph sensor to work, with zerrrrrrrrro spillage...", "mmm the trick was using the word \"probe\" while googling", "poke hole in soil and test it. clean probe after usage.", "\ni suppose if you dont feel like testing the sample you collect, and want to just probe the ground itself, we could dry the probe after cleaning so we havent spilled liquid :/", "probes are $100ish havent searched hard enough yet"], "contents_details_more": ["wide angle panorama showing full context of site and immediate surrounding area. panorama indicates cardinal directions, has indication of scale", "close up, well focused, high res picture with indication of scale (can be done post capture) at sampling site", "gps coordinates of each site including elevation and accuracy range. thorough documentation is especially crucial for the sample that is returned"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/47", "issue_title": "Rover Onboard Computer Hardware Options", "issue_status": "Open", "posted_on": "Oct 5, 2014", "issue_contents": ["Current options to consider:", "\nsabrelites", "\nIntel NUC", "\nlaptop", "Intel NUC info:", "\n", "\nIntel NUC specifications:", "\n", "Sabrelite user manual", "\n"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/49", "issue_title": "Arm Design references", "issue_status": "Open", "posted_on": "Oct 18, 2014", "issue_contents": ["\n            ", "\n          ", "Mechanical analysis of excavator boom/arm with FEA", "\n", "Excavator Terminology for further searching:", "\nfrom wikipedia ", "\ncabin,body, etc = house", "\narm attachment = swing frame", "\narm attached to house = boom", "\narm with attachment (bucket, etc) = stick", "Other soil sampling devices:"], "contents_details_more": ["soil probe/core sampler: extracts a column of soil by driving a metal tube into the ground", " (skip to 1:20)", "auger: screw head drills into soil (more commonly used for hole digging)"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/55", "issue_title": "Create ROS Launch Nodes", "issue_status": "Open", "posted_on": "Mar 28, 2015", "issue_contents": ["We need ros launch nodes so we don't have to type in 20 commands everytime we run anything.", "We need a launch node in the rover package that does the equivlant of setting these environment variables", "\nROS_IP=192.168.1.3", "\nROS_HOST=192.168.1.3", "\nROS_MASTER_URI=", "\nIf you can detect the computers IP address (not host name) try that one", "It needs to run the camera script, and the drive controler (which I'm going to move to the rover package)", "In the gui package", "\nROS_IP=192.168.1.15", "\nROS_HOST=192.168.1.15", "\nand run the navigation gui", "useful roslaunch tutorial: "]}
,{"url": "https://github.com/bluesat/numbat_software/issues/58", "issue_title": "Map Integration GIS file", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Time: 2 weeks + 1 week after dependencies"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/61", "issue_title": "ROS topic Arm Ultrasonic", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["\n            ", "\n          "]}
,{"url": "https://github.com/bluesat/numbat_software/issues/56", "issue_title": "Video feed control on/off get state for Nav GUI", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["\n            ", "\n          ", "Some information here: ", "We need topics that relay the status of the videos, and keep track of if they are running, and if they are avialable (using ls on /dev or something similar)", "Check the wiki for existing topics, and the git for existing code as I think there may already be some", "Use branch: video", "\nMerge working versions to: widgetGui"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/62", "issue_title": "Implement Fine Control & Sensing GUI", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["\n            ", "\n          ", " Design"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/64", "issue_title": "Implement Obstacle Locations on Map - Autonomous GUI", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Work Time Estimate: 4 weeks 1 person", "use branch: widgetGui (may need to merge positioning, autonomous)"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/68", "issue_title": "Miniature Arm Model to Rover Arm Control", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Time: 2 Saturdays", "use branch: arm_model.", "\nNote: this is an optional task"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/65", "issue_title": "Combined Positioning System", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Take in all Position Sensor Data output to Position & Direction", "\nTime: 3 Weeks, ongoing", "use branch: positioning"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/70", "issue_title": "Interfacing Board Driver", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Time: 2 Saturdays", "\nHardware Data: Out, In Combine", "Create branch: board"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/69", "issue_title": "Motor Control - Drive", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Time: 3 Saturdays", "use branch: controls"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/71", "issue_title": "Test Different Wheel Motor Power Level", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Time: 1 Saturday", "Started here: ", "\nWe need to try different vectors to see if we can make it turn. The idea was that it would turn at a set speed when a button was pressed on the joystick", "Use branch: controls"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/73", "issue_title": "Path Finding for Autonomous", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["Time: 3 weeks", "Use branch: autonomous"]}
,{"url": "https://github.com/bluesat/numbat_software/issues/72", "issue_title": "Obstacle Detection Ultrasonic for Autonomous", "issue_status": "Open", "posted_on": "Apr 18, 2015", "issue_contents": ["\n            ", "\n          ", "Use branch: autonomous"]}
,{"url": "https://github.com/airalab/robonomics_comm/issues/33", "issue_title": "Listener node fails with web3 BadFunctionCallOutput", "issue_status": "Closed", "posted_on": "Oct 1, 2018", "issue_contents": ["While parity synced ", " service fails, please consider a log below. I'll try to collect more data to specify reproduction steps.", "Are your chain is fully synchronized?", "When I find it, parity had latest block number. I'd like to have a tool for logs capture to take it all automatically even when I have no time to leave system unavailable. Are they all in /var/lib/<service_user>/logs or you like to have any more on failure occurred?", "Currently this error should not crash service, you are free to reopen issue when more data will collected."], "issue_code": ["robonomics_liability"]}
,{"url": "https://github.com/at-wat/neonavigation/issues/37", "issue_title": "Add comments and unit tests.", "issue_status": "Closed", "posted_on": "Jun 27, 2017", "issue_contents": [], "contents_details_more": [" costmap_cspace ", " planner_cspace ", " ", " ", " map_organizer", " obj_to_pointcloud", " safety_limiter", " track_odometry", " trajectory_tracker"]}
,{"url": "https://github.com/AcutronicRobotics/moveit2/issues/24", "issue_title": "OS X, issues with orocos_kdl", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["Deleting the corresponding install folder of the package did it:"]}
,{"url": "https://github.com/airalab/robonomics_comm/issues/90", "issue_title": "Promisee and promisor fields in robonomics market ROS messages", "issue_status": "Closed", "posted_on": "Jan 8, 2019", "issue_contents": ["In offer and demand processing it would be great to know an ID of a party sent this message. Could we add this filed to market ROS messages, I mean Offer and Demand types?", ", please see ", ", seems cool, but need an example.", ",", "\nyou can use robonomicsMessageUtils.get_result_msg_sender_address(result) method for getting sender of Result.msg message", "\nand", "\nuse sender field directly for Demand and Offer messages"]}
,{"url": "https://github.com/airalab/robonomics_comm/issues/38", "issue_title": "Executor node fails on attempt to get an account address", "issue_status": "Closed", "posted_on": "Oct 6, 2018", "issue_contents": ["I set ", " parameter but node attempts to get address from a client. Seems like parameter namespace error. Please consider log and launchfile screenshot below.", "\nGet actual account from a ", " or at least from ", " parameter with no error.", ", please try again. Issue is fixed now."], "issue_code": ["account_address", "1", "robonomics_liability", "keyfile", "account_address"]}
,{"url": "https://github.com/airalab/robonomics_comm/issues/93", "issue_title": "Demand/Offer: reducing nonce field and add sender", "issue_status": "Closed", "posted_on": "Jan 25, 2019", "issue_contents": ["related to "]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/294", "issue_title": "wf_simulator  bug  report", "issue_status": "Closed", "posted_on": "May 3, 2016", "issue_contents": ["Firstly ,thanks for your open source code and answering my question.", "I have got a bug in Autoware when I am trying to use wf_simulator in my own computer.", "\nMy operations are listed below:", "\n1.launch Autoware and open rviz ,then open \"default.rviz\" in rviz.", "\n2.select the .pcd file and click \"point cloud\" button.", "\n3.select \"tf.launch\"  file and click the \"TF\" button. Now in rviz i could see the Points Map.", "\n4.select .csv file and click \"waypoint_loader\" ,then I could see the waypoints in the rviz.", "\n5.select \"lane_rule\" and click the \"APP\" of \"lane_select\" , and click to select \"Driving\".", "\n6.click the \"APP\" of \"wf_simulator\" ,but nothing shows up.", "All my operations are following the instructions in the video \"Planning with wf_simulator\". In the video in  step 6, there should show up a dialog to choose from \"Rviz\" \"ndt_loclizer\" \"GNSS\". However in my computer ,nothing shows up.", "and the tips in the console is listed below:", "Traceback (most recent call last):", "\nFile \"/home/linfeng/Autoware/ros/src/util/packages/runtime_manager/scripts/runtime_manager_dialog.py\", line 680, in OnHyperlinked", "\nself.OnHyperlinked_obj(event.GetEventObject())", "\nFile \"/home/linfeng/Autoware/ros/src/util/packages/runtime_manager/scripts/runtime_manager_dialog.py\", line 688, in OnHyperlinked_obj", "\ndlg = klass_dlg(self, pdic=pdic, gdic=gdic, prm=prm)", "\nFile \"/home/linfeng/Autoware/ros/src/util/packages/runtime_manager/scripts/runtime_manager_dialog.py\", line 2595, in ", "\nself.panel = ParamPanel(parent, frame=frame, pdic=pdic, gdic=gdic, prm=prm)", "\nFile \"/home/linfeng/Autoware/ros/src/util/packages/runtime_manager/scripts/runtime_manager_dialog.py\", line 2295, in ", "\nvp = VarPanel(self, var=var, v=v, update=self.update)", "\nFile \"/home/linfeng/Autoware/ros/src/util/packages/runtime_manager/scripts/runtime_manager_dialog.py\", line 2401, in ", "\nself.choices_sel_set(v)", "\nFile \"/home/linfeng/Autoware/ros/src/util/packages/runtime_manager/scripts/runtime_manager_dialog.py\", line 2572, in choices_sel_set", "\nself.obj.SetStringSelection(v)", "\nFile \"/usr/lib/python2.7/dist-packages/wx-2.8-gtk2-unicode/wx/_controls.py\", line 2437, in SetStringSelection", "\nreturn ", ".RadioBox_SetStringSelection(_args, *_kwargs)", "\nTypeError: String or Unicode type required", "I am really sorry to disturb you, and hope my bug report could help you make Autoware much stronger.", "Please update latest version.", "\nIn ", " I fixed that bug.", "\nIn addition , you have to delete \"param.yaml\" in ros/src/util/package/runtime_manager/scripts/ after update.", "Thank you very much!", "\nThe Autoware works fine now."]}
,{"url": "https://github.com/airalab/robonomics_comm/issues/96", "issue_title": "Objective player for services without liability contract", "issue_status": "Closed", "posted_on": "Feb 11, 2019", "issue_contents": ["When I use robonomics protocol to let user use a CPS service for free, I mean when on ", " recieved I do something immediately and return a ", " type message, I currently have to implement my own objective player.", " response packing support necessary as well. You may consider manually packing ", ".", "Closed in "], "issue_code": ["Demand", "Result", "Result"]}
,{"url": "https://github.com/ANYbotics/elevation_mapping/issues/58", "issue_title": "ERROR: cannot launch node of type [elevation_mapping/elevation_mapping]: can't locate node [elevation_mapping] in package [elevation_mapping]", "issue_status": "Closed", "posted_on": "Jan 26, 2018", "issue_contents": ["I'm using ROS kinetic and the compilation worked. What could be causing this?", "Hi,", "can you please share the whole error you get in the console and not paste it as title?", "\nAlso, did you source your catkin workspace after you compiled the package and before you tried to launch the node?", "marco-tranzatto, I used catkin_make_isolated for this, wich created a devel_isolated dir.", "\nSo to solve it the source has to be changed to:", "source devel_isolated/setup.bash"]}
,{"url": "https://github.com/at-wat/mcl_3dl/issues/200", "issue_title": "Define custom pcl point_type to have beam origin", "issue_status": "Closed", "posted_on": "Sep 18, 2018", "issue_contents": ["Current implementation stores id in ", " field.", "\nThe type should have ", ".", "PointXYZIL (xyz, intensity, label)"], "issue_code": ["intensity", "beam_origin_id"]}
,{"url": "https://github.com/at-wat/neonavigation/issues/346", "issue_title": "How to enable trajectory_tracker to do reverse driving?", "issue_status": "Closed", "posted_on": "Jul 9, 2019", "issue_contents": ["Hi", "i tested the trajectory_tracker and found that its result(forward driving) is nice. I wonder if i want to use it to do reverse driving, for whatever path that sent to it. How shall i enable this? using service call will be good.", "Best,", "\nSamuel", "Path message with poses facing opposite to the direction of the path makes robot backward running.", "Or, do you want to make the robot running reversely on the recorded path?", "\nIf yes, trajectory_tracker doesn't provide it.", "\nIn this case, please make a node or a tool which creates a reverse-path message from given path message.", "Hi ", "I recently get the robot to be able to run reversely on the recorded path. Neither changes are needed to be made onto the trajectory_recorder nor the trajectory_planner which generate path. The robot will follow the given path reversely. Only slight modification is needed to achieve this. Will share with you soon.", "Best,", "\nSamuel", "Hi ", "For doing the reverse tracking the path that generate by the planner, just need to apply a reverse direction frame (", ") that has its x-axis and y-axis in opposite direction of ", ". And make the trajectory_tracker to use this ", ", instead of ", ". At the same time multiplying the cmd_vel.linear.x with -1.0 will do the reverse tracking.", "Here i attached the screenshot for your reference.", "\n", "Best,", "\nSamuel"]}
,{"url": "https://github.com/at-wat/neonavigation/issues/349", "issue_title": "planner_cspace, costmap_cspace: move cost remembrance feature in planner_3d to costmap_3d_layer", "issue_status": "Closed", "posted_on": "Jul 16, 2019", "issue_contents": ["\n            ", "\n          ", "remembered cost is handled in float, costmap_cspace can only handle int8."]}
,{"url": "https://github.com/AutonomyLab/ardrone_autonomy/issues/168", "issue_title": "navdata returning 0", "issue_status": "Closed", "posted_on": "Dec 9, 2015", "issue_contents": ["In my program after subscribing navdata using callBack function, the values returning as zero. The values inside the callBack is giving the actual rot.Z data, but after exiting the function the value 'roll' returned as 0.", "This is probably due to a bug in your ", " program. Please consider posting the question to answers.ros.org"], "issue_code": ["C++"]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/394", "issue_title": "VSCAN min and max heights and velocity_set min and max detection heights", "issue_status": "Closed", "posted_on": "Jul 9, 2016", "issue_contents": ["The VSCAN ceiling and floor values : Are they wrt to ground or wrt localizer pose?", "\nAlso the config values for min(detection_height) ad max (detection_height) in velocity set config are wrt localizer pose or wrt ground values?", "\nI did study the vscan publication at  ", "\nbut am confused on how the measurements are taken. Also what does beamnum signify? I have a 16 ring lidar. Does beamnum coincide with ring number or does it have some other significance?", "The VSCAN ceiling and floor values : Are they wrt to ground or wrt localizer pose?", "They are wrt localizar pose.", "Also the config values for min(detection_height) ad max (detection_height) in velocity set config are wrt localizer pose or wrt ground values?", "They are also wrt localizer pose.", "I did study the vscan publication at ", "\nbut am confused on how the measurements are taken. Also what does beamnum signify? I have a 16 ring lidar. Does beamnum coincide with ring number or does it have some other significance?", " Can you answer this question?", "Thanks for your patience.", "The VScan's ceiling and floor values are wrt Velodyne pose (aka localizer).", "\nThe measurements are based on the flood-filled method filtering road surface and getting obstacles.", "\nThe beamnum defined how many beams you need in the 2D VScan.", "\n16-ring is a kind of vertical resolution, and the beamnum is related to a kind of horizontal resolution."], "issue_quotes": ["The VSCAN ceiling and floor values : Are they wrt to ground or wrt localizer pose?", "Also the config values for min(detection_height) ad max (detection_height) in velocity set config are wrt localizer pose or wrt ground values?", "I did study the vscan publication at ", "\nbut am confused on how the measurements are taken. Also what does beamnum signify? I have a 16 ring lidar. Does beamnum coincide with ring number or does it have some other significance?"]}
,{"url": "https://github.com/AutonomyLab/create_autonomy/issues/6", "issue_title": "Add support for bumpers and wheeldrop", "issue_status": "Closed", "posted_on": "Mar 8, 2016", "issue_contents": ["Expose ", " function calls at ROS API level.", "Added custom message for all bumper information: two contact sensors and six light sensors.", "\nAvailable in the ", " branch. Will be included in version ", "."], "issue_code": ["devel", "1.1.0"]}
,{"url": "https://github.com/at-wat/neonavigation/issues/347", "issue_title": "costmap_3d node dies by bad_alloc if map_overlay is outside of a global map", "issue_status": "Closed", "posted_on": "Jul 15, 2019", "issue_contents": ["When the ", " node subscribes ", " topic which is completely out of  a global ", ", this node dies by ", ".", "This is because that internally negative value can be assigned to width or height of CSpace3DUpdate in this situation. (", ")", "\nAs a result, message that has too large (nealy max of uint32) size of data array in ", " will be published and finally the node dies.", "It would be better to ignore publishing ", " topic  if ", " topic is completely out of the global map."], "issue_code": ["costmap_3d", "/map_overly", "/map", " std::bad_alloc", "CSpace3DUpdate", "/costmap_update", "/map_overly", "$ roslaunch neonavigation_launch demo.launch \n... logging to /home/ykoga/.ros/log/d1a3381e-a6f2-11e9-834d-186590dd30e5/roslaunch-ykoga-20734.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://ykoga:41849/\n\nSUMMARY\n========\n\nPARAMETERS\n * /costmap_3d/ang_resolution: 16\n * /costmap_3d/footprint: [[0.35, -0.22], [...\n * /costmap_3d/layers: [{'overlay_mode':...\n * /costmap_3d/linear_expand: 0.08\n * /costmap_3d/linear_spread: 0.3\n * /costmap_3d/static_layers: [{'type': 'Costma...\n * /largemap_to_map/hz: 1.5\n * /largemap_to_map/width: 60\n * /neonavigation_compatible: 1\n * /planner_3d/freq: 2.0\n * /planner_3d/freq_min: 0.5\n * /planner_3d/local_range: 2.0\n * /planner_3d/num_threads: 2\n * /planner_3d/use_path_with_velocity: True\n * /rosdistro: kinetic\n * /rosversion: 1.12.14\n * /spur/curv_forward: 0.2\n * /spur/dist_lim: 0.5\n * /spur/goal_tolerance_ang: 0.05\n * /spur/goal_tolerance_dist: 0.05\n * /spur/hz: 30.0\n * /spur/k_ang: 3.0\n * /spur/k_avel: 4.0\n * /spur/k_dist: 4.5\n * /spur/limit_vel_by_avel: True\n * /spur/look_forward: 0.1\n * /spur/max_acc: 0.3\n * /spur/max_angacc: 1.0\n * /spur/max_angvel: 0.6\n * /spur/max_vel: 0.5\n * /spur/no_position_control_dist: 0.05\n * /spur/path_step: 1\n * /spur/rotate_ang: 0.2\n * /spur/stop_tolerance_ang: 0.04\n * /spur/stop_tolerance_dist: 0.04\n\nNODES\n  /\n    costmap_3d (costmap_cspace/costmap_3d)\n    dummy_robot (planner_cspace/dummy_robot)\n    largemap_to_map (costmap_cspace/largemap_to_map)\n    map_server (map_server/map_server)\n    map_server_local (map_server/map_server)\n    patrol (planner_cspace/patrol)\n    planner_3d (planner_cspace/planner_3d)\n    rviz (rviz/rviz)\n    spur (trajectory_tracker/trajectory_tracker)\n    stf1 (tf2_ros/static_transform_publisher)\n\nauto-starting new master\nprocess[master]: started with pid [20744]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to d1a3381e-a6f2-11e9-834d-186590dd30e5\nprocess[rosout-1]: started with pid [20757]\nstarted core service [/rosout]\nprocess[costmap_3d-2]: started with pid [20774]\nprocess[map_server-3]: started with pid [20775]\nprocess[planner_3d-4]: started with pid [20789]\nprocess[spur-5]: started with pid [20797]\nprocess[patrol-6]: started with pid [20808]\nprocess[dummy_robot-7]: started with pid [20828]\nprocess[stf1-8]: started with pid [20838]\nprocess[map_server_local-9]: started with pid [20847]\nprocess[largemap_to_map-10]: started with pid [20860]\n[ INFO] [1563189746.615713931]: New static layer: unknown\n[ WARN] [1563189746.615801620]: overlay_mode of the static layer is not specified. Using MAX mode.\nprocess[rviz-11]: started with pid [20879]\n[ INFO] [1563189746.674812552]: New static layer: overlay0\n[ INFO] [1563189746.814571052]: New layer: overlay2\n[ INFO] [1563189746.866349534]: New layer: overlay1\n[ INFO] [1563189746.956683713]: 2D costmap received\n[ INFO] [1563189748.707477684]: Map received\n[ INFO] [1563189748.707561722]:  linear_resolution 0.05 x (400x400) px\n[ INFO] [1563189748.707596183]:  angular_resolution 0.39 x 16 px\n[ INFO] [1563189748.707625633]:  origin -10.000 m, -10.000 m, 0.000 rad\n[ INFO] [1563189749.258053933]: x:7, y:7 grids around the boundary is ignored on path search\n[ INFO] [1563189773.705720901]: New goal received (264, 293, 4)\n[ERROR] [1563189778.758714546]: Position jumped (12.282/1.000, 0.245/1.500); clearing history\n[ WARN] [1563189778.758859055]: Oops! You are in Rock!\n[ERROR] [1563189779.209776987]: You are on the edge of the world.\n[ WARN] [1563189779.209847529]: Oops! You are in Rock!\n[ WARN] [1563189779.705791926]: Oops! You are in Rock!\n[ WARN] [1563189780.214125122]: Oops! You are in Rock!\n[ WARN] [1563189780.705717191]: Oops! You are in Rock!\n[ WARN] [1563189781.222154753]: Oops! You are in Rock!\n[ WARN] [1563189781.705698776]: Oops! You are in Rock!\n[ WARN] [1563189782.205709201]: Oops! You are in Rock!\n[ WARN] [1563189782.710184015]: Oops! You are in Rock!\n[ WARN] [1563189783.205734355]: Oops! You are in Rock!\n[ WARN] [1563189783.718114774]: Oops! You are in Rock!\n[ WARN] [1563189784.205734110]: Oops! You are in Rock!\n[ WARN] [1563189784.726240091]: Oops! You are in Rock!\n[ WARN] [1563189785.206115052]: Oops! You are in Rock!\n[ WARN] [1563189785.705756589]: Oops! You are in Rock!\n[ WARN] [1563189786.214203820]: Oops! You are in Rock!\n[ WARN] [1563189786.705734343]: Oops! You are in Rock!\n[ WARN] [1563189787.222203333]: Oops! You are in Rock!\n[ WARN] [1563189787.705742892]: Oops! You are in Rock!\n[ WARN] [1563189788.205723615]: Oops! You are in Rock!\n[ WARN] [1563189788.710222021]: Oops! You are in Rock!\n[ WARN] [1563189789.205718794]: Oops! You are in Rock!\n[ WARN] [1563189789.718224281]: Oops! You are in Rock!\n[ERROR] [1563189789.884400708]: a message of over a gigabyte was predicted in tcpros. that seems highly unlikely, so I'll assume protocol synchronization is lost.\n[ WARN] [1563189790.205727747]: Oops! You are in Rock!\n[ WARN] [1563189790.726136511]: Oops! You are in Rock!\n[ WARN] [1563189791.206243766]: Oops! You are in Rock!\n[ WARN] [1563189791.705721752]: Oops! You are in Rock!\n[ WARN] [1563189792.214127410]: Oops! You are in Rock!\n[ WARN] [1563189792.705723930]: Oops! You are in Rock!\n[ WARN] [1563189793.222183517]: Oops! You are in Rock!\n[ WARN] [1563189793.705767665]: Oops! You are in Rock!\n[ WARN] [1563189794.205719503]: Oops! You are in Rock!\n[ WARN] [1563189794.710166384]: Oops! You are in Rock!\nterminate called after throwing an instance of 'std::bad_alloc'\n  what():  std::bad_alloc\n[ WARN] [1563189795.205720843]: Oops! You are in Rock!\n[ WARN] [1563189795.718130766]: Oops! You are in Rock!\n[costmap_3d-2] process has died [pid 20774, exit code -6, cmd /home/ykoga/sq_ws/devel/lib/costmap_cspace/costmap_3d __name:=costmap_3d __log:=/home/ykoga/.ros/log/d1a3381e-a6f2-11e9-834d-186590dd30e5/costmap_3d-2.log].\nlog file: /home/ykoga/.ros/log/d1a3381e-a6f2-11e9-834d-186590dd30e5/costmap_3d-2*.log\n[ WARN] [1563189796.205688303]: Oops! You are in Rock!\n[ WARN] [1563189796.705858076]: Oops! You are in Rock!\n[ WARN] [1563189797.206289667]: Oops! You are in Rock!\n[ WARN] [1563189797.705738850]: Oops! You are in Rock!\n[ WARN] [1563189798.205808489]: Oops! You are in Rock!\n^C[rviz-11] killing on exit\n[largemap_to_map-10] killing on exit\n[map_server_local-9] killing on exit\n[stf1-8] killing on exit\n[dummy_robot-7] killing on exit\n[patrol-6] killing on exit\n[spur-5] killing on exit\n[planner_3d-4] killing on exit\n[map_server-3] killing on exit\n[rosout-1] killing on exit\n[master] killing on exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\nneonavigation (master $% u+16) \n"]}
,{"url": "https://github.com/AutonomyLab/bebop_autonomy/issues/105", "issue_title": "[Question] Max tilt of the camera", "issue_status": "Closed", "posted_on": "Apr 11, 2017", "issue_contents": ["How can I point the front camera to the ground? (Similar to the FreeFlight App camera tilt.)", "\nUsing ros I'm stuck at something like 30 degree maximum camera tilt.", "\nBasically, how can I overcome this limit?", "Have you pulled the latest version of the repo and built it? The latest version should allow you to tilt the camera to 83 degrees.", "Just tested here, the new repo fixed my problem. Thanks for the reply!"]}
,{"url": "https://github.com/AutoRally/autorally/issues/11", "issue_title": "Rename muri-laptop machine tag and update corresponding launch files", "issue_status": "Closed", "posted_on": "Jun 8, 2016", "issue_contents": ["\n            ", "\n          ", "Resolved in ", "Fixed in release "]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/82", "issue_title": "(utils) Documentation of motion library", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["Should have an abstract and a detailed explanation for new contributors.", "\nAlso explain use with VARUN and in gazebo.", "Please look at Wiki/DevGuide for details"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/81", "issue_title": "(utils) Documentation of IMU code", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["\n            ", "\n          ", "Please see Wiki/DevloperGuide for details"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/109", "issue_title": "Serving Drinks Scenario", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["This is kind of a meta issue for the \"Serving Drinks\" scenario of RoboCup 2019 to keep track of what needs to be done. I will open more issues for every point listed below.", "In \"Serving Drinks\" the robot has to find all party guests without a drink, take orders and deliver the drinks.", "Can you please update the to-dos for this issue?"], "contents_details_more": [" Create a state machine for the scenario (see ", ")", " Create action \"choose_action\" for serving drinks, to decide what the robot is going to do next (get order / get drink / is done)"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/110", "issue_title": "Take out the garbage", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["This issue is created to keep track of the progress towards completing the ", " task.", "The robot takes out the trash bags from the two bins in the apartment and move them into a specified collection zone.", " Grasp verification can in theory be done (there is code for that in the ", " package in the ", " repository), but the verification depends on an object-specific threshold. This will thus need some testing."], "issue_code": ["mas_hsr_gripper_controller", "mas_hsr"], "contents_details_more": [" Create a state machine for the execution_manager [", "]", " Component to turn head to a certain point [", "]"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/113", "issue_title": "Recognize if there is a lid on the trash bin", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["Ideally, we should be able to recognize if there is a lid on the trash bin so Lucy can remove it by itself.", "In case we want to score the extra points for removing a lid from the bin, we would need to first recognize if there is a lid on top of the bin. This is not a ", " for the task, but desirable for the  ", " [see ", "] task."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/112", "issue_title": "Recognize if object is a bin", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["We need to recognize if an object in Lucy's field of view is a bin or not.", "This is needed for the ", " [see ", "] task. We need to be able to tell apart if the object is a bin or not so we can proceed with the task."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/114", "issue_title": "Recognize the trash bag inside the bin", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["We need to recognize the trash bag inside the bin so that Lucy can pick it.", "This is needed for the  ", " [see ", "] task. Once the bin has been detected, we need to recognize the trash bag in order to pick it. The rulebook specifies that the trash bag is of a different color than the bin, so it should be possible to achieve this."]}
,{"url": "https://github.com/autowarefoundation/autoware/issues/396", "issue_title": "NDT Mapping and matching and Voxel Grid Size | Resolution For Sparse Feature locations areas", "issue_status": "Closed", "posted_on": "Jul 13, 2016", "issue_contents": ["I attempted to create a map of a parking lot with  some mature trees. I generated the map using default voxel grid size = 1 and resolution = 1. Then I persisted the in memory map with voxel grid size .2, .5 and 1.  I find that localization fails when used with map persisted using voxel grid size .2 . However it does localization correctly when used with voxel grid size .5. Does this mean that localization works better with higher grid size in sparse feature regions? What criteria should be used to determine voxel grid size?  Can you explain the concept of resolution in ndt mapping or matching. ? Is it the scan points per meter? Or distance between scan points? Should it be set higher for sparse feature regions? Please provide some guidance on setting ndt matching and mapping  parameters", "Does this mean that localization works better with higher grid size in sparse feature regions?", "Theoretically, the larger grid size of voxel grid filter, the worse of accuracy of localization.", "What criteria should be used to determine voxel grid size?", "Computation cost becomes smaller if you use larger voxel grid size. We use 2.0 meter by default because it keeps accuracy of localization and computation finishes in the measurement interval of scan.", "Can you explain the concept of resolution in ndt mapping or matching. ? Is it the scan points per meter? Or distance between scan points?", "If you want to know the concept of NDT, please refer to following papers.", "\n", "\n"], "issue_quotes": ["Does this mean that localization works better with higher grid size in sparse feature regions?", "What criteria should be used to determine voxel grid size?", "Can you explain the concept of resolution in ndt mapping or matching. ? Is it the scan points per meter? Or distance between scan points?"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/116", "issue_title": "Student projects for SDP", "issue_status": "Closed", "posted_on": "Mar 6, 2019", "issue_contents": ["For next semester, it would be good if we can have some projects to propose to the SDP students. The projects should preferably", "If you have any ideas that you think would fit, please open an issue for it, and add the link as a comment here.", "cc ", " once you open the issue with the planning actions, please link it here.", "Depending on what's done in Koblenz, extension and improvement of ", " may be a fitting task.", "This could be a small, but useful project: ", "I've opened ", " to discuss the missing planning actions."], "contents_details_more": ["have a \"starting point\", e.g. some existing code, a tutorial, some old package that you found somewhere else", "have a longer scope than SDP"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/115", "issue_title": "Dectect where to grasp the trash bag from", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["Ideally, the robot should pick the trash bag from a specific part so that we can decrease the probability of damaging or tearing the trash bag.", "For the ", " [", "] task, the rulebook states that tearing the bag will result in a score reduction of 50 point for each bag. Having a way to detect the ideal place from were to grasp the trash bag should decrease the chance of it tearing."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/122", "issue_title": "Travis builds fail", "issue_status": "Closed", "posted_on": "Mar 9, 2019", "issue_contents": ["Travis builds for mas_domestic_robotics fail for every opened pull request because some dependency fails to build.", "Issue ", " needs to be fixed and ", " merged. With those changes the Docker container builds fine for me."], "issue_code": ["Starting >>> ae_test_scenario_visualiser                                       \nFinished <<< ae_msgs                                            [ 5.9 seconds ]\nStarting >>> ae_test_scenarios                                                 \n_______________________________________________________________________________\nErrors << ae_test_scenario_visualiser:install /kinetic/logs/ae_test_scenario_visualiser/build.install.000.log\nerror: package directory 'ros/src/ae_test_scenario_visualiser' does not exist\nCMake Error at catkin_generated/safe_execute_install.cmake:4 (message):\n  \n  execute_process(/kinetic/build/ae_test_scenario_visualiser/catkin_generated/python_distutils_install.sh)\n  returned error code\nCall Stack (most recent call first):\n  cmake_install.cmake:118 (include)\n  \nmake: *** [install] Error 1\ncd /kinetic/build/ae_test_scenario_visualiser; catkin build --get-env ae_test_scenario_visualiser | catkin env -si  /usr/bin/make install; cd -\n...............................................................................\nFailed << ae_test_scenario_visualiser:install                 [ Exited with code 2 ]\nFailed <<< ae_test_scenario_visualiser                        [ 3.3 seconds ]  \n"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/117", "issue_title": "mdr_listen_action fails to build", "issue_status": "Closed", "posted_on": "Mar 7, 2019", "issue_contents": ["The error message is as follows:", "It seems like the folder ", " is missing. ", " could you look into this?", "I could not really reproduce this error... Does it still appear after you fixed issue ", "?", "I also can't reproduce the error, but since the package doesn't seem to have a ", " directory, it makes sense to update the ", " so that it doesn't try to set up a non-existing package.", "To reproduce the error, build the packages with Docker, just as Travis does:", "This takes some time but eventually it will (still) fail with the error I posted above.", "The code expects some messages to be in ", ", while ", " points to the nonexistent ", " directory. I'm not sure where the automatically generate message files from ", " go, but it seems like ros also generates a package for that.", "Anyway, I tried building it with the reference to ", " in ", " removed and it seems to build and run just fine. If no one does, I'll create a PR tomorrow.", "I think the issue is this ", ", which points to a non-existent folder. Should I add that to my PR, or should it be handled in a separate PR ", "."], "issue_code": ["______________________________________________________________________________________________________________________________________________\nErrors     << mdr_listen_action:install /home/henrik/ros/mas_stable/logs/mdr_listen_action/build.install.004.log                              \nerror: package directory 'common/src/mdr_listen_action' does not exist\nCMake Error at catkin_generated/safe_execute_install.cmake:4 (message):\n  \n  execute_process(/home/henrik/ros/mas_stable/build/mdr_listen_action/catkin_generated/python_distutils_install.sh)\n  returned error code\nCall Stack (most recent call first):\n  cmake_install.cmake:118 (include)\n  \n\n\nmake: *** [install] Error 1\ncd /home/henrik/ros/mas_stable/build/mdr_listen_action; catkin build --get-env mdr_listen_action | catkin env -si  /usr/bin/make install; cd -\n..............................................................................................................................................\n", "common", "common", "setup.py", "mdr_listen_action.msg", "mdr_listen_action", "common", "Listen.action", "mdr_listen_action", "setup.py"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/141", "issue_title": "Using the web-based socket for HRI", "issue_status": "Closed", "posted_on": "May 8, 2019", "issue_contents": ["Could we integrate the code we used for the screen during ", " into this repository? Or can we get a fork from the ", " repo? I think that visual information on the robot is super useful, and we are currently wasting the screen.", "The one from roboland is very amy specific. I created a new repo on my git for the GO. I would need to clean it up and remove GO specific stuff. Also, we could develope it further and add more things.", "Link to the repo: ", "Yes. I will create a repository for that and we start with what we have developed in Magdeburg, but should clean it and improve it quickly.", "Suggestion from Alex and me regarding the repository name: ", "should we fork this one and start from there? or is it really better to start from scratch?", "Can we fork, but rename it? If yes, then we should do it. Otherwise I would prefer the more general name for the repository. In that case I would ask Roberto to push to the newly created repo and we continue the work from there."], "issue_code": ["where is this?", "RoboLand", "mas_hri_interface"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/142", "issue_title": "Integrating operator detection into where is this", "issue_status": "Closed", "posted_on": "May 8, 2019", "issue_contents": ["I believe we already had a state for this, but it would be nice to test this thoroughly and maybe make the whole scenario a bit more human-friendly."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/140", "issue_title": "README has broken link", "issue_status": "Closed", "posted_on": "May 7, 2019", "issue_contents": ["Has a link:", "b-it-bots members can use ", " to setup a complete development environment for all our robots.", "For external users, the following instructions should get you a working system:", "The section does say \"b-it-bots mebers\" but I ( ", " ) still think that the file which is being linked to should be open as it helps anyone who might want to re-use this code.", "The link to work", "The link doesn't work", "make that repo open source, or move that portion of the repo to the public somewhere else.", "I ( ", " ) was giving some advice to go look at some of your code for an example.", "Any browser", "Just confirming, I filed that ticket from someone else's computer.", "I don't see anything in the dev-env repo which is worth keeping private, it's basically a newer version of 3 other repos, which it cites at the bottom of it's readme.", "For Context, I've asked ", " to investigate two open-sourced approaches to solving a task.", "So she was trying to get something from here running on her robot.", " this can be closed as a duplicate. I knew I ran into this issue before.", "I still haven\u2019t had the chance to clean up that repo, which is why it\u2019s still private. I\u2019ll give access to ", " later today.", "FWIW the repo is unfortunately not updated with all the dependencies, so PRs are welcome if you run into problems.", "Thank you for giving me access!"], "issue_quotes": ["b-it-bots members can use ", " to setup a complete development environment for all our robots.", "For external users, the following instructions should get you a working system:"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/143", "issue_title": "Testing find my mates", "issue_status": "Closed", "posted_on": "May 8, 2019", "issue_contents": ["This one doesn't really require further description, but ", " you can add any further things that remained as todos.", " could you help us out with running this test (maybe with the help of ", " if he has some available time during exam season?)?", "The important thing out of this is finding out already things that don't work or need to be updated to work on the lab instead of GO.", "I met with ", "  and we identified a few problems when trying to run this scenario. Since issues/ pull request have been raised, I will link them:", " and I will need to work on the people finding action on Friday/Saturday; this is related to his R&D, but we'll tell you how it goes (and we'll fix any issues as we face them)."], "contents_details_more": [" Speech models was not pointing to the correct directory. Solved [", "]", " NLU Rasa models for find my mates is not present. (needed for \"find\" intend and for names and description of persons) [", "]", " Find people package was not running. (Tried running the package by itself, but does not seem to return a result.)  [", "]"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/158", "issue_title": "[manipulation] move DMP parameters to a configuration file", "issue_status": "Closed", "posted_on": "May 24, 2019", "issue_contents": ["Currently, a lot of the parameters in ", " are hardcoded. They should be moved into a configuration file.", "This issue was addressed by PR "], "issue_code": ["mdr_planning/mdr_actions/mdr_manipulation_actions/mdr_move_arm_action/ros/src/mdr_move_arm_action/dmp.py"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/167", "issue_title": "Clean up mdr_speech_matching", "issue_status": "Closed", "posted_on": "May 24, 2019", "issue_contents": ["The current version of the package contains comments in the ", " and ", " files. These files also need to be updated to handle the dependencies correctly.", "There are also a few files that are hardcoded directly in the node, which should be changed to parameters and added to the launch files."], "issue_code": ["package.xml", "CMakeLists.txt"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/170", "issue_title": "Refactoring the picking and placing actions", "issue_status": "Closed", "posted_on": "May 25, 2019", "issue_contents": ["The following actions need to be rewritten to avoid code duplication:", "See ", " for more information.", "I wouldn't want to touch these at the moment since any refactoring has implications on the ", " and the implementation of actions in general. I thus suggest we close this issue for the time being."], "issue_code": ["mdr_behaviours/place.py", "mdr_behaviours/place_based_on_category.py", "mdr_place_action/action_states.py", "mdr_pickup_action/action_states.py"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/126", "issue_title": "Planning actions for new robocup scenarios", "issue_status": "Closed", "posted_on": "Mar 15, 2019", "issue_contents": ["For some of the SDP projects, I would like students to develop missing skills or actions. Could you please add a list here of the things you have come up so far? Please add to which scenarios they belong in case we have too many, this will help with prioritization.", "Related to ", ", ", ", ", ", ", ", ", ".", "Related to the ", " task [", "]:", " ", " ", " the deadline for this is Friday, has there been any progress? I still need to work on the proposals a bit after you guys suggest them.", "I'm not sure which scope those SDPs are supposed to have, but here are some things that need to be done for \"serving drinks\":", "Hey, besides the actions or skills we could need a JSHOP2 dispatcher for ROSPlan. At the moment we are using the standard dispatcher which has a slightly different plan representation. At the moment shop is modified to generate the plan in the same way the popf planner does.", "Otherwise ", " ", " and ", " have already defined the skills needed for the tasks.", "I think at one point we should discuss about what each skill is doing and the naming of the operators and predicates for the preconditions, add and delete actions they will add to the KB, these should also be defined so that the students stick to these conventions, this will save a lot of refactoring once they get integrated.", "naming of the operators and predicates for the preconditions, add and delete actions they will add to the KB", "Let's add that to this discussion thread, so I can refer back to it when writing the proposals.", "\nCould you maybe add those for the actions that are already listed?", "i started with the once we allready use in the domain.", " The client of ", " does more than what you have in the table: for each object, an ", " and ", " predicates are added + the plane is marked as ", ". Also, before perceiving, all objects on the plane are deleted from the KB. Implementation ", ". The other actions match what we have.", "ok, i add those things, we should then progress with the explore and locate behaviour.", "explore - would be exploring something unknown like a room, piece of furniture or whatever.", "locate - would be identifying an object at specific location.", "so explore would mostly also include the locate action, but the locate action can be used when we  already know the location but not the exact position."], "issue_code": ["perceive_plane", "on(object, plane)", "object_category(object, category)", "explored"], "issue_quotes": ["naming of the operators and predicates for the preconditions, add and delete actions they will add to the KB"], "contents_details": ["explore - would be exploring something unknown like a room, piece of furniture or whatever.", "locate - would be identifying an object at specific location."], "contents_details_more": [" Component to turn head and /or body to a certain point (LookAtAction)[", "]", "Detect all people in a room", "Detect faces of (detected) people and remember them (there might already be some code for the detection part ", ")", "Recognize what a person is holding in their hands", "Create an action/behaviour for picking up specific objects"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/171", "issue_title": "Clean up of the mas_hri_behaviours", "issue_status": "Closed", "posted_on": "May 25, 2019", "issue_contents": ["\nThese files also need to be updated to handle the dependencies correctly.", "This also means moving the models to the ", " repository.", "Some clean up was already done in ", " if you have some time this week, could you help out with this? Then we can finally merge ", "Yes, I will work on this on Friday. "]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/108", "issue_title": "Define Scenario \"Farewell\".", "issue_status": "Closed", "posted_on": "Mar 5, 2019", "issue_contents": ["We have prepared the scenario storing groceries already. Now it is time to proceed with the next task and set it up in parallel. The scenario suggested in this issue is the \"Farewell\" scenario.", "The \"Farewell\" scenario is described in the newest rule book of the RoboCup competition (2019).", "Similar to storing groceries a scenario in mdr_robocup_tasks needs to be defined."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/173", "issue_title": "Clean up the mdr_listen_action", "issue_status": "Closed", "posted_on": "May 25, 2019", "issue_contents": ["The launch file still contains some HSR-specific topics."]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/172", "issue_title": "Clean up of mdr_find_people", "issue_status": "Closed", "posted_on": "May 25, 2019", "issue_contents": ["The current version of the package contains comments in the package.xml and CMakeLists.txt files. These files also need to be updated to handle the dependencies correctly.", "This topic should be passed as an argument ", "\n      ", "\n    ", "\n         Line 18\n      in\n      ", "\n    ", "And we need an updated launcher for ", " which then modifies that topic to the HSR-specific one.", "One other thing that needs to be done is reimplementing the action using FTSM.", "duplicate ", "?", "I don't think this is a duplicate. This issue is about small refactoring of the existing package; your issue is about the logic behind the action itself, which I see as considerably more involved.", "The cleanup has already been done (commits ", ", ", ", and ", "), so I'll close the issue. The migration to FTSM can/should be a separate issue."], "issue_code": ["mas_hsr"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/199", "issue_title": "Fix place action name in place client", "issue_status": "Closed", "posted_on": "Jul 11, 2019", "issue_contents": ["To match the name case in the ", ", which uses camel case starting with a capital letter for the action names, the names of the action in the ", " and the ", " should be changed to ", " rather than ", " and ", " rather than ", " respectively.", "Resolved by ", "To avoid issues like this in the long run, maybe it's better to start using snake case in the ontology, although that would violate various OWL naming conventions (e.g. ", ", where the use of camel case is recommended for classes)."], "issue_code": ["Release", "release", "Throw", "throw"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/202", "issue_title": "Migration to melodic", "issue_status": "Closed", "posted_on": "Jul 24, 2019", "issue_contents": ["I've opened a different issue from ", " since I don't want to mix things up.", "\nSince I had to set up 18.04 on my laptop, I gave it a try with our repos. One thing I noticed, is that it seems there are no packages in bionic for the HSR. Can someone else double-check?", "I have an e-mail where they say they support 19.04, but nothing for 18.04. If that's the case, melodic may be out of the question.", "Even the update manual for 19.04 only says it works with ROS kinetic, so I guess melodic is not yet supported by Toyota. It may make sense to ask them directly what the timeline for this upgrade is.", "It\u2019s possible to write Kinect/Melodic compatibles code- MoveIt is Kinetic/Melodic compatible.", "Just build and test master on both versions.", "If you want to run Melodic/Kinetic mixed, then you can do that with Docker, as long as the msgs haven\u2019t changed between versions, and most the standard msgs haven\u2019t changed.", "19.04 isn\u2019t a targeted version of Ubuntu for any ROS release.", "The decision was made to only target LTS versions of Ubuntu, and ROS Noetic will be the last version of ROS1. After N-turtle everyone should switch to ROS2.", "18.04 is LTS, and ROS1 Melodic / ROS2 Bouncy are both LTS.", "Supporting 19.04 and Kinetic is very strange.", "Now that I'm back, I had a bit of time to look a bit more carefully. The 19.04 refers to ", " package version numbers, not the OS. As far as I could tell, they are still only supporting ", ". I assume no one touched base with Toyota, so I've sent an email asking just in case.", "I'll close this since it's not high on our priority list anymore."], "issue_code": ["kinetic"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/217", "issue_title": "Grasp verification improvements", "issue_status": "Closed", "posted_on": "Feb 5, 2020", "issue_contents": ["Continuation of ", "As a precursor for recovery in the ", " action, we need to improve the current force sensor-based grasp verification (currently used the HSR), which is not very flexible with respect to different objects. For this, we could start with some of the ideas from last semester's FDD projects:", "There were some potentially interesting strategies there that may be worth exploring.", " made changes about this in ", ", so I'll close the issue. The new strategy compares the opening angle of the gripper before and after grasping and reports a grasp if the angular difference is larger than an experimentally determined threshold.", "This seems to work well for most objects, but not for slim objects, such as a towel. For such objects, we plan to incorporate vision-based verification as a second step (i.e. if the angle-based method reports no grasp, we would check whether an object is actually grasped by processing the HSR's hand camera)."], "issue_code": ["pickup", "mas_hsr_gripper_controller"], "contents_details_more": ["pull action: ", "push action: ", "handle opening action: "]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/83", "issue_title": "(utils) Documentation of Image processing nodes", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["Explain the algorithm and the how to use it.", "Please look at Wiki/DeveloperGuide for details"]}
,{"url": "https://github.com/b-it-bots/mas_domestic_robotics/issues/174", "issue_title": "Clean up of where is this", "issue_status": "Closed", "posted_on": "May 25, 2019", "issue_contents": ["The rasa models we are loading are right now hard coded, see ", " for an example. It probably makes sense to move the models to ", " as well.", " ", " thoughts?", "There's a ", " that may make it easier to get files from packages' relative paths. We can put these type of files in the model repository as a ROS package and access them this way.", "Sounds good to me, can you open a PR for that?", "Do you mean to upload a model? Shouldn't ", " handle this?", "I meant the utility function. Need a proposal on how to handle it, e.g. do we want to make ", " dependent on ", "?", "I don't think being dependent on ", " makes sense in that context. It would be better if such utility functions are stored elsewhere, perhaps in a dedicated repository, since they could be useful in different places (e.g. we need something similar in the execution manager for handling the state machine hierarchies in the code rather than through launch files).", "Edit: Perhaps we could create a repository similar to the ", " repository, which exposes a package with various useful functionalities - in that case, for working with MongoDB databases and black box data?", "I'm for a small utility repository for this kind small functionalities. We should try to keep it compact and as a single package though, to avoid another ", ". ", "? ", "?", "I\u2019m in favor of that. Any preference on the name?", "I have a small preference for ", ", but only because I'm biased by ", ". :)", "Either way is fine for me. I would also want to migrate some of the utilities from ", " there for reuse.", "Here it goes ", "should it be a ROS package? I suppose there's the possibility for it to contain both C++ and Python.", "I'd say yes, mostly because ROS deals with package paths nicely.", "you can now access the function with ", " do you have an estimated date on when you could push the fixes mentioned here? For now this seems to be one of the blocking issues for ", "Yes, I'll get it done today.", "The rasa models we are loading are right now hard coded, see ", " for an example. It probably makes sense to move the models to ", " as well.", " ", " thoughts?", "The checkpoint file is automatically generated by the training script of rasa. I am almost certain that it is not needed for the implementation part because it would've complained already when I put the model folder in lucy.", " should I just move everything the way it currently is inside  ", " under a rasa_nlu/where_is_this folder?", "Yes, but make it a ROS package, I would say: ", ". Then edit the ", " package to load the models from the right place.", " can give you further info on how to use the utilities from ", "Minh is at lunch at the moment, so I'll answer. Currently ", " only has a single script inside the package - ", ", which exposes a single function, namely ", " (the documentation has an example call).", "Basically, if ", " is a package, then you can find the absolute path of the model directory with the following code snippet:", "Is this done? Was there a different PR for this issue other than the one that was closed?", " ", " shall I close this?", "Yeah, I think so."], "issue_code": ["mas_models", "mas_models", "mas_perception_libs", "mas_perception_libs", "mas_common_robotics", "mas_utils", "mas_tools", "mas_tools", "black-box-tools", "dataset_interface", "from mas_tools.ros_utils import get_package_path", "mas_models", "rasa_nlu_models/common/where_is_this", "mdr_where_is_this", "mas_tools", "mas_tools", "ros_utils", "get_package_path", "rasa_nlu_models", "from mas_tools.ros_utils import get_package_path\nmodel_path = get_package_path('rasa_nlu_models', 'common', 'where_is_this')\n"], "issue_quotes": ["The rasa models we are loading are right now hard coded, see ", " for an example. It probably makes sense to move the models to ", " as well.", " ", " thoughts?"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/88", "issue_title": "Yaw publish on gazebo", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["right now gazebo is publishing only raw data. You need to publish the yaw angle for calibrating turn pid on gazebo"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/86", "issue_title": "(utils) Send node names, topic names and anyother string from launch file", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["The code of nodes should not have string constants in them. For example if I change the name of forwardServer from \"forward\" to something else then everything will stop working. Better would be to give these strings to node from the launch file so that we can see all such strings in one place. also we can use ", " ros pkg to simplify our ros launch files."], "issue_code": ["xmacro"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/90", "issue_title": "(hardware_layer) publish `roll` `pitch` also from hardware_imu", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["publish roll and pitch just like yaw from hardware_imu", "\nalso make sure to do so to the same topic as gazebo", "not really required"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/93", "issue_title": "(debug_layer) Gravity in gazebo", "issue_status": "Closed", "posted_on": "Jul 7, 2016", "issue_contents": ["Add gravity in the gazebo for the upward motion before the upward PID calibration.", "not really required"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/92", "issue_title": "(task_handler_layer) Fix line detection and line following nodes", "issue_status": "Closed", "posted_on": "Jul 7, 2016", "issue_contents": ["Right now our bot stops the thrusters on detecting the line but goes on drifting due to inertia and the line following node just take the angle and align it to the angle of the line. By this our bot gets displaced and it aligns with the line but the center is not at the line. ", "  you need to fix this problem. What we can do is first align the bot to the line and then move the bot sway until the line center and the bot center coincides. The direction of sway can be obtained by the position and the angle of the detected line.", "closed by "]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/87", "issue_title": "(hardware_layer) Launch files for hardware layer", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["launch file for hardware layer is empty right now it needs to start imu, cameras, arduino node", "Right now we are only able to launch imu from it. there is some problem in launching arduino node as we  have to send ", " as commandline argument.", "It also needs to launch camera node twice for both cameras and publish the data correctly."], "issue_code": ["/dev/arduino"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/89", "issue_title": "launch files: write launch files for pid callibration", "issue_status": "Closed", "posted_on": "Jul 6, 2016", "issue_contents": ["launch files represents the mode in which we will run code. Ideally we should not need to run nodes ourself. Right now we have 2 set of launch files. One for launching everything with gazebo and one with hardware_layer. But now we have a third case where we would need to run some specific nodes for callibration of pid (on gazebo and on hardware) so I propose we write a new set of launch files which would start hardware layer and gazebo with topic remapping. meaning that the topics from sensors will be renames to topics we use to tell motion nodes what is the current position. Like ", " should be remapped to ", " during callibration so that ", " node pid can be callibrated using pressure sensor data. similarly for ", " should be mapped to ", ". So we can have launch file in ", " which launches ", " along with ", " and launches one of ", " or ", ". Where ", " are the respective launch files with topic remapping."], "issue_code": ["/varun/sensors/pressure_sensor/depth", "/zDistance", "upward", "/varun/sensors/imu/yaw", "/yaw", "motion_commons", "motion_nodes.launch", "rqt_reconfig", "hardware_nodes_pid.launch", "debug_nodes_pid.launch", "xyz_pid.launch"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/94", "issue_title": "Test nodes for PID calibration", "issue_status": "Closed", "posted_on": "Jul 7, 2016", "issue_contents": ["Write the test nodes  for all the motion  for calibrating pid, also ensure that the sensors do not publish the data directly to the motion library, the data should go through the test nodes and the task handlers.", "closed by "]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/95", "issue_title": "(task_handler_layer) Add on-off switch to IP nodes", "issue_status": "Closed", "posted_on": "Jul 7, 2016", "issue_contents": [" to control IP nodes from task handlers, we directly use ", ". But we want to run all the nodes (all ip nodes and task handlers) in parallel so we need to add a switch that will turn on and off the ip nodes. It has already been done in task buoy. The server publishes on a topic (boolean data type) which is subscribed by the ip node that changes a global boolean variable which is checked in a if condition in the main while loop. If that boolean is then only the whole processing of images starts. So you need to add similar check in all the other IP nodes including line detection and line following node.", "closed by "], "issue_code": ["system(rosrun ...)"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/96", "issue_title": "Add stabilization to the task handlers", "issue_status": "Closed", "posted_on": "Jul 7, 2016", "issue_contents": ["In all motions, bot  performs an unexpected yaw motion. To prevent this, add goal in the task handlers with infinite loop time that will ensure the yaw angle to be fixed.", "closes "]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/115", "issue_title": "(task_handler_layer) Launch file to launch all nodes together", "issue_status": "Closed", "posted_on": "Jul 9, 2016", "issue_contents": ["Also on launch all nodes should be inactive. Only when they get a goal should they start doing image processing or sending motion goals to motion library", "Closed by "]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/98", "issue_title": "(hardware_layer) Add ip preprocessing node", "issue_status": "Closed", "posted_on": "Jul 7, 2016", "issue_contents": ["Add a preprocessing node which takes image from ", " and ", " and fixes camera distortion and publishes to ", " topic which can then be used by ip nodes. Since camera distortion is in both hardware and gazebo so this can be a part of hardware layer.", "not really required"], "issue_code": ["varun/sensors/front_camera/image_raw", "varun/sensors/bottom_camera/image_raw", "varun/sensors/front_camera/image"]}
,{"url": "https://github.com/AUV-IITK/Varun-Software/issues/161", "issue_title": "Dynamic Reconfigure for all parameters", "issue_status": "Closed", "posted_on": "Aug 28, 2016", "issue_contents": ["Add dynamic reconfigure for all the parameters in the code, i.e. in all the motion lib nodes and in all the IP and task handlers."]}
,]