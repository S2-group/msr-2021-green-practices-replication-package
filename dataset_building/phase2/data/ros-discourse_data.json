[{"title": "Don't miss EU funding for ROS developments via ROSIN FTPs - Next cut-off dates: June 14 & Sept 13, 2019", "thread_contents": ["The EU H2020 ", " has the goal to advance open-source robot software for industry and the robotics community as a whole. One of the main activities of the project is a grant program with a total amount of ", " for Focused Technical Projects (FTPs) on ROS software development.", "Sofar, we have funded almost 40 FTP projects. Many (intermediate) results can be found here:", "\n", " Do you have a good idea for a ROS/ROS-Industrial related project and want to work on ROS(-I) software components, documentation, standardisation or a related topic?", " Are you or your company located within the European Union (or any of the ", ")?", " Then submit an FTP proposal and apply for a ROSIN grant by following the steps outlined in the ", ".", "FTP proposals may be submitted all year long but are evaluated 3-4 times a year.", "The next cut-off date are: ", " .", "Proposals are short (a few pages) and concise. Applicants are guided through the process by an application wizard and a guide is provided.", "To submit your FTP, please visit ", ".", "If you have any questions about the process, whether your idea or project would qualify, send me or one of my colleagues a message either through ROS Discourse or by email (see the ", " on the site for addresses).", " Too much text and no video?", "\n", " Watch our project coordinator explaining ROSIN project and it\u2019s funding opportunities at last year\u2019s ROSCon in less than 200 seconds", "\n          ", "\n", "or follow the 30 minutes presentation from ROS Industrial Conference (followed by four 15 minutes presentations of FTPs already funded by ROSIN):", "\n            ", "\n", "As a result of discussions at ROSCon2018, ROS-Industrial Conference 2018 and exchange of ideas in early 2019, we would like to solicit \u201c", "\u201d (or pre-baked FTPs) for upcoming ROSIN FTP calls in June and September. These suggestions shall give a focus on the topics we fund with our EU funding, but is not meant to not limit the scale and scope of future FTPs. Please check (upcoming) ", " from more than 38 ROSIN FTPs, to see what projects are in the pipeline or have been finished.", "\nWhich manipulator is suitable for a specific task? What is the ideal location of an object (e.g. to be polished) within a workcell for a given manipulator? Compare different solutions (in terms of computation time, energy consumption) for a specified task. Currently within MoveIt! it is possible to benchmark OMPL planners only ", "\nCurrently, mobile bases are approximated with floating joints within MoveIt! and it is not possible to execute these trajectories. Only other possibility is to plan for the arm (MoveIt!) and the mobile platform (navigation stack) separately. ", "\nAlthough FTP in similar direction has been approved (Move-RT): ", "\n(MoveIt!'s transition to ROS 2.0)", "\nMake MoveIt\u2019s interfaces more generic which will simplify integration of standalone components (kinematics and collision checking). and also external planners (Descartes) and optimizers (TrajOpt).", "\nKinematicsBase API", "\n", "\n", "\nCollision checking API", "\n", "\nAllow for \u201cpartial\u201d SRDF definitions to use arms with a combination of end-effectors / mount objects (tables, mobile base), similar to XACROs? Have a unified robot description (URDF, SDF, SRDF, COLLADA). Multiple formats introduces repetition of link names, etc.", "\nTiming and memory analysis tools for ROS nodes.", "\nAlthough some tools have been mentioned in the link: ", " ,", "\nit may not be the best solution for multi threaded, multiprocess applications", "\nEspecially, (most commonly used) Valgrind seems to be mainly for single-threaded applications and runs on VM, which distorts the actual results.", "\nA tool by OSRF (", ") has not been updated since 2014.", "\nHardware-in-the-loop / model-in-the-loop testing", "\nPackage versioning (possibly a REP)", "Kind reminder for the European ROS Community.", "\nOur next cut-off date is approaching.", "\nWe will be happy to evaluate all FTP proposals that are submitted before ", " end-of-day.", "\nNext cut-off is then on September 13, 2019 .", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/dont-miss-eu-funding-for-ros-developments-via-rosin-ftps-next-cut-off-dates-june-14-sept-13-2019/8999"}
,{"title": "Safety working group meeting and call for participation", "thread_contents": ["The ROS 2 Safety working group will be meeting this week on ", ".", "We haven\u2019t met since early December and we have not done a great job of achieving our goals, so this meeting will be as much about trying to inject some energy back into the group as anything else.", "The safety working group currently is focusing on producing a catalogue of architectural patterns commonly used in safety-critical systems and how to implement them using ROS 2. You can find the catelogue (currently empty) on the ", ".", "We also welcome any other ideas on things the working group could achieve that would help the ROS community use ROS in building safe robots.", "Join Zoom Meeting", "\n", "Meeting ID: 266 509 195", "One tap mobile", "\n+19292056099,266509195# US (New York)", "\n+16699006833,266509195# US (San Jose)", "Dial by your location", "\n+1 929 205 6099 US (New York)", "\n+1 669 900 6833 US (San Jose)", "\n+81 524 564 439 Japan", "\n+81 3 4578 1488 Japan", "\n+49 69 7104 9922 Germany", "\n+49 30 3080 6188 Germany", "\n+49 30 5679 5800 Germany", "\n+34 917 873 431 Spain", "\n+34 84 368 5025 Spain", "\n+44 203 051 2874 United Kingdom", "\n+44 203 481 5237 United Kingdom", "\n+44 203 966 3809 United Kingdom", "\n+44 131 460 1196 United Kingdom", "\nMeeting ID: 266 509 195", "\nFind your local number: ", "I\u2019ll join this one a bit late but will be there!", "+1 for the \u201ccall for participation\u201d, ping to groups involved in security matters.", "involved", "+1  for the \u201ccall for participation\u201d", "Minutes are available here:", "ROS safety working group repository", "Apologies ", " and everyone else in the call I could not join at the end. Something urgent came up. Will try my best on the next telco.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/safety-working-group-meeting-and-call-for-participation/12489"}
,{"title": "ROS Answers needs your help", "thread_contents": ["Or to be more specific: all of us posting on ROS Answers need your help.", "?", "Not counting the duplicates and the ones that are easy to answer (\u201cdid you source ", "?\u201d, \u201cNo, pkg X is not released into ROS version Y\u201d), I easily see a hundred new questions on ROS Answers each week.", "While many are about installation, deployment or integration issues, the ones that make ROS Answers really worth the time are those that ask about advice on configuration of specific packages. How to tune the PIDs on a ", " based driver fi, what sensors to choose for ", " in an underground cave or whether a 2D navigation stack can actually be used by a 6D drone \u201c", "\u201d.", "A large number of volunteers from the community help out by answering quite a few of these questions, or at least try to help getting closer to a solution \u2013 and they deserve a lot of recognition and thanks for that. I\u2019ve noticed however somewhat of a decline in the number of interactions of what I would call ", " with the ROS Answers community: users that have extensive experience with specific functionality such as mapping, localisation, motion planning, application design, human-robot interaction, behaviour modelling, system architecture, high-volume datastream processing, multi-robot systems, etc.", "It\u2019s exactly those kinds of interactions that make ROS Answers an invaluable resource, and it would be unfortunate if we lose that.", "This post is therefore a ", " (or perhaps even a ", "): if you have experience with one or more packages and you feel you could help out a fellow ROS user that is perhaps just starting out, take 15 minutes at the end or start of your day to check whether there are any questions you could contribute to.", "Even a short comment pointing the poster in the right direction has the potential to save someone hours of debugging or searching. Don\u2019t understimate how much you can help someone, even if you feel you are not a \u201cROS expert\u201d (whatever that may mean).", "And if you\u2019re just starting out yourself, you could consider ROS Answers a good way to get familiar with ROS: pick a question and try to figure out what the answer could be. I\u2019ve learned quite a few interesting things about ROS that way which I would not have known otherwise.", "Thanks for the call ", "! Always good to give back.", "Thank ", " for taking the time to answer a few questions ", "If ", "\u2019s verbal argument is not sufficient, here\u2019s a cold hard statistic.  As of this post, only ", " of questions have an accepted answer.", "Indeed ROS Answers is a great community resource and it does rely on the community to give back to make it effective.", "In addition to visiting the site periodically you can also sign up to be notified when specific tags are mentioned. So if there\u2019s a topic that you feel that you\u2019re able to help out on add tags associated with that to the \u201cInteresting tags\u201d section in the side bar and make sure you have \u201cEntire forum (tag filtered)\u201d enabled in your \u201cemail alerts\u201d settings.", "Maintainers we suggest subscribing to tags relating to your packages too. It\u2019s a good way to learn where people are getting tripped up as well as can be an early indicator of a bug or issue with the code.", "If ", "\u2019s verbal argument is not sufficient, here\u2019s a cold hard statistic. As of this post, only ", " of questions have an accepted answer.", "I\u2019m not sure how you\u2019re computing your ratio, but I find the ", " a little bit more representative of the state of things. Questions that are \u201cclosed\u201d as duplicate or off topic will never have an \u201caccepted\u201d answer but also aren\u2019t waiting for one. And the outstanding \u201cunanswered\u201d questions are only 12837 out of 40548 total which is closer to 68% resolved.", "If everyone who reads this could try to answer at least one question per week the backlog would be cleared quite quickly.", "As with many things, the answer is ", ". It does ignore closed questions.", "However, the Askbot API isn\u2019t perfect, so I may be missing some values.", "Although I can\u2019t find a reference, there was at least once a coordinated day for burning down unanswered questions wasn\u2019t it? My calendar still marks Septermber 19th is world ", " day (not sure when, not sure if that was the title either).", "If ", "\u2019s verbal argument is not sufficient, here\u2019s a cold hard statistic. As of this post, only ", " of questions have an accepted answer.", "\u2026 And the outstanding \u201cunanswered\u201d questions are only 12837 out of 40548 total which is closer to 68% resolved.", "The original claim was \u201cI\u2019ve noticed however somewhat of a decline in the number of interactions of what I would call topic experts\u2026\u201d. To substantiate, prove or refute it numerically, a ", " in a timeseries needs to be provided.", "\nThe ROS metrics reports are a little more helpful in that respect.", "The current ratio, be it 44% or 68%, is a quite meaningless number without the context to judge it. Maybe even 44% could be quite a lot and a sign of good health of the community.", "Perhaps OSRF needs to hire some support staff if these metrics really are trending down (which we have yet to see proved in this thread). Or, perhaps at ROSCON we get some companies pledging to contribute support hours to the community. There are a lot of companies making a shit ton of money using ROS, it\u2019s only fair they give back, right?", "Next year, in order to get \u201cPlatinum\u201d sponsorship, you also have to prove your company has contributed XX support hours to the community.", "Just an idea.", "Or, for individual supporters/not affiliated, maybe those with enough internet points karma (can\u2019t remember what ROS answers uses) they can get their ROSCON registration fee waived/top contributors full expense paid and sponsored to come to ROSCON.", " do you have suggestions on how to find questions about", "application design, human-robot interaction, behaviour modelling", "in ROS Answers? I\u2019ve been using ROS for creating interactive behaviors and running HRI experiments last couple years. I\u2019ve been wanting to answer some questions in ROS Answers but had difficulty in getting started. I understand there are tags, \u201cSort By\u201d, etc., that I could use to navigate the answers but I had difficulty in spending time searching for questions that are in my domain.", "Any suggestions would be greatly appreciated and thanks for staring this thread.", "Wow, quite some responses.", "First: re: \u201cunproven claims\u201d: I deliberately used the words \u201cI\u2019ve noticed\u201d and nuanced it with \u201csomewhat of\u201d. That makes this a personal observation. I\u2019m in a university, so I\u2019m all for hard numbers and facts, but I don\u2019t have them. I\u2019m also wondering how I would gather such statistics, as unfortunately Askbot doesn\u2019t have a ", " badge.", "Or, for individual supporters/not affiliated, maybe those with enough internet points karma (can\u2019t remember what ROS answers uses) they can get their ROSCON registration fee waived/top contributors full expense paid and sponsored to come to ROSCON.", "Motivation of community members to volunteer for or contribute to certain hard-to-sell tasks is something we\u2019re also looking at in the ROSIN Quality Working group sessions (", "). It\u2019s not trivial. Other than financial incentives a lot of things come down to karma or status.", "We do get quite some input from companies that they will, as part of their hiring process, look at someone\u2019s position and contribution to the community. However, this is typically not so much to gauge what sort of nice guy the candidate is, as it is to see what his technical experience is.", "re: sponsored ROSCon: that is a nice idea. That could certainly be a nice way to recognise community contributions without resorting to just handing out money.", " do you have suggestions on how to find questions about", "application design, human-robot interaction, behaviour modelling", "in ROS Answers? I\u2019ve been using ROS for creating interactive behaviors and running HRI experiments last couple years. I\u2019ve been wanting to answer some questions in ROS Answers but had difficulty in getting started. I understand there are tags, \u201cSort By\u201d, etc., that I could use to navigate the answers but I had difficulty in spending time searching for questions that are in my domain.", "Any suggestions would be greatly appreciated and thanks for staring this thread.", "Quite some ROS Answers posts have either:", "This makes it hard to find posts relevant to your topic specifically. I\u2019m afraid I wouldn\u2019t know any real tips that solve your problem unfortunately.", "Personally I\u2019ve found just spending 15 mins quickly reading each new question is enough to get a feeling for whether it\u2019s something you have an affinity with. I also try to retag questions if I feel that\u2019s necessary.", "+1000 for wanting to answer questions btw. You must have experience with quite some packages and infrastructure by now. Don\u2019t limit yourself to HRI ", "sponsored ROSCon: that is a nice idea. That could certainly be a nice way to recognise community contributions without resorting to just handing out money", "Just giving my though on this, It\u2019s a good idea but it will maybe launch a \u201cKarma War\u201d from some people to answers all the questions, with a risk of bad answers, just to get Karma (If the OP of the questions mark as answered, It gives Karma nonetheless.", "In addition to visiting the site periodically you can also sign up to be notified when specific tags are mentioned. So if there\u2019s a topic that you feel that you\u2019re able to help out on add tags associated with that to the \u201cInteresting tags\u201d section in the side bar and make sure you have \u201cEntire forum (tag filtered)\u201d enabled in your \u201cemail alerts\u201d settings.", "In a more \u201caggresive\u201d way, we could also setup a small bot who read tag and associate it with the maintener mail on package.txt and send periodic summary of the number of unanswered new questions about the package (or make a post on a specific Discourse channel).", "I think this is a good time to thank gvdhoorn for his great work on ROS answers. I visit that place from time to time and in perceived most of the questions there is at least a helpful comment from him.", "First off, thanks ", " for bringing this up and for all the energy you put into answering questions lately, I\u2019m sure you\u2019ve helped a bunch of people! I\u2019ve been slacking off in answering questions myself, and I\u2019ll try to spend the 15 minutes each day from now on.", "I believe it\u2019s best to encourage people to answer questions out of intrinsic motivations instead of paying them to do so (because it will lead to sub-standard answers). When I started answering questions 7 years ago, what motivated me was:", "While many are about installation, deployment or integration issues, the ones that make ROS Answers really worth the time are those that ask about advice on configuration of specific packages.", "Would it make sense to let a package maintainer know about those that they can be referenced and (later) added in some FAQ section on the packages wiki page?", "A large number of volunteers from the community help out by answering quite a few of these questions, or at least try to help getting closer to a solution \u2013 and they deserve a lot of recognition and thanks for that.", "I got most answers from you ", " so far. I appreciate your effort very very much! Thx!", "I have no real feeling for this outside ", ", but I wonder how many questions that get asked have been answered before. I know from personal experience that I\u2019ve answered the same non-trivial question on multiple occasions.", "Part of this, I think, comes down to search. For example, I often get asked, \u201cCan I use robot_localization with just a GPS and IMU?\u201d If users search for that on ROS Answers, ", ". None of those initial results are all that relevant to the search. If we search using Google, ", ".", "Obviously, part of this is a failure of the package documentation, too, and ", "\u2019s idea to reference the questions in their package wikis is a good one. I\u2019m just wondering if there\u2019s anything we can do to make previous answers easier to find.", "Regardless, I learned a lot from ROS Answers when I started using ROS, and I think we owe it to new users to give them the same level of help that we received. I\u2019ll try to carve out more time to answer questions, and will encourage my team members to do the same.", "sponsored ROSCon: that is a nice idea. That could certainly be a nice way to recognise community contributions without resorting to just handing out money", "Just giving my though on this, It\u2019s a good idea but it will maybe launch a \u201cKarma War\u201d from some people to answers all the questions, with a risk of bad answers, just to get Karma (If the OP of the questions mark as answered, It gives Karma nonetheless.", "This is something we would need to be careful with, I agree. However, we do still have other users and the moderation system. Answers (and questions) can also be downvoted. The idea of this being of course that it\u2019s a self-balancing system (in the end).", "I\u2019ve been slacking off in answering questions myself, and I\u2019ll try to spend the 15 minutes each day from now on.", "My OP was more a call-to-action, as in \u201cwe can do better\u201d, but if you feel you\u2019ve been \u201cslacking off\u201d then that sounds like a good motivator ", "I believe it\u2019s best to encourage people to answer questions out of intrinsic motivations instead of paying them to do so (because it will lead to sub-standard answers).", "As I wrote in reply to ", "\u2019s comment, I believe this can be mitigated by the self-correcting nature of the system with up- and down-votes.", "I do agree completely that intrinsic motivation is best. For some people money is a good intrinsic motivator though ", " But perhaps altruism might be a good one too.", "When all the easy questions were already gone, I started researching answers to questions that I didn\u2019t immediately know the answer to. After a while, I noticed that this really helped me to gain broad understanding of everything there is in ROS, even though that wasn\u2019t my original intent when answering those questions (I wanted to get karma!). I believe this is really the best way to become a ROS expert.", "I completely agree with this one too. It\u2019s basically what I wrote at the end of my (perhaps too long) OP.", "While many are about installation, deployment or integration issues, the ones that make ROS Answers really worth the time are those that ask about advice on configuration of specific packages.", "Would it make sense to let a package maintainer know about those that they can be referenced and (later) added in some FAQ section on the packages wiki page?", "That is a good idea, and would even not be too difficult to implement. It would just take some time.", "Perhaps the easiest would be \u2013 seeing as not all questions are properly tagged \u2013 for someone to browse ROS Answers and collect questions about a certain package which he could then collate and post on the issue tracker of that package / those packages.", "Rather low-tech, but perhaps even already enough.", "I have no real feeling for this outside ", ", but I wonder how many questions that get asked have been answered before. I know from personal experience that I\u2019ve answered the same non-trivial question on multiple occasions.", "Part of this, I think, comes down to search. For example, I often get asked, \u201cCan I use robot_localization with just a GPS and IMU?\u201d If users search for that on ROS Answers, ", ". None of those initial results are all that relevant to the search. If we search using Google, ", ".", "This is definitely true. The number of duplicates or near-duplicates is just about bearable at the moment.", "The search is rather sub-par at the moment: I\u2019ve even opened an issue about that: ", " (which you ", "ed).", "I ", " use Google: ", ".", ": is this an opportunity to revisit ", "?", "Obviously, part of this is a failure of the package documentation, too, and ", "\u2019s idea to reference the questions in their package wikis is a good one. I\u2019m just wondering if there\u2019s anything we can do to make previous answers easier to find.", "Would this perhaps warrant a post over in the ", " category?", "Regardless, I learned a lot from ROS Answers when I started using ROS, and I think we owe it to new users to give them the same level of help that we received. I\u2019ll try to carve out more time to answer questions, and will encourage my team members to do the same.", "Great, thanks ", ". And thanks for the support for ", " ", ".", "Although I can\u2019t find a reference, there was at least once a coordinated day for burning down unanswered questions wasn\u2019t it? My calendar still marks Septermber 19th is world ", " day (not sure when, not sure if that was the title either).", "I\u2019m glad someone still celebrates ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a title that does not accurately (or even at all) reflect the topic / subject of the post", "a poorly chosen set of tags (the obligatory ROS distro tag has helped a bit here)", "an ", "\n", "Helping people. You really get a good feeling for having helped someone who otherwise wouldn\u2019t have solved their problem, and who perhaps has nobody else to turn to for help.", "Getting karma. It sounds silly to pursue fake internet points, but the gamification aspect definitely worked for me. Getting karma and earning new badges by answering questions feels like leveling up in a game.", "When all the easy questions were already gone, I started researching answers to questions that I didn\u2019t immediately know the answer to. After a while, I noticed that this really helped me to gain broad understanding of everything there is in ROS, even though that wasn\u2019t my original intent when answering those questions (I wanted to get karma!). I believe this is really the best way to become a ROS expert."], "url": "https://discourse.ros.org/t/ros-answers-needs-your-help/5147"}
,{"title": "Which board/microprocessor should I use? I want to use ROS Kinetic on Ubuntu 16.04, along with packages like move_base, laser_scan_matcher for indoor autonomous navigation", "thread_contents": ["I am confused as to which board will be able to handle the necessary computation to run autonomous navigation for my turtlebot-like-robot.", "\nI want to run Ubuntu 16.04 OS and ROS Kinetic, and need packages like move_base, laser_scan_matcher.", "\nI\u2019ve heard that Raspberry Pi 3 and its Arm Cortex A53 isn\u2019t enough for the purpose.", "\nWhat are your views?", "That highly depends on your budget, software requirements and space limitations.", "\nThe packages alone also don\u2019t say  that much about the required computation power.", "\nE.g., what kind of LIDAR are you using (how many data points per second does it produce?), what other kinds of sensor (e.g., cameras) are you using?", "\nFor cameras etc. the USB bandwidth might be a limiting factor that has to be kept in mind.", "Without more information, I can only suggest some generally viable small form factor options depending on your computation power requirements:", "\n", " You could try the Pi 4 which is a lot faster than the Pi 3. The 4GB RAM might not be enough depending on your use case but I assume it should be able to handle autonomous navigation with the kind of LIDAR you would put on a Turtlebot-like robot.", "\n", " A mini-PC such as an Intel NUC would surely be enough to handle almost anything you throw at it.", "what kind of LIDAR are you using (how many data points per second does it produce?)", "Hey ", ", I\u2019m using RPLidar A1, and using it at 5Hz with 2000 data points.", "\nNUC seems too bulky, actually. Looking for something much more compact like Pi.", "\nWould you recommend Beaglebone or Odroid XU4?", "Hi, if you don\u2019t have budget constraint you can try UP board or jetson nano or Beaglebone. Working with ROS , Lidar and some kind of application that is to be done like turtlebot alike of robot if designed.", "I agree with ", " on the UP board if budget is not a constraint.", "\nI wouldn\u2019t recommend a Beaglebone for this use case (I assume you want something simple to work with and develop on) because they have very little RAM (unless there\u2019s a model I missed) and that can lead to frustrating issues for inexperienced developers, e.g., you may not be able to compile directly on the board but will have to cross-compile on your host machine and deploy the binaries manually.", "\nA jetson nano really only makes sense if you also have a camera and want to use graphics accelerated algorithms or deep learning otherwise the Pi 4 is faster IIRC.", "\nHowever, if I were you I would try a Pi 4 since you\u2019re not dealing with a lot of data and if the Pi is not enough, it\u2019s a cheap mistake to make and maybe you can still use it for something else.", "\nAlso, the Pi is widely used and has the highest chance of finding someone who can help if you run into issues.", "\nI don\u2019t know how the Odroid compares to a Pi 4.", "Later on, you can still switch to something that is smaller / more energy efficient but harder to work with.", "Looks like you mised the ", " (Cortex A15, 1GB RAM) and the ", " (Cortex A15, 2GB RAM)", "I have had great luck using the TX2 with the ", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a Q&A site which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/which-board-microprocessor-should-i-use-i-want-to-use-ros-kinetic-on-ubuntu-16-04-along-with-packages-like-move-base-laser-scan-matcher-for-indoor-autonomous-navigation/11488"}
,{"title": "Ros_comm - request for help", "thread_contents": ["If you have created a pull request against the ROS 1 ", " repository in the past you have likely experienced a long wait until you have received feedback - and in several cases your patch might not have received any attention yet. Unfortunately this has become a more frequent problem in the recent past (and I am really sorry for that negative experience). So why is that the case?", "I am the assigned maintainer of almost all packages in the ", " repository. Beside that I am also the designated maintainer of almost all ROS 1 packages which make up the ", " metapackage as well as higher level packages like ", ". ", " is standing out here due to its prominent content but other \u201chigh profile\u201d repositories like ", " are similar in this regard. ", " receives by far the most tickets - issues as well as pull requests. In the early ROS days it was often sufficient to file a ticket describing a problem or feature idea and \u201csomeone\u201d would just go ahead and fix / implement it for you. That has been much less the case in recent years. At least on the pull request side I was able to review contributions in regular intervals (something around every few weeks). Unfortunately not only has that interval continuously increased also the time I am able to spend on actually reproducing problems, testing patches etc. has decreased significantly. That resulted in both: patches got merged which weren\u2019t sufficiently tested and introduced regressions as well as many patches which haven\u2019t been \u201caccepted\u201d until now.", "To understand why that is the case I have to mention a bit about my work time at Open Robotics. As you\u2019ve surely noticed over the past few years, our development focus is on ROS 2. We\u2019re still maintaining ROS 1 on a best-effort basis, but my day-to-day priority is shifting more and more towards ROS 2 development. That priority is the result of both our decision as a company on where to focus our energy and the reality of what type and scope of work can be supported by the industry partners and government agencies that supply the bulk of our funding. As a result any time that I do spend on ROS 1 maintenance necessarily takes away from our current projects. And even without targeted funding, the company decides to continuously invest quite some discretionary resources into ROS 1 maintenance (through each of our employees) - consider the effort it takes us every year to prepare a new ROS distribution - beside the continuous work on issues and pull requests, running the buildfarm, etc.", "Open Robotics doesn\u2019t have the resources available to allocate to these tasks so we can\u2019t expect any additional time to \u201cmagically\u201d become available. Luckily there is already one external ROS developer helping to maintain the ", " repo (Thank you, Mike Purvis / Clearpath Robotics!). Without his efforts the current situation would be much worse. But the trend of me having less and less time as well as the load continuously increasing leads to the current unsatisfying situation as well as a not very positive outlook for the future.", "So what can we do about it? Ultimately we need more people to volunteer to help with the effort of maintaining these packages. These contributions can come in many forms: review pending patches (which includes reproducing the problem, applying the proposed patch, reporting that the problem is fixed, additional testing to check for potential regressions), review incoming issues and provide users with help to fix them, help with other maintainer tasks like creating and testing patch releases, considering patches for backporting into \u201colder\u201d ROS distributions which are still being supported, investigating flaky tests, etc. A good way to start is to browse through the existing tickets and subscribe to the notifications on the repo so you will get emailed when new issues / pull requests are coming in, and some may pique your interest. While any kind of help is highly appreciated we are looking for people which want to help for a longer time period and on a regular base with these tasks. If you would like to \u201ccommit\u201d to that kind of support please let us know and we are happy to get you more actively involved.", "I am interesting at maintaining the old package in ROS1 how to involve the great process ?", "I can volunteer some time to help out with some of these tasks. Plus I always wanted to learn more about ROS packaging and the build farm process.", "I\u2019d like to help support this.", "To understand why that is the case I have to mention a bit about my work time at Open Robotics. As you\u2019ve surely noticed over the past few years, our development focus is on ROS 2.", "You left out the part where OSRF decided to make all of ROS2 independent and incompatible to ROS1, causing a split in the community, and duplicating all maintenance efforts. Don\u2019t you think that this is part of the reason?", "So what can we do about it? Ultimately we need more people to volunteer to help with the effort of maintaining these packages.", "You do not mention the possibility of migrating the ROS1 community as a whole to ROS2. But maybe even OSRF has given up hope of achieving that anytime soon after making the migration path so difficult.", "Open Robotics doesn\u2019t have the resources available to allocate to these tasks so we can\u2019t expect any additional time to \u201cmagically\u201d become available. While any kind of help is highly appreciated we are looking for people which want to help for a longer time period and on a regular base with these tasks.", "You really mean OSRF is looking to hand over ROS1 to some other organisation, like an actual open-source robotics foundation?", "So will OSRF make a presentation at next ROSCon about how ROS1 is dead, given ros_comm is unmaintained? Might go nicely along with other presentations showing new ROS1 packages.", "Hi Dirk,", "I would like to help, but I cannot really justify it to my employers at this point, when we are focusing on ROS2.", "Also, I think QA on ROS1 has always been a problem and I don\u2019t see a good solution.", "So, how about trying the idea of a library shim, either instead of or in addition to adding maintainers to ros_comm?", "I mean a ROS1 API on top of rcl. We could transition everybody much more quickly that way.", "I know this has been talked about before, but not realized so far. Not sure why, but if it\u2019s about the effort, a shim ", " be something I and probably many others could get involved in, because it\u2019s about helping with the future ROS.", "Cheers,", "Ingo", "I like Ingo\u2019s proposal of a ", ". I\u2019d be interested in helping on the Python part.", " ", " ", "Thank you for offering your support. Please see ", " where I tried to give a high level overview how to help. Please feel free to ask questions on that ticket.", " ", "If you would like to contribute towards an API shim that would be great. We have had discussions about the topic in the past about how viable / complicated it would be, what to cover, what to do when the existing ROS 1 API can\u2019t be mapped to ROS 2 etc. I would suggest to start a discussion in the \u201cNext Generation ROS\u201d category. I could also create repositories for this effort (e.g. ", ") if you think that would be helpful to get things rolling?", " lets start a discussion, indeed. You may have a better insight of how much work and how doable it is.", "Any updates? Maintenance still seems dead.", "\nSome people did volunteer in this thread, is there an ongoing process to find/add maintainers?", "Any updates?", "\nSome people did volunteer in this thread", "While there were some comments offering help there was unfortunately no actual help contributed afterwards.", "is there an ongoing process to find/add maintainers?", "I am not sure what you mean with \u201congoing process\u201d? We are still hoping for others to help but have reached out numerous time with no effect and don\u2019t know what else to do from our side.", "I am spending every few months a little bit of time on catching up with pull requests. Currently I am still working through the tickets on lower level repositories before getting to ", ". And with the amount of PRs pending in that repo I doubt that I will have time to review all new ones before I have to drop the ball again for another few months. (I am sorry if that isn\u2019t satisfactory but I want to set clear expectations.)", "Just to clear up a potential misunderstanding: The process isn\u2019t to just add maintainers to ros_comm. Instead, ", " has lined out how you can help here: ", "Any updates?", "\nSome people did volunteer in this thread", "While there were some comments offering help there was unfortunately no actual help contributed afterwards.", "Sorry i just read the three volunteer prompts and missed the follow up over the Ros1/2 shim rant.", "\nMaybe everyone else missed that too ", "I am not sure what you mean with \u201congoing process\u201d? We are still hoping for others to help but have reached out numerous time with no effect and don\u2019t know what else to do from our side.", "The steps from the ticket include reading issues and creating pull request. Maybe people did read tickets and created pull requests: what now? Thumps Up PRs?  I can imagine this is why none of the volunteers did help. I fell somewhat lost after reading the ticket - No Offense, just presenting my own thought process. ", "I am spending every few months a little bit of time on catching up with pull requests. Currently I am still working through the tickets on lower level repositories before getting to ", ". And with the amount of PRs pending in that repo I doubt that I will have time to review all new ones before I have to drop the ball again for another few months. (I am sorry if that isn\u2019t satisfactory but I want to set clear expectations.)", "It feels like too much for a single person - ", ": There should be additional maintainers, right?", "There should be additional maintainers, right?", "Probably yes. However, since ros_comm is the most important repo of ROS1, I hope that there\u2019s a vetting process for new maintainers; like, potential new maintainers should first help answering issues, reproducing and testing PRs etc. without maintainer status (as described in the issue mentioned above). Once they\u2019ve demonstrated that they are diligent and know what they are doing, they could get direct push access to the repo.", "The steps from the ticket include reading issues and creating pull request. Maybe people did read tickets and created pull requests: what now? Thumps Up PRs?", "You might want to read through the instructions of the referenced PR which describe in detail what kind of help we are looking for: ", "And no, just \u201creading issues\u201d isn\u2019t helping much - the goal is to comment on issues with the finding according to the triage process described in the above ticket. Just to copy some of the bullets from there:", "Also \u201cthumbs up PRs\u201d isn\u2019t helping much since it is unclear what the reviewer actually did (beside that nobody gets a notification when you add an emoji to a ticket). Maybe the person just wants the problem to be fixed. Again some bullets on what we are looking for:", "I fell somewhat lost after reading the ticket.", "Then please consider to ask for clarification.", "Some people probably want to help, but also might simply be intimidated/lack confidence (similar to what the MoveIt! maintainers ", ").", "Not sure what could help other than to tell people (myself included) to \u201cjust do it,\u201d since your instructions on ", " definitely provide sort of a starting point.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Check if it is still valid", "If it is a bug try to reproduce the reported problems", "Consider creating a pull request to fix the bug / implement the feature request (always ", " the original issue)", "If you don\u2019t have the permission (yet) to set labels or close obsolete tickets please just comment with the suggestion and someone will follow up on it.", "Check if it is still valid\n", "All pull requests must target the latest development branch which is commonly also the default branch (currently  ", " ), the only exception is if a patch is only applicable to older branches and not necessary on the default branch", "\n", "If the patch already has feedback which hasn\u2019t been addressed by the author consider creating a new PR addressing the pending feedback to make the patch mergable (always referencing the original ticket)", "Review the patch which includes:\n", "Check the diff for problems (unnecessary / unrelated changes, incorrect / incomplete logic, API / ABI breakage, code style matching surrounding code)", "Reproduce the problem without the patch", "Apply the patch", "Reproduce that the problem has been addressed and try to make sure that it doesn\u2019t introduce any regressions", "Ensure that it passes the tests", "\n"], "url": "https://discourse.ros.org/t/ros-comm-request-for-help/5840"}
,{"title": "Announcing tf_remapper_cpp and static_transform_mux: More power to your TF!", "thread_contents": ["Have you ever run a ", " on a complex system with tens of nodes subscribing TF? Have you looked at the ", "? ", " Stop wasting energy and CPU cycles by moving to ", ". The API is a superset of tf_remap, so you won\u2019t need to alter anything but one line in your launch file. As a bonus you get a node that works natively with TF2, supports remapping of /tf_static (which the original tf_remap cannot do correctly), you get the ability to \u201cremove\u201d frames, and the node can also work bidirectionally (listen on both old and new TF topics).", "Have you ever faced weird problems with ", "? Yes, there is a big problem - ", "! It only publishes the last message, but ideally it should publish all so-far-seen static transforms. This is where ", " comes into play. Just launch this node early enough and it will take care of your static transforms. There\u2019s one limitation, though - you still have to replay the bag from the beginning (or just play a few seconds of it and then you can seek further if you keep static_transform_mux running).", "Give it a try and report if you find these packages useful!", "Happy transforming ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcing-tf-remapper-cpp-and-static-transform-mux-more-power-to-your-tf/9219"}
,{"title": "Current status of FreeRTPS?", "thread_contents": ["I\u2019m evaluating the availability of communication between ", " and ", ".", "I tried to make the native-posix version of FreeRTPS communicate with the ROS2 on FastRTPS. But it seems that the native-posix sample(e.g. talker_no_rosidl) won\u2019t be able to communicate with the ROS2(e.g. listener_best_effort).", "Is there anyone who can make FreeRTPS communicate with ROS2 or knows which features to be developed?", "\nIt seems that the talker_no_rosidl is able to add its messages to the internal history of FastRTPS. But the upper layers won\u2019t retrieve the messages.", "Greetings! Thanks for your interest in the project. Development of FreeRTPS has been paused for a while. Current development efforts are using FastRTPS and RTI Connext.", "At some point it would be to resume development of a microcontroller-friendly middleware option, but at the moment we (OSRF) are not spending time on it. Of course, this is all open-source, so if you have time and energy to spend on FreeRTPS, it\u2019s certainly there for the taking. It would need a significant amount of effort, though. Cheers!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/current-status-of-freertps/1396"}
,{"title": "Research about the battery in robots", "thread_contents": ["Dear community,", "We are researching on how to make robots programming more efficient and of a better quality. If you are into robot programming, specially drones, please help us by filling this survey. It takes no more than 3 minutes.", "Kind regards", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/research-about-the-battery-in-robots/5566"}
,{"title": "Looking for Kobuki power supply / battery charger", "thread_contents": ["I have a Turtlebot2 / Kobuki base but no charger for it. I\u2019ve searched online and can\u2019t seem to find any for sale anywhere. If anyone has a link to where I might purchase one, I\u2019d appreciate it.", "Thanks.", "Hi Matt,", "I looked at the spec sheet and you need an Input: 100-240V AC, 50/60Hz, 1.5A max; Output: 19V DC, 3.16A power supply.", "Here\u2019s an example on Amazon: ", "As for the barrel connector, I measured mine at 5mm. The one linked above is 5.5mm, which I assume would also probably work with a little tough love.", "S", "Thanks ", ", I\u2019ll give that one a try.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looking-for-kobuki-power-supply-battery-charger/8754"}
,{"title": "Regional Robotics Sailing Winter Academy at Zhejiang University", "thread_contents": ["Sailing robots are autonomous surface vehicles (ASV) that use wind power as the only propulsion source. The robot can sailing on the sea on a much longer duration compared to the battery-powered counterpart and potentially could serve as a platform for the long-term ocean observation. In order to help teams get start with robotics sailing, we are organising a two days winter academy for students and professionals.", "\nThe lecture will be given by Yu Cao (Southampton Sailing Robot Team) and Prof. Chao Xu (Zhejiang University).", "\nDec.15 and 16, 2018 @ Zhejiang University, China // Free of charge", "\nRegistration can be made online via ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Introduction on robotics sailing \u2013 history and the challenges", "The physics on sailing craft", "Hardware and software structure of an autonomous sailing robot", "Software tutorials on the basics of ROS, python and git version control", "Sailing robot path planning and lower level control", "Sailing demonstration given by ZMART team (subject to weather condition)"], "url": "https://discourse.ros.org/t/regional-robotics-sailing-winter-academy-at-zhejiang-university/6980"}
,{"title": "A do-it-yourself Turtlebot", "thread_contents": ["I am teaching a class to Computer Science students (clever software but not hardware types). I thought it would be a good experience to actually build or own turtlebot-like robot, not literally from a kit, but from a pre-acquired set of parts that I knew would go together. It would give them a more visceral experience of building. Our goal is to do mobile navigation with ROS, so we are also using off-the-shelf Robotis Turtlebot3 but they are very small and a \u201creal\u201d Turtlebot2 costs like $2K. Any links or suggestions from those of you who have traveled the same road?", "(Not sure if this question should\u2019ve been on ", ".)", "Hi pitosalas, while not exactly the same specs as Turtlebot, I am working on a ROS + OpenCV Raspberry Pi based robot kit called ROSbots - ", ".", "It is a differential drive robot that has a camera, wheel encoders, and a motor driver, and off course runs ROS and OpenCV pre-installed.", "Would that fit your needs?", "Jack", "Hi Jack", "Yes that\u2019s along the lines but I am looking for something a bit bigger and with a more powerful main computer. Do you have any other leads? Thanks!", "Pito Salas", "\nBrandeis Computer Science", "\nVolen 134", "If you don\u2019t want to build the TurtleBot2 from a kit, the full designs are available to build it from scratch at: ", " Many people have done that. And while you\u2019re at it you can modify the design for your usecase or application.", "Hi Pito,", "I apologize if you had already thought of this but if you can tolerate the network latency and your robot isn\u2019t too sensitive to slightly \u201clate\u201d sensor data (but still accurately timestamped), you can always have a more powerful computer (ie laptop or desktop) run the more computationally heavy ROS nodes as secondary processors - no need to have these processors actually on the robot (which just sucks up more power and requires heavier duty motors). Just another architectural thought to help your students keep cost down while still being able to build their own full fledged ROS robot.", "Happy to continue to hear your thoughts, and \u201cdiscourse\u201d (harhar!).", "Cheers", "\nJack", "The TB2 \u201cfull designs\u201d look quite intimidating. Did you say that the components are available to buy or is the idea that I literally 3d print the parts as specced out there?", "Hi Jack. Yes I was aware of that option but I also am aiming (maybe mistakenly) for building up the experience and skills to allow us to make a robot that will be able to carry a small load (1kg say) and eventually go outdoors. I know that\u2019s a lot. But that\u2019s why I am looking for something with a base the size of an iRobot create or something else\u2026 Thoughts?", "Hi Pito,", "You can check out ", " . It\u2019s not exactly a Turtlebot but it offers a \u201cblueprint\u201d for building a variety of DIY ROS compatible robots (2WD, 4WD, Mecanum Drive, and Car-Like steering) . The platforms are computer agnostic so you can choose a much capable board.", "Hope this helps. Cheers!", "You can buy individual parts for the Turtlebot 2 from Dabit Industries for around $800/kit plus the cost of a computer", "For a cheaper cost, you can grab a iRobot Create2 base ($200), iRobot Create2 Mounting Kit ($125), Orbbec Astra ($150), and some computer.", "\n", "**The Kinect and iRobot Create2 are NOT included in this kit \u00a0 This is a kit meant to screw directly into the iRobot Create2 Boss locations, and provide an adaptable and scalable base for all sorts of fun projects! \u00a0 Included in this kit: 1 x Middle...", "\n    ", "\n", "As for computers:", "For SBCs, you can take a look at:", "\nRaspberry Pi3: ", "\nASUS Tinker Board: ", "\nODROID-C2: ", "As for laptops, you can check out the following recommended Lenovo laptops:", "\nrefurbished x240 at $419:", "\n", "\n  \n  \n  \n  \n  ", "\n", "\n", "\nLenovo 11e at $369: ", "Hi Pito, you got some great suggestions from the community. Last but not least, while not exactly a kit but definitely a platform you can build on (capable of huge payload) - check out the Magni platform from the awesome folks at Ubiquity Robotics -", "\n", "Jack", "Besides the Ubiquity platform, you can download both their RPi sd card or Virtual Box images. The small Loki platform may become available this Spring if crowd funding is successful.  Also check out the SV-ROS github for how to convert a Neato Botvac to a Turtlebot like platform. I published two articles in Servo Magazine entitled Roll your own Turtlebot. So I think it is easiest to go with an existing platform. Putting together a platform from scratch, with motors, encoders and controllers from scratch just takes too long.", "Some of the autonomous racing folks are building around a rc car platform, but I don\u2019t think that\u2019s good for education. I believe Cousera used the Neato in their course on intro to autonomous robots.", "Disclaimer, I coordinate SV-ROS, and am a member of Ubiquity Robotics", "Good luck. The only way to get better at robotics is to build better robots.", " - do you happen to have pdfs of your two articles? Reading them online in servo is hard ", " Thanks!", "I can send you the botvac article plus youtube videos of  interest.  I have to look for the original Roll Your Own Turtlebot article. (about 3 laptops ago!)   Sending raw odt:", "Roll Your Own Turtlebot Part II", "By Alan N. Federman (Dr. Bot)", "Several years ago I wrote a \u201cServo\u201d article showing how you could convert an old Roomba vacuum cleaner and a Microsoft Kinect into a robot training platform capable of teaching yourself  ROS, the Robot Operating System from Willow Garage.  Willow Garage has now closed, and Clearpath Robotics can sell you a complete brand new Turtlebot for about $2100. Recently OSRF and Robotis has announced a less expensive Turtlebot, but this is still a bit out of the range for most serious amateurs  What if you could easily build the equivalent of a Turtlebot for under $300?  I am going to show you how easy it is to convert a Neato robotic vacuum cleaner into a fully functional training platform in less than a day.  Very little hardware skill or special tools are needed. Everything is available COTS, and the software is all Open Source.", "What you need to get, if you don\u2019t already have them:", "A laptop or wifi connected desktop running Unbuntu. This should be at least 14.04 and running ROS Indigo, but it would be better to upgrade to the same versions if running cross platform. ROS versions are usually matched to Ubuntu releases.", "\nA Neato Botvac or equivalent (I have seen used XV-12s for under $200, and new basic models for under $300)", "\nA Raspberry Pi 3 (camera is optional but highly recommended) ~$50", "\n16 Gig Sd Card for Pi ~$10", "\nRechargeable 5v power pack (Those for recharging cell phone are fine) ~$20", "\nUSB cables for battery pack to Pi (micro) and Pi to Botvac (can be mini or micro depending)", "\nSmall scraps of aluminum or a tin from can.", "\nSmall scraps of flat plywood or acrylic", "\nVelcro, double sided tape or other easy to remove adhesives", "Step 1 Modifying the Botvac", "Depending on your model, you may chose to ignore any hardware modifications entirely. Then if you mess up, you can just use it to cleanup your house!  I removed the brushes, the dust bin and used a stip of metal to disable the bin detector switch (See Photo 1)", "STEP 2 Preparing the Pi and attaching to Botvac", "Artfully arrange the Pi, battery pack and optional camera on a 6\u201d by 6\u201d flat piece of wood or plastic. Attach with double sided tape. On the bottom of the assembly, attach a piece of Velcro or similar quick release fastener. Attach the matching Velcro to the top of the Botvacs Lidar unit. Lastly plug in the USB cables.  You might want to charge your batteries.  It would be a shame to have all the software loaded and than have to wait to test it.", "STEP 3 Loading the software onto the PI.", "At the time of this writing, an official version of Ubuntu 16.04 was not available fro the Pi 3. I used the Ubuntu Mate (pronounced \u201cma tay\u201d) version. Instructions for loading Mate are found here:", "Download a copy of Ubuntu MATE", "And follow the instructions for 16.04 \u2013 Raspberry PI 2/3.", "You can use an HDMI TV and attached keyboard to initially set up the Pi, using the Graphical Environment.  It also helps to have a direct Ethernet connection when doing the initial set up, because you need to load a lot of software initially. Using the Desktop, it is pretty easy to get WiFi working.", "I suggest creating an 8 gig image on a 16gig SD card. After the initial software is on and you can bring up a graphical desktop, follow the intro screen and click on Raspberry PI info \u2013 it will enable you to expand the image to 16gig. It also will allow you to configure your WiFi.  I suggest you still use the Ethernet Connection, but you can at this point open a terminal, type sudo graphical disable, and then use ssh over WiFi to complete the installation.", "Once Ubuntu is working, continue loading ROS onto the PI,", "(", "\nYou may have to maintain your Ubuntu distributions; the following commands are useful:", "sudo apt-get update", "\nsudo apt-get upgrade  (must run both in sequence)", "and sometimes to clear dpkg errors:", "sudo dpkg \u2013configure -a", ")", "Summary:", "sudo sh -c \u2018echo \u201cdeb ", " $(lsb_release -sc) main\u201d > /etc/apt/sources.list.d/ros-latest.list\u2019", "sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116", "sudo apt-get update", "\nsudo apt-get install ros-kinetic-desktop-full", "(if -full is not available, just get ros-kinetic-desktop)", "sudo rosdep init", "\nrosdep update", "\necho \u201csource /opt/ros/kinetic/setup.bash\u201d >> ~/.bashrc", "\nsource ~/.bashrc", "\nsudo apt-get install python-rosinstall", "ROS Catkin Workspace installation", "mkdir -p ~/catkin_ws/src", "\ncd ~/catkin_ws/src", "\ncatkin_init_workspace", "\ncd \u2026", "\ncatkin_make", "And then edit .bashrc to change the source /opt/ros/kinetic/setup.bash to ~/catkin_ws/devel/setup.bash", "\nalso it helps to add the following line if the Pi is hosting the robot:", "export ROS_MASTER_URI=http://$HOSTNAME.local:11311", "Where \u201c$HOSTNAME\u201d is the name you have in the /etc/hostname file", "Your /etc/hosts file should look like this:", "127.0.0.1\tlocalhost", "\n127.0.1.1\t\u201cyour hostname\u201d", "::1     ip6-localhost ip6-loopback", "\nfe00::0 ip6-localnet", "\nff00::0 ip6-mcastprefix", "\nff02::1 ip6-allnodes", "\nff02::2 ip6-allrouters", "This will support ROS networking.", "You also may wish to install \u201cchrony\u201d to synchronize the time different \u2018nodes\u2019 are running at, this is because the Pi doesn\u2019t have a real time clock, and if your Wifi is not connected to the Internet, the Pi will have the wrong time.", "Next I suggest you load the following ROS packages into your catkin_ws/src workspace:", "ROS by Example part one (RBX1) from Patrick Goebel", "     This should go on both your laptop and the PI", "and", "the SV-ROS Botvac nodes courtesy of mostly Mr.  Ralph Gnauck", "Repository of packages and info for the SV-ROS Intro To ROS training series - SV-ROS/intro_to_ros", "follow the instructions in the README files to install and test.", "Example", "cd ~/catkin_ws/src", "git clone ", "  (this also should go on both)", "cd \u2026", "catkin_make", "TESTING", "On the laptop:", "roscd teleop", "If nothing is found,", "sudo apt-get install ros-kinetic-teleop-twist-keyboard", "On the Botvac, turn on the pi, and sign on via a terminal window from the laptop.", "I like to launch a custom base only node on the Pi", "roslaunch bv80bot_node bv80bot_njoy.launch (code included at the end of this article)", "Then you should hear the Neato Lidar unit start to spin.", "On the Laptop open up another terminal window and", "set up the ROS_IP  and ROS_MASTER_URI environment variables via the \u2018export\u2019 command.", "Test to see if you are getting topics:", "rostopic list", "and scans", "rostopic echo /scan", "Finally launch teleop", "rosrun teleop_twist_keyboard teleop_twist_keyboard.py", "You should be able to drive your robot.", "If you open RVIZ in another window, you should see the LIDAR returns.", "What Next?", "With just this simple robot, you can begin to learn how to accomplish advanced robotics tasks and begin to learn the subtleties of autonomous navigation. Because of the Neato\u2019s XV-11 LIDAR unit, you can simultaneously accomplish localization and obstacle avoidance. Support for webcams and the Raspberry Pi Camera are available through ROS nodes. I have gotten teleop via a blue tooth joystick to work through the laptop, but not directly on the Rpi.  Please note that though the Rpi has 4 USB slots, there is seldom enough power to run more than the Botvac interface and a WiFi dongle. A typical USB webcam will draw too much current and crash the Rpi.", "With a WiFi connected phone and some ingenuity, you would be able to issue voice commands. So you can call up your home robot from the office and ask it to find the cat. The Botvac is a little underpowered for bringing you a snack from the kitchen, but when the next more powerful platform is available, you\u2019ll know just how to program it.", "Figures and Code", "LISTING 1  /catkin_ws/src/intro_to_ros/bv80bot/bv80bot_node/launch/include/bv80bot_njoy.launch", "LISTING 2  Terminal output from launching startup nodes:", "roslaunch bv80bot_node bv80bot_njoy.launch &", "rostopic list", "/button", "\n/cmd_vel", "\n/cmd_vel_mux/active", "\n/cmd_vel_mux/parameter_descriptions", "\n/cmd_vel_mux/parameter_updates", "\n/joint_states", "\n/mobile_base_nodelet_manager/bond", "\n/odom", "\n/raw_cmd_vel", "\n/robot_cmd_vel", "\n/rosout", "\n/rosout_agg", "\n/scan", "\n/sensor", "\n/smoothed_cmd_vel", "\n/teleop_velocity_smoother/parameter_descriptions", "\n/teleop_velocity_smoother/parameter_updates", "\n/tf", "\n/tf_static", "Figures:", "   Fig 1  XV-12 dustbin removed", "  Fig 2  XV-12 Name Plate", "    Fig 3  XV-12  Brush Removed", "   Fig 4 RPi 3 mounted", "Youtubes of Turtlebot I from create base and voice control ~2013-2014? :", "Other Youtubes of interest:", "   Botvac 2015", "  Magni 2016", "  Loki 2015", "Very interesting and thank for your information. I am interested in building a robot that can carry a larger weight (>100kg). But I am lost when Raspberry Pi control the motor because, in comparison with Turtlebot, this robot will have a more powerfull motor. In your article it does not appear any other MCU, so my question is how is the motor controled?", "\nThank you.", "The Botvac has its own microprocessor, motor drivers, etc. The Pi connects via a USB cable. There is no interface required. This was not the case on the original Turtlebot 1 which required a USB to serial TTL converter.", "I am going to make your project and i am a beginner in ROS can you provide more details to build this project", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-do-it-yourself-turtlebot/3978"}
,{"title": "Turtlebot3 - Joule stalled", "thread_contents": ["Hi", "Turtlebot 3 - Waffle \u2026 Joule - after installing Joule following the official documentation (Ubuntu 16.04 LTS) : everything works fine\u2026problem: Joule stops working/running after approx 10min up - independently if operated on 12V/3A external power supply or with the battery provided with the Turtlebo3 kit. Logs don\u2019t show any weird symptoms\u2026", "Any ideas? pls help\u2026", "Tnx in advance", "\nml", "First thing to check, what is the temperature doing? Is the module overheating?", "Also, does anyone know what the replacement for the Joule is, now that they are EOL?", "\nThanks ", "Tnx Ilia for the hint\u2026checking continously temp via /sys/class/thermal zones 0-7 - max. 45C \u2026 assuming that isn\u00b4t the root cause. Keep on digging\u2026Best ml", "\u2026quick update: if you push the Power button (SW2) the \u201cstalled\u201d system recovers - even the Unix sessions\u2026", "\n\u2026did I miss something here - kind of \u201csleep mode\u201d /   hibernation mode - some BIOS settings?", "Interesting.  Our bot was doing the same thing, with symptoms just like a software crash, so that\u2019s what I assumed.  Maybe it is going into a sleep mode of some sort.", "I experience the same stalling that you describe.  Have you managed to figure out a workaround?", "Hey ashoed,", "unfortunately not - tried all 3 BIOS versions, including the recommended ", " \u2026 same negative results.", "Any luck on your side?", "Best ml", "Hi Michael,", "Have you checked to see if there are settings within Ubuntu 16.04 for screensaver or power savings?", "You should also check if you can update to the latest Linux kernel.", "If you can type  ", " in the terminal on your Intel Joule and post the output, I may be able to load up my Intel Joule with your versions and check to see if I encounter the same issue.", "Please keep us updated on any other efforts you may try.", "Hi MyNameIsCosmo,", "tnx for your support\u2026some progress but not 100% done yet:", "all updates based on \u201capt-get update\u201d installed", "HDMI not working - need to work with cli", "uname -a:", "\nLinux turtle 4.4.0-1000-joule #0+joule21-Ubuntu SMP PREEMPT Thu Mar 16 14:46:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux", "following your hint \u201cpower saving\u201d -> focussing on Ubuntu not Joule I did the following:", "sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target", "-> unix processes will not get stopped/suspended anymore -> seems ok !", "\n-> issue: every 30min the network manager gets a request to \u201csleep\u201d (syslog) -> wifi connection goes down", "\n-> can\u2019t restart it automatically (service network-manager restart -  nmcli network on - ifup -a)", "No clue what causes the request for network manager to sleep\u2026given that it is reproducible it seems a \u201cfeature\u201d. Any idea how to tackle it? Seems I didn\u2019t disable all \u201cpower saving\u201d options (or what soever causes the nm shutdown\u2026) - can\u2019t find anything on the net that fixed it\u2026", "Every hint/comment welcome\u2026", "SOLVED: guess the \u201cGUI\u201d environment triggered the request to sleep after 30min - given that I run the Joule headless: disabling \u201cGnome\u201d and \u201clightdm\u201d did the trick - system up and running for hours (didn\u2019t reverse the masking of sleep, suspend, hibernate, hybrid\u2026)", "\nThank to all of you for the hints - Best Michael", "Had the same issue, disabling gnome and lightdm did the trick! Thanks for sharing!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/turtlebot3-joule-stalled/2739"}
,{"title": "Using NUC with Kobuki", "thread_contents": ["Hi,", "It seems to becoming harder to find netbook size laptops to use with the Kobuki base and so I\u2019m looking into using the Intel NUC (e.g. NUC6i5SYH). I want fully autonomous operation where the Kobuki can dock and charge its batteries and the PC, so I wondered if anyone was aware of a power solution e.g. external battery that can connect to the Kobuki 19v output AND will allow the NUC to get a reading of the battery level e.g. via USB. Alternatively, is it possible to use a dc-dc converter with one of the other Kobuki power outputs (e.g. 12v 5A) to power the NUC?", "Or if anyone knows of a good choice of small laptop to use then that might be preferable.", "Thanks,", "\nLee.", "Hi,", "Have a look at the ", ", it\u2019s a DC-DC converter as well as UPS (6-30V input, 6-24V output). It supports multiple battery chemistries (e.g. Li-Po, SLA) and has an adjustable DC output.", "It has a USB interface, and Linux drivers, supporting ", ".", "I\u2019ve been using this device on my robot, I wrote a very simple Python ROS interface for this, which just publishes the charging state and individual Li-Po cell voltages. I\u2019ll upload this script and link it on this thread when I get the chance (I\u2019m currently away from my robot).", "Alex", "Thanks for the response. After posting I did end up looking at that and it looks like a good solution. Good to know that you have it up and running and that the Linux drivers work etc.", "I\u2019m thinking of using 6 of these batteries:", "\n", "7926", "\n", "\nwhich based on my very quick calculations should drive a NUC for about 6 hours (assuming NUC consumes 15W)?", "I asked the OpenUPS company if they could recommend any enclosures - did you find one that works well?", "Hi,", "we have a NUCi7 on our Turtlebot. We ordered an i5 but got an i7 due to", "\nsome issue while ordering. The i5 had a notebook powerbank with it which", "\nis not powerful enough for an i7 (it lasts about 30 minutes). Thus we", "\nare now testing a custom LiIon battery. It seems to be able to run the", "\ni7 for about 4-5 hours now but we are still working on a battery status", "\nintegration. Additionally, we need about 4-6A for charging the robot", "\nnow. From the internal wiring, this should not be a problem, but the", "\nwall charger only gives us 3.2A. So we also still need to get a powerful", "\nwall charger (right now using adjustable lab power supplies).", "Once the battery runs well, I can send you our setup. The OpenUPS looks", "\nvery nice as well, especially the already integrated battery status and", "\ncharging state. If you deploy it to your Turtlebot, could you give", "\nfeedback about it?", "Best,", "\nLasse", "Hi!", "We are also using Turtlebot (version 1) with an i7 NUC which actually requires 19V / 65 W. So far, we didn\u2019t use any DC/DC converters or something similar, but instead, we powered the NUC directly from the 12V / 5A output port on the Kobuki.", "This is certainly not ideal, and we experienced the NUC shutting down because of a \u201cprocessor thermal trip\u201d after some time (sometimes only 30 mins when the Kobuki battery would still last for long). First we thought it\u2019s a problem with the NUC (old thermal paste or something), but later we realized that the output voltage from the Kobuki is monotonically decreasing and will be too low at some point. This seems to have caused the overheating problems then.", "Either ", " or ", " seem like good solutions for this problem, but I\u2019m not very experienced with DC-DC converters or UPS and I\u2019m glad for any advice.", "Can anyone tell me if this would be a good buy for more reliable mobile NUC operation?", "We had a similar problem and solved it by purchasing a power bank. It charges from the kobuki base, provides 19V to the NUC and does not kill the Kobuki battery. We use one of the 19V connectors on the back of the Kobuki and make an adapter to charge the powerbank when the robot is on the charger.", "This is the one we used: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-nuc-with-kobuki/489"}
,{"title": "ARM TechCon: Best Contribution to an Open-Source Software Project", "thread_contents": ["Hello, everyone. ", "I\u2019m pleased to announce the news that we got finalists of the Arm TechCon 2017 Innovation Awards.", "\nI think this is an achievement and good news for our ROS community.", "TurtleBot3, in cooperation with OpenRobotics and ROBOTIS, was awarded ", " at \u2018ARM TechCon2017\u2019 in Santa Clara, California from October 24 to 26.", "\n", "\n", "OpenCR in TurtleBot3 is powered by an Arm Cortex-M7 and be able to connect the servos and battery. TurtleBot3 take advantage of Raspberry Pi\u2019s full capabilities as a central computing system for the robot platform. The hardware, firmware and software of TurtleBot3 are open-source allowing users to download, modify and share source codes. All components of TurtleBot3\u2019s 3D CAD data are available for 3D printing.", "\n", "\n", "\n", "Although we missed the final prize, we are proud to announce that TurtleBot3, which was only five months, was finally nominated for an innovation in the open source software project at an international event. We are also proud to be recognized as a platform by many people who want to learn the ROS.", "We are especially grateful to have a great opportunity and wonderful project with Tully, Morgan, and Brian. I want to make a lot of fun things for the ROS community in the future. ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/arm-techcon-best-contribution-to-an-open-source-software-project/3129"}
,{"title": "[TB3] The Turtlebot3 Teleoperation Example", "thread_contents": ["Hello ", "The Turtlebot3 would be teleoperated by various devices.", "\nWe tested it using several wireless devices e.g. PS3, XBOX360, ROBOTIS RC100, etc.", "\nThis example is operated by ROS on Ubuntu mate 16.04 with Raspberry Pi 3 (except that it tested by LEAP Motion) and OpenCR which controlls Dynamixel XM-430.", "See you soon with next video ", "Darby Lim", "I would like to use my upcoming Waffle as a part-time telepresence robot, controlled over the internet. This is specifically so that a housebound friend of mine can go to a science fiction convention. This couldn\u2019t just be a standard remote-controlled robot-with-a-camera and face. Science fiction cons are usually extremely busy and even real people have problems not running into other people, especially in the dealer\u2019s rooms.", "The Waffle would be modified with a few more avoidance sensors and a tall head. This should make it approximately 3.5-4 feet tall.", "I am assuming that the avoidance routines would have to be done on-robot and that my friend would be able to \u201csteer\u201d the Waffle with a goal and the Waffle would choose how to implement this goal.", "I\u2019d use a larger lithium battery so that it could last at least an afternoon or day.", "It would go along with me (or I along with it) so that it wouldn\u2019t get \u201clost.\u201d", "I\u2019m also assuming that I might have to run the Internet off of a cell phone rather than wifi so that it might have lower bandwidth and higher latency than normal.", "Has anybody made a telepresence robot specifically for these conditions?", "Hi ", "Thank you for your interest in TurtleBot3", "\nYour project is really nice! We expect a lot of people like you to transform TurtleBot3 into the robot they want.", "So We have already make some friends of TurtleBot3. I think that your robot is simillar as ", "This friend are help your project ", "Thanks.", "Thank you for the suggestion.", "I\u2019ve looked at Carrier and it doesn\u2019t seem to have the sensors that I need. It needs the capability of traveling on uneven floors as well as perhaps some grassy areas.", "Maybe a combination of Carrier and Monster. It must be extremely stable because I\u2019m sure it will be bumped a lot in a crowd. Think of any kind of crowded situation where there are small tables and thin maze-like passageways. That is an SF con\u2019s dealer rooms.", "And to make human recognition more difficult, many of the people will be in costume and there will be other robots there.", "Part of the reason for the head is that I can put additional sensors there to locate things that a floor-level sensor would miss. The other reason is so that people will think of this device as a person. Another reason is so that the human on the other end of the link can see through the camera on the head.", "I might need to plan on making the head a variable distance high. For portability, it might need to be short, but for telepresence, it might need to be tall. I\u2019ll have to see if the camera I\u2019ve ordered can see in a wide enough field of vision or I might need to make the head more complex than I\u2019d like for now.", "Just thinking out loud.", "Hi Routiful", "I wanted to buy the waffle but it is out of stock.", "I would like to build the \u201ccarrier\u201d do you have the complete partlist\u2026 the list i found here doesnt seem complete ", "And do you have the instruction for building the carrier?", "Cheers", "\nian", "Hello ", "  ", "Thank you for your inquiry.", "\nTB3 waffle was sold out last month. ", "\nBut you can buy it next year!!", "We don\u2019t officially support instruction for building series of TB3 Friends. However, you can refer to ", " in OnShape.", "Thanks", "\nDarby", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Related packages and applications", "\nros-kinetic-teleop-twist-joy", "\nros-kinetic-teleop-twist-joy-drivers", "\nros-kinetic-joy", "\nros-kinetic-rosserial-python", "\nros-kinetic-teleop-twist-keyboard", "\nros-kinetic-wiimote", "\nros-kinetic-leap-motion", "\nturtlebot3_core (developed)", "\nlearning_wiimote (modified)", "\nrosleapmotion (modified)", "\nOpenCR Arduino", "\nAndroid App (ROS Teleop)"], "url": "https://discourse.ros.org/t/tb3-the-turtlebot3-teleoperation-example/865"}
,{"title": "SBS (Smart Battery System) SMBus over USB?", "thread_contents": ["Hi all, this isn\u2019t ", " ROS-related, but I\u2019m trying to integrate a SBS laptop battery into a ROS-based robot I\u2019m building, and it\u2019s been a difficult road. Wondering if anybody here has had any experience with this and could share their perspective.", "I\u2019ve had minor amounts of luck with a few approaches", "I\u2019m hoping to discover a fairly \u201cout of the box\u201d solution to reliably retrieve data from the SMBus. The built-in state of charge measurement, and other metrics, is really attractive from these smart batteries. However, I don\u2019t have a lot of experience implementing these protocols on microcontrollers, so it\u2019s fairly daunting to try and get it working reliably.", "Once the communication is working, I\u2019m looking forward to making a node to talk to the SMBus communication layer and publish BatteryState messages", "For the curious I am currently using the following battery and charger from RRC - they\u2019re a bit pricey but seem really nice", "\n", "4S1P Standard-Batteriepack RRC2054 mit 14.40V/3.45Ah/49.70Wh. H\u00f6chste Performance, weltweit zugelassen, direkt verf\u00fcgbar. Entwicklungskosten & Zeit sparen!", "\n", "\n", "RRC-PMM240 Lademanagement-Modul als integrierte L\u00f6sung zum Laden unserer Standard-Batteriepacks. Mit 240W max. Ausgangsleistung & 82W max. Ladeleistung.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Arduino using ", ", which let me speak SMBus (Wire library doesn\u2019t format communication correctly for smbus) - but the library seems to not handle the multi-master situation well and freezes up the communications quickly", "I\u2019ve also tried this little project ", " which also seems to freeze up on me"], "url": "https://discourse.ros.org/t/sbs-smart-battery-system-smbus-over-usb/12836"}
,{"title": "What do you want to see in an educational ROS platform?", "thread_contents": ["We\u2019ve started developing a table top / ground ROS robot for use in educational environments. We\u2019d like to hear from educators that really want to teach ROS based robotics, but don\u2019t feel they can because currently available robots are either too expensive, too complicated or too [insert your adjective here].", "We\u2019d also like to hear from educators that are teaching ROS robotics but want to broaden what they do, or find alternative platforms.", "If you are an educator who offers courses and would like to do so with ROS can you respond to this thread with the one thing that you\u2019d like to see in a educational platform for ROS. If enough people express interest we will do a couple of group calls so we can hear what people would like and make trade-off decisions. Then we will do our best to build what the group wants with open source repos to go along with it!", "Please respond with the one thing you\u2019d like to see in an ROS robot platform oriented for education.", "David", "P.S. Thanks to Tully and Kat for suggesting this!", "I\u2019m not using ROS in education yet but I\u2019ve been looking for a platform like this a while back. I was looking at this from a hobbyst perspective, so a low price was a must (Turtlebot looks good but it\u2019s a bit too expensive if someone wanted to buy it to get into ROS).", "Some of the platforms I looked at didn\u2019t have wheel encoders but I think it\u2019s a very good to have if you are teaching about odometry.", "Here are some things I was looking for in the robot platform:", "Functionality wise I wanted to be able to build a full ROS stack on the platform, starting with the drivers, through ros_control and ending with navigation and localization. I was hoping that with a platform like this I could guide students through all levels of software design for a robot.", "Sorry it\u2019s not a single thing but I thought some of these observations could be useful for you.", "Perfect educational resource to my mind is github repo with whole bunch of small working examples. Each of them compiles, runs, has reach comments and illustrates particular piece of ROS functionality.", "\nE.g.", "\nexample 1: how to use tf2", "\nexample 2: how to use can_bridge", "\nexample 3: how to use pointcloud and access individual points", "\nexample 4: you name it\u2026", "Awesome list! I agree price is the key thing that is missing here.", "How about if we designed it so that you\u2019d have ROS topics shared between a workstation computer and the robot itself and the heavy lifting was done on the workstation? Do you think that would be workable? This is one strategy to keep costs down for the robot - it also is a great example of the power of ROS.", "This reminds me a bit of the vector_ros: ", ". I can see a value in that for learning some high level concepts.", "The reason I was initially looking at low level concepts is because I think there is a niche in this area. At the time I was looking into it I couldn\u2019t find any tutorials on how to create a robot from scratch with ROS (I remember it took me quite a while to figure out how ros_control works).", "If you were to share the topics with the user\u2019s workstation would a user be able to add their own sensors on the robot? Because to me that is the single best thing about ROS and projects like vector_ros are very interesting but because you are not able to extend the robot the educational value might be a bit limited.", "That is definitely possible and workable, given decent WiFi. ", " is nontrivial I guess for complete noobs, but once that\u2019s taken care of, running code from a workstation removes the need to sync code from workstation to the robot.", "Running hardware interfaces of course needs to happen on the robot.", "Two places too look at are the SV-ROS github, which uses a Neato Botvac as a ROS platform, and the ROS by Example books by Patrick Goebel, available from Lulu. It is possible to build a ROS mock Turtlebot using a Raspberry Pi, and a Botvac that has a \u201cLidar\u201d for $300.", "In my experience with new comers (or in some cases even research labs that have been using ROS for years) is that network issues tend to cause them a lot of grief. I think the idea to have some of the logic run on the workstation is good but that would require the existence of very thorough noob friendly tutorials about how to get your network just right.", "Yes network provisioning is usually messy.", "We made it significantly less messy at Ubiquity Robotics by building PiFi. On bootup it scans all the available networks then boots in to AP mode with a unique network name (something like ubiquityrobotXXXX) where XXXX is the last 4 digits of the MAC address.", "You can connect to the robot via AP mode then when you do it presents a list of available networks and you can elect to connect to one of those or you can just stay in AP mode.", "It works well, although educational institutions some times have problems with new WiFi networks and also don\u2019t always make it easy to connect to the available infrastructure networks. Our solution is pretty slick and I can\u2019t think of a better one - but I am all ears for suggestions.", "I have used turtlebot3 for a while and for what it\u2019s intended for (as an introduction to ROS), it doesn\u2019t really justify the price of it.", "In the case of turtlebot3, the heavy lifting such as SLAM, map navigation is done on a workstation. But, I found this rather limiting.", "Ideally the onboard computer can function as a WiFi hotspot, possibly with a 4G/LTE dongle, so it can connect to internet if it needs to. In this case any computer could connect to such network. This will minimize network infrastructure.", "With waffle-pi, beginners could run the examples, launch the ros nodes and configure it with the available setup. But, that\u2019s pretty much the extent of what they can do. If they want to level up eventually, they will seek for better sensors and onboard computer.", "It\u2019s hard to scale the turtlebot or to upgrade it without replacing the core components (raspberry pi 3 model B, the dynamixel motors, etc.). When I plug the OpenCR to an Intel NUC, it doesn\u2019t immediately work out of the box, without some configuration and re-testing the Arduino code. This is what most beginners are not aware of.", "I also noticed people have used more powerful computers such as the Jetson TX2 to run processing onboard because tracking or depth cameras such as realsense / zed are too heavy for a raspberry pi 3, and yet they are quite popular among robotics researcher.", "Perhaps something like the PULP platform, OpenMV camera is a good alternative considering the costs.", "On the software side, it\u2019s not clear how to migrate to ROS 2 while keeping the software stack to be the same. This is if we still want to run the same SLAM / navigation packages. Though I understand it really depends on whether there\u2019s an upgrade on dependencies.", "In short, I would suggest that there should be a guidance on such \u201cupgrade\u201d path and scalability issues. As probably those who started to learn robotics are here to invest their time and skills in the long run.", "My apologies I\u2019m not too familiar with the tb3 but trying to understand why SLAM (I assume against an rplidar) qualifies as a heavyweight algorithm. What\u2019s being used as a default SLAM algorithm for tb3?", "The default setup (as per documentation) is to run the SLAM algorithm and navigation on a remote computer. Running gmapping (the default) or hector with rpildar A3 on the raspberrypi 3 model B is fine and I have tested that. But, I think cartographer is heavier. The other default option is to run frontier_exploration.", "To build the DWA local planner and map_server on the rpi3 itself requires pcl_ros to be installed as a dependency, which is unnecessary in most cases. So, up until now I never run the DWA local planner + map_server onboard.", "Then, once I setup a realsense T265 as a tracking camera to improve odometry, the realsense nodelets used up about 50% of memory on average. Occasionally the realsense camera manager crashes. If I run SLAM onboard.", "If I just use a 3S LiPo 5000mah battery (it\u2019s already larger than the default). The realsense fisheye camera nodes are not streaming their images. I assume there\u2019s not enough power from OpenCR. Although these camera images are fine if I use a larger capacity battery and higher voltage, such as 6S.", "For sure, I would need a better computer and more battery cells + capacity to run something like RTAB-Map with D435 + T265 for example.", "The impression is the rpi3 seem to only be utilised to run the turtlebot bringup or at most gmapping. I am in the process to at least migrate to rpi4 for its USB3 as realsense cameras worked best with USB3.", "At Stanford, one researcher ran a compute intensive Turtlebot II with extra  18v battery packs for a gaming Nvidia equiped laptop.  Runs up to 1 hour autonomous were obtained.", "But, I think cartographer is heavier.", "For what it is worth, I successfully ran cartographer on a ", " attached to a Turtlebot 2.  The caveat was that I had to run it in 2D mode; in 3D mode, it was too heavyweight.  You can see my short presentation from ROSCon 2017 about it ", ".", "Interesting, thanks for sharing your talk!", " does seem to have a better setup from what I\u2019ve seen. It doesn\u2019t have waffle plates. But, they are not really necessary.", "It comes with ", " camera or ASUS Xtion pro live if it\u2019s bought from clearpath robotics. Also dimension is better, while turtlebot3 require additional plates and plate support.", "I\u2019ve been teaching with Turtlebot 2 for several years now. In terms of hardware, it is hard to beat:", "There are a few downsides:", "I\u2019m at the point where I need to re-equip our robotics classroom, and I\u2019m at a loss. It looks like Turtlebot 3 is the default choice at this point, but it doesn\u2019t have any of the pros I list above.", "So\u2026 to answer your question. The platform I\u2019m looking for is something that looks a lot like TB2, with nice clean ROS and ROS2 packages.", "At Ubiquity Robotics we have been running platforms off RPi for a couple of years. We have a simplified navigation node called move-basic that is suitable for student learning and runs on low powered CPUs.", "I going to suggest a few things to make this discussion more productive.", "Please make it clear if you are an educator or not and if so what your student body looks like. Not all students are the same and it helps us if we understand the different constituencies of users.", "If you have a request or an idea please frame it in terms of the subject matter you want to teach not the hardware. Hardware changes from quarter to quarter; fundamentals more slowly.", "I\u2019m not an educator.", "But however we\u2019d like to consider robotics as an abstraction like how software is. It\u2019s not the same, hardware is also an important subject.", "A junior engineer could quickly drop a robot platform that doesn\u2019t do all the things he/she could see on videos of latest research.", "What they couldn\u2019t see is the efforts, workarounds, to make an algorithm work in a certain environment with certain setup of hardware.", "If I would to educate someone, be it a student or an engineer. I would emphasize:", "The JPL Mars rover is probably also a good reference as an educational platform.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Aforementioned wheel encoders", "IMU", "Onboard computer running ROS (extra points for this to be an option, lot\u2019s of people already have embedded computers)", "Ease of integration of custom sensors (consideration for space, robot payload)", "Some built-in DC-DC converters would be great to power external sensors (3.3, 5 and 12V would be superb)", "The size is good.  It is small enough to be safe and portable, but big enough to operate on human-scale problems like office delivery, tour guide etc.", "RGBD sensors provide a nice bang for the buck.  We can do 2d-slam, 3d-slam, computer vision etc.", "Using a laptop for computation makes it much easier to use in the classroom.  Trying to get networking set up correctly and keep it working for multiple robots is a pain.  It is much easier to just write some code on a laptop and plug it in.", "It would be nice if it were cheaper (though honestly, I don\u2019t think the price is unreasonable).", "The ROS Turtlebot packages are not very easy for novices to make sense of.  TB2 is relatively easy for beginners to use, but it is hard for beginners to modify. It would be nice to see an educational platform that serves as a clear, well-documented, example of how to set up a robot with ROS support.", "As far as I can tell, TB2 is going away. It\u2019s not clear if the Kobuki base is even being manufactured anymore.", "\n", "\n", "\n", "\n", "Environments that a robot should operate. This will cover perception, map, navigation, kinematics (if we\u2019re considering a more complicated movement than 2-wheeled differential drive robot)", "Some mechanical / electronics foundation (enough to get something working)", "OS & networking fundamentals, why Python / C++ as a common programming language. Although anyone can use other languages, but these two are the most common.", "Upgrade / scalability problems", "Algorithms", "Software management", "User Interface", "Security (this would be the advanced subject)"], "url": "https://discourse.ros.org/t/what-do-you-want-to-see-in-an-educational-ros-platform/10958"}
,{"title": "Intel\u00ae Euclid\u2122 Development Kit is open for pre-order!", "thread_contents": ["Intel\u00ae Euclid\u2122 Development Kit is open for pre-order! (", ") ", "The Intel\u00ae Euclid\u2122 Development Kit features the integration of Intel\u00ae RealSense\u2122 depth camera technology, a motion camera, and an Intel\u00ae Atom\u2122 x7-Z8700 Quad core CPU to produce a compact and sleek all-in-one computer and depth camera in the size of a candy bar! The Intel\u00ae Euclid\u2122 Development Kit is designed to be operable out of the box with pre-installed software including an Ubuntu\u00ae 16.04 operating system and ROS Kinetic Kame, which makes it perfect for Making Robots and other makers projects!", "\nIt is computer combined with a depth camera,  Fisheye camera, IMU, WiFI, and Bluetooth along with some other sensors like GPS and environmental sensors in a very small size. It comes with a battery so it is completely stand alone.", "\nYou can check out the Euclid in action on a turtlebot here: (all processing is being done on the Euclid device)", "Limited quantity is available, so get it before it runs out ", "euclid community site is live: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/intel-euclid-development-kit-is-open-for-pre-order/1828"}
,{"title": "TurtleBot3 software and firmware update and 'waffle_pi'", "thread_contents": ["Hello everyone ", "I announce that TurtleBot3 is huge updated!!!", "\nThis update considered many issues and requests from users. We are sincerely thankful to them.", "\nMore interest makes more progress. If you have any issues or suggestions, please feel free to get ", "Existing users need to download new version in master branch of turtlebot3 and turtlebot3_msgs", "Direct download in repository ", ", ", "or using command line", " sudo rm -rf turtlebot3/", "\n$ git clone ", " sudo rm -rf turtlebot3_msgs/", "\n$ git clone ", "Open Arduino -> Toos -> Board: -> Board Manager\u2026 -> Update (v 1.0.15)", "Best regards!", "\nDarby", "I assume the Waffle Pi is a lower-cost waffle with a Raspberry Pi instead of the Joule. This is great news!", "Is there any progress on a Burger or Waffle version that comes with no sensors or compute boards, for those of us who have all that stuff already?", "Hi ", " thanks alot for the update.", "\ni have a question\u2026 Do we need to update the OpenCr board? and How? i get \u201cChecksum does not match\u201d error . thanks alot\u2026 ", "Hello ", "You can follow below instruction", "or you can find more detail in ", "Thanks", "\nDarby", "I am interested in using the new software update on my TB3 and thank you for these instructions. Are there also updates planned or available to the documentation to describe the ROS commands necessary to show the new Publisher /diagnostic, /battery_state information and to connect a speaker to the RasPi3 or OpenCR to hear the generated sounds? Ross", "Hello ", " ", "We have a plan to be compatible with ROS2 though we don\u2019t have any details.", "\nWe provide a ", " and it will be added more information soon for these software updates.", "\nAdditionally, you don\u2019t need to connect separate speaker due to OpenCR already has it ", "Thanks", "\nDarby", "Darby, Following your good instructions and compiling the revised catkin_ws/src, I easily updated the ROS SBC TB3 and OpenCR software and very pleased that it all works well, including \u201crostopic echo /diagnostics\u201d node that is very informative. I look forward to the new wiki to learn how to connect the new /sound Subscription that roswtf reports as unconnected. I note there is a reference to a Adafruit display driver in the TB3 github-Is that a future additional feature? A small request is make available the pdf version of the previous wiki.Thank you and your colleagues for their hard work and producing an excellent, fun and educational robot :).", "I\u2019m still getting a checksum mismatch. I upgraded to OpenCR 1.0.15 and also tried 1.0.16. I upgraded with turtlebot3.git and on turtlebot3_msg.git on both my RemotePC and TurtleBot.", "This is the execution", "\n<", "\n$ roslaunch turtlebot3_bringup turtlebot3_core.launch", "\n\u2026 logging to /home/eepp/.ros/log/5a7d18a4-0545-11e8-9b6b-080027c0cb1e/roslaun", "\nch-orras-3438.log", "\nChecking log directory for disk usage. This may take awhile.", "\nPress Ctrl-C to interrupt", "\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS", "NODES", "ROS_MASTER_URI=http://10.0.0.159:11311", "process[turtlebot3_core-1]: started with pid [3447]", "\n[INFO] [1517266129.977476]: ROS Serial Python Node", "\n[INFO] [1517266130.051262]: Connecting to /dev/ttyACM0 at 115200 baud", "\n[ERROR] [1517266132.304481]: Creation of publisher failed: Checksum does not ma", "\ntch: 427f77f85da38bc1aa3f65ffb673c94c,d537ed7b8d95065b6c83830430b93911", "\n[INFO] [1517266132.362502]: Note: publish buffer size is 1024 bytes", "\n\u2026", "\n/>", "Nice! we are preparing updated wiki including your requests.", "\nThanks you for your interest ", "Hello ", " ", "Have you set network config??", "\nYou can check how to config network btw RemotePC and TB3 on ", "Best regards", "\nDarby", "ROS guys ", "Thank you for your interest on TB3 ! (I am happy as if i get a sweet coffee)", "\nBut ", " is not proper page to create issue ", "If you have any question for TB3, please use Github ", " or ", ".", "\nYou can meet me in there ", "Thanks", "\nDarby", "Darby,", "Thanks for your quick response. This is how I set it up", "Remote PC", "$ ifconfig", "\nenp0s3    Link encap:Ethernet  HWaddr 08:00:27:c0:cb:1e", "\ninet addr:10.0.0.159  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.159", "\nexport PATH=$PATH:$HOME/tools/arduino-1.8.5", "turtlebot3", "$ ifconfig", "\nwlp1s0    Link encap:Ethernet  HWaddr a0:c5:89:4b:3a:e3", "\ninet addr:10.0.0.158  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.158", "\nexport TURTLEBOT3_MODEL=waffle", "Ed", "I followed the instructions in 7.1.5  for Porting OpenCR1.0 to Arduino IDE. I want to reason through my issue. I\u2019m not familiar with Arduino checksum generation. I connected my OpenCR to my Remote PC. I ran the Arduino IDE from there and loaded the bootloader. So I assume I can load the OpenCR from any machine as long as I have the right bootloader version. Where and how are the checksums being generated and compared? Will I have the same bootloader checksum as everyone else in the world? If that is correct, someone should be able to tell me if I have the correct one and tell me which side is incorrect. If not correct, does the checksum depend on my hardware and/or software.", "ed", "Hi Darby", "Just wanted to confirm when you pointed to this article for updating, you meant: ", "Is it possible to update the board- manager as defined in the wiki using dfu-util?", "Or you meant some other steps such as: ", "Can you please confirm?", "Thanks", "\nSandip", "Hi ", " ", "You don\u2019t need to enter dfu mode.", "\nIf you have used OpenCR, you just update it using ", ".", "Please follow below link, it will help you", "Thanks", "\nDarby", "Thanks Darby for the reply,", "\nI updated the Board Manager on Arduino IDE to 1.0.15 (also tried 1.0.16) --> the tried updating the Bootloader using Arduino IDE (but failed) with the following error message:", "I made sure the", "lsusb", "is showing the STMicroelectronics (NOTE: There is no DFU mode written as you mention in the emanual - but this entry in lsusb starts showing up only after DFU mode triggered on using Boot + Reset button)", "Can you let me know some tips to go ahead with debugging?", "Thanks", "\nSandip", "Hi ", ",", "This is a space for discussion, so the question is not appropriate.", "\nI will continue on the issue page of the link below.", "\n", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "\n", "Thanks!", "Exactly same Checksum error here - before I start digging\u2026was it solved? any ideas? Tnx Michael", "BIG SORRY - my fault\u2026didn\u00b4t read the error messages after uploading correctly\u2026no issue anymore!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["turtlebot3_controller - include RC100(for remote control) library", "turtlebot3_diagnosis - include diagnostic functions", "turtlebot3_motor_driver - include DYNAMIXEL SDK", "turtlebot3_sensor - include functions for IMU, battery, magnetic filed and analog Input", "/version_info - Contains the hardware, firmware and software information", "/battery_state - Contains battery voltage and status", "/magnetic_field - Contains magnetic field information", "/diagnostic - Contains self diagnostic information", "/sound - Output beep sound", "/motor_power - Dynamixel torque on/off", "/reset - Reset odometry and re-calibration IMU", "add Sound.msg", "Simple command makes USB setup", "It shows state of IMU, motor, lidar, battery, button and version information", "Add Waffle PI", "Now, we are preparing new version of TurtleBot3 called ", ".  Meet Waffle PI in ", " before you get an it.", "Software (v 1.0.0)", "Firmware", "I did a roscore on my RemotePC", "a ssh into the TurtleBot", "did a \u201croslaunch turtlebot3_bringup turtlebot3_core.launch\u201d on the TurtleBot", "/rosdistro: kinetic", "/rosversion: 1.12.12", "/turtlebot3_core/baud: 115200", "/turtlebot3_core/port: /dev/ttyACM0"], "url": "https://discourse.ros.org/t/turtlebot3-software-and-firmware-update-and-waffle-pi/3729"}
,{"title": "Roboware for ROS beginner dealing with BLDC controller and ROS melodic", "thread_contents": ["hi guys,", "when i was trying to start, first thing in my mind is like \u201chow ROS handle the motor controller?\u201d coz so far as i know (CMIIW), not all BLDC controller is ROS friendly.", "\nso i can only place my bet for any controller which has protocol like CANopen and RS232. trying to plug the controller directly to PC (like intel NUC as the brain of the robot) as well as cheap LIDAR sensor and hoping it will auto magically work by the time ROS installed.", "\n*idiot mode. i know\u2026 i\u2019m very newbie, with only 2 weeks experience in ROS ", "just wondering if roboware is compatible for the latest ROS melodic version and ubuntu 18.04.", "\nfiguring out to work with roboware designer and studio to deal with a chinese BLDC controller. is it possible?", "\nespecially roboware and BLDC controller from the same city in jinan ", "i see chance in roboware designer for any newbies like me to play with simple autonomous robot using ROS. but yes, are all controllers (BLDC, brushed DC) and cheap sensors compatible with roboware and ROS?", "i saw many great example using state of the art lidar (sick, hokuyo, and even $4K velodyne for sale which is still obviously out of my wallet ", " ) while i have eyes only on $200-300 2D lidar (they have ROS driver but still don\u2019t have any idea, but at least i have hope as they said they have ROS driver ", "  ) and intel realsense for 3D vision (i\u2019m sure this camera will work as i saw it in many threats although i\u2019ve never tried this until today).", "so here is my steps as newbie from scratch to autonomous wheeled robot:", "do you think my steps are correct? or i\u2019m still lost in the jungle and still long way to go?", "\nplease kindly guide me by giving me a clue, even a small clue or direction is really appreciated.", "anyway, thanks for creating such this great tool.", "\ni do really appreciate it.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["install ubuntu and ROS (18.04 ubuntu and ROS melodic? or just keep it with kinetic for now?)", "install roboware", "connect BLDC controller to any usb port or serial port (sure connect the motor to the controller)", "connect lidar to any usb port", "connect intel realsense camera to usb 3.0 port", "get the power/battery on", "and play with roboware for doing small baby steps"], "url": "https://discourse.ros.org/t/roboware-for-ros-beginner-dealing-with-bldc-controller-and-ros-melodic/4915"}
,{"title": "Cozmo with ROS2 without SDK", "thread_contents": ["Hi there,", "for all of you using Cozmo from Anki", "I implemented a simple wrapper for using it with ROS2 without depending on the Cozmo SDK. Please, check it out and let me know what you think:", "At the moment, it allows to move the robot (wheels, head and lift) using a modified version of the teleop_twist_keyboard and publishes the images from the camera to a topic.", "Feel free to contribute to the repo.", "That\u2019s super cool, and the price point is pretty good for the features (~$115US on Amazon). Looks like there is quite a bit more possible. Pycosmo supports:", "Sensors:", "Actuators:", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Hi,", "Good news! So far Cozmo publishes:", "Now that it is working, the rest is quite straightforward (and boring ", ") so some help with the implementation would be much appreciated.", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Not yet but I will try. In theory the pycozmo wrapper I am using for it supports it. I just have to convert the data to ROS2 msgs.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Camera", "Cliff sensor", "Accelerometers", "Gyro", "Battery voltage", "Cube battery voltage", "Cube accelerometers", "Wheel motors", "Head motor", "Lift motor", "Backpack LEDs", "IR LED", "OLED display", "Speaker - work progress", "Cube LEDs", "Platform LEDs", "Odometry", "Image", "Imu (orientation, gyro and accel)"], "url": "https://discourse.ros.org/t/cozmo-with-ros2-without-sdk/11689"}
,{"title": "Autoware v1.12-alpha.1", "thread_contents": ["The first alpha for the next version of Autoware, ", ", is now available for testing. We encourage all Autoware users to try it out and wring out any remaining bugs before the code-freeze date of June 24th.", "Get the source from ", " and install it using the ", ".", "Docker images are available at dockerhub:", "\n", "The following changes have been made since 1.11.", "The following changes planned for 1.12 have not yet been made (we expect to do a second alpha for these in the next couple of weeks). You can follow progress on the ", ".", "Following on from the first alpha, we now have alpha.2.", "GitLab.com", "The major change in this alpha is the move to GitLab and the split repositories. The new install instructions have not yet been written, but in a nutshell here is how to get and build Autoware 1.12.0-alpha.2 from source:", "\nIt seems like Google Chrome changes the file name to autoware.ai.txt when downloading ", " from above link. Content does not change so just change the filename after download. (Firefox worked fine)", "I created the feedback issue about 1.12.0-alpha.2 release based on our experiment on the real vehicle. Please check and let us know what do you think.", "\n", "We tested 1.12.0-alpha.2 release on the real car, Autonomous Lexus with 4 Velodyne LiDARs. Based on the results of this experiment, the issues to be solved as the fix merge...", "\n", "I\u2019ve updated the instructions to fix the issue that ", " mentions above. There are also instructions there for the upcoming 1.12.0-beta.1.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["MPC waypoints follower (", ")", "Fix setting files for rosbag demo (", ")", "Modify stop states (", ")", "Fix autoware launcher readme. (", ")", "Split drive state (", ")", "melodic support (", ")", "add battery charging state (", ")", "modify mssion state for fms (", ")", "add install to the resources and plugin.xml (", ") (Fixes ", ")", "Add tests around Openplanner utilities (", ")", "Support full functions of g30esli_interface (", ")", "Update Docker cross-build Image (", ")", "add predicting convex hull (", ")", "Fix Segmentation fault at velocity_set by asynchronous base_waypoints and closest_waypoint (", ") (", ")", "Fix gazebo not found in colcon_release_cross synquacer (", ")", "fix behavior_state in imm_ukf_pda_track (", ")", "runtime_manager: change params definition from int to float (", ")", "Fix don\u2019t show run time in runtimemanager ", " (", ")", "ekf_localizer (", ")", "[fix]: make sure cost is set with expanding_polygon_size (", ")", "Remove points2costmap (", ")", "fix install directive for waypoint extractor (", ")", "Adding install directive for node (", ")", "Fix bug that limits the data rate of DataRateCheckerPlugin (", ")", "Updated paths in quick_start launch files and parameters in default.rviz (", ")", "Remove enablePlannerDynamicSwitch (", ")", "Add .repos file for setting up an Autoware workspace using vcs (", ")", "Split repositories and shift to using ", " for install", "Clean up the quick start documentation", "Download the ", " file ", " and remember where you saved it.", "Run the following commands to download and build Autoware", "\n"], "url": "https://discourse.ros.org/t/autoware-v1-12-alpha-1/9396"}
,{"title": "Autoware 1.12 released", "thread_contents": ["Autoware 1.12 is now available for general use.", "\nTo install it from source, follow the ", ".", "\nPlease note that with this release, Autoware has changed its repository structure and its install method.", "\n", "Docker images are available at Dockerhub:", "\n", "Once you have Autoware installed, you can try it out using some recorded data by ", ".", "This is a list of the major changes in Autoware 1.12:", "There are some known issues in 1.12.", "As always, the current reported issues can be ", " and the current", "\nproposed bug fixes and new features can be ", ".", "Going forward, will the \u201cReleases\u201d section of gitlab be used?", "\n", "GitLab.com", "\n", "Or will release summaries be primarily done here on discourse?", "We haven\u2019t had any discussion about that, but I can say that since Discourse is our primary communications channel we will need to do something here.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["New MPC waypoints follower for more accurate path following", "New Enhanced Kalman Filter (EKF)-based localiser", "Improved decision maker state machine, including support for better stop states and battery charging", "Convex hull prediction", "Support for ROS Melodic Morenia (but see the known issues, below)", "Shift to GitLab as the host of the code", "Split the repositories along functionality lines to improve CI times", "Shift to using ", " to install Autoware", "Numerous bug fixes and smaller changes; see the full list of changes below", "MPC waypoints follower (", ")", "Fix setting files for rosbag demo (", ")", "Modify stop states (", ")", "Fix autoware launcher readme. (", ")", "Split drive state (", ")", "melodic support (", ")", "add battery charging state (", ")", "modify mssion state for fms (", ")", "add install to the resources and plugin.xml (", ") (Fixes ", ")", "Add tests around Openplanner utilities (", ")", "Support full functions of g30esli_interface (", ")", "Update Docker cross-build Image (", ")", "add predicting convex hull (", ")", "Fix Segmentation fault at velocity_set by asynchronous base_waypoints and closest_waypoint (", ") (", ")", "Fix gazebo not found in colcon_release_cross synquacer (", ")", "fix behavior_state in imm_ukf_pda_track (", ")", "runtime_manager: change params definition from int to float (", ")", "Fix don\u2019t show run time in runtimemanager ", " (", ")", "ekf_localizer (", ")", "[fix]: make sure cost is set with expanding_polygon_size (", ")", "Remove points2costmap (", ")", "fix install directive for waypoint extractor (", ")", "Adding install directive for node (", ")", "Fix bug that limits the data rate of DataRateCheckerPlugin (", ")", "Updated paths in quick_start launch files and parameters in default.rviz (", ")", "Remove enablePlannerDynamicSwitch (", ")", "Add .repos file for setting up an Autoware workspace using vcs (", ")"], "url": "https://discourse.ros.org/t/autoware-1-12-released/9817"}
,{"title": "Announcing tf_remapper_cpp and static_transform_mux: More power to your TF!", "thread_contents": ["Have you ever run a ", " on a complex system with tens of nodes subscribing TF? Have you looked at the ", "? ", " Stop wasting energy and CPU cycles by moving to ", ". The API is a superset of tf_remap, so you won\u2019t need to alter anything but one line in your launch file. As a bonus you get a node that works natively with TF2, supports remapping of /tf_static (which the original tf_remap cannot do correctly), you get the ability to \u201cremove\u201d frames, and the node can also work bidirectionally (listen on both old and new TF topics).", "Have you ever faced weird problems with ", "? Yes, there is a big problem - ", "! It only publishes the last message, but ideally it should publish all so-far-seen static transforms. This is where ", " comes into play. Just launch this node early enough and it will take care of your static transforms. There\u2019s one limitation, though - you still have to replay the bag from the beginning (or just play a few seconds of it and then you can seek further if you keep static_transform_mux running).", "Give it a try and report if you find these packages useful!", "Happy transforming ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcing-tf-remapper-cpp-and-static-transform-mux-more-power-to-your-tf/9219"}
,{"title": "Autonomous Selfie Drone by Crazyflie using Deep Learning Models", "thread_contents": ["Post retracted at the request of the author due to business-related sensitive information contained in this post.", "The information is expected to be re-announced with more information in the future with more details.", "Hi, nice project ", "Is all the processing done on a jetson tk1?", "We implement this on a Linux box with more power and then port it for Jetson TX2.", "Running on Tk1 might be possible if we rewrite some code for arm 32bit and low gpu power.", "(Also, we plan to optimize this for embedded board without gpus. )", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/autonomous-selfie-drone-by-crazyflie-using-deep-learning-models/2414"}
,{"title": "PICA: Drone Made to Learn ROS with MCU and FPGA", "thread_contents": ["If you are a new learner of ROS and want to own your R&D drone? Do you want to learn ROS along with MCU and FPGA? PICA would be the choice for you to experience science, research and education all together on one plate form. PICA is specially designed for Research and development and it would give you a thorough insight of ROS. In a very short period of time you can be skillful in the field of ROS Robotics. PICA is a highly capable platform for Unmanned Aerial Vehicles. PICA is packed with serious processing and computing power, deploying an advanced FPGA, MCU and on-board computation module. The Education Package would give you full access to a learning environment. It has tutorials and instructions inside, and could help you to leap a quick start towards robo-science.", "Gaitech Robotics have also a perk for the customers, to support the consumer, there are learning courses available, which includes not only the hardware training but also software drills will be available.", "PICA is now available on ", " !", "Hello,", "Nice to see a new drone open source! Is your autopilot new or based on PX4 or ArduPilot ?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pica-drone-made-to-learn-ros-with-mcu-and-fpga/2542"}
,{"title": "Hardware Accelerated Depth Camera Simulation", "thread_contents": ["Hello,", "I\u2019ve made a depth camera simulation that utilizes OpenGL for hardware acceleration, but otherwise has fairly minimal dependencies. It is most useful when you want to quickly simulate depth scans but don\u2019t really need the full blown power of Gazebo.", "I welcome feedback and contributions.", "\n", "I wonder if any others users have need of a headless (or offscreen) rendering tools. Maybe machine learning applications?", "Hi Jonathan,", "Some years ago I developed a very similar library ", ", in the context of my PhD thesis. The library also used OpenGL for fast computation of virtual laser scanners on a 3D environment. These virtual scans were used as \u201cexpected measurements\u201d and were compared with actual measurements for a particle filter map-based localization ", ". The tool worked pretty well, and very fast, allowing to update hundreds of particles at rates of 5-10Hz.", "I also developed the version for depth cameras, which I wanted to use later for camera-gripper calibration for robot manipulator applications, see picture below:", "\n", "The main feature of this package, specially designed for 2D lidar simulation, was that the rendering window, which can be obviously hidden, was set with a size according to the resolution of the sensor you want to simulate, so no extra pixels are rendered. This resulted with very small windows to simulate lidars, since they typically have lower angular resolutions than cameras. Small rendering windows lead to fast renderings, so fast depth computations.", "Just wanted to post here, so we can be in touch, and to point out two potential applications of this approach (mobile robot map-based localization and camera-hand calibration)", "If I have some time I\u2019ll try to dive a little bit in your code. Feel free to contact me for further discussion.", "best", "Andreu", "Math details about the library can be found in the SIMPAR\u20192010 publication:", "\nCorominas Murtra, A., Trulls, E., Mirats Tur, J.M., Sanfeliu, A. ", ". Lecture Notes in Artificial Intelligence -LNAI6472. International Conference on Simulation, Modelling and Programming for Autonomous Robots (SIMPAR\u201910). Darmstadt, Germany. November 2010.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/hardware-accelerated-depth-camera-simulation/4817"}
,{"title": "Fully Open Source (Industrial) Robotics Controller", "thread_contents": ["I\u2019m unsure if this is the right forum to discuss ROS related business ideas or not, if not please let me know where the right place is.", "I have this vision of a fully open source industrial robotics controller based on ROS-I. This includes hardware and software, basically, everything that you need to drive a robot from the motor drivers up or alternatively to drive a robot with an existing proprietary controller.", "I have the feeling that it\u2019s still quite hard to get started on using ROS for an industrial application, because of many factors, not only involving the complexity of the ROS platform.", "What\u2019s missing from ROS at the moment are following pieces. I\u2019ll also add my ideas on how to solve them.", "The opportunity I see here is to build such a platform. For a client, this platform would give following benefits, compared to using a proprietary robotics controller.", "How to make money with this business?", "The goal of this project is to make using industrial robots more affordable and accessible by leveraging the power of ROS.", "Status: Idea, partially proof of concept", "I would very much appreciate your feedback if you think this is a reasonable idea and what kind of applications you see for such a platform.", "I\u2019d suggest to wait for ROS 2 and consideration of TSN in DDS. ", " could be interesting for you. From the papers conclusion:", "\u201cFor  higher  layers,  we foresee a contending landscape where the integration of TSN", "\nin different middleware solutions focused on interoperability", "\nsuch  as  OPC-UA  and  DDS  promise  to  deliver  a  bottom-up", "\nreal-time communication solution.\u201d", "I love the idea of an end to end open robot controller. The nature of my work (contract research at SwRI) means that custom hardware is almost never allowed for safety, cost, and support reasons. I think you will find a lot of folks here that have similar restrictions.", "What a lot of us ", " collaborate on is the user facing end of your proposed stack: the programming language, virtual pendant, and communication interface between PC & hardware. I\u2019m just musing now, but recently we\u2019ve seen more robot vendors provide a \u201cdumb servo\u201d interface to their hardware. We should have (but do not) the basic software in place to easily replicate what robot controllers can do now: jog, point to point move, cartesian move. Save paths, load paths.", "If we have our own controller, that is ROS based, it almost eliminates the need for a robot command language.  However, I do think there is value in providing a programming interface that is more familiar to industrial robot programmers.  We are actively developing something called ", ", that attempts to simplify programming, while still benefiting from power of ROS.", "Additional must-haves:", "How to make money with this business?", "This is by far the harder question.  An open source controller still requires a robot.  Not everyone is going to build a custom robot (or not enough to make it a worth while business).  If the controller worked with off the shelf robots, that would help, but I don\u2019t see getting the cooperation of existing robot vendors as very likely.", "Very interesting topic indeed! Thanks ", " for bringing it up and ", " for referring to our previous work (we will release before ROSCon a follow up on that!).", "We actually looked at a somewhat similar idea a while ago and have an ", " prototype that interfaces with two of the most common collaborative robots seamlessly (in our opinion, ", " is indeed right about the opportunity here). From a modularity perspective (which is what we\u2019re mostly interested in), such a concept (an Open Source (Industrial and ", ") Robotics Controller) is something very appealing but we did not obtain good input from the robot vendors we spoke with (which I guess was expected).", "We already expressed this to ", " directly but to motivate others to contribute, we from Erle Robotics wouldn\u2019t mind allocating some resources and contributing with such a project.", "E.g.: if there were to be enough interest, we could produce a limited number of our existing ", "  and distribute it to those interested parties.", "If the controller worked with off the shelf robots, that would help, but I don\u2019t see getting the cooperation of existing robot vendors as very likely.", "I wouldn\u2019t be so pessimistic about that. If they see it as a way to sell more robots, they might be more interested. You need to approach them with the idea that it will expand the market for their robots, rather than as something to cannibalise their controller business. I\u2019ve seen it work before.", "Very interesting topic indeed! Thanks ", " for bringing it up and ", " for referring to our previous work (we will release before ROSCon a follow up on that!).", "You are welcome. I totally agree with the papers statement and appreciate the effort invested into modularization concepts HRIM, H-ROS, etc as well. I am looking forward to the follow up.", "First: I\u2019d love to have a full open source stack! Specifically the part where you just want to have a robot do some things.", "For this idea to succeed (you want to make money doing this too), you need to know your targets. Will that be companies? A robot manufacturer (startup?) looking to offload support to a community? A manufacturing company which makes products? Why would such a company decide to use another platform if they already have spent serious money on a robot brand, including educating staff and writing programs (sometimes processes are signed of by their customer, which will make it even less interesting to switch)?", "In my opinion, in industry, users need an easy way of programming so they \u201cget things done\u201d. These users don\u2019t go designing robots, they don\u2019t have the knowledge/time/money, they don\u2019t go tinkering, they don\u2019t have the time/knowlege/task to dive into writing nodes and worrying about dds, realtime etcetera. They have a task and want to use a tool to solve a problem.", "\nCompanies themselves struggle to hire staff who can perform these (for the ROS community simple) tasks. On a normal level, coming from school. These men and women do not have a university degree.", "I think that one of the big missing things in a full stack is the programming language and easy programming for a factory user. All your points about the hardware side are doable.", "If you want to have success:", "How to make money with this business?", " I was thinking about something end-user facing for the programming language, like a turnkey solution that works similar to what typical industrial robot controllers offer. But maybe I\u2019m thinking to much \u201cin-the-box\u201d on this topic.", "\nAnd yes, support for off-the-shelf robots is a necessity in my opinion, even if you don\u2019t get the support of the vendors themselves, the vendors provide ROS support, so it shouldn\u2019t be too hard to add support for their robots. But it would be of course better if the vendors collaborate, selling more robots, for additional user groups, could be an opportunity for them as well.", " Thank you for contacting me. I\u2019m very interested in hearing and seeing more at ROSCon.", " I agree.", "RT support of the middleware is definitely helpful, but I don\u2019t think it\u2019s a minimum requirement for such a controller. As pointed out, Machinekit or any other motion control layer needs to be RT capable, but the middleware itself could also operate in userland.", "The RT support of the middleware, however, is necessary when we are talking about distributed motor control. Which is something ", " is targeting for example.", "I agree. Supporting closed source robot hardware would be possible, even without the vendors actively helping with the integration.", "On the other hand, vendors could be interested in offering ROS support not only as a software package but also on the controllers they sell. Just musing.", "Dear ", ",", "thank you for your post . And thank you ", ", ", ", ", ", ", ", ", " & ", " for adding valuable answers to this thread.", "And to answer your introductory questions: Yes, ROS Discourse is the right forum to discuss this ", "From a point of view of ROS-Industrial Consortia (in the Americas, Europe and Asia-Pacific) you mention a lot of issues that we 've looked at it in the past, and as my previous speakers/posters have already expressed: some of it is tempting, most of it could be possible, and all have been on the radar (which does not make it less right or wrong).", "As you remember from our last collaborations (thanks again for your support at sprint tech workshop & ", " at IPA), the devil is in the details (e.g. see comments from ", " like: safety, cost, and support reasons). Therefore, we use official industrial robot controllers.", "If you want to challenge this (and think, you can find potential customers that don\u2019t have that restrictions), I personally would encourage you and others to further investigate in developing a fully Open Source Industrial Robotics Controller. In that respect: great offer, ", ", for offering to allocate some resources and contributing with such a project.", "So far there was not a sufficient alignment of the ", " resources involved in such effort (think about the safety certification costs alone) to pull this off. If anybody has more links, please add them to this post/thread.", "As you and ", " are both based in Europe, you could even apply for some EU funding for a Focused Technical Projects (", ") via the ROSIN project (", ") and give it a try.", "Good luck!", ", ", ", ", ", and Thilo", "If people coming from school knowing this programming language and if you\u2019re lucky some hardware/software concepts, you\u2019ll also solve the problem of companies trying to find educated personnel. Might be an opportunity for the ROSIN project and education.", "As you and ", " are both based in Europe, you could even apply for some EU funding for a Focused Technical Projects (", ") via the ROSIN project (", ") and give it a try.", "+1 for this. Thanks ", " and ", ". After the reactions, we are considering doing so. I\u2019m thinking more in the ROSin FTP direction though. Although the percentage funded is lower, it certainly (at least it does to me) matches a bit better the objective of a potential project (maybe someone could argue this?).", "Creating a small consortium where each partner had access to a different industrial robot and could contribute with their corresponding abstractions for a common Open Robotics Controller (ORC?, ", ")  may make sense.", "If there\u2019s anyone interested, please say so.", "Very interesting idea indeed. I like the suggestion of ThiloZimmermann for a EU funding. So, If ", " and ", " are considering to apply it. Can I propose to joint this proposal as an academy collaborator? Thanks.", "Hello ", ",", "Can I propose to joint this proposal as an academy collaborator? Thanks.", "Thanks for your interest. That\u2019s certainly very feasible. The current status of the project proposal is as follows:", "Currently, the group is composed by:", "We will need a bit more of information about you and your group but feel free to write me if you feel you could contribute. Please specify which robot targets you\u2019d be willing to use for the project.", "Hi ", " ", " I\u2019ve been working on and off on an OS robot (hardware, so that dimensions can be adapted for each case), running on Machinekit called ", " Currently I\u2019m working towards changing the design from prototype for use in practical colleges in NL. I too would like to enquire how can collaborate this project.", "Hi ", " ", " I\u2019ve been working on and off on an OS robot (hardware, so that dimensions can be adapted for each case), running on Machinekit called ", " Currently I\u2019m working towards changing the design from prototype for use in practical colleges in NL. I too would like to enquire how can collaborate this project.", "Pretty interesting! Why don\u2019t you write me directly at \u201cvictor at ", "\u201d and describe a bit more the contributions you have in mind, the entity you belong to and the robot targets you have in mind?", "Thanks!", "Hi ", ",", "It looks an interesting initiative!", " is willing to collaborate in this project. We can contribute with our mobile robots!", "We\u2019ll write you directly.", "Thank\u2019s for your idea, I really agree!", "\nThe good news is: There is an industrial controller (controller + IO-system + Linux + Real-time + \u2026) available which supports ROS.", "\nPlease send a short e-mail for more information (", ") or have a look a this ", "Very nice and exciting topic! I would love to make a robot and understand everything about how it works ", "From my experience the only thing that makes a robot industrial is the fact that companies are able to buy it and legally use it (certifications / safety). It\u2019s not about weight, payload, speed, range, being collaborative or even how the maintenance will be done, these are to be discussed after a certification is issued.", "The dream robot for me would be:", "I think there is a real marker opportunity in making cheap capable robots, robots that do not cost much (mechanics, electronics) but have nice software inside, which would easily allow tasks that known industrial robots struggle with (vision etc.).", "For every robot that I have used so far the problem was always in the software:", "I think one big barrier is the mechanics cost: making a decent robot is complicated and expensive.", "\nThere is already a lot of open source robots that can be 3D printed, however most of them use stepper motors or servomotors which may not be the best if scaling up the robot. It could be a great start using these projects with AC or DC servos instead of servomotors.", "I can help for:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a robot command/programming language, equivalent to what you have for industrial robots - that\u2019s only necessary if you want to skip the vendor\u2019s controller", "An easy to use online programming HMI, well, that\u2019s doable.", "A low-level real-time capable platform to drive the robot if you are not interested in using the vendors\u2019 controller. ", " work for this.", "A reference hardware platform. I know applications are diverse, but having a common ground to start from just makes things a lot easier. An RT Linux capable platform, preferably with low-level IO access, which is powerful enough to support CV and machine learning applications should do. Turns out the ", " is a good candidate.", "No vendor lock-in.", "Easy and intuitive programming, thanks to the great HMI and teaching capabilities.", "Leverage the whole ROS eco-system for your application.", "Rebrand the product for your application.", "A reference platforms makes it easy to get started, more focus on your core business.", "Sell the hardware/software as a turnkey solution.", "Provide consulting services around ROS, robotics hardware and so on.", "Use dual licensing for commercial and open source applications.", "Provide paid premium support for the platform.", "a robot command/programming language, equivalent to what you have for industrial robots - that\u2019s only necessary if you want to skip the vendor\u2019s controller", "Trajectory generation, planning, and smoothing at the joint level is also missing.  This would have to be added to the low level controller in order to achieve industrial robot level motion.", "Collaborative functionality.  Might as well jump into this growing market, rather than focus on legacy applications.", "you need ", " that companies can buy, proving they can get going. They are mostly interested knowing if the tool can do the job, and what it yields them. Managers mostly decide about purchasing these tools. Concepts as ROS, Open Source or vendor lock-in are second round considerations.", "staff must be able to work with your stack solving a problem, They will help leverage the decision if they tell their Manager that they are able to work with the tools. Possibly even tried out a demo without hardware.", "If people coming from school knowing this programming language and if you\u2019re lucky some hardware/software concepts, you\u2019ll also solve the problem of companies trying to find educated personnel. Might be an opportunity for the ROSIN project and education.", "do not underestimate developing and selling a product, and providing support if you\u2019re going to provide hardware/software as turnkey solution.", "consultancy would be where you have a starting point.", "Open source hardware\n", "Mechanics (dimensioning, CAD / drawings, most interchangeable parts as possible)", "Electronics (CAD / BOM)", "\n", "Open source software\n", "ROS native", "Access to the closed-loop control (monitoring, configuring etc.)", "\n", "Certified", "Not being able to run tasks concurrently", "Robot not following the setpoint (not because of the mechanics)", "Missing (paid) options", "Low memory", "ROS driver not exposing all controller functions", "And the list goes on\u2026", "The mechanics", "The software (from real time to GUI)"], "url": "https://discourse.ros.org/t/fully-open-source-industrial-robotics-controller/5832"}
,{"title": "Proposal - New Computer Vision Message Standards", "thread_contents": ["Hello computer vision users,", "Please help us define a new set of computer vision specific ROS messages by reviewing the ", " and providing your feedback, either here or on the repository.", "At OSRF, we are in the process of defining a new standard set of ROS messages for the computer vision community, and we\u2019d like your help. This need was identified from our computer vision ", " as a first step towards improving the ROS computer vision ecosystem, so thank you for the feedback!", "The end result of this effort may be a new message package in ", ", a REP, or both. Our goal is to capture as many common computer vision use cases as possible, with the exception of navigation. (We feel that navigation and localization are already well-defined by the community and ", ".) Object recognition and image classification are two primary targets we are hoping to hit, and we want to cover both 2D and 3D use cases.", "The ", " we have created is very much a work in progress, and only with your feedback can we make it better. Any feedback is welcome, bu here are a couple of questions I have identified:", "Thanks!", "Nice effort! What about also define interfaces for annotations?", "We also try to define \u201cgeneric\u201d interfaces:", "image_recognition - Packages for image recognition - Robocup TU/e Robotics", "However, it\u2019s hard to capture things as:", "I think you would also like to define something as detection or feature groups that belong together. Maybe take a look at how ", " does it.", "Currently, what is the difference in roles between the two poses in ", " vs ", " nested inside it?:", "As in, what is the relationship that would afford the use the nested ", " to convey the detection\u2019s frame_id?", "I suppose this is a larger question of semantics or dichotomy, but perhap I\u2019m of the thought that classifications derive from detections, such as ROI\u2019s, as opposed to viersa. In whichever case, I think the relationship should be made clarified if we are starting to nest standard message types.", "To just throw this out here, I\u2019ve been using SPENCER recently, and I\u2019m beginning to really appreciate the message type layout they\u2019ve used. Perhaps we could take some hints from the project:", "\n", "spencer_people_tracking - Multi-modal ROS-based people detection and tracking framework for mobile robots developed within the context of the EU FP7 project SPENCER.", "\n", "Hi,", "\nwhen talking computer vision message standards one thing comes to mind, features.", "\nIt is not unusual to have different nodes exploiting the same kind of features (think SIFT/SURF etc), so that rather than extracting several time the same features, a single \u2018extraction\u2019 node does the job and publishes them. They are in turns exploited by (several?) others. A standard message may not be straightforward and I\u2019m not sure this problem fits the scope of this proposal, however it certainly would be useful.", "Cheers.", "I think features should be regarded as an implementation detail of the classifier/detector and should therefore specifically not exposed in these messages.", "I really like this proposal, there should be a standard for this in my opinion.", "In detail, what I like more about Reinzor\u2019s message definitions, is that there is a shared ", ", which takes the role of the ids+scores combination in ", "I think using the CategorialDistribution reduces a little bit on complexity on the user end, but the difference is very small. Another benefit is that the message definition is reusable.", "This topic\u2019s header is \u2018Computer Vision Message Standards\u2019 however the discussion seems to focus on classification. Did I misunderstood the point or is the title not appropriated ?", "It\u2019s not clear to me if the proposal supports per-pixel segmentations. There is the ", " field of ", " and ", "  which might be used for segmentations, but from the documentation I\u2019m not entirely sure if it is also meant for segmentations or not. The name ", " is a bit confusing to me.", "There might also be other types of detection besides bounding boxes and segmentation that I\u2019m not currently thinking of, though the two seem like a pretty solid base for now.", "Why XML? Why not YAML or JSON, or completely implementation defined? Or what about the name of a tree of parameters on the ROS parameter server?", "It indeed says computer vision and not classification/detection only. My bad.", "\nWhat kind of messages would be needed for tasks beyond those 2?", "No problem ", ", it\u2019s just that I have a specific use case in mind. It is as follows:", "\nLocal features (a point and a descriptor -> SIFT/SURF etc) are one of the basic component of CP and is used for geometry algos as well as for appearance-based algos. In feature-based Visual-SLAM (e.g. ORB-SLAM) you rely on feature both for the poses estimation (geometry) and place recognition (appearance). Those two tasks can be executed in parallel threads. Assuming you are using the same features for both tasks one could communicate a ", " (or such) to the other.", "\nIt is something (a ", ") I have been hackily doing here and there, feeding different classifiers - different processes for that matter.", "\nI am just wondering here if a standardized way of moving such objects around would not make sense ?", "ps : To be fair local features are also used from other sensor readings (e.g. laser scan, point cloud) so my question my be a little out of the scope of this thread.", "Thanks for the awesome feedback, everyone! I\u2019ll try to address everything that was brought up.", "First, let me start off by noting that although I only created Classification and Detection messages, I think it makes sense to keep this as a general ", " package, and additional computer vision-related messages can be added as time goes on. I think it\u2019s more useful than making a too-specific ", " or similar.", ", thanks for linking to your message definitions! I think that annotations are already covered under the existing implementation. You could provide the bounding box coordinates in a ", " message, and the most likely label as the only result in the class probabilities. If we want to add other information, such as color of the outline, etc. then maybe this would be a better fit for ", " or another package.", "On another note, is human pose estimation standardized enough to make a custom message type for it? Or is it best described by a TF tree, arbitrary set of ", ", or some other existing ROS construct? I\u2019m thinking of the fact that different human detectors provide different levels of fidelity, so it might be difficult to standardize.", ", My idea with having two poses is that the bounding box could actually have a different pose from the expressed object pose. For example, the bounding box center for a coffee mug might have some z-height and be off-center wrt the body of the mug, but the expressed object pose might be centered on the cylindrical portion of the mug and be at the bottom. However, maybe it makes sense to forego the bounding box information, as this could be stored in the object metadata, along with a mesh, etc.", "On the topic of nesting, I\u2019m open to the idea of flattening the hierarchy and having Classification/Detection 2D/3D all include a  new ", " message. I\u2019m not sure how much message nesting is considered standard practice, so I\u2019ll look at some other packages to get an idea.", ", I like the idea to add a standardized ", " or other similar message, as long as there is some common baseline that can cover a lot of feature types. From my own understanding of visual features, there\u2019s usually a lot of variation in how the feature is actually defined and represented, so I\u2019m not able to find a \u201clowest common denominator\u201d from my own experience. If you feel there\u2019s something there that could be broadly useful, please feel free to post it here or make a pull request. I agree with ", " as well, although many classifiers use features internally, this should be hidden in the implementation except in special cases like the SLAM case described.", "I didn\u2019t design the current messages to support per-pixel segmentation, and I\u2019ll have to look into how that is usually represented to get a good idea of how to craft a message for it. My initial guess is that it will be a separate message type from ", " and ", ".", "On the topic of the parameter server, I think it\u2019s worth having a discussion about representation format. From talks with other OSRF folks, I don\u2019t think it\u2019s a good idea to use a tree of parameters; a single parameter would be better. For example, if you are loading the ImageNet class names, that\u2019s 1000 items on the parameter server, just to store the names. Add object meshes, sizes, etc., and it could balloon very quickly.", "While JSON/XML/YAML might work equally well in terms of expressive power, with XML, we can be sure that both C++ and Python will have the ability to read the database. TinyXML is already included as a low-level dependency in ROS C++, but the same can\u2019t be said for a YAML or JSON parser. Rather than allow people to use whatever\u2019s convenient, I think it\u2019s worth it to restrict/recommend everyone to use a format that can be parsed from more languages. We could do it in the REP, but not enforce it, so if someone really wants to use YAML in their Python-only implementation, they could do so. That\u2019s my position, but I\u2019m interested in hearing other ideas.", "I\u2019ve updated the repository with changes as discussed above \u2013 ", " and flattened the message hierarchy accordingly.", "On the topic of dense pixel segmentation, is there a reason that ", " is inadequate?", "In regard to pixel labeling, I\u2019ve also seen ", " used as a means to publish, along with some custom structure to define the mapping between pixel values and labels on a separate topic. It would be cool to also have a message type to publish a array of convex bounding polygon verticies with label IDs. That\u2019s\u200b a common use case when labeling regions of an image, and would be good compressed representation to transmit instead for classification modalities that utilise that format.", "For pixel-based segmentation, I imagine a message PixelSegmentation.msg like", "where the pixel value of each pixel in the mask corresponds to an index in the results-array.", "This looks like it would be a clean implementation. Just to be sure (since I\u2019m a segmentation newbie), the size of ", " would be the number of pixels in the ", "? There\u2019s a distribution for each pixel?", "First, let me start off by noting that although I only created Classification and Detection messages, I think it makes sense to keep this as a general vision_msgs package, and additional computer vision-related messages can be added as time goes on. I think it\u2019s more useful than making a too-specific classification_msgs or similar.", "I am not an expert in this field, but I would like to clarify whether the classification and detection messages are specific to 2D image processing, or if they can also be used for 3D point cloud processing or even 2D laser scan processing. If there is a possibility that they may be used outside of image processing, then perhaps a ", " or similar package actually is appropriate. Just something I think should be considered.", "A CategoricalDistribution for every ", ". E.g pixel [45, 89] has value 21. That means it\u2019s labeling can be found at", "That CategoricalDistribution e.g. determines that pixel is either a pear or banana with corresponding values in the distribution.", "Pixel [45, 90] also has value 21, referring to the same CategoricalDistribution, though pixel [56,94] has value 2 so refers to results[2], which says that pixel is most likely an apple etc.", "The max value of the image +1 corresponds to the length of the .results-array.", "Is it likely that many pixels in the image will have identical distributions? It seems that \u201capple\u201d pixels near the edge of the apple would have a different probability distribution than those near the center. All the ML-based segmentation systems I\u2019ve seen either predict a single output class for a pixel (such as a binary classifier), or they produce probability vector", "It seems like a bit of a halfway solution to define a small set of distributions that the image uses as an index, then transmit that set with every result. I feel that these two options would work based on use case:", "The image is segmented in some small finite set of output classes, which do not have probability distributions that vary in space/time: use an Image message where the lookup value of the pixel is the output class. If desired, static probability distributions for each class can be communicated in a one-time fashion, such as via a single CategoryDistribution[] message, or via the parameter server", "The output segmentation includes varying probability distributions that are calculated per-pixel or per-small region: use a CategoryDistribution of length the size of the image, where each pixel has its own unique distribution that may change every frame.", "Let me know if I missed something! If you have some code available for a use case, that\u2019s really helpful. I\u2019m currently in the process of writing example classifiers to use the Classification/Detection messages and finding it a useful exercise.", "3D point cloud processing generally falls under the topic of \u201ccomputer vision.\u201d But I had not considered laser scan processing, good point. The package name will probably be subject to review from more senior OSRF architects, and we\u2019ll keep that in mind!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Are there major use cases or edge cases not covered by this set of messages?", "Is this set of messages broad enough to encompass both handcrafted and machine learning-based approaches to computer vision?", "\n", ": as a result from ", "\n", "Poses e.g. ", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/proposal-new-computer-vision-message-standards/1819"}
,{"title": "Discussion on detection of people and obstacles in a dense crowd", "thread_contents": ["Hi,", "I\u2019m not at the point of being able to ask a proper question yet. However, I am starting on a project that will hopefully work as a telepresence robot for a friend of mine who is housebound. This would, hopefully, allow her to attend science fiction conventions.", "The problem is that science fiction conventions are extremely crowded and I don\u2019t want the robot to be able to run into anybody or anything, at least no more than a human would in a similar situation.", "The robot will have a lot of sensors to detect objects at various levels, but I think that vision is the best way to detect people in this situation. And I\u2019m sure that it won\u2019t help that many of the people will be in costume.", "Does anybody have any ideas about how to approach this problem?", "The planner to determine the route of the robot would include the above data, in addition to the goals of the remote user and also keeping me within a certain distance. But right now I am only concerned about detecting the \u201ccrowd.\u201d Finding a way through the crowd is different problem. It is similar to the problem of a child in a similar circumstance with orders to keep their parents in sight and not to run into people.", "Yes, it would be nice if the robot were fully autonomous.  But actually I think a robot like this one would be teleoperated, that is moved with a joystick.    So there is a human driver using the vision system remotely.    Still there would be some lag and human drivers are not perfect.   I think it might be good enough that if the robot detected an imminent collision would simply apply the brakes hard and stop.  The human driver would then figure out what to do.", "I\u2019m using the common HC-SR04 as a last-resort collision sensor.  In theory my robot should never crash as there are sensors to prevent this.  But hc-sr04 is a backup.   Inside the base controller there is a loop that runs once for every twist message.  But the base controller looks at the distance reading from the ultrasonic sensors and decides if it \u201cwants to\u201d execute the twist message.   This decision is made completely outside of ROS.  The point is that we should only detect an obstacle if there is a failure in the ROS based system so the base controller, the process that actually commands the traction motors does the \u201cpinging\u201d itself and will refuse to power into a fixed object.", "You may or may not want to use this same design but if operating in a crown and the robot owner is not within arm\u2019s length with a hand ready to punch an \u201ce-stop\u201d bottom you need maybe TWO not one fail safe checks.   In other words if the ROS planner says \u201cgo\u201d it also must have an OK from TWO independent non-ROS based systems.  Perhaps the second one is a mechanical switch that detects physical contact between a bumper bar and the obstacle.    For your robot  I\u2019m thinking of a ring that encircles the robot and is held by springs and if a spring moves say 1/4 inch it means that your vision system, the humans driver and the ultra sonic sensors have all failed to detect the obstacle.  So this system that uses mechanical switches disconnects the motors from power using a mechanical relay (no software in the loop)", "Safety is hard.  The easy task is to design the machine to be safe when it is operating as designed.  The harder task that is 100% required to operate remotely in a crowd is that the machine remains safe even after unanticipated failure modes.    As an example I had an electrical fire last week in a prototype.  Lithium batteries have high power density and I ended up vaporizing a power cable.  My design was not fail safe, in that obviously there was a failure mode that could cause a fire.", "So your anti collision system must work even if there is a bug in the software and even if there is a mechanical fault.  Vision is CLEARLY to complex to be unconditionally safe but could be a very good primary system given enough  redundant backups.", "Back to my work, I\u2019d like to be able to use one camera but I\u2019m undecided.  getting 3D data from one camera requires very good motion estimation.  I don\u2019t think my IMU and odometry will be good enough so I may need stereo vision.   I think in your mixed indoor/outdoor use case, moving in a crowd you will need stereo vision to get usable depth.  Your obstacles are all moving and you will need to snap the pair of images simultaneously, not sequentially.", "Summary:  I think stereo vision is a great primary sensor.  But for remote operation you\u2019d better have multiple independent backups that are each so simple they can\u2019t fail and being truly independent the chances of both failing at the same time is the product of the probabilities, a tiny number.", "Telepresence seems easy at first, basically it is a remote control car with a webcam glued to the top but the problem is that telepresence by definition means the operator is not present.  I think that implies extreme reliability and safety.", "People tracking for people who don\u2019t look like people all the time, interesting!. In a crowded environment no less.", "You might look into the ", " (who had a robot driving around on Amsterdam Schiphol Airport: ", ")", "Following people in a less crowded environment is a task in RoboCup@Home. At TechUnited, we use vision to detect a person and then a laser scanner at torso-height to track the operator.", "Thank you very much, Loy.", "And I forgot to mention: while most of the people in the crowd are adults, there are many children and probably a few R2D2 robots. This is going to be an interesting project. I suppose I will start out with collisions in my motorhome where there is just me to damage. Then I will add in other factors by inviting other people in and putting random boxes down to simulate a changing environment.", "Then I might bring Groucho outside and see how he handles there, with me close by with a remote cut-off switch. Unfortunately, I have to carry him in and out of the motorhome. This puts an upper limit on his size and weight, though I will make him able to be split into multiple pieces if possible.", "Jay", "I agree about the need for safety.", "In some of my previous robots, I had a \u201creflex system\u201d that handled some failure cases. This was a simple processor that took in input from certain sensors and stopped the robot and then sent a message to the planner about why it had stopped. Until the planner sent the reflex processor a \u201cchill out\u201d message, the robot didn\u2019t move.", "The reflex processor killed the motors with relays so that nothing could turn them on until the reflex processor was satisfied.", "I used touch-sensors and floor sensors to make sure the robot wasn\u2019t heading over some stairs.", "These were very simple robots compared to Groucho and they didn\u2019t use ROS.", "I usually design my robots\u2019 brains with multiple functions/subsystems. The reflex processor is the one that is guaranteed to be a separate processor that is very simple.", "I will use multiple segments to my touch-ring. This will give me a better idea of where the robot was touched the object. I may even have certain touch-sensors control the motor relays themselves after a given amount of pressure.", "I have two Intel RealSense cameras that I plan to use for Groucho. I also have six webcams coming so I\u2019ll be able to handle using stereo vision if need be. I am still designing Groucho\u2019s head. I want it to look metallic while also looking like a caricature of Grouch Marx. Plus I will take some ideas from ", ". This is the most expressive head I\u2019ve seen while still looking like a robot. I have 12 Dynamixel AX-12 servos that can be used for just the head if need be.", "There will be at least one camera in the head for additional vision at a taller height. There will be at least 3 webcams in Groucho, in addition to the two RealSense cameras.", "I\u2019ll be using an Intel NUC (latest generation) for Groucho\u2019s main brain. I may preprocess some of the cameras with another computer if that is needed. Or perhaps use a second NUC for vision processing. At this point, I haven\u2019t done much vision processing so I will have to do more before I can say anything. The base will have enough room for several processors.", "DangerousThink, AKA DT, Jay", "The Jack Robbot program at Stanford is using ROS to understand the robot SLAM in social situations.  For example, A person would never walk between to other people having a conversation.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/discussion-on-detection-of-people-and-obstacles-in-a-dense-crowd/2109"}
,{"title": "[URDF-NG] ROS2 URDF2 discussion", "thread_contents": ["What does the next version of URDF look like?", "There was", " around URDF and SDF in 2.0.", "Later a ", ". Summary ", ".", "The discussion so for seems to be leaning towards harmonizing URDF and SDF (using SDF as a foundation).", "What are the next steps? What are the important details?", "My bold position is that ROS could adopt SDF wholesale. Most, if not all, of the wishlist items for URDF2 are already captured in SDF. It would be a shame to reinvent the wheel.", "I have a feeling this idea is somewhat controversial. If so, would someone be willing to speak to potential issues associated with ROS using SDF?", "It would be a shame to reinvent the wheel.", "Didn\u2019t SDF do that, given that people were trying different ways to extend URDF already (mimic tags, srdf, etc.)? ", "Jokes aside, I think SDF\u2019s ability to have frames with multiple parents in the kinematic chains is a distinct and fundamental advantage over URDF, which happens to also be a requirement of sorts when working with physic engines. However, I do think it shows that there are appropriate situations in which yet another standard makes sense.", "If so, would someone be willing to speak to potential issues associated with ROS using SDF?", "In order to use SDF in ROS, there needs to be a way to do tf with graph (rather than tree) based kinematics, i.e. how to handle multi-parent reference frames with tf. This is brought up every time this is suggested and though some ideas have been discussed in person, there\u2019s never been a well formed proposal on how to deal with that.", "Specifically ", " and I have talked at length about spanning trees for the SDF graph and how they could be used to address this. Maybe ", " can speak to that point.", "There may be other issues, but that\u2019s the fundamental one that comes to mind for me.", "Wheels are strange beasts, and like to be reinvented. Gazebo did have an XML format for describing robots and worlds before ROS existed. In fact, URDF looks surprisingly like Gazebo\u2019s original format\u2026", "The graph issue is a good point. Adding something to SDF to break a graph into a tree sounds like a reasonable approach.", "An impopular position perhaps, but personally I would really like to see ROS2 use an established scene/robot description format if possible.", "SDF is a good improvement over URDF already, but it\u2019s still a custom format, with almost no uptake outside the ROS-Gazebo universe. I know of one (export only?) plugin for a (commercial) 3D modelling tool. Newer Gazebo versions have the model editor, but that is still limited.", "With ROS2 using an established and industry grade middleware as the foundation to build its communication abstraction on top of, it would be great to see if we can do something similar for some other key parts, the robot description format being one of them.", "So I have just consumed most, if not all, of the discussion so far (hurray for lazy Fridays) including the recording of the meeting. Here are a few observations:", "All the formats seem to have arisen from fire-fighting small problems in small problem domains rather than intentional design. Everyone participating in the last year of discussions is representing their stakeholders and their requirements; but the conversations seems to be \u201cHow can we make the current robot description techniques less broken?\u201d rather than \u201cWhat set of stakeholders and requirements will give us the best foundation for the future?\u201d", "Given the number of people volunteering to do anything, moving to and incrementing on SDF seems like the best bet.", "At the heart of a robot description, the most important thing is that the mental model, and any given element of that model, is documented in enough detail so that you, and I, and everyone else understand the context and agree on what the element is. After that we need a defined data representation, with a common, easy to implement format being nice to have.", "I\u2019m seeing a general confusion between configuration (robot A has a gripper and robot B does not) and the dynamic state of things (robot A picks up an object and we add a rigid constraint between the pose of the EE and object to represent that). There is nothing wrong with bootstrapping any dynamic models with the robot description, but they are a different thing.", "There was some discussion about moving the robot_description parameter to a latching topic. Fine idea. Outside the scope of a robot description format.", "The robot description is a storage/wire format. There is a point in time when all data retrieval and parsing is finished at which point there is a robot model in memory. What happens after that point is application specific. Declaring which end effectors are available is part of the robot description. Signaling the switching of tools or the mounting of a tool at t=0 is not part of a robot description.", "There has been some discussion on compose-ability, extensibility,  and whether a core description (or well defined sub-descriptions) should be used. Dependency hell issues were raised with regards to plugins, URI\u2019s and the like.", "I don\u2019t have anything new to add to that discussion. The core description thing seems to be an artificial issue created by assumptions about the responsibility of the tools using the description to their downstream users. The robot description should not care about those third party consumers. Any parseable and coherent subset of the robot description should be fine.", "Dependency hell is not a reason to disallow dependencies. If managing external connectivity or packaging of multiple elements is beyond your project, then you should keep everything in one file.", "There was discussion about xacro, template engines, and GUI editors. I think these are outside the scope of the robot description, beyond indicating a format that has good libraries available.", "One final observation is that while it appears that SDF is the easiest path, the SDF creators have a mental frame that constrains their thinking. Nothing wrong with that, but it\u2019s worth taking a careful survey of others using rigid body robot models. People deploying robots in experiments or the field. People transferring CAD or optimizer designs. I think most types of users have been participating in the discussions so that\u2019s good.", "In order to use SDF in ROS, there needs to be a way to do tf with graph (rather than tree) based kinematics, i.e. how to handle multi-parent reference frames with tf. This is brought up every time this is suggested and though some ideas have been discussed in person, there\u2019s never been a well formed proposal on how to deal with that.", "Specifically ", " and I have talked at length about spanning trees for the SDF graph and how they could be used to address this. Maybe ", " can speak to that point.", "I\u2019m really hitting a mental wall with why a graph topology is a problem. Which frames have multiple parents (or rather, how is it possible for a frame to have multiple parents)? Can you elaborate the problem?", "Since it\u2019s a rigid body model it seems like any spanning tree would give you a valid results. The default spanning tree via the order joints are in the file would be fine no?", "(EDITED to clarify the question)", "I\u2019m really hitting a mental wall here. Which frames have multiple parents? Can you elaborate the problem?", "Since it\u2019s a rigid body model it seems like any spanning tree would give you a valid results. The default spanning tree via the parent-child relationships would be fine no?", "I\u2019m guessing ", " is thinking of kinematic structures with cycles in them, like grippers with parallel / coupled links, delta robots, etc.", "An impopular position perhaps, but personally I would really like to see ROS2 use an established scene/robot description format if possible.", "What formats do you have in mind?", "SDF is a good improvement over URDF already, but it\u2019s still a custom format, with almost no uptake outside the ROS-Gazebo universe.", "Open source projects are difficult to track. Based on word of mouth and interactions with other people I can say that simulators, such as Moby and Drake, use SDF. Organizations like FIRST, Robocup, and NASA use SDF. There is also a solidworks to SDF exporter, and AutoCAD has expressed interest in developing their own exporter. None of this means SDF is good, but are people who rely upon SDF.", "Can you clarify what you mean by \u201ccustom format\u201d and \u201cestablished format\u201d. To me it seems that a format created for a particular project (let\u2019s say SDF and URDF) can become an established format through general acceptance. This acceptance is an indicator that a format is worth using, or improving upon. Whereas a format created by an organizing body (let\u2019s say Collada) does not make the format good.", "Newer Gazebo versions have the model editor, but that is still limited.", "Yes, it is limited. The approach has bee to develop and release incremental improvements, rather than wait until a feature complete version is ready. We\u2019d love to have help with the model editor.", "All the formats seem to have arisen from fire-fighting small problems in small problem domains rather than intentional design.", "Just to clarify, both SDF and URDF have been carefully crafted by groups of people. It\u2019s difficult to foresee all potential use cases and problems. What may seem like fire-fighting is the normal processes of incremental improvements.", "I\u2019m guessing ", " is thinking of kinematic structures with cycles in them, like grippers with parallel / coupled links, delta robots, etc.", "Yes, tf requires a tree representation, but many robot configurations can have cycles. It\u2019s also jsut a good idea to support representation of a graph as a tree.", "I would say the only other one we should consider is collada.  This is because it is an industry 4.0 standard.", "If SDF is it then we need push to get it as an industry 4.0 standard as well", "industry 4.0 standard", "What is involved in becoming an industry 4.0 standard?", "I have not looked into it.", "Paul Hvass or Shaun Edwards might know more about this.", "Gjis?", "I\u2019m guessing ", " is thinking of kinematic structures with cycles in them, like grippers with parallel / coupled links, delta robots, etc.", "Yes.", "Since it\u2019s a rigid body model it seems like any spanning tree would give you a valid results. The default spanning tree via the order joints are in the file would be fine no?", "One strategy would be to pick an arbitrary spanning tree (first in order, last in order, random, etc.). However, you could also let the user specify the spanning tree, something that ", " was pretty interested in doing. Also, when you are dealing with a distributed tf system you can easily get into a situation where two spanning trees can disagree. This isn\u2019t an issue when the description is being used in a simulation because the simulation ensures that the spanning trees agree (or at least it should) and it\u2019s able to do so because it is not distributed and it has a \u201cperfect\u201d model.", "So allowing the user to pick, with a reasonable default if they don\u2019t care, seems best. But there are a lot of details about how to represent this and communicate it in the ROS graph and in the API.", "There was some discussion about moving the robot_description parameter to a latching topic. Fine idea. Outside the scope of a robot description format.", "Both of the previous two points, by the way, are examples of why I have to disagree with your general sentiment that how the description is transmitted and used in context is out of scope. The description isn\u2019t used in a vacuum and I don\u2019t think it should be designed in one either. I do understand the desire for the format to be portable to different frameworks and therefore it\u2019s design shouldn\u2019t be unduly influenced by just one of those possible frameworks, but I think you can accomplish that while considering how it will be used if you\u2019re conscious of that fact.", "One strategy would be to pick an arbitrary spanning tree (first in order, last in order, random, etc.). However, you could also let the user specify the spanning tree, something that ", " was pretty interested in doing. Also, when you are dealing with a distributed tf system you can easily get into a situation where two spanning trees can disagree. This isn\u2019t an issue when the description is being used in a simulation because the simulation ensures that the spanning trees agree (or at least it should) and it\u2019s able to do so because it is not distributed and it has a \u201cperfect\u201d model.", "A spanning tree export is certainly possible, and something that I think we should explore. But a naive spanning tree computation will lead to significantly degraded performance of tools like tf. As mentioned, it needs to be consistent between all participants. For good performace the tree needs to be both deterministic and also consistent over time. If the tree changes topology you loose the ability to interpolate between time updates. There\u2019s also value in having the tree be similar in topology to the physical linkages since traversing fewer joints to compute a transform will result in less error accumulation.", "My opinion on what should be done in the short term has changed over the past few months. While I still find it annoying that Gazebo and ROS do not have a common format, I don\u2019t believe that switching to SDF for all ROS applications (or vice versa) makes sense, because:", "On top of these technical issues, there is the higher level decision of who decides what goes in the spec, and what the process is for that. One reason we have so many different robot description formats is that many of the interested parties prefer to have the flexibility to decide on their own what goes into a format. For TF/moveit/rviz/etc to share the same description format with gazebo (for example) people from all of those groups would need to have input into the spec. ", " would you be ok with changes to sdformat being chosen by a committee, where gazebo was only one of several participants?", "Longer term, I do think that defining a format (or set of formats) that are broadly used in the robotics community is extremely important, but I\u2019ll post some thoughts about that separately in ", "EDIT: Removed reference to \u201clong term section\u201d - I\u2019m going to post those thoughts on the next gen robot description thread.", "Switching to SDF doesn\u2019t provide most of the things that I want out of a robot data exchange format", "Do you mean the action of switching, or that there are a bunch of features/characteristics missing? If features, what\u2019s missing in SDF?", "I believe switching to any new format would require significant effort.", "\nlibsdformat has conversion from URDF to SDF, and the reverse is partially", "\ncomplete. This would make a transition less onerous.", "What are the items that SDF is missing?", "SDF elements are already chosen by committee. That committee happens to be", "\ngazebo developers. Interested parties are welcome to propose changes, and", "\ncomment on pull requests. Are you talking about a more formal committee?", " i mean the act of switching", " yes - switching to any new format would require significant effort, which is why I\u2019m suggesting not switching.", "I do mean that I the committee would have to be not just gazebo people. Yes, anyone can propose changes and offer input, but for something so crucial as a robot description format, I think that the make up of the committee that has actual decision making power should reflect the people who use it, and so if URDF switch to SDF, then i would expect the committee with decision making power for sdf to include people from developers for the various tools that now use URDF, as well as the developers for gazebo.", "To be clear I\u2019m not asking for that - I\u2019m bringing it up as an example of why I don\u2019t think we should switch from URDF to SDF.   I think that SDF is an excellent format created by engineers who know their stuff and  who have a clear application in mind that guides their decisions.", "I\u2019m still really struggling to see how the graph causes issues. Does anyone have any example cases? This feels like it\u2019s straightforward and a non-issue to me and given the people here I must be missing something.", "Here\u2019s my assumptions:", "\nLoading a graph into a tree with a hand written parser can be done with a brute force loop detection with just a few lines of code, and given the typical number of loops there is no performance problem. So no penalty for the little guy.", "\nIf one is working with a large graph structure that would cause a performance impact, then they have probably have the ability to write more elegant loop detection.", "\nAll spanning trees are equivalent and it never matters which one is given to TF.", "Where am I going wrong?", "A spanning tree export is certainly possible, and something that I think we should explore. But a naive spanning tree computation will lead to significantly degraded performance of tools like tf.", "How does the computation for breaking the loops in the graph interact with TF and change TF\u2019s performance?", "As mentioned, it needs to be consistent between all participants. For good performace the tree needs to be both deterministic and also consistent over time. If the tree changes topology you loose the ability to interpolate between time updates.", "What naive algorithms for breaking a graph into a spanning tree are not deterministic and not constant in time? Can you come up with some code that gives non-deterministic results when applied successively to the same graph or any expected evolution of that graph? Is this just a theoretical problem? (I understand a tree topology change causes a real problem).", "There\u2019s also value in having the tree be similar in topology to the physical linkages since traversing fewer joints to compute a transform will result in less error accumulation.", "How much error are you really going to accumulate (or save) on any robots in current (or in URDF2\u2019s future) use with TF, with a non-optimum spanning tree?", "I think that the rigid body assumption (or the mathematical nature of a frame) means that there is no spanning tree that is better than any other from a topological view.", "From a problem solving view there is a benefit if I can get the upstream tools to use the same spanning tree as my tools (maybe this is what you meant about matching topology to physical linkages?) and we have a solution for that in annotated graphs. I didn\u2019t see any proposals, but I would expect that a \u2018loop\u2019 element, or something similar would close the loop and all tools reading \u2018joint\u2019 elements would just naturally see the spanning tree that is the \u2018best\u2019 from the users view.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Switching existing tools that use URDF over to SDF would require a large effort. Tons of tools/libraries/etc use URDF, and switching would be non-trivial.", "Switching to SDF doesn\u2019t provide most of the things that I want out of a robot data exchange format", "As ", " and ", " have pointed out, TF and associated tools need to have a tree in order to work. There are solutions (provide a way to specify a root node for the tree, or to specify the entire tree), but my feeling is that this is only the first of many edge cases that would be run into.", "The existing tools for taking a robot described by a URDF, converting it to SDF, and spawning it in gazebo work \u201cwell enough\u201d for me.", "This isn\u2019t just a question of urdf/sdf. On top of those I have an SRDF for moveit, yaml files for describing other robot-specific formats that aren\u2019t in any spec yet, etc."], "url": "https://discourse.ros.org/t/urdf-ng-ros2-urdf2-discussion/511"}
,{"title": "[TB3] How to leverage the Intel ROS Project", "thread_contents": ["I brought up the Intel ROS Project and don\u2019t know how to test it, port it to the TB3, or leverage it.", "\n", "I installed the Intel ROS Project because the diagram on it home page (see link above) suggested I should be able to use it to do interesting things. It also assures that the Movidius and RealSense APIs work well together. It was difficult to install because of the number of packages and instructions were written using different documentation, installation and test conventions. But I managed to complete that install after a couple false starts.", "In the following paragraphs I document installation and test status to provide confidence that the install is promising. I am currently stuck. I don\u2019t know what the next steps are.", "I executed the following in separate terminal windows. They executed without crashing and I could display an interesting rqt_graph.", "rviz displays \u201cNo Image\u201d in its Image window. Clearly I have a publisher/subscriber message mismatch. My /camera/realsense2_camera node is publishing /camera/realseanse2_camera_manager/bond. I see nothing in the rqt_graph subscribed to this message.", "/moving_object subscribes to the /object_analytics/tracking message from /object_analytics and it also subscribes to /tf_static from /camera. This demonstrates that a lot is working. rqt_graph image below.", "QUESTIONS", "The rqt_graph indicates that I have a fundamental disconnect about how to integrate the capabilities I have installed. Where do I go from here? Is there an example implementation and description that I can use to integrate all this into a proof of concept?", "I think rviz is supposed to display the output of the RealSense camera. How do I make that happen?", "I don\u2019t have a well defined project. I\u2019m having fun learning something new.", "My current setup supports Kobuki. I have a tb3. What messaging capabilities must I port to tb3 to start that process? Does this question even make sense?", "Support appears to be migrating to ROS2 more quickly than I expected. I am hoping to get a basic understanding of the ROS Intel Projecting using ROS1 before moving to ROS2. The transition will probably take me several months so I am getting this project to work in ROS1 will provide the reenforcement I need while I work on that transition. I want to see the power of the ROS Intel Project to motivate me. Is that reasonable?", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a forum which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Movidius and RealSense ROS demos work.", "I had to install the Kobuki message package before I could complete the ROS Moving Object install: ros-kinetic-kobuki-msgs.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/tb3-how-to-leverage-the-intel-ros-project/8655"}
,{"title": "LIDAR of Turtleblot3 starts spinning once power on?", "thread_contents": ["Just wondering is it normal that the LIDAR starts spinning when I switch on TB3???", "In my understanding, the LIDAR should start spinning after we roslaunch turtlebot3_bringup turtlebot3_robot.launch\u2026", "Anyone face the same condition?", "Hi,", "Since October, 2017, the LIDAR which comes with the TurtleBot3 gets the firmware that makes automatically run from booting on. It is not a malfunction.", "Look here:", "and hopefully (as Tully always says):", "The TurtleBot category here on ROS Discourse is the right forum for general discussions.", "For questions with direct answers or debugging please use ", " with the tag turtlebot or turtlebot3 with waffle or burger depending on your model.", "There are guidelines for asking questions at ", "Thanks", "Leon", "Thanks Leon for your reply and the reminder of ", " ^^", "hello, I am also having the same problem. have you solved this problem? if you solved please help me to get out of this\u2026", "\nUnfortunately, spinning the LDS with power up is an inherited feature of the sensor firmware itself since 2017.", "ok.", "\nDoes turtlebot3 needs host computer?", "\ncan you help me to do without host computer if possible\u2026", "Generally if you are going to need a machine to ssh into the turtlebot and kick-off a launch file. I would suggest that if you have general turtlebot questions you ask Robotis directly or use ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["#include <hls_lfcd_lds_driver/lfcd_laser.h>", "#include <std_msgs/UInt16.h>", "\n", "namespace hls_lfcd_lds", "{", "LFCDLaser::LFCDLaser(const std::string& port, uint32_t baud_rate, boost::asio::io_service& io)", ": port_(port), baud_rate_(baud_rate), shutting_down_(false), serial_(io, port_)", "{", "serial_.set_option(boost::asio::serial_port_base::baud_rate(baud_rate_));", "\n", "// Below command is not required after firmware upgrade (2017.10)", "boost::asio::write(serial_, boost::asio::buffer(\"b\", 1));  // start motor", "}", "\n", "LFCDLaser::~LFCDLaser()", "{", "boost::asio::write(serial_, boost::asio::buffer(\"e\", 1));  // stop motor", "}", "\n", "void LFCDLaser::poll(sensor_msgs::LaserScan::Ptr scan)", "{"], "url": "https://discourse.ros.org/t/lidar-of-turtleblot3-starts-spinning-once-power-on/3332"}
,{"title": "[TB3] TurtleBot3 with Laser Distance Sensor (LDS)", "thread_contents": ["Hi everyone ", "\nTurtleBot3 release come just next month.", "\nAre you curious about the LDS on the TurtleBot3?", "LDS can be rotated 360 degrees.", "\nIt is a key part in create slam and navigation.  even create 3D map!!!", "\nAnd less than half the price of a lidar sold on the market. But the functions are almost the same.", "You can see it soon. ", "\nThank you ", "This is the cheapest 360 degrees LiDAR in the world I know. ", "\nFor more information; Please see the detailed spec on TurtleBot3 wiki.", "\n", "The design reminds me of the RP-LIDAR (", ").", "Can you comment on expected uptime? E.g., if I begin to operate the sensor and do not stop, how long before the device may need to be rebooted?", "I am motivated to ask because I noticed recently that after approximately 3 days of continuous operation of an RP-LIDAR that I have, the motor stopped and would not return to operation until after I cycled power. I plan to conduct a more thorough study of product lifetime and durability of the RP-LIDAR.", ", do you have any information on the cost?", "I think it is hard to expect long operation life in low-cost LiDAR.", "\nI will ask the engineer about the expected time for the operation time. ", "After the launch of TurtleBot 3, the products for each accessory are going to start selling in June.", "\nThe price is still undecided, but we think it is under $150. ", " did you get any data about expected lifetime under continuous operation?", "The following attachments include the contents like basic performance, measurement performance, mechanism layout, optical path, data information, pin description, command. But, I can\u2019t find information about the lifetime from the manufacturer.", "14.56 KB", "Ask you engineers what is the mean time between failures (MTBF) data they should have and hopefully share.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-turtlebot3-with-laser-distance-sensor-lds/1644"}
,{"title": "Waffle - HDMI no signal", "thread_contents": ["Hello,", "I need help installing Ubuntu on TurtleBot3 Waffle.", "\nI flashed Joule BIOS with version 193 (since 1J2 didn\u2019t worked and seemed a lot of people had trouble with it).", "\nLooked good :", "However after that I can\u2019t get any screen with micro HDMI-HDMI cable, still no signal.", "\nI plugged power supply one openCR, keyboard, mouse and USB with Ubuntu on USB hub (which is connected to the Joule). All leds are green, plus the white/blue one.", "Any suggestion ?", "\nThanks", "Hello r00t,", "\nThe below link is the method that worked for us to recover the joule.", "\nIt seems like the sequence of connecting and disconnecting cables do matter.", "\n", "Thank you for your reply.", "Tried everything, still can\u2019t get it working.", "\nFirst connecting type C USB, first connecting power supply, waiting 10min first boot before connecting HDMI, really everything. With both bios version 193 and 1J2. Even re-done exactly like in the video tutorial. Did it with two different Joule board (we got three of them), same thing.", "Flashing is always fine, but can\u2019t get that output working.", "Anyone with an other idea ? (Hardware is fine, test moves worked)", "According to McCool, sometimes specific monitors would not connect to the Joule.", "\n", "\nWere you able to connect them with your current monitor before updating firmware?", "Yep, it was working before updating\u2026", "I\u2019m trying to reproduce the problem with my Joule and if I figure out what the problem is, I\u2019ll share the solution for it.", "Well, I took my chance installing directly the \u201cAlternative Ubuntu for Joule\u201d on a second Joule not flashed (since this way I did have acces to BIOS to boot on USB), and it works properly\u2026 (ROS install and turtlebot teleop working with no problem)", "\nSeems weird 'cause there must be a reason the guide tells us to flash before doing it, but i am not complaining, it works.", "I\u2019ll keep trying make the first one work, and come back to you if I find a solution.", "Thanks ", "Hello r00t,", "As far as I understand, BIOS 193 is required in order to install Ubuntu with a USB flash drive. Since it was one of requirements from Ubuntu Developer(", "), I didn\u2019t doubt about updating the BIOS firmware.", "\nAt least you figured out your solution with the second Joule which is definitely a good news ", "\nPlease let me know if your second Joule has different BIOS version other than 193(", ") so I can also try with the same version.", "\nCheers!", "I was able to use joule-firmware-2017-06-26-1J2-public, specifically the debug bin file.  My initial flash with the release file would not boot, nor display anything on screen.  With the debug bin the board does run, although bootup times are very slow.", "That said, the Joule board has proven to be unstable.  I\u2019m unsure on the root cause, be it BIOS, Ubuntu, or hardware, but since Intel has discontinued the board I don\u2019t expect much in the way of software improvements.  At present, I\u2019ve swapped over to a Raspberry Pi, and have an Odroid XU4 which will eventually end up in the Turtlebot.  I\u2019ll go back to the Joule at some point, but it\u2019s not a long term solution for development.", "Hello,", "\nI got the same problem. The board was displaying the bios using hdmi but when I flashed bios version 193 I could not get hdmi output anymore. The flashing process didn\u2019t give any errors. I also tried newer bios versions with same negative results.", "\nI can connect to the board using serial connection. It seems to recognize the display when I execute xrandr -q. If I reboot the board it displays the whole boot sequence. These are the last commands before stopping:", "[   12.378641] Bluetooth: RFCOMM ver 1.11", "\n[   12.820056] HDMI HDA Codec ehdaudio0D1: HDMI: failed to get afg sub nodes", "\n[   12.827761] HDMI HDA Codec ehdaudio0D1: Failed in parse and map nid with err: -22", "\n[   12.836166] HDMI HDA Codec: probe of ehdaudio0D1 failed with error -22", "It seems to be some sort of hdmi error. Any idea how to correct the issue?", "\nThanks", "Just a data point.", "\nMy Joule running 193 works with my Dell and Samsung monitors, with a generic type A to type A HDMI cable, and generic type A to type D adapter.", "\nIt won\u2019t work with my Elecrow monitor, its included type A to C cable, and the above generic type A to type D adapter", "\nIt will work with my Elecrow monitor, its included type A to C cable, and a Rocketfish type A to D adapter.", "Ed", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Downloading BIOS", "Clearing NvStorage\u2026"], "url": "https://discourse.ros.org/t/waffle-hdmi-no-signal/2687"}
,{"title": "Kobuki base external power supplies: can be set OFF by default?", "thread_contents": ["When you switch on the robot, the external power supplies already provide current, until you publish OFF state on mobile_base/commands/external_power topic.", "Can be somehow disabled by default on robot switch on and then enabled by publishing ON on mobile_base/commands/external_power topic?", "Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/kobuki-base-external-power-supplies-can-be-set-off-by-default/271"}
,{"title": "Turtlebot3 - Joule stalled", "thread_contents": ["Hi", "Turtlebot 3 - Waffle \u2026 Joule - after installing Joule following the official documentation (Ubuntu 16.04 LTS) : everything works fine\u2026problem: Joule stops working/running after approx 10min up - independently if operated on 12V/3A external power supply or with the battery provided with the Turtlebo3 kit. Logs don\u2019t show any weird symptoms\u2026", "Any ideas? pls help\u2026", "Tnx in advance", "\nml", "First thing to check, what is the temperature doing? Is the module overheating?", "Also, does anyone know what the replacement for the Joule is, now that they are EOL?", "\nThanks ", "Tnx Ilia for the hint\u2026checking continously temp via /sys/class/thermal zones 0-7 - max. 45C \u2026 assuming that isn\u00b4t the root cause. Keep on digging\u2026Best ml", "\u2026quick update: if you push the Power button (SW2) the \u201cstalled\u201d system recovers - even the Unix sessions\u2026", "\n\u2026did I miss something here - kind of \u201csleep mode\u201d /   hibernation mode - some BIOS settings?", "Interesting.  Our bot was doing the same thing, with symptoms just like a software crash, so that\u2019s what I assumed.  Maybe it is going into a sleep mode of some sort.", "I experience the same stalling that you describe.  Have you managed to figure out a workaround?", "Hey ashoed,", "unfortunately not - tried all 3 BIOS versions, including the recommended ", " \u2026 same negative results.", "Any luck on your side?", "Best ml", "Hi Michael,", "Have you checked to see if there are settings within Ubuntu 16.04 for screensaver or power savings?", "You should also check if you can update to the latest Linux kernel.", "If you can type  ", " in the terminal on your Intel Joule and post the output, I may be able to load up my Intel Joule with your versions and check to see if I encounter the same issue.", "Please keep us updated on any other efforts you may try.", "Hi MyNameIsCosmo,", "tnx for your support\u2026some progress but not 100% done yet:", "all updates based on \u201capt-get update\u201d installed", "HDMI not working - need to work with cli", "uname -a:", "\nLinux turtle 4.4.0-1000-joule #0+joule21-Ubuntu SMP PREEMPT Thu Mar 16 14:46:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux", "following your hint \u201cpower saving\u201d -> focussing on Ubuntu not Joule I did the following:", "sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target", "-> unix processes will not get stopped/suspended anymore -> seems ok !", "\n-> issue: every 30min the network manager gets a request to \u201csleep\u201d (syslog) -> wifi connection goes down", "\n-> can\u2019t restart it automatically (service network-manager restart -  nmcli network on - ifup -a)", "No clue what causes the request for network manager to sleep\u2026given that it is reproducible it seems a \u201cfeature\u201d. Any idea how to tackle it? Seems I didn\u2019t disable all \u201cpower saving\u201d options (or what soever causes the nm shutdown\u2026) - can\u2019t find anything on the net that fixed it\u2026", "Every hint/comment welcome\u2026", "SOLVED: guess the \u201cGUI\u201d environment triggered the request to sleep after 30min - given that I run the Joule headless: disabling \u201cGnome\u201d and \u201clightdm\u201d did the trick - system up and running for hours (didn\u2019t reverse the masking of sleep, suspend, hibernate, hybrid\u2026)", "\nThank to all of you for the hints - Best Michael", "Had the same issue, disabling gnome and lightdm did the trick! Thanks for sharing!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/turtlebot3-joule-stalled/2739"}
,{"title": "[TB3] TurtleBot3 Burger Assembly Video", "thread_contents": ["Hi all,", "I\u2019ve been planning to post this assembly video[43minute] from long ago and today is the day!", "\nSometimes reading assembly manual seems hectic and often confused.", "\nI have to confess that connecting power pin to the Raspberry Pi 3 board required me some googling :\u2019(", "\nAlthough this video cannot be a perfect manual, saving at least one Raspberry Pi board would be awesome.", "Also, there is an amazing series of ", " recorded by Michael Overstreet so go ahead and check his videos as well.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-turtlebot3-burger-assembly-video/2340"}
,{"title": "OpenCR and encoders", "thread_contents": ["I want to use my OpenCR board to interface with two quadrature encoders for odometry and wanted to get your thoughts on different approaches.", "I did some searching and the STM32F7 is capable of reading quadrature encoders (I\u2019m not sure how many yet) but this may conflict with the current firmware settings on the board.", "\nAnyone know if the Low-power timer (LPTIM1) is being used for anything?", "\n", "My other option would be to use external counters and interface to them via SPI\u2026", "\n", "Other methods I should consider?", "I\u2019m leaning towards the external counters but  is there anything I\u2019m missing that would interfere with the above shield?", "Any suggestions would be greatly appreciated!!!", "Have you looked at the Turtlebot3 source? You can find it in the OpenCR code. The turtlebot3 uses encoders for odometry. It uses dynamixel\u2019s sdk, which you can definitely find the source for.", "I have slowly been working my way through the source, but haven\u2019t made it to the odometry section yet. The Dynamixel motors are \u201csmart\u201d servos and perform their own position/velocity control and just report this info back the the Opencr board.", "Sorry I wasn\u2019t very clear with my plans. I want to use the Opencr board exactly as the TurtleBot3 does, but swap out the Dynamixel\u2019s for larger motors with encoders. This will require the Opencr board to take on the servoing of the wheel motors and calculate odomerty on its own. I feel comfortable modifying the source to do this but wanted to see if anyone had advice or suggestions on adding encoders to into the mix.", "I have experimented with the STM32 processor and did want to use it\u2019s builtin ability to read quadrature encoders.   I had to give up because of a conflict over timber use.", "But I did find the CPU has zero trouble at all with VERY high interrupt rates  I have four motors each spins up to about 11,000 PRM and has 64 pulses per revolution.   The processor can handle the maximum rate and still do quite a lot else.  that would be (4 x 11,000 x 64)/60 interrupts per second.    That said the interrupt handler in C++ is very short and fast.", "You can further speed  up the  processing by a large factor if you remember the last time the motor changed directions and then you can ignore that the encoder is using quadrature.  And your handler becomes \u201ci = i + direction\u201d  Just one statement.", "So the simplest way to to do the encoder handing inside an interrupt handler in software.", "But if you can use the STM32 counter hardware that just seems so much more efficient but in my case I ran out of timers", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/opencr-and-encoders/2335"}
,{"title": "Turtlebot 3: Successfully upload Alternative Ubuntu Desktop 16.04 to Joule?", "thread_contents": ["When I set Turtlebot 3, I met some problems:", "\nIt is very hard to upload the Ubuntu image to Intel Joule board. I am not able to get my PC to connect to Joule board through USB-C successfully. And After that I am not able to upload Ubuntu image to Joule board successfully. It is very hard to find any useful info online regarding these issues.", "After some study, I finally got it work. Here I would like to share with you some critical info.", "1, Connect your PC with Intel Joule board via USB-C connector.", "\nBefore doing anything, we need to make our PC recognize and connect to Joule board. We should see \u201cIntel DnX device\u201d under \u201cUniversal Serial Devices\u201d in Device manager. If not, you have a problem.", "\nIf you get it work by following Intel\u2019s description. It is great. If not, it is normal. Here is what you need to do:", "\nAt first, follow the link ", ".  User \u201cvraoresearch\u201d provides a solution. (Thank you, vraoresearch.)", "\nIf you still cannot get it work, here is my suggestion:", "\nStep#1, disconnect 12VDC power supply.", "\nStep#2, Connect PC to USB-B of the Intel Joule board, if you want to see how Joule runs through UART terminal.", "\nStep#3, In Device manager in your PC, select \u201cShow Hidden Devices\u201d. And choose \u201cUninstall device\u201d on all failed devices.", "\nStep#4, Connect PC to USB-C of the Intel Joule board, at the same time press \u201cDnX\u201d button on the Joule board. Wait for PC to detect the DnX device and then release the button. If it does not work, try it multiple times.", "2, Upgrade the BIOS image to Intel Joule board.", "\nThere is a bug in the file DNX/Flash.bat in the downloaded and unzipped Joule BIOS image. \u201cclearRpmbFlag\u201d needs to changed to \u201cClearRpmbFlag\u201d. And its value should be defined as \u201cTRUE\u201d", "When running Flash.bat, you should see the following output:", "\nClearing NvStorage\u2026", "After this, you can disconnect PC from USB-C of Joule Board, and connect the 12VDC power supply, the board should be able to boot up properly, Then you go ahead to follow the description to use a USB flash drive to upload the Ubuntu image to Joule board.", "By the way, the bug I mentioned is found in the version 1J2 of Joule BIOS image. Since Intel discontinued Joule. I believe this is the last release of Joule BIOS image.", "Hi rknlhrqy,", "Thank you so much for sharing your method for solving Joule\u2019s USB-C issue when updating BIOS!", "\nI\u2019ll create a suggested solution link to your post from TurtleBot3 wiki FAQ page if you don\u2019t mind.", "Sincerely,", "\nWill", "Hi Will,", "\nNot at all. It is my great pleasure.", "\nThank you.", "\n\u2013Kening", "I\u2019m sooooooooooooooooooooooooooo happy \u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c  you are SOOOOOOO genius!!! I have suffered for a long long long!!! time. Because I couldn\u2019t enter the BIOS even after installing the BIOS successfully\u2026 At first I thought that the connection method of my HDMI wire would be a problem, but when I tried to connect the serial port with putty, I could not enter the BIOS too!!! I searched all the questions related to the bios of the joule board to find the answer. \u201cjoule Bios not boot\u201d, \u201cjoule board putty serial bios\u201d, \u201cjoule hdmi not output\u201d etc\u2026 For a very very!!! long~~~~~~ time \u2026 and I was able to solve the problem through \u201c\u201d\"\"\u201cyour answers\u201d\"\"!!!. My BIOS was the latest version, and Flash.bat was False. After I changed this to True, I was finally!!! free from all the problems. I can sleep well now. It\u2019s all for you. I really love you!!!", "Hi kingbob,", "\nI am thrilled by your email.    Thank you very much for the nice words.", "\nI am so glad that it did help people.", "-Kening", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/turtlebot-3-successfully-upload-alternative-ubuntu-desktop-16-04-to-joule/2224"}
,{"title": "Workshop on Security and Privacy in Robotics - Call for contributions", "thread_contents": ["========================================================================", "========================================================================", "IMPORTANT DATES", "Submission deadline: March 31, 2020", "Notification: April 15, 2020", "Camera-ready deadline: April 25, 2020", "Submission site: ", "SCOPE", "The workshop on \u201cSecurity and Privacy in Robotics\u201d addresses recent security", "\nand privacy challenges with the robotic systems. The trend of integrating", "\nrobots with information and communication technologies (ICTs) such as cloud", "\nservices and IoTs has imposed cybersecurity risks. Interdisciplinary", "\napproaches that bridge cryptography, communication networks, data sciences,", "\ncontrol systems, and robotic operating systems will be examined and discussed", "\nto address the emerging challenges. This workshop will gather experts in this", "\nemerging area of research and provide a perspective on relevant challenges", "\nand opportunities for the academia and industry. The workshop will focus on", "\nrecent advances in areas such as ROS security, cloud robotic security, and", "\nprivacy issues in robotics. It will give the audience an overview of the", "\nsystematic security solutions to robotic systems in these areas and provide a", "\nplatform to discuss new research directions. Conclusions will be summarized", "\nin a report released to the community.", "TOPICS OF INTEREST INCLUDE (BUT ARE NOT LIMITED TO)", "SUBMISSION GUIDELINES", "The (WiP) session provides an opportunity to present and discuss", "\nnew challenges and visions, showcase early research results, and", "\nexplore novel research directions. The specific aim of the WiP", "\nsession is to provide a forum for timely presentation, discussion", "\nand feedback for novel, controversial, and thought-provoking ideas.", "Demonstration submissions should list any special requirements", "\n(tables, power, wireless connectivity, etc.) n a separate page to", "\nthe submitted abstract (see the instructions below). This page is", "\nnot part of the technical content of the abstract, can be formatted", "\nat the discretion of the authors.", "Authors are requested to submit original, unpublished abstracts of", "\nno more than 500 words or 2 pages of extended abstract. All submissions must be", "\nin PDF format and uploaded through the online system", "This workshop aims to addresses recent security and privacy\u00a0challenges with the robotic systems. The trend of integrating robots with information and communication\u00a0technologies (ICTs) such as cloud services and IoTs has imposed...", "The submitted abstract describing the work or demo will be", "\nevaluated based on technical merit, innovation, and the potential", "\nto stimulate lively discussions at the conference.", "For any further questions on the submission or workshop format,", "\nplease don\u2019t hesitate to contact us (email addresses given below).", "WORKSHOP ORGANIZERS", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "Robot security architectures", "Secure deployment of robotic systems", "Accountability", "Safety and Security modeling of robot systems", "Lessons learned from practice", "Demonstrations of practical solutions", "Cloud operated robots", "Cryptography and Applications in robotics", "Cross-disciplinary topics", "Resilient robotic systems", "Security of multi-agent robotic systems", "submission of work-in-progress", "submission of demos", "Quanyan Zhu, New York University, USA, quanyan.zhu@nyu.edu", "Stefan Rass, Universitaet Klagenfurt, Austria, stefan.rass@aau.at", "Bernhard Dieber, Joanneum Research, Austria, Bernhard.Dieber@joanneum.at"], "url": "https://discourse.ros.org/t/workshop-on-security-and-privacy-in-robotics-call-for-contributions/12780"}
,{"title": "Lecture notes specifically for \"Programming Robots with ROS\"", "thread_contents": ["For better or for worse I\u2019ve concluded that the best \u201ctextbook\u201d for teaching ROS to Python developers is \u201cProgramming Robots With Ros\u201d by Quigley, Gerkey and Smart. It is a few years old and uses Indigo. Many of the code and other instructions don\u2019t work exactly as written. But the sequence of explanation and examples is really excellent.", "My question: have any of you developed course notes, powerpoints or similar to go with that particular book that you\u2019d be willing to share? I am starting to write some myself to see how it goes and thought I would reach out.", "If you see this book, you may change your mind", "\n", "Hi everyone, \ud83d\ude42  I\u2019m happy to announce a new ROS book: \u201cROS Robot Programming, A Handbook is written by TurtleBot3 Developers\u201d. Now, this book has been published English and Chinese versions. You can download the pdf of this book.  The authors of the...", "\n    ", "\n", "I\u2019ve looked very closely at that book and its quite good. However it uses only C++ while I think for teaching purposes python is much better suited.", "For teaching students they should be well versed in both C++ and Python, and perhaps any other language tricks of the trade. It would be doing them a disservice as with ROS in my experience I actively use both, all the time, sometimes both in the same day to achieve a goal.", "I have been teaching ROS with C++ at least two times at Universities, and it was clear that students struggled more with C++ syntax and build requirements that with ROS concepts.", "\nLast ROS courses I used Python with student with zero background on Python and it was much more easier for them to grasp faster ROS concepts.", "\nSo, I think, unless you have students with good background in C++, it will be painful for them.", "\nIt seems with new C++ 11 supported in ROS2, programming in C++ with ROS will be less tedious.", " I totally agree and have reached the same conclusion. Our students know Java and Python (and many know additional languages. But C++ is a rare bird.) I do agree that to get a \u201cjob\u201d in the ROS world you probably are going to need to know C++ but for teaching Python is the way to go.", "(Anyway I have no desire or intention to start a debate on this. Everyone has their own experiences.)", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/lecture-notes-specifically-for-programming-robots-with-ros/5729"}
,{"title": "Claiming maintainership for ros-perception/{pcl_msgs, perception_pcl, ...}", "thread_contents": ["Hi,", "Long time stalker of ros-perception, fairly new maintainer on image_pipeline here.", "There\u2019s an effort going on to get PCL items in ROS released that have stalled out mostly from PRs going un-reviewed and unmerged.", "Discussion here", "\n", "\n", "Examples", "\n", "ROS package containing PCL-related messages . Contribute to ros-perception/pcl_msgs development by creating an account on GitHub.", "\n", "\n", "\n", "While I could ask for just maintainer access to these repos, in practice I want to take on some maintainership of the larger ros-perception ecosystem as I have backgrounds in nearly all the packages there and I\u2019m taking the attitude ", ". I\u2019m not necessarily promising that I can port everything, but I am committing to getting reasonable PRs submitted reviewed and merged, particularly as relates to ROS2.", "So I\u2019d like to request to be added to the ros-perception org with write access. Any projects with active maintainers I don\u2019t plan on getting in the way.", "Pinging here, as I\u2019m trying to release PCL msgs into Dashing / Eloquent. ", " can you bump the versions on the ", " branch in pcl_msgs so I can continue with the release here (", ") in the meantime while I\u2019m waiting on this to be discussed above?", " We want to let the maintainer of pcl_msgs have a chance to respond and/or do the releases themselves.  In this case, it looks like you\u2019ve already opened ", ", so we\u2019ll see if the maintainer responds and go from there.", "Sounds good!", "(Something something 20 characters)", " pinging back, its been 2 weeks. We want that PR into pcl perception and release the messages for eloquent.", "Update: Awesome we got pcl_msgs shipped thanks to ", ". Off to perception_pcl", "Steve", "Hi ", ", as I\u2019m sure you\u2019ve picked up, my attention span for perception_pcl maintainership has been low. I\u2019m not spending much time on perception ", " ROS2 at the moment. If you\u2019ve got the time and motivation to help review and clean up the ROS2 porting/releasing/etc, I\u2019d be happy to tag you in.", "Sure thing, I\u2019d like to help get this out!", "Could the powers that be (", "?) please add ", " to ", "?", "Done! Happy maintaining!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/claiming-maintainership-for-ros-perception-pcl-msgs-perception-pcl/10813"}
,{"title": "Information for those using Raspberry Pi, Ubuntu Xenial, and ROS Kinetic", "thread_contents": ["This is \u201chopefully\u201d a point in time statement:", "The following refers to images from this source: ", "For those who are using or plan to use the Ubuntu 16.04 Xenial server builds for Raspberry Pi 3 (and 2), it is important to immediately disable ", " and avoid ", ".", "There is a temporary solution to prevent a system from upgrading critical kernel files: ", "There are known issues with various patches which have been introduced after 1034. These patches make the Raspberry Pi unstable - primarily during the boot process. The result is the Raspberry Pi will sometimes freeze during boot. Several re-attempts (power cycling) and the system may eventually boot. Occasionally even this is insufficient.", "By installing the original images and not permitting upgrades of core kernel files, the RPi + Ubuntu Xenial + ROS Kinetic is stable.", "Background:", "Launchpad thread: ", "[spoiler]", "\nUPDATE: 2017-02-15: There is a better workaround available. Rather than use the \u201chold\u201d feature, users may edit the config.txt and change the device_tree_address (", "). The result should be the following lines:", "\n", "Even with this improvement, completing apt-get dist-upgrade will break wlan0.", "UPDATE: 2017-02-16: The config.txt has been repeatedly reported to work; however, I get boot issues.[/spoiler]", "UPDATE 2017-02-17: The update from the 15th has proven unreliable. The suggestion of using a non-server version from ubuntu-pi-flavor-maker is proving more promising.", "UPDATE: 2017-02-15: There is a better workaround available. Rather than use the \u201chold\u201d feature, users may edit the config.txt and change the device_tree_address. The result should be the following lines:# set extended DT area    #device_tree_address=0x100    #device_tree_end=0x8000    device_tree_address=0x02008000", "Just to be clear, we would modify the ", " file to comment out the old device_tree parameters, then only set the ", " to be ", ".", "In the launchpad thread you linked to, there was some references to ", ":", "\n", "\nDid you try those yet?", "\nDoes say the ubuntu-mate-16.04.2-desktop-armhf-raspberry-pi.img encounter that same issues?", "\nI\u2019m going to try it with ROS in a bit.", "I did not attempt the alternate image because of the statement at the end of the thread \u2026", "Shuhao (shuhao) wrote on 2017-02-13:\t", "\nThe images listed on ubuntu-pi-flavour-maker and the one listed on ", " is exactly the same. The same issue should exist on both systems.", "The non-server ubuntu-pi-flavor-maker builds are built using a different process, and don\u2019t seem to have this issue. They come with rpi-update and can easily run the RPi \u2018mainline\u2019 kernel (not the Ubuntu fork).", "I have been using lubuntu-16.04.2-desktop-armhf-raspberry-pi.img.xz with no issues.", "Rohan", "Today I am getting undiagnosed issues so I will try using the desktop image and then disable the GUI to have a facsimile of a server image.", "[quote=\u201crohbotics, post:4, topic:1205\u201d]", "\nThe non-server ubuntu-pi-flavor-maker builds are built using a different process, and don\u2019t seem to have this issue.[/quote]", "I can confirm. I have a functioning instance of my LoCoRo project. I started with the Lubuntu image from ubuntu-pi-flavor-maker and disabled LXDE. Then treated like an Ubuntu server image. Everything worked as expected.", "The Ubuntu Bugs thread has demonstrated to me that \u201cofficial Ubuntu support\u201d for the Raspberry Pi is dubious at present and has an uncertain future. When the April drop of Ubuntu 17.04 occurs, things should be clearer one way or another.", "For ROS, it would be best for the community if it supported buildfarm installable packages for a \u201csupported Raspberry Pi operating system\u201d. This translates to Raspbian. I\u2019ve been trying to promote Ubuntu but I have to concede, its not an ideal situation for ROS.", "I have talked with Canonical about the problem of Raspberry Pi support. They told me the lack of work on the ubuntu raspberry pi is do to the focus on ubuntu core. I agree with their logic. There is little reason to run a full version of ubuntu on a RPI.", "I do see the attention to Ubuntu Core. This is why my previous conclusion was to suggest ROS to consider Raspbian or even one of the other fully supported Linux distributions for the Raspberry Pi audience.", "Here is where I see a divergence between ROS and Ubuntu as far as the Raspberry Pi is concerned.", "The majority of Raspberry Pi owners (and the focus of the Raspberry Pi org) is the experimentor, student, tinkerer, maker, learner, creator.", "Ubuntu Core appears to focus on IoT which is more of a commercial direction.", "Unless a lot more packages are constructed as Snaps, it leaves very little usefulness for a large segment of the Raspberry Pi user base. (You can\u2019t just \u201capt-get install\u201d on Ubuntu Core.", "Is ROS considering a focus on Ubuntu Core?", "My personal activity with ROS is more \u201cresearch\u201d and \u201ccreator\u201d and not IoT. None of the Taspbery Pi based projects I\u2019m involved in are looking at Ubuntu Core.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/information-for-those-using-raspberry-pi-ubuntu-xenial-and-ros-kinetic/1205"}
,{"title": "Using ROS in commercial OpenEmbedded environment?", "thread_contents": ["Can someone suggest resources (web links, blogs, books, etc\u2026) that discuss the implications of running ROS in a commercial product.  Specifically, a commercial product running a Yocto/OpenEmbedded derived custom Linux distribution.  Although, a ROS product based on a commercial distribution of Linux would likely have all the same issues.", "I realize there is a whole spectrum of issues that might be involved.  A few of which might be: handling ROS version issues, ROS system configuration, open source license management and compliance, build and testing management, and then the more nitty-gritty stuff that might be tweaked in packages to better support commercial use where customers expect things to just work without a lot of tweaking on their own.", "I have a feeling a lot of the service and industrial robots that are using ROS are solving these types of issues on their own, but I am wondering if there is a watering hole on the web where people concerned with these issues gather to share knowledge and best practices so ROS support from a variety vendors looks more unified from an end-customers standpoint.", "Thanks,", "Mike", "Hi Mike!", "I\u2019m neither ROS nor OpenEmbedded expert, but I\u2019ve spent some time working with both projects and my gut feeling is that there\u2019s maybe just a couple of companies using ROS on top of OpenEmbedded in production internally at the moment. I\u2019m pretty sure nobody builds real commercial products or something that goes beyond a research projects. And I know what it takes to build a product (In my previous life I worked on the integration system used to build Nokia N9).", "I guess the best way to find out what people are concerned about is to have a look at the programs of ROS (and ROS Industrial) conferences. You\u2019ll see that the majority of ROS users are researchers happy with Ubuntu as a baseline for ROS.", "BR,", "\nDmitry", " - this is something that I am interested in as well.", "Dmitry, thanks for the response.", "For good historical reasons, ROS seems strongly focused around Ubuntu as a base platform, but Ubuntu isn\u2019t really suitable platform for actual embedded products intended to work in a ROS ecosystem.  Its akin to placing an aircraft carrier nuclear power plant into a speed boat.  I\u2019m very happy and grateful to see that others had already blazed the trail on getting ROS working over OpenEmbedded as it at least sets a direction and makes the integration possible, even if it is not easy.", "I\u2019m just at the beginning of this process, but I\u2019ll be happy to share my experience with others as our development progresses on a commercial product based on ROS running on an OpenEmbedded-based custom Linux.  Hopefully I\u2019ll find others on a similar journey with which to compare notes.", "Mike", " I can not say much about ROS on OpenEmbedded. But can give you some advice on OpenEmbedded/Yocto for product development. We are using Openembedded for about 4 years for our camera firmware development. I kow none other system which provides you the services OpenEmbedded do.", "\nWe do automatic builds based on Jenkins and create full software update packages bundled with our application software.", "Best,", "\nChristian", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-ros-in-commercial-openembedded-environment/1428"}
,{"title": "Meta-ros Kinetic runtime issue with roscore", "thread_contents": ["Hey everyone,", "I\u2019m adding ros-meta to a Yocto/OpenEmbedded build of a Linux distribution for a custom Xilinx Zynq based camera board and it looks like I\u2019m running into some runtime dependency issues when running roscore.", "I\u2019m using the experimental kinetic distribution of ros-meta at:", "My bblayers.conf file looks as follows:", "Added the following line to by ", " file:", "Used the following command to create my own image.", "Build completed without any errors.  Created an SD card including the kernel", "\nand file system that includes ROS.", "On the booted Xilinx Zynq based system running the Linux environment I just built, I\u2019m configuring ROS Kinetic with the", "\nfollowing:", "I have the following problems running roscore.", "From what I can find on ROS Answers, it looks like defusedxml was as a fairly recent dependency added to ROS Kinetic.  I assume this dependency probably didn\u2019t make it into the meta-ros dependencies.  Any suggestions on how I go about adding it to the Yocto/OpenEmbedded build of the system?", "Thanks,", "Mike", "Hey Mike,", "Nice to hear that the \u201ccustom Xilinx Zynq based board\u201d is still going ", "Defusedxml is a python package that was added to facilitate adding security to ROS, as it provides protection against a bunch of different XML attacks. This is part of the ", " effort to add some security and hardening to ROS.", "Anyway to get roscore to run, have you tried adding ", " as a dependency here: ", "I have only done some looking around at OpenEmbeded stuff, and never got around to actually using it so I am not sure if this is the \u2018right\u2019 way to fix this, or if it would even work.", "Out of curiosity what is the size of the image that comes out the build process? I would think it would quite small relative to a Ubuntu image.", "Rohan", "Hey Rohan,", "I resolved this issue by adding the following recipe for defusedxml to:", "The contents of the recipe is:", "Then adding a dependency to roslaunch_1.12.2.bb for this python module.", "The following commit is a complete fix.", "The resulting image file for the entire Linux OS including roscore is about 47MB in size which is considerably smaller than an Ubuntu image.  However, this is for the absolute minimum user tools and ROS install.", "Mike", "Hi Mike,", "I am interested in doing something similar using a Microzed, but honestly don\u2019t know where to even start, can you point me to some good resources, so I might be able to do the same?", "Thanks!", "Hi Qnetjoe,", "I would propose that you start as follows:", "Get an understanding of yocto and bitbake following the Yocto Quick Start:", "\n", "\nAfter stepping through that document, you should be able to build and start a small Linux image in the qemu emulator.", "Build and run an image with ROS for the qemu emulator following the instructions at:", "\n", "Build and run a minimal image for the Microzed board, using the microzed configuration provided", "\nin the meta-xilinx layer at:", "\n", "\n", "\nBy the way, I found this by searching on ", ",", "\nand it quickly linked me to:", "\n", "Combine what you learned in Step 2 and Step 3, and build an image", "\nwith ROS for the Microzed board. When combining, you will probably need", "\nto choose the build configuration that is a compromise between the build", "\nconfigurations you used in step 2 and in step 3. This will then probably need", "\ncertain adjustments in meta-ros.", "Here some further advice:", "Get a build machine with a lot of CPU power, RAM and a SSD disk to", "\nbuild images much quicker. You will not get away with some laptop", "\nand some Linux virtual machine. when bitbaking the images.", "Report issues you encounter on the meta-ros github issue tracker. Please", "\nalways provide at least the build configuration you use, and possibly even", "\nthe bblayers.conf file, so that others can reproduce the reported issue.", "I hope this helps.", "Lukas", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/meta-ros-kinetic-runtime-issue-with-roscore/1407"}
,{"title": "TechRepublic: The hottest thing in robotics is an open source project you've never heard of", "thread_contents": ["Commentary: The Robot Operating System (ROS) doesn't get a lot of press, but it increasingly powers the robots upon which industrial automation and other functions depend.", "With discussion at ", "Never heard of that thing\u2026 what is it?", "Yes Loy, what are you on about? What is a ROS?", "\nOh and b.t.w. Thilo:", "The Robot Operating System (ROS) doesn\u2019t get a lot of press,", "Looks like you should be even more active on all forms of social media than you already are ", " haha", "Good questio\u2026", "What is ROS?", "even better question, how come its so widely used, yet the confrences are so small\u2026 last year ROSCON japan and france, didnt attract 300 professionals combined\u2026?", "If its so common among robotic profesionals, why so little attendance?", "Both ROSCon JP and ROSCon FR are local conferences for their respective countries, dealing with a still rather niche area (open-source robotics software), so it is not surprising that they don\u2019t have a huge attendence.", "The global ROSCon attracted nearly 600 people last year, which is similar to the number of attendees that PyCon was attracting 10 years ago. I think this is an impressive number given that the field of potential attendees is smaller (I think there are probably far less robotics engineers in the world than Python programmers).", " Conference in Stuttgart, Germany, could also attract \u201conly\u201d 150 people in ", " and ", " respectively. Such events are just very technical and (so far) we do not attract much more people. But who knows, maybe we need to book the ", " (football stadium) in 2030 for the event \u2026", "\u201cROS\u201d? But it is not a robot nor an operating system. who names this stuff?", "Varies by region? ", " had 570 developers, Taiwan ROS Summer School had 200.", "\n", "The summer schools in China get hundreds of attendees every year as well, seemingly without effort.", "Of course there is a difference between knowing a  conference\u2019s subject, knowing that there is a conference and actually attending a conference. If I had the time & money I\u2019d visit much more conferences than I currently do.", "\nMy guess is that since the Python community is so much larger than ROS\u2019s, ROSCon has a much higher turnout ratio.", "I didn\u2019t meet anyone in the robotics field who didn\u2019t hear of ROS, even if they don\u2019t use it (maybe my pool of samples is just too biased though :P). However, as Geoffrey pointed out, robotics is still a growing, but comparatively small field. Sure, IROS/ICRA grew a lot in the last 10 years, but just check out computer vision conferences since 2012\u2026", "And honestly, I am thrilled by these consistently high numbers of attendees at worldwide ROS-related events. In 2007 we organized a Player summer school in Munich, when Player was already quite established, and I don\u2019t remember there being this many & large events. Such a list was considered long back then: ", " (maybe now a \u201ccould be using ROS but don\u2019t\u201d list would be shorter :P). Then in 2010 we organized a ROS Fall School, where it was still about getting people on board. Then ROSCon started in 2012, and now there are regional conferences, tons of events, etc.", "The title is just clickbait for non-roboticists, in the field ROS does not need an introduction, which is amazing progress! Keep up the great work ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/techrepublic-the-hottest-thing-in-robotics-is-an-open-source-project-youve-never-heard-of/12229"}
,{"title": "ROSCon JP 2019 report", "thread_contents": ["The second officially licensed Japanese ROSCon event, ROSCon JP 2019, was held in Tokyo, Japan on the 25th of September. ROSCon JP 2019 was, like ", ", held in conjunction with the Open Source Robotics Foundation.", "We sold over 210 tickets, and had 204 participants on the day (excluding sponsors\u2019 invitations and staff), a small growth over last year. The livestream again had a steady 60 to 70 people watching at any one time. Thanks to all the participants, our sponsors and their staff, and most of all the excellent event staff who made sure things Just Happened", ", ROSCon JP was an excellent event for all. Presentation slides and videos will be published on the website in a few weeks.", "Everything at the event, including the invited talks, the submitted talks, the lightning talks, and the exhibitors\u2019 booths, was of even higher quality than last year and showed a level of committment from the local ROS community and sponsors that we can only say makes us, as organisers, very happy to be a part of this community. We consider the event to have been a massive success, building on the success of our first event last year, and feel that the Japanese ROS community has now established a solid foundation on which to grow the event to provide an enjoyable venue for the ever-growing community to come together.", "This year we tried a new addition to the programme, and gave a full-day tutorial on ROS 2 and navigation2 for 51 participants. The morning was taken up by learning the basics of ROS 2, including how to write nodes and use topics, services and actions. In the afternoon, ", " gave a marathon 3-hour lesson on setting up navigation2 from scratch on a real robot, building a map using Cartographer, and then navigating around in it.", "The response to the tutorial was good, and I think the participants all enjoyed the chance to learn about ROS 2 before diving into the community head first on the main conference day. In our follow-up survey, which had 33 responses, every respondee said they would consider participating again.", "The first invited talk was given by Ryan Gariepy of Clearpath Robotics/Otto Motors. Rather than being specific to ROS, Ryan discussed why safety is important for all those robot products that are being produced now that ROS has Solved All Our Problems, and gave tips on where to start. The content of the talk was probably unexpected by the majority of the audience, but the feedback was positve and it was clear that they all were happy to have heard the talk.", "The second invited talk was given by Louise Poubel of Open Robotics. She talked about Gazebo and Ignition, giving some information on where Gazebo came from and where Ignition is going. With the default simulator for ROS going through a transition to greater power (and greater responsibility?), this talk was timely for the audience. And, as usual for a member of the Gazebo team, the talk was given in Gazebo, which apparently was a new thing for most of our participants! There was an audible response from the audience when the first slide turned out not to be a slide.", "Although we provided real time translation from English to Japanese for the participants, Louise gave her entire talk in Japanese.", "ROSCon JP 2019, in keeping with the ROSCon method, established a programme committee and made a public call for submissions. This year we received 23 submissions, a slight drop over last year, but fortunately most were of high quality.", "After review by the programme committee, 13 presentations were selected and given presentation slots ranging from 10 minutes to 30 minutes. (One presentation unfortunately had to drop out.) The presentations were selected covered a broad range of ROS-related topics, such as", "On the other hand, the lightning talks session was full this year, unlike last year where we struggled to get participants. Most likely, having seen last year what it is about, a lot more people wanted to do one this year. The hour was fun for all and every presenter was very enthusiastic. We even had a live demo of a ROS-controlled tiny-scale quadcopter, done perfectly in just a minute and a half!", "To help cover the cost of holding a conference in central Tokyo, where meeting space is at a premium, the organising committee invited sponsors. To our amazement, we sold out of exhibition space and had three companies waitlisted to sponsor!", "As with ROSCon, sponsors were provided with dedicated exhibition space in a separate room from the presentations. This allowed them to give lively demonstrations and hold discussions with participants throughout the day. The exhibition hall was well-attended, with participants talking to exhibitors even during the invited talks.", "ROSCon JP 2019 was sponsored by the following companies and organisations:", "Gold: Analog Devices, The Autoware Foundation, eSOL, iSiD, Rapyuta Robotics, Renesas, Seqsense, Tier IV, Ubuntu", "Silver: Acutronic Robotics, ADLink, Argo, Concurrent Real-Time, Mamezo, Robotis, SoftBank, Whill", "Bronze: AVNet, Fixstars, RT Corporation, TechMagic, TokyoRobotics, Vstone", "To encourage networking between participants, ROSCon JP provided both a catered lunch and a catered reception. Attendees were able to enjoy a good meal without leaving the conference venue, giving them more time to mix with each other and attend the exhibitors\u2019 booths.", "The reception in particular was well-attended, even by those who had to travel several hours to get home that night. Once again we were forced to drive people out of the reception when the hotel wanted their room back!", "A survey of participants at the conclusion of the conference received 109 replies. Here are some select results:", "Building on the 2019 event, ROSCon JP 2019 was a huge success. We saw many repeat visitors, and the buzz around the event beforehand showed us that people were genuinely excited about attending. The event seems likely to continue next year!", "Congratulations to ", ", ", ", and the rest of the ROSCon JP team for putting on another great event!", "Congratulations ", " and everyone else involved in the organization!", "Awesome! I\u2019m glad to hear it went so well, especially the Nav2 tutorial, very cool! ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["using Alpine Linux to rapidly deploy ROS-based software to in-production robots with minimum dependencies;", "how ispace is using Gazebo to assist with designing its moon rover (due for launch soon!);", "the combination of machine learning and Matlab\u2019s ROS support to provide higher-quality highly-functional robot software; and", "two talks on how to do hard real time with ROS 2.", "56% of respondents were engineers at companies. A further 22% were in commercial R&D and 6% in business.", "Just 15% of respondents were on the academic side, with the vast majority of those being students. This shows again that there is a strong commercial interest in ROS in Japan.", "94% of respondents rated the quality of the sessions as high or very high, an improvement over last year. Just 2 people rated them as below average (again).", "By far the majority of participants considered the sessions just right in length and number.", "100% of respondents stated they intend to participate if ROSCon JP is held again. Out of this, 70% were unconditional (i.e. not based on schedule or content).", "28% of respondents expressed a desire to present at the next ROSCon JP. A further 41% stated they would present a lightning talk."], "url": "https://discourse.ros.org/t/roscon-jp-2019-report/10860"}
,{"title": "Which board/microprocessor should I use? I want to use ROS Kinetic on Ubuntu 16.04, along with packages like move_base, laser_scan_matcher for indoor autonomous navigation", "thread_contents": ["I am confused as to which board will be able to handle the necessary computation to run autonomous navigation for my turtlebot-like-robot.", "\nI want to run Ubuntu 16.04 OS and ROS Kinetic, and need packages like move_base, laser_scan_matcher.", "\nI\u2019ve heard that Raspberry Pi 3 and its Arm Cortex A53 isn\u2019t enough for the purpose.", "\nWhat are your views?", "That highly depends on your budget, software requirements and space limitations.", "\nThe packages alone also don\u2019t say  that much about the required computation power.", "\nE.g., what kind of LIDAR are you using (how many data points per second does it produce?), what other kinds of sensor (e.g., cameras) are you using?", "\nFor cameras etc. the USB bandwidth might be a limiting factor that has to be kept in mind.", "Without more information, I can only suggest some generally viable small form factor options depending on your computation power requirements:", "\n", " You could try the Pi 4 which is a lot faster than the Pi 3. The 4GB RAM might not be enough depending on your use case but I assume it should be able to handle autonomous navigation with the kind of LIDAR you would put on a Turtlebot-like robot.", "\n", " A mini-PC such as an Intel NUC would surely be enough to handle almost anything you throw at it.", "what kind of LIDAR are you using (how many data points per second does it produce?)", "Hey ", ", I\u2019m using RPLidar A1, and using it at 5Hz with 2000 data points.", "\nNUC seems too bulky, actually. Looking for something much more compact like Pi.", "\nWould you recommend Beaglebone or Odroid XU4?", "Hi, if you don\u2019t have budget constraint you can try UP board or jetson nano or Beaglebone. Working with ROS , Lidar and some kind of application that is to be done like turtlebot alike of robot if designed.", "I agree with ", " on the UP board if budget is not a constraint.", "\nI wouldn\u2019t recommend a Beaglebone for this use case (I assume you want something simple to work with and develop on) because they have very little RAM (unless there\u2019s a model I missed) and that can lead to frustrating issues for inexperienced developers, e.g., you may not be able to compile directly on the board but will have to cross-compile on your host machine and deploy the binaries manually.", "\nA jetson nano really only makes sense if you also have a camera and want to use graphics accelerated algorithms or deep learning otherwise the Pi 4 is faster IIRC.", "\nHowever, if I were you I would try a Pi 4 since you\u2019re not dealing with a lot of data and if the Pi is not enough, it\u2019s a cheap mistake to make and maybe you can still use it for something else.", "\nAlso, the Pi is widely used and has the highest chance of finding someone who can help if you run into issues.", "\nI don\u2019t know how the Odroid compares to a Pi 4.", "Later on, you can still switch to something that is smaller / more energy efficient but harder to work with.", "Looks like you mised the ", " (Cortex A15, 1GB RAM) and the ", " (Cortex A15, 2GB RAM)", "I have had great luck using the TX2 with the ", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a Q&A site which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/which-board-microprocessor-should-i-use-i-want-to-use-ros-kinetic-on-ubuntu-16-04-along-with-packages-like-move-base-laser-scan-matcher-for-indoor-autonomous-navigation/11488"}
,{"title": "Announcing LaMa: An alternative localization and mapping package", "thread_contents": ["Dear ROS users,", "We would like to announce the release of the ", " package.", "\nIt includes a framework for 3D volumetric grids (for mapping), a localization algorithm based on scan matching and two SLAM solution (an Online SLAM and a Particle Filter SLAM).", "The main feature is ", ". You can even run the Particle Filter SLAM in a Raspberry Pi.", "We provide ROS integration with the ", " package.", "Fell free to try it and provide any feedback.", "Very nice. I\u2019ll definitely take the localization out for a spin", "You should also give SLAM a chance ", "Looking forward to test it!", "Hi ", ",", "\nCongratulations on the release!", "\nI am wondering if you have found any differences comparing IRIS LaMa localization to amcl implementation that\u2019s already in ROS. Same for mapping, with comparison to popular ones out there.", "\nI am really curious to know.", "I am wondering if you have found any differences comparing IRIS LaMa localization to amcl implementation that\u2019s already in ROS. Same for mapping, with comparison to popular ones out there.", "\nI am really curious to know.", "Yes, I did compared my solutions with popular ones found in ROS. In the ", " file you can find a few papers where I compare LaMa\u2019s algorithms with solutions such as AMCL and GMapping. But here are my selling points:", ": In general both provide good accuracy but (by default) AMCL does not use all data to compensate for particle filter\u2019s overhead and that can result in some errors. Scan Matching can be 5x times faster or more. I still use AMCL in applications where information is reduced and noisy.", ": I think that GMapping is a wonderful piece of technology but very slow. I remember, back in early 2012, using GMapping online was ", ". LaMa PF SLAM is kinda like a ", " GMapping or ", " GMapping if you activate multi-threading. LaMa Online SLAM is the turbo version, it can generate the Intel map in 5seconds. Here is the result: ", ": I used the ", " to compare with other SLAM solutions and we did good ", ". I believe that ", " also used the same benchmark.", ": ", " is another top reference in robotics. I only developed SDM because OctoMap\u2019s main focus is occupancy grids and I needed more flexibility. The inner structure of SDM is model agnostic and provides the same features for any type of grid map. Those features include Copy-on-Write and Online Data Compression.", "Thanks! BTW that map looks awesome!", "Both mapping and localization work pretty well, but I am particularly happy with the latter.", "Congratulation for the awesome work !!!", "Would you mind sharing a video of your tests? A video is worth a thousand images ", "Looks very interesting!", "\nDid I miss the link to the benchmark results somewhere?", "\nA comprehensive quick overview demonstrating the performance both in terms of speed and accuracy in comparison to the other methods would also help a lot.", "Did I miss the link to the benchmark results somewhere?", "No, you did not miss the link. Some of the benchmarks are in published articles.", "\nBut I can provide a short summary of the results.", "I used this ", " (like other have). This software provides a mean ", " and ", " errors.", "Here is short version of the errors:", "The following table show how long each solution takes to process a given dataset:", "  The values that are presented here were not obtained using the same computer, therefore it may not be a ", " comparison. Nonetheless, the difference in scale is obvious.", "\nThe ", " values are taken from the author\u2019s paper.", "Accuracy is on pair with known (and established) solutions such as GMapping with very good performance. I omitted the ", " but I can say (with confidence) that it also offers good results.", "You should give it a try!", "But GMapping uses Particle Filter right?", "Yes, GMapping uses a Particle Filter.", " congrats for the packages, and thanks for sharing it! ", "What exact ", " files have you used as inputs for the SLAM benchmarking?", "I\u2019d like to know specifically what computers each of those tests were run on, I\u2019m not sure you could say the scale is obvious if its a 5th generation laptop vs an 8th generation desktop CPU and had a fair comparison of settings.", " congrats for the packages, and thanks for sharing it! ", "What exact ", " files have you used as inputs for the SLAM benchmarking?", "Thank you!", "\nFor the benchmark I used the raw log files available at ", ". These are CARMEN ", " log files (if I am not mistaken). To use these logs with ROS I created a small program to convert ", " to ", ".", "I\u2019d like to know specifically what computers each of those tests were run on, I\u2019m not sure you could say the scale is obvious if its a 5th generation laptop vs an 8th generation desktop CPU and had a fair comparison of settings.", "I took the time to redo the tests and you are correct, the scale is not obvious.", "The original GMapping values were taken from a HP ProLiant with two Intel Xeon CPU E5-2640 0 @ 2.50GHz (Max Turbo Frequency 3.00 GHz) running Ubuntu 16.04 TLS.", "Here are some values taken from the same computer, a Thinkpad L480 with an Intel  i7-8550U CPU @ 1.80GHz (Max Turbo Frequency 4.00 GHz) running Ubuntu 18.04.3 LTS.", "Maybe I should also take the time to test ", ".", "I like the direction this thread is taking\u2026", "Lot of us care about the quality of localization and it is hard to figure out the pros and cons of different algorithm, being also Karto one I would add to the list.", "It would be very nice if we figure out a way to make these benchmark easily reproducible, maybe with a Docker image and a Gthub repository where people can add their own algorithm to the test.", "If we dream big, we may even associate a continuous integration machine and get updated about new results!!! ", "Something that I consider valuable is the amount of tuning that a certain algorithm need.", "With Cartographer, I always have the feeling that I can play with hard to understand parameters to make it work. I know that there are parameters that EVENTUALLY will give me an amazing map, but it could be frustrating.", "Something I appreciate of ", " is that it just worked, at least with my dataset.", "Benchmarking different SLAM algorithms (for map/localization quality and speed) is not a straightforward task. Benchmarks such as the ", "  try to be objective (and it kinda is), but, the final result can (and most likely will) be influenced by the ", " of the algorithm. This is most evident If I am the one trying to do the benchmark without fully understanding what the parameters do. And just like ", " said, this can be frustrating.", "Nonetheless, I do believe that benchmarking is necessary. It creates a healthy competition that can result in improvements. For example, the ", " has a leader board where the authors of a solution submit their code with proper parametrization for evaluation. Maybe something like this could exist for SLAM? I usually have to search for papers to find this kind of information.", "Something I appreciate of ", " is that it just worked, at least with my dataset.", "I think that everybody likes things that just work ", "\nThe response of a system to change in parameters was something that I discussed quite often with my colleagues.", "Hi,", "I\u2019ve been playing with this today and wanted to share my very preliminary results.", "I would independently verify that its much lighter weight than other options I\u2019ve seen recently for the optimizer based SLAM option. I\u2019m seeing the CPU grow but overall pretty consistent over short trajectories at around 10% CPU on a 6th gen i7. Over the same trajectories with my package I\u2019m seeing less consistent but generally hovering around 30% \u2013 both with more or less the same memory utilization.", "I\u2019d say though the rastered map image out isn\u2019t as good as slam toolbox and I\u2019m not seeing it accomplish loop closures as responsively. That may not be a big deal for many users. For the datasets it works with, it works pretty well to keep as a reasonable option on the table. For the datasets it doesn\u2019t work with, I have no idea what\u2019s going on. See below, the same robot, on the same day in the same environment 2 datasets were taken, one works fine, the other does this:", "It worked for about 10-20 updates and then just started blowing up. No warnings or errors thrown. I\u2019m also going to have to figure out why LaMa has so much of a CPU drop from Slam toolbox, it looks like it uses much of the same techniques and it may lie in the dependency libraries since I use Ceres as my LM solver & a bunch of outside libraries so I can swap out with new technology trends \u2013 though I\u2019m sure you get a really nice speed up from the distance field work as well.", "Overall I think this is a pretty good option, but needs to expose more of the parameters, documentation, and hardening \u2013 which in SLAM isn\u2019t the hard stuff.", "If there\u2019s any interest in writing and maintaining long term a ROS2 port of this work, I\u2019d support this as a genuine option for us on the ROS2 Navigation Working Group/TSC to consider at for the \u201cdefault option\u201d in ROS2. I think its well written and enables a number of applications on lower power machines, though being able to scale from small examples to 200,000+ sqft facilities remains to be evaluated.", "Edit: I didn\u2019t evaluate the localization stuff.", "\nEdit2: I was thinking about those numbers, which seemed high and remembered that I didn\u2019t build in release mode so those are going to be higher than you\u2019d see in production.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/announcing-lama-an-alternative-localization-and-mapping-package/10916"}
,{"title": "Discourse - A very powerful mailinglist", "thread_contents": ["Preface: I know it\u2019s horribly hard to organize the community and osrf tries to do the job, thank you!", "\n< rant >", "\nUntil recently I simply ignored discourse because it looks like a huge soup of everything, sorted by web 3.0 standards and only used by a handful of people. I\u2019m sure I\u2019m not the only one who looks at it that way.", "Now, as OSRF keeps pushing people towards this website by corporate policy (e.g. ", "),", "\nI tried to integrate it with my mailbox.", "\n< /rant >", "\nThis is because I skim through my mails every morning, but do not plan to login to a website everyday just to scan through things I\u2019m probably only marginally interested in.", "This brought up two issues for me:", " here is the constructive criticism you asked me for. Well more like a \u201ccriticism\u201d and a \u201cconstructive\u201d part, but anyway.", "Thanks in advance for pointers to solutions!", "Hi ", " thanks for the feedback,", "Indeed discourse is a more modern front end, it is a little different than many of us are used to but that\u2019s one of the things that is valuable about it.", "In the current state on the mailing lists I often find myself finding threads on nabble rather than the main site when I search. Discourse is focused on both supporting active members via the realtime email notifications as well as being easily searchable and browseable which is highly valuable for the majority of ROS users who are not actively subscribed. From the ", " we have approximately twice as many people with wiki accounts, 5x the users on ", ", and 20x the unique IPs as subscribers to ros-users. We want to make things better for this large majority of the users. Improving browseability and ease of search significantly improves this experience.", "One of the main impetus\u2019 behind the switch was ", " To find that thread, I searched my personal inbox, found the title and then searched online for the archive so I could link to it. Without cheating and remembering exact strings from that thread it\u2019s very hard to find the thread using google search. Discourse is designed to be easily crawled by search engines and the results are picked up quite quickly with good rank.", "We\u2019ve had many reports of people unsubscribing from ros-users due to the volume being too high. As the user community has grown the ros-users mailing list subscriber base has not grown proportionally. Typically it\u2019s  a subcategory of content which individuals are not interested in and they unsubscribe because it\u2019s filling up their inbox too often. Whether it being job postings, development discussions etc with discourse and your personal settings now you can easily pick the topics you\u2019re interested in subscribing to and let the rest be.", "We\u2019ve also seen a trend that our SIG\u2019s get created populated, and then the drift off into a slow quiet decline because they are not discoverable. It requires knowing there\u2019s a mailing list to find, finding the mailing list, browsing the individual archive. Often after the initial impetus has petered out, there are a bunch of people on the list and if a new person joins the members will respond, but rarely does the membership grow after the initial publicity due to the SIGs being hard to discover whereas now they can be a category and new people joining the community can easily discover them through the front page of the forum.", "Discourse can scale and support many more users. If you browse through: ", " you\u2019ll see forums with tens of thousands of users. Clearly that level of activity requires filtering and categorizing, but that\u2019s what discourse empowers the user to do.", "To answer your specific questions:", "In general, how do I send a mail to a given category to start a conversation?", "You should be able to send a message to ros+[categoryname]", " if you are at trust level 1 or higher. To get to trust level 1 you need to enter 5 topics, read 30 posts and spend at least 10 minutes on the site. This is primarily an anti-spam measure. From your ", " badge I see that you\u2019ve already reached level 1 from just looking around the site.", "You can find the ", " in the url if you browse to the category. Note that subcagetories are separated by a ", " not a ", " so it\u2019s ", " where the url reads ", "From your reply on github you implied that the domain did not resolve for you. I received the notification of your post from that domain so as far as I know it\u2019s resolving and operating just fine.", "How do I subscribe to a specific category only? I changed my profile to \u201cmailing list mode\u201d and only category I \u201cwatch\u201d is \u201cMoveIt! Developers\u201d, but still got mails for ", " and ", " . I know that some colleagues face the same problem.", "In terms of filtering, mailing list mode means that you get everything on the forum sent to you, just like a mailing list would. If you want to filter you should turn that off and then you should only get notifications on any categories you\u2019ve setup to watch.", "I never said there are no reasons to use such a system, but anyway thanks for the lengthy elaboration of osrf\u2019s reasoning.", "\nI didn\u2019t know that short thread got so much attention, but it kind of makes sense. It actually made me laugh when someone suggested the user list got too active.", "\nEspecially as it feels like every mail that is sent by someone other than yourself gets a fast reply saying \u201cgo to ", "\u201d, hehe.", "\nI\u2019m pretty sure one day of subscription to the Linux Kernel Mailing List would change their mind (and their way of handling mails). ", "In general, how do I send a mail to a given category to start a conversation?", "You should be able to send a message to ros+[categoryname]", " if you are at trust level 1 or higher.", "\nFrom your reply on github you implied that the domain did not resolve for you.", "I forgot to look up their MX entries(actually just one). I simply weighted it more likely that you made a typo", "\nthan that all mails sent for ", " discourse instance are send over a central mail server.", "\nStill sounds somewhat fishy to me.", "\nSo I suppose it works as you described and I\u2019ll test it eventually. Thanks!", "How do I subscribe to a specific category only?", "In terms of filtering, mailing list mode means that you get everything on the forum sent to you, just like a mailing list would.", "I enabled mailing list mode only recently because I didn\u2019t get a number of replies from ", " .", "\nAlthough I received some other posts from the watched category\u2026", "\nLooking into it a bit, I suspect that discourse did not automatically send mails to me for replies to the thread because the thread got created before I added the category to the watch list (and if I remember correctly also before I created the accout).", "\nI\u2019m not sure this is the case, but it might actually be a bug in the system?", "I turned off mailing list mode again and I expect that everything works as expected now.", "\nIf I notice any further problems, I\u2019ll come back to this.", "Thanks for your time.", "I forgot to look up their MX entries(actually just one). I simply weighted it more likely that you made a typothan that all mails sent for any discourse instance are send over a central mail server.", "This is because we\u2019re using their hosted service. Originally we had to setup our own mail server SPF/DKIM entries etc and we\u2019d done it successfully. But for most of their customers that\u2019s too much so they\u2019ve moved to processing all the emails centrally for the hosted accounts.[quote=\u201cv4hn, post:3, topic:389\u201d]", "\nI enabled mailing list mode only recently because I didn\u2019t get a number of replies from ", " .Although I received some other posts from the watched category\u2026Looking into it a bit, I suspect that discourse did not automatically send mails to me for replies to the thread because the thread got created before I added the category to the watch list (and if I remember correctly also before I created the accout).I\u2019m not sure this is the case, but it might actually be a bug in the system?", "I turned off mailing list mode again and I expect that everything works as expected now.If I notice any further problems, I\u2019ll come back to this.", "Thanks for your time.", "\n[/quote]", "You\u2019re welcome. We want to hear feedback and understand how things are working or not so that we can fix it. Discourse is under active development/maintenance. Well formed bug reports  ", "  can easily be diagnosed, a fix developed, and deployed on the site within a week.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["In general, how do I send a mail to a given category to start a conversation?", "How do I subscribe to a specific category only? I changed my profile to \u201cmailing list mode\u201d and only category I \u201cwatch\u201d is \u201cMoveIt! Developers\u201d, but still got mails for ", " and ", " . I know that some colleagues face the same problem."], "url": "https://discourse.ros.org/t/discourse-a-very-powerful-mailinglist/389"}
,{"title": "Announcing the Hardware Robot Information Model (HRIM)", "thread_contents": ["Hello everyone,", "It\u2019s a pleasure for me to announce the release of the ", " version of the Hardware Robot Information Model (HRIM) available at ", ".", "Briefly,", "\nHRIM is a common interface that facilitates interoperability among different vendors of robot hardware components with the purpose of building robot modules and thereby, modular robots. HRIM focuses on the standardization of the logical interfaces between robot modules, designing a set of rules that each device has to meet in order to achieve interoperability. It tackles the problem of incompatibility between robot components that hinder the reconfigurability and flexibility demanded by the robotics industry. In a nutshell, HRIM presents a model to create plug-and-play robot hardware components. HRIM builds upon the ROS component model and although we envision its expansion to support other framework alternatives, currently a ROS 2.0 implementation is available at ", ".", "In short, the robot modules have been classified in 6 (actually, there\u2019s a seventh, composites that is a work in progress) types of modules which correspond to the task they can perform: sensing, actuation, communication, cognition, user interfaces or power. Each type is composed by sub-types or devices, related to the functionality of the component. For example, a camera is a sub-type of the sensor type. The following image pictures the HRIM component model for each device (or sub-type):", "HRIM has been presented to national experts at the International Standardization Organization (ISO) within the scope of the standardization in the field of robotics, excluding toys and military applications (ISO/TC 299). Particularly, it has been introduced within the ongoing standard ISO/CD 22166\u20131 which treats modularity for service robots.", "\n", "\n", "In addition, Fraunhofer IPA \u2014 the largest research organization for applied research in Europe \u2014 is currently looking at HRIM to adopt it and extend it for several projects. Mirko Bordignon, group manager at Fraunhofer IPA said:", "After years of experience designing, developing, and deploying software on robotic systems, we fully subscribe to the objectives stated in the HRIM manifesto: working towards true \u201cplug and play\u201d hardware modules through a standardized information model, which merges the inputs and feedback gathered from open-source communities with the stability and platform-independence required by standardization bodies. We look forward to contribute our experience to further advancing HRIM towards this goal!", "Similarly, HRIM picked the interest of the ROS-Industrial (ROS-I) Europe consortium during the last ROS-I conference where HRIM was introduced.", "Feedback, criticism and contributions are more than welcome. A complete writeup and description of the release is available at ", ".", "Regards,", "Great work, Victor and everyone working with you! I hope this is adopted widely!", "I have some questions about your relationship with standards.", "Hello ", "!", "How exactly are you compatible with the (draft) ISO 22166-1?", "I/we don\u2019t claim that HRIM is compatible with the draft of ISO 22166-1. Doing so would be incorrect as the ongoing standard covers (or that\u2019s my hope for its final shape) much, much more than a model. As far as I know, the existing draft does not even propose a model and the closest thing included is a pointer to HAL4RT (more about this below).", "What we claim is that HRIM has been ", " and ", " to the experts pushing the ongoing standard. We received feedback already from several of those experts and our hope is that we can inspire some additional content on the working document which currently, IMHO, lacks of software abstractions that facilitate interoperability.", "How are you compatible with HAL4RT? There are two versions of that specification proposed; the first has died and the second was rejected because it doesn\u2019t contain anything of use to robotics.", "Although what you say is right and HAL4RT 2.0 isn\u2019t still official (AFAIK JASA will revise and submit the new version to OMG in June this year but you probably know it better than I do), we based ourselves on the latest draft of HAL4RT 2.0 available (verified just a few days ago with Kenichi Nakamura when he facilitated the last version after I introduced HRIM to him (leading HAL4RT)). We claim we conform with the statements indicated in this last draft and which refer to specific sections within HAL4RT 2.0 document. The conformance of each one of this sections is discussed in Appendix I at ", ".", "Taking into account these aspects, we believe that claiming conformance is valid but probably, stating that we\u2019re on the path to conform (formally) with HAL4RT 2.0 is more correct since as you point out, (HAL4RT 2.0) isn\u2019t official yet.", "The proposed APIs are also not compatible with ROS.", "I\u2019m not really sure we agree on this. Could you mention which sections make you think this way?", "How do you support OpenEL 3.0, which is a private commercial project (not an international standard)? Do you provide an implementation of its APIs? They don\u2019t seem compatible with ROS, to me.", "This is likely the claim I can defend the least since most of the information we found is in Japanese and it was a tremendously hard task to understand it right. In fact, although we looked into it, we were not going to mention anything about it however, a few days ago I got the following from Kenichi:", "At this time, OpenEL 3.0 and HAL4RT 2.0 are almost the same.", "\nSo there is no document for OpenEL 3.0 in English now.", "Everything we saw (and understood), make us think this way as well so on this basis, we included that sentence.", "What we claim is that HRIM has been presented and introduced to the experts pushing the ongoing standard. We received feedback already from several of those experts and our hope is that we can inspire some additional content on the working document which currently, IMHO, lacks of software abstractions that facilitate interoperability.", "Sorry, I misread your post.", "Unfortunately, based on this week I don\u2019t think that 22166-1 is going to go in the direction you and I desire.", "Taking into account these aspects, we believe that claiming conformance is valid but probably, stating that we\u2019re on the path to conform (formally) with HAL4RT 2.0 is more correct since as you point out, (HAL4RT 2.0) isn\u2019t official yet.", "I\u2019m not really sure we agree on this. Could you mention which sections make you think this way?", "OK, after reading that document I understand how you are looking at HAL4RT. I looked at the interfaces you have and I do not think you are correct to claim conformance with HAL4RT 2.0. You can claim that you correlate on capabilities with HAL4RT 2.0, but to conform you would need to have identical interfaces, which you do not. You have similar capabilities (as well as significantly extended capabilities) but you do not match the model defined by HAL4RT 2.0. Which I think is a good thing because your model is significantly better for our needs, in my opinion.", "This is likely the claim I can defend the least since most of the information we found is in Japanese and it was a tremendously hard task to understand it right. In fact, although we looked into it, we were not going to mention anything about it however, a few days ago I got the following from Kenichi:", "At this time, OpenEL 3.0 and HAL4RT 2.0 are almost the same.", "So there is no document for OpenEL 3.0 in English now.", "Everything we saw (and understood), make us think this way as well so on this basis, we included that sentence.", "OK, on that basis if you feel confident then you could claim correlation with OpenEL 3.0 (I wouldn\u2019t claim compliance with something I hadn\u2019t seen; \u201calmost the same\u201d is not the same as \u201cidentical\u201d). I wouldn\u2019t claim it\u2019s an international standard, though.", "If you want help with the Japanese, feel free to ask. ", "For completeness, I\u2019m leaving here a short comment about our updates:", "although we will keep in touch with the groups behind HAL4RT and OpenEL efforts, after an offline discussion, Iweve made an update to the original announcement and crossed over the HAL4RT and OpenEL claims until we clarify exactly the status of those documents/efforts.", "A new version of the paper has been submitted to arXiv and should be available tomorrow. Thanks ", " for the support.", "Needless to say, HRIM will keep growing as we get more and more contributors, supporters and (hardware) devices captured in the model.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["How exactly are you compatible with the (draft) ISO 22166-1?", "How are you compatible with HAL4RT? There are two versions of that specification proposed; the first has died and the second was rejected because it doesn\u2019t contain anything of use to robotics. The proposed APIs are also not compatible with ROS.", "How do you support OpenEL 3.0, which is a private commercial project (not an international standard)? Do you provide an implementation of its APIs? They don\u2019t seem compatible with ROS, to me."], "url": "https://discourse.ros.org/t/announcing-the-hardware-robot-information-model-hrim/3906"}
,{"title": "XEL Network first application + Distributing XEL devices 100 set for free in ROScon2018!", "thread_contents": ["Hi, all!", "We released a video about simple application using ", "We set up ROS2 topics for each XEL using the XEL Network GUI tool and created a mobile robot by creating ROS2 nodes that utilize these topics on the PC. This demo will be shown at ROScon2018.", "And,", "\nCelebrating our XEL Network Project Release, we will be distributing 100 sets for free in ROSCon2018.", "The configuration of this free set is as follows.", "\n", "The XEL Network dramatically reduces the learning curve in the use of sensors, ROS2 communication(DDS) and power monitoring, providing an easy-to-use experience.", "If you want to join the XEL Network project, you can always go with us through Github.", "\nWe release almost all the files related to project development for free.", "\nFor example, schematic, firmware source code, etc.", "\nIf we can contribute to creating interesting things, there is nothing more better than this.", "You can find more information from", "For our other similar project, please refer to ", "And, We specially thanks to ", " developers.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["GitHub Repositories :\n", "\n"], "url": "https://discourse.ros.org/t/xel-network-first-application-distributing-xel-devices-100-set-for-free-in-roscon2018/6115"}
,{"title": "Addition of Radar-Specific Message(s) to sensor_msgs", "thread_contents": ["I work for a company that is very closely tied to the automotive industry. Many people in our industry are starting to use ROS as the basis for research and development efforts into autonomous vehicles. One of the primary sensing modalities is radar. Unfortunately, there isn\u2019t really a basic message type in ", " that currently fits the output from a radar. The closest available is ", ". However, while there are ", ", ", ", ", " and ", " properties associated with a radar detection, there are also other intrinsic properties like ", " (a measure of the power of the returned signal - also applies to the other light-based sensor readings represented by Range) and ", " (the angle of the detection within the lateral field of view - since radars have horizontal discrimination). Soon, there will also be 3D radars with vertical discrimination necessitating a splitting of ", " into lateral and vertical components.", "Furthermore, many radars do not actually output the raw detection information but only output \u201ctracks\u201d which are filtered and grouped abstractions of single or multiple detections. Because of the tracking over several scans, they contain all the same readings as ", " with the following exceptions:", "We have our own versions of messages representing these data (see our package on Github [1] - specifically RadarDetection and RadarTrack) but I am now aware that these do not comply with REP 117 [2] and would much rather contribute to the standardized set of messages in ", ".", "To the point: Does it make sense to try to extend ", " to include the properties of a radar detection or should a new message be created? What about a track (when they are the only data available)?", "[1] ", "\n[2] ", "I think the best way to go is to create a new message, where you include", "\nthe existing  sensor_msgs/Range  and add the missing fields.", "\nExtending the current  sensor_msgs/Range is not a good idea because it", "\nwould change the message checksum and all hell would break loose for those", "\nalready using it.", "Thanks for the feedback, Procopio. Someone here at ROSCon mentioned this would likely be a problem too but I wasn\u2019t sure how worried the community was about it. I\u2019ll make the pull request.", "Hi ,", "\njust to comment that some radars provide, for each target detected, the Doppler velocity as a raw measurement made directly by the device. So it would be important to differentiate \u201crange_rate\u201d which, as JWhitleyAStuff said, the polar longitudinal velocity of tracks, from the \u201cdoppler_velocity\u201d , which is also a polar longitudinal velocity, but issued from a wave signal processing step, and not issued from an object tracking process.", "\nFrom the robotics perspective, doppler velocity is a raw measurement, like range.", "\nbest,", "\nandreu", "Thanks to everyone for the feedback. I think it\u2019s unavoidable to change ", " in, at least a minimalistic way - e.g. adding an enum value to ", " for RADIO. I\u2019m going to go ahead and make the pull request and we can discuss the naming and architectural semantics there.", " Thanks for starting the thread and also opening ", ".", "Now that common_msgs is relatively mature we generally want to make sure that messages are already in use and have been tested, instead of developing them in the abstract here. There\u2019s some notes on contributing ", "We generally want things to have been validated by real use in the field before merging them into common_messages so that we can be confident that they are useful. Sometimes new messages here will also be a merge of two or more messages to standardize. Such as ", " for the BatteryState message.", "There may be several classes of Radar returns that would be good to clarify/separate. There are potentially low level interfaces (range, intensity, velocity)  as well as high level interfaces (object position, velocity, and scale). Surveying the field of sensors and understanding which ones can be covered by any given interface would be valuable in this process. And I\u2019m sure that there are users with radar messages already that would be great to get their feedback and look for an already tested version of a radar message.", "A good amount of people seemed to be interested in these messages. Just a note that they have  now moved from ", " to ", ". All packages under these repos just got an update and have been released to ShadowFixed.", "They are now available in the ROS official repos. You can ", " from Ubuntu or Debian.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They do not contain \u201camplitude\u201d since this is meaningless for a grouped/tracked single or set of detections.", "They also contain the following additional fields:", "\na. ", ": The lateral velocity of the object within the arc of the field of view of the radar (polar lateral velocity).", "\nb. ", ": A lateral measurement of the \u201ctrack\u201d from the most extreme lateral detections that make up the track.", "\nc. ", ": A measurement of the longitudinal velocity of the track with respect to the detected angle (polar longitudinal velocity).", "\nd. ", ": A measurement of the longitudinal acceleration of the track with respect to the detected angle."], "url": "https://discourse.ros.org/t/addition-of-radar-specific-message-s-to-sensor-msgs/2724"}
,{"title": "Announcing new packages for TurtleBot3 in ROS2 (including Cartographer and Navigation2)", "thread_contents": ["Hi guys ", "We are happy to announce new packages for ", "!!", "\nThe packages includes ", "(", ") and ", "(", ")", "Now, you can launch those packages using simple commands in ROS2 Crystal Clemmys with TurtleBot3.", "\nIf you already have TurtleBot3, you could try teleoperation, SLAM and navigation through ROS2 frameworks after you setup for ROS2.", "\nIf you don\u2019t have TurtleBot3, you could load TurtleBot3 into Gazebo simulator and launch everything I said.", "This release only support ", " yet, but we are going to update that packages to support ", " after few days later.", "[Github repo]", "[Issue page]", "[E-Manual]", "\n", "\n", "Please consider to release your packages using ", " in order to provide Debian packages for users so that they don\u2019t have to build the packages from source.", "I want to thank ", " for the great work he did here! I was working on this from the Navigation2 side and bugged him many times for help and updates, and he patiently helped me and worked through the issues to get it working! Thank you!", " Of course, we have plan to release tb3 packages ", " Thank you for your kind comment ", " Navigation2 and your team makes me driven to focus that packages. Thank you again and let\u2019s make more greatness!", "Hi ", ", I am preparing to release the TurtleBot3 packages using bloom in order to provide Debian packages for users. Fortunately, the way to register with ", " is the same as before. One thing I\u2019m curious about is that I have used ", ", but ros2 does not seem to be able to use it. What should I use instead of this? I want to hear your advice. Thanks. ", " Please post any follow up conversation to a different place since this category is only intended for announcements etc. and reaches too many people. Either to the ", " category or ", ".", "One thing I\u2019m curious about is that I have used ", ", but ros2 does not seem to be able to use it.", "I guess the right statement would be: ", " doesn\u2019t support ROS 2 (rather than the other way around).", "What should I use instead of this?", "The official ROS buildfarm has the same capabilities for ", " as for ", ". When releasing your repositories you should make sure to:", "If you don\u2019t want to leverage the official ROS buildfarm you can still use the logic provided by the package which powers it by using the scripts locally or withing a CI provider like Travis: see the ", " for more information about that.", "I am not aware which of the other ", " support ROS 2 atm.", "Cheers,", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["add a ", " entry to ", ". Based on that a ", " job will be created which will build new commits on the target branch shortly after they are pushed (and in case of problems send notification emails to the maintainers listed in the packages).", "choose to enable pull request testing. Based on that a ", " job will be created which will build commits on PRs targeting the specific branch and report back with the status through the GitHub UI.", "Dirk"], "url": "https://discourse.ros.org/t/announcing-new-packages-for-turtlebot3-in-ros2-including-cartographer-and-navigation2/7694"}
,{"title": "Looking for Kobuki power supply / battery charger", "thread_contents": ["I have a Turtlebot2 / Kobuki base but no charger for it. I\u2019ve searched online and can\u2019t seem to find any for sale anywhere. If anyone has a link to where I might purchase one, I\u2019d appreciate it.", "Thanks.", "Hi Matt,", "I looked at the spec sheet and you need an Input: 100-240V AC, 50/60Hz, 1.5A max; Output: 19V DC, 3.16A power supply.", "Here\u2019s an example on Amazon: ", "As for the barrel connector, I measured mine at 5mm. The one linked above is 5.5mm, which I assume would also probably work with a little tough love.", "S", "Thanks ", ", I\u2019ll give that one a try.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looking-for-kobuki-power-supply-battery-charger/8754"}
,{"title": "[Beta Test] DYNAMIXEL Wizard 2.0", "thread_contents": ["Happy belated New Year!", "We hope you are having a great start of the year.", "Here\u2019s a good news for all of you especially advanced users and Linux users.", "\nAll new DYNAMIXEL Wizard 2.0 is about to be released, so we\u2019d like you to try it and tell us what you think.", "\nThe DYNAMIXEL Wizard 2.0 supports Linux and Windows, and going to support OS X as well in the near future.", "DYNAMIXEL Wizard 2.0 comes with various advanced features that will allow users to get the most out of DYNAMIXEL.", "Below is the list of advanced features \u201ccurrently\u201d implemented in the software.", "Please refer to the below eManual for more information.", "Software can be downloaded at below download link.", "Please feel free to report any bugs or issues.", "\nThanks!", "Well, on windows, it only downloads a file without file extension. This file cannot be run even when I rename it to have exe or msi extension\u2026", "\nYou might have downloaded Linux version.", "\nI just downloaded windows version and which has an .exe extension on it.", "Ah, you\u2019re right, I\u2019ve probably misclicked ", " Thanks", "Is this compatible with the U2D2 usb-serial adaptor sold by Trossen?", "\n", "(I am also back-feeding 12V with a power hub)", "I\u2019ve tried to get the wizard2 software to work on both Linux & Windows 10 with no luck.", "I also downloaded the RoboPlus software and DynamixelWizard1.0 it is not working on Windows10", "I tried scanning 1.0 & 2.0 and every possible Baud rate.", "I also have an Arbotix-M board, and I can use that to talk to the servos, so I know that the servos work.", "Hi ", ",", "ROBOTIS officially distributes U2D2 for DYNAMIXEL and it should work well as long as you have correctly installed the driver(usually Windows installs it automatically)", "\nCould you elaborate more on \u201cwhat did you do and how did it fail\u201d part?", "\nVideo or picture would be very helpful in this case.", "I think I might be experiencing hardware failure. the device was working fine yesterday. With dynamixelwizard2 the scan feature works and the lights blink in the U2D2 but it never finds a servo.", "\nAnd when I try to run dynamixel_workbench_single_manager package  I\u2019m getting \u201cincorrect status packet\u201d", ",", "\nWhen scanning for DYNAMIXEL do you see both TX(green), RX(blue) led on U2D2 blink?", "\nWhat DYNAMIXEL do you use?", "\nAlso, go to Options menu and make sure that you are selecting correct COM port and baudrates.", "Yes both lights blink but it never finds a servo. it doesn\u2019t matter if I connect to an XM540 or and XM430. I purchased another U2D2 and it can see the servos so there is something wrong with this unit", "It looks like there is an issue with the DPI setting in Qt.", "A packet log to be able to view some amount of lines (10000?) of rx/tx packets for diagnostic/debugging purposes would make a great addition.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Graph plotting feature : User can select Control Table items to plot data on the graph", "Packet Monitoring & Generation : User can easily generate packets and log TX/RX packets to analyze", "DYNAMIXEL Diagnosis : Enhanced diagnosis helps to figure out the problem of DYNAMIXEL.", "Enhanced Firmware Updates : Multiple DYNAMIXEL with different ID can be updated at the same time.", "Multi Baudrates, Ports are supported.", "English : ", "\n", "Korean : ", "\n", "Download for Linux : ", "\n", "Download for Windows : ", "\n"], "url": "https://discourse.ros.org/t/beta-test-dynamixel-wizard-2-0/7819"}
,{"title": "Sphero RVR Needs ROS Interface", "thread_contents": [" has a new robot the RVR. This is a serious, robust, and capable small tank robot for a good price.", "Creating the ROS interface would be a good undergrad or advanced high school project. I\u2019d pursue it myself but have other activities over the next months that limit the time I can spend on it. Plus, bluntly, I don\u2019t know enough about ROS internals, working with URDF, etc to address this.", "Here\u2019s why I think the RVR should receive attention.", "It provides a serial port for control by another controller mounted on the chassis. The serial API provides access to control the motors and read gyro, accel, imu, speed, velocity, and location. Sphero is saying a 1st quarter 2020 release will provide access to the magnetometer and other drive functions. The API also accesses power and drive status, and controls LEDs. The Sphero link above provides access to the documentation on the API.", "Sphero has Python code that runs on the Raspberry Pi. I\u2019ve reverse engineered the protocol for C++. I\u2019ve run my code on an Up Board which is a quad core 1.7 Ghz SBC mounted on the RVR. My code is at ", ".", "If someone is interested I can be of some assistance.", "What do we need to do?", "\nRight now I am using roslibpy to publish topics to rosbridge since RVR is in Python 3 and kinetic and Melodic for Python 2 for the PI images I am using.", "I\u2019m not sure what is needed which is why I posted the message. Maybe someone can suggest needs to be done? Some thoughts / questions;", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Need to be able to use Move Base.", "Is Differential Drive suitable? It needs URDF definitions from looking at the code.", "How to get from Twist to the specific motor commands?"], "url": "https://discourse.ros.org/t/sphero-rvr-needs-ros-interface/12163"}
,{"title": "Perception control unit", "thread_contents": ["We are looking to design platforms which can be used to support (multiple) Lidar and camera based vision subsystem with auto-grade heterogeneous socs, sufficient computing power to handle in-car network communication as well as interfacing with ECU through on board MCU.", "Hey ", "  it is a little unclear what should the people respond here.", "\nAre you looking for algorithms folks to give you requirements?", "Do you want feedback on above sketch?", "Etc?", "Sorry for late response - just got back from travelling.", "Yes, we are looking for two groups of feedback:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Requirements from algorithms and system integration folks regarding the hardware", "Feedback from silicon vendors for chips we can use"], "url": "https://discourse.ros.org/t/perception-control-unit/8472"}
,{"title": "Autoware Working Group 20190904", "thread_contents": ["The next meeting of the Autoware working group will be held at ", ". The meeting information is below.", "Please join my meeting from your computer, tablet or smartphone.", "\n", "You can also dial in using your phone.", "\nUnited States: ", "Access Code: 547-575-093", "More phone numbers", "\nAustralia: ", "\nAustria: ", "\nBelgium: ", "\nCanada: ", "\nDenmark: ", "\nFinland: ", "\nFrance: ", "\nGermany: ", "\nIreland: ", "\nItaly: ", "\nNetherlands: ", "\nNew Zealand: ", "\nNorway: ", "\nSpain: ", "\nSweden: ", "\nSwitzerland: ", "\nUnited Kingdom: ", "New to GoToMeeting? Get the app now and be ready when your first meeting starts:", "\n", "I\u2019m trying to catch up with the minutes. The goal of this WG also includes the following, right?", "We need software design for Autoware.", "Autoware API", "\nROS interface (nodes, topics, message types, etc.)", "\nData structure and format", "These are all essentially the same thing, and yes, they are in part covered by this working group, but also in part by other working groups.", "Many others upcoming, such as safety and security", "I think it is likely we will have additional working groups on those topics once we get enough activity and expertise in those areas, but for now we intend to cover them in this working group and rely on the existing ROS 2 working groups.", "I am Seiya Maeda Saitama University.", "\nI attend to next meeting and discuss followings.", "The minutes for the Autoware Working Group meeting ", " have been posted to the wiki:", "Project for managing the Autoware Foundation's open-source activities (working groups, project structure, etc.)", "Unfortunately attendence at the meeting was very poor. I think that the TSC will likely have to take some actions to increase participation in the Autoware WG. Currently we have ", " and virtually no humanpower available to do them.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Autoware API", "Data structure and format", "ROS interface (nodes, topics, message types, etc.)", "Many others upcoming, such as safety and security", "Requirement specification", "Function specification", "System test", "Unit test"], "url": "https://discourse.ros.org/t/autoware-working-group-20190904/10388"}
,{"title": "ROS2: Security tools for development and production", "thread_contents": ["Hi all, my team at Amazon is looking for your input on security tools!", "Developers looking to secure their robots and actively develop new nodes will have to manually create keys and policies, as well as debug their node connectivity should security not be set up properly. Conversion from an unsecure DDS to secure DDS may also have a performance impact, therefore developers will need to test and develop with security enabled for production.", "SROS2 has done an excellent job creating their own wrappers to demonstrate the security capabilities of FastRTPS and RTI Connext. With this in mind, we want to bring forth three tools to make security quicker and less error prone for developers.", "\nThis is error prone for developers since we often use docker containers, different machines, direct development on the robot, and all the amazing ways we like to create robots.", "\n", "\nWe would like to present a cmake macro for generating the key and keystores during build time. Our macro uses SROS2 on the back end, but the macro definition should stay the same should a team want to define their own security generator. This enables each developer to have the expected authentication without having extra steps at run time. The ROS security environmental variables will still need to be set outside the macro.", "\n", "\n", "\n", "\n", "\n", "\n", "\nA release engineer or developer would need to inspect the node graph, all services, and soon actions to create policies for each node. We are considering creating two tools for easing deployment to production security and debugging.", "\n", "\nDevelopers will be using ROS2 for a myriad of applications, and teams will want to know the effects of choosing certain providers over others, and the level of security they need. ApexAI provides their repository (", ") which we will use to run a series of performance tests with and without security to prevent regression and understand system impact.", "\n", "Thanks for putting this together! Although not formally a command line tool, our team at Alias Robotics has been working on the Robotics CTF (RCTF). An online environment for penetration testing. RCTF can run both in our servers or locally through Dockerized scenarios (", ").", "We are making very active use of this with our clients and are more than happy to contribute with support and maintenance to the community.", "\nIn addition to the availability of the RCTF, we\u2019d be happy to put person-power and create custom scenarios for sensitive security aspects within ROS 2.0. This way, evaluation is simplified and easily accessible to a wide variety of security researchers", "\nStatus: In Code Review", "\nDeadline for release: 11/7/2018", "\nStatus: Blocked on Node Graph API", "\nDeadline for release: 11/14/2018", "\nStatus: Started for eprosima Fast-RTPS", "\nDeadline for release: 11/14/2018", "\nSee: ", "\nStatus: Upcoming", "\nDeadline for release: 11/16/2018", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A node graph inspection tool will be available to take a snapshot of the node graph and generate policies for each node. It will print feedback on nodes which advertise services and actions, which developers will use to add permissions for client nodes.", "(OPTIONAL) rqt_graph modified with permissions, such that a developer can inspect their system and visualize the access control of their nodes."], "url": "https://discourse.ros.org/t/ros2-security-tools-for-development-and-production/6487"}
,{"title": "ROS2 Navigation - Input requested", "thread_contents": ["Hello ROS and ROS2 users and developers! I have been in discussion with David Lu and have reached out to OSRF to start an effort to develop the ROS2 Navigation stack. We are currently investigating the needs for the next-generation Navigation stack in ROS2.", "I\u2019d like to invite you to reply with things that:", "I have my own wish list but I don\u2019t want to seed the discussion, I want to hear other users thoughts.", "Also, if you are an active community member and want to be involved in the development effort, please let us know that also. We have a small team right now, but desire to do this in the open with community involvement.", "How to be involved in the development ?", "Happy to know this kickoff. ", "Per my experience of using ROS Navigation Stack, I would like to add below items into the wish list:", "Thanks for the feedback, a couple questions.", "Peter, I sent you an email, we can talk offline", "This is a rather ambitious list, and perhaps some of the navigation tasks applicable to these maps types may be too domain specific, but I\u2019ll just float some far ideas here:", "I suppose I\u2019d like to see navigation planners that could interoperate map format types that are more memory efficient, compressible, dynamic, human relatable, e.g. less pruly metric based like voxels or occupancy grids. I\u2019d also to see ROS navigation planners generalize beyond the classic 2.5D mobile robot on a planer workspaces or perhaps appropriate other environment data in a map as navigational heuristics like for packbots, quadrotors, ROVs that climb, fly or swim in 6DoF.", "Any dedicated Discourse/Slack or other mean for getting involved ??", "One of the main frustration I experienced with ROS nav stack is the lack of flexibility (e.g. ", " inner state machine) that eventually got partially addressed by the community later on - e.g. ", " let you use the state-machine of your choice under the hood.", "\nSimilarly, as it has been mentioned already, the possibility to use other types of map representation would be awesome.", "I\u2019m interested in a similar desire to extend the typical map layers into more semantic meanings. Would add wifi, ble beacons and other rf land marks to the annotated affordances idea mentioned be ", "I haven\u2019t used the navigation stack, but I understood David Lu to say at the last ROSCon that it doesn\u2019t support Ackermann-steered vehicles. That is an important use case.", "I agree with ", ", decouping the move_base in separate modules in a state machine would greatly improve its flexibility.", "Regarding features, a nice little thing to have IMO would be to be able to pass the goal tolerance (xy and yaw) in the goal message to move base.", "Ok, let me see\u2026", "Things to keep about the current navigation stack:", "Things that I will change:", "\nPlease, correct me if I am wrong. Right now, if you want to use your own planner (local or global), you have to use the C++ API. The class that you provided was extremely convenient to minimize the flow of information and speed up the full Navigation Stack work.", "\nHowever, ROS2 should be more efficient while handling messages right? Is it fast enough to deal with the idea of using a node for each planner? This node could receive all the information throw messages or is too much?", "\nIf this is the case, we should study the possibility of detaching the nav_core and the planners.", "\nMaybe the community will be more comfortable with a publish/subscribing paradigm. Is just a thought, but this will make everything even more modular right?", "Is the navigation stack too dependent on the position or is just my impression? If the robot does not start in his position, everything goes really crazy. The local planner and the local cost map can be more independent of the positioning system, right? Is a bad configuration of my navigation or is like that?", "Things that I will add:", "\nA simple system that tracks common problems. E.g: Hey, I am waiting for a map and it is not coming! Hey, I am sending cmd_vel commands but the position does not change! Things like that.", "A simple undocking algorithm; I do not want my robot to move backward unless a recovery behavior specifies so.  A tiny algorithm at the beginning that moves the robot backward x meters could be handy. Moreover, this can be attached to a simple boolean topic. There, a sensor can publish if the robot is at the docking station or not. If the robot is at the docker, you execute the undocking before moving, if not, just move as usual.", "+1 to the idea of specifying the goal tolerance.", "I\u2019m not an expert navigation researcher but I am a user and an experimenter.", "For me, the biggest desire is for navstack2 to be a navigation framework built using ROS2 principles rather than a monolithic navigation solution, as it is in ROS1. This means defining the separate components (local planner, global planner, local map probider, global map provider, planner supporters such as costmap providers, rescuers, path follower, and so on), defining the interfaces between them, and specifying those as ROS2 messages, services and actions. We should be able to say \u201ca node (or set of nodes) that provides a global planner compatible with navstack2 should publish/subscribe/use these topics, services and actions and provide these parameters, at a minimum\u201d. The key thing here becomes defining the APIs between the different parts of the navigation stack.", "Navstack2 should take advantage of the capabilities of ROS 2 to make things nodes but then keep them in the same process so that message passing is nearly cost-free.", "Being possible to specify via a configuration file what the global planner is, what the local planner is, etc. would also be possible. This is to have it not just be a bunch of nodes with defined interfaces, but be more of a framework where it is simple to build a complete navigation stack without feeling like you are plugging things together manually.", "If we do this, then we can achieve the goal of allowing different planners to be plugged in, etc. that is frequently stated in this thread.", "It will also ensure that it is inherently easy to introspect the internal navigation process, because we can intercept all the messages flying around between the different parts.", "Building on this, navstack2 should then provide a default configuration that provides an equivalent or better navigation functionality to the ROS1 navigation stack.", "As someone who has a strong desire to participate but is something like 1000% over-committed in time, I would also like to see a central place where I can comment on design decisions and implementation choices made when I have time, without losing track of where everything is. Please make a github project so we can make issues and discuss them, if you haven\u2019t already.", "A simple undocking algorithm", "I think this is too application-dependent. But, it should be ", " easy make things like this default behaviour for your robot in navstack2.", "wow,  very detailed, and right, it\u2019s what I meant\u2026 ", "All, thanks for the input so far, keep it coming!", "A few comments.", "Keep the feedback coming!", "To enable autonomous navigation, you have to allow the robots to sense the environment around to create its map and enable collision avoidance, In other words, sensor data input to any algorithm is important.", "\nNow there are various sensor solutions serving for this purpose, for example, Sonar, Lidar, MMW, Vision and so on they serves different preference and it\u2019s possible to sensor fusion with them in the real autonomous navigation. Free to think ROS2 navigation can consider more:", "Late to the party, but here\u2019s my $0.02.  Generally, +1 to all of Geoff Biggs\u2019 coimments.  I\u2019d like to see:", "I\u2019d be interested in having our group here at Oregon State help with some of this, depending on where it goes.", "cheers", "\u2013 Bill", "Not really a navigation stack user myself, so just passing by, but I was surprised ", " wasn\u2019t / isn\u2019t mentioned more. Only ", " mentioned it once earlier in this thread.", "From the ", " at ROSCon and ", " it seems it\u2019s gone into the direction that ", ", ", " and some others sketch:", "Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the ", " and the ", ". For navigation on costmaps see ", " and ", ".", "Would seem to be a good idea to get some input from its maintainers (", " et al.).", "Yes, I was overjoyed when I saw ", " announced at ROSCon last year. I consider it a ", " step in the direction I want to see the navigation stack go. I haven\u2019t had time to try it out myself yet, but I agree with ", " that any effort to develop a new navstack for ROS2 should consider it the biggest input into design.", "Hi all,", "\nWe\u2019ve created a repo for the ROS2 Navigation project here:", "\n", "ROS2 Navigation. Contribute to ros-planning/navigation2 development by creating an account on GitHub.", "\n", "We\u2019re going to collect all design inputs, here, starting with high level use cases and requirements:", "\n", "Please submit your use cases and requirements via pull requests so we can have design discussions there.", "Thanks,", "\nMatt", "A bit late, but I figured I would chime in (having been a maintainer of the navigation stack for going on 5 years now).", "First, I concur with the several comments about better modularity. I\u2019d almost suggest that the ROS2 navigation stack shouldn\u2019t include any planners in the main repo (as is the case with MoveIt). There have been several newer (and probably better) planners developed \u2013 but users assume they should use only the \u201cdefault\u201d planners. At the same time, maintaining a code base that includes \u201call the planners\u201d is just not feasible. Having better modularity, and having things like local and global planners exist in other repos, makes it far easier to have more maintainers involved, and for development to proceed quicker (if you don\u2019t like planner X, go write and release planner Y).", "While splitting those things out to other repos, I would suggest providing some basic/core code to build planners on. At some point a Willow Garage intern started to refactor base_local_planner in that direction \u2013 but it was never really finished.", "On the subject of 2d/3d \u2013 I think there is a fine balance to walk here. While most research is pushing more in the 3d direction, commercialization tends to push towards cheaper/smaller processing power \u2013 and some of the optimization in terms of 2d/2.5d in the navigation stack is important here. While a full 3d mode is awesome, requiring the whole system to always act as 6Dof pose + 3d terrain may make it unusable on smaller platforms like Turtlebot.", "With regards to not being monolithic \u2013 I think this will be a serious challenge. One of the things that ROS1 does a really poor job of is synchronizing the operations in multiple nodes. I\u2019m not sure how much ROS2 really helps in that regard.", "But here\u2019s my most important feedback: we need better testing. One of the reasons we have a hard time merging things in the current navigation stack is that there is just almost NO test code (similar issues with MoveIt). I have spent an enormous amount of time physically testing code in simulation or on real robots to try and be sure something contributed works \u2013 only to find out that it actually breaks some particular feature that someone was using. If you\u2019re going to largely overhaul/rewrite things \u2013 do it in a test-driven way, and make sure those tests are meaningful so that the system can actually be maintained.", "Has anyone looked at what other ROS2 dependencies are missing? AFAIK, there is no equivalent of actionlib yet (which is probably a pre-req to actually building most robot applications in ROS2). I\u2019m also not sure the status of things like parameter management or dynamic reconfigure (highly required for people to actually tune a navigation setup in a reasonable amount of time).", "-Fergs", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You like about the current ROS Navigation stack and would like to see kept the same (or equivalent functionality)", "Things you think can be improved on and would like to see done differently, aka. your wish list", "make a more flexible mechanism for plugin implementations, especially for recovery plugins.", "Support multi-thread and heterogeneous computing", "Adopt AI gene (e.g. Reinforcement learning) into path planning and collision avoidance.", "Support more map types", "it\u2019s better to support 3D path planning and CA", "I think you mean AI Gym, right? If so, are you aware of the ", " project? I have been using it for some RL work in this space. I believe it could be ported to ROS2 fairly easily also.", "What map types are you thinking in particular? Google Maps? Non-2D occupation grid maps?", "3D is on my wish list too, for applications like drones.", "What map types are you thinking in particular? Google Maps? Non-2D occupation grid maps?", "Semantically oriented Maps\n", "Indexed Points of Interest\n", "rondevu points", "moving goals", "\n", "Labeled region boundaries\n", "property borders", "tolls or crossing costs", "exclusion zones", "\n", "Annotated Affordances\n", "Doos, Elevators, Appliances, Chargers", "Departments, Faculties", "\n", "\n", "Vector Maps\n", "Floor plans or 3D scale models\n", "Google maps/earth", "Architectural blueprints", "\n", "Roadways maps\n", "Turning lanes, intersections, crosswalks, etc", "Congestion, Traffic density", "\n", "\n", "Geo Maps\n", "Topological\n", "Elevation and grade", "Underwater terrain", "\n", "Weather\n", "wind and tide velocities", "Dynamic time series forecasts", "\n", "Approximate at scale\n", "WGS84 vs local cartesian", "Alternate ", "\n", "\n", "\n", "All the dynamic reconfigure for sure.", "All the representations in rviz (necessary for tunning as well)", "I will keep the algorithms that are currently implemented in ROS1, if possible with the same parameters. This will make the transition smooth.", "I plan to create a ", " repo for this (hopefully under ", " namepace, working on that), and will start capturing the wish list items and requirements, as well as documenting design decisions, etc. This thread is just the primer for that.", "The many different types of maps requested, to me means we need a more abstract data type for maps, that can represent many different types of maps that might be inputs to the system. I\u2019m not sure what that will be exactly yet, but to me, this illustrates that we need to decouple the map type as much as possible from the system.", "I also agree on the use of ROS2 nodes for the low level plug-ins like global and local planning. That will improve decoupling and make it easier to replace those components with other algorithms, and will also ease the debug effort as pointed out before. We can do this using shared memory pointer passing so that the performance overhead is small.", "general flexibility to smoothly use their output in different phase of autonomous navigation", "how to adapt their combination or sensor fusion well while engaging with ros2 navigation stack", "consideration to certain solution with upcoming trend or innovation to extend, for example, for vision based, it may not require to create map firstly to navigation in the future.", "More pluggability in the elements of the nav stack, and the ability to hot-swap implementations.", "A common API that will allow people to add their own underlying representations.", "The ability to handle time.  I\u2019ve been toying with the idea of a map that changes throughout the day (as corridors become congested, and such), so the ability to integrate this into the system would be important to me as a specific use case.", "The ability to run more than one algorithm at a time, compare the results and mux them.  This is important for localization, where it can be used to compare the performance of two algorithms, or to use different algorithms at different time.", "Factoring things up as finely as possible.  As Geoff pointed out, this should be more lightweight in ROS2", "Ability to develop in Python or C++.", "Use floats or doubles as the underlying representation, not some fixed-point hack.  Actually, it might be nice to use arbitrary underlying representations (of probabilities).", "Some more modern algorithms from the literature.", "Being able to swap maps in and out of core seamlessly, so that I don\u2019t have to keep all of campus in memory at the same time.", "Multiple floors in a building.  Hybrids to let me get from one building to another."], "url": "https://discourse.ros.org/t/ros2-navigation-input-requested/4884"}
,{"title": "ROS2 Real-time Working Group Online Meeting 3 - Oct 2, 2019, 7AM PDT (UTC-7)", "thread_contents": ["Hi real-time folks, time for our 3rd meeting.", "Proposed agenda:", "Last meeting ", " and ", ".", "Respond to this conversation by answering to:", " ", " ", " ", " fyi", "Hi Dejan,", "We (", " and I) will be attending this meeting.", "The version we will discuss is a POC on the latest stable dashing release. We are currently also working on a PR (master) version.", "thanks for taking care of this thread, i would like to  join at this time.", "thanks", ",", "Thanks for the invitation. I\u2019m available.", "\nThese would be my topics:", "Kind Regards,", "\nAndrei", "Hi all, below are the meeting invite details.", "Topic: ROS2 Real-time Working Group Online Meeting 3 - Oct 2, 2019, 7AM PDT (UTC-7)", "\nTime: Oct 2, 2019 07:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "Meeting ID: 571 727 736", "\nPassword: 074189", "One tap mobile", "\n+16699006833,571727736# US (San Jose)", "\n+19294362866,571727736# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 571 727 736", "\nFind your local number: ", " thanks a lot.", "Can you and ", "  then present this in details tomorrow?", " Will you be able to attend the meeting tomorrow (4PM German time)?", "If yes - do you have any update on this topic:", "Repository of tools for static and dynamic code analysis", "Ralph Lange and Jan Staschulat will be there.", "Hi Dejan,", "I\u2019ve made a small presentation that shows the differences between the current executor and our proposal. I think it is easy to follow even for people who don\u2019t have a deep understanding of how the current executor works. I will need about 10-15 min. to explain the executors and their differences, but I will need presenter view for this (screen-sharing). Is this possible? I have no experience with Zoom Meetings.", "The presentation will be quite short, but if people are interested we can have a discussion after the presentation about the possible next steps and features people require. The executor we made shows possible CPU gains and gives an alternative for specific use-cases, but a proper all-round executor will need additional features.", "\nFeatures that are currently on the TODO list:", "If there is no time during the meeting to discuss these things then people can just post their thoughts on the PR ", ".", "Kind Regards,", "Martin", "I will need about 10-15 min. to explain the executors and their differences, but I will need presenter view for this (screen-sharing). Is this possible? I have no experience with Zoom Meetings.", " yes, this is absolutely possible and easy.", " we started with the long list of goals couple of months back: ", ".", "In that thread I proposed  an overall goal which would be to make ROS 2 hard real time. This would then mean that we\u2019d need to \u201canalyse an entire stack, from the hardware platform to the applications written with ROS 2\u201d and improve the realtime across the entire stack.", "However the guys have, rightfully so, ", " that this is simply too much work and it depends too much on the use case and selected underlying stack (ECU, RTOS, network interfaces, \u2026).", "As a result we agreed to work on the following sub-goals:", "Additional goals are of course welcome but we will also need someone to work on them.", "All (12 participants), thanks for attending.", "Old link: ", "New link: ", "Check out zhe rmw implementation - all", "Looking for 1-2 candidates to join Erik and his team in creating ", " on top of  ", "  - ", "Create a system for documentation sharing (GDrive, Github project, \u2026) - ", " is this something that you could maybe do?", "Get the presentation from Martin - ", " could you paste it here?", "Present the mini buildfarm with resource guarantees at Apex.AI - ", "Carlos has been working on the demo that shows some of the real-time characteristics. - ", " can you share the demo with us?", "Dejan to take content and conclusions of this meeting to OSRF and TSC- ", "PRs for the static executor - all have a look and comment", "ROS 2 Real-time WG to meet every 2 weeks (Wednesdays 7AM PST)", " - would you be able to join the meetings for this group?", "Yes, I\u2019m happy to join any meeting you think I could help with, please ping me directly for each meeting you\u2019d like me to attend. If you need me on a regular basis I can do that too, though selfishly I\u2019m trying to stay out of the way as much as possible.", "ROS 2 Real-time WG to meet every 2 weeks (Wednesdays 7AM PST)", "Is it at all possible to shift this? It clashes perfectly with AWF board meetings.", "Hey everyone,", "Discourse won\u2019t let me upload the presentation as a powerpoint and the presentation in pdf format is not really usable (the transition of images on a single slide is how I show the working of the executors). Instead I will share some links here, while I look for a way to share the powerpoint.", " this link contains a README where the executors are explained with flowcharts. The flowcharts + explanations get the ideas across of each executor.", " This is the fork we use for the pull request that is up to date with master (slightly behind by now probably).", " we also made a version that we merged with ", " PR for improved intra_process communication.", "For the people that are not aware of why we worked on this static executor here is a very short history. There is more links in the links pointing to more links if you feel like going on an adventure.", "\n", "\n", "\n", " (related, but less interesting)", "Hopefully this gets everyone what they are looking for. If you want more information or help with the code or understanding the STE or static executor, don\u2019t hesitate to send me a message.", "Your flow chart for your static executor is missing some labels on the decision branches.", "I do not have a lot of experience making flowcharts, nor do I know the exact rules to making them. I hope that despite the inaccuracies they are still readable. Sorry for the possible inconvenience.", "Any diamond is a decision. Each arrow going ", " of the diamond needs a label to say what the condition is for that branch being taken.", "I added the powerpoint that I used during the meeting for my presentation to ", " in the presentation folder. Please let me know if it works by downloading it and looking at it in presentation view (to cycle through the images).", "Sidenote:  The dashing version now uses the nodes\u2019 guard_conditions as event trigger to rebuild the executable list and the wait-set. Making the use of the static executor less restrictive (semi-dynamic, rebuild only when necessary). We will continue work on a master (eloquent) version and look into a multi-threaded version.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Single process, real-time rmw => ADLink", "Repository of tools for static and dynamic code analysis => ", "\n", "Performance testing on target hardware => Apex.AI", "I would propose to hold meetings regularly, every 2 weeks", "Are you available?", "Do you have additional items for  the agenda?", "Can you expand on your agenda items?", "Yes we are available", "We created a POC StaticExecutor for rclcpp that can be added without breaking existing source code. If someone wants to use the StaticExecutor instead of the SingleThreaded executor, this only requires a change to one line of code at user-code level.", "The code and preliminary results can be found here ", ". This StaticExecutor significantly reduces CPU usage. We can quickly explain how we changed the existing executor. We have aligned with Ingo and his team to collaborate on a better executor for ROS2. Our focus is on ", " whereas Ingo and his team are working on proper ", ", both of which are important for RT.", "go via AIs from the last meeting", "discuss probably the goals of the RT WG (Personally I\u2019m not sure I understand them, and during the last meeting I got a feeling that those goals are quite different for the RT WG participants)", "Different scheduling mechanism (Bosch, create RT safe scheduling, add priority)", "Semi-dynamic setup (Nobleo, rebuild the system based on event triggers, i.e. something added to a node)", "Provide a use case description and requirements", "Implement real-time able rmw (single threaded, static executor)", "Present findings on real-time audit in rcutils, rcl and rclcpp and continue improving", "Create a shared repository for tools for static and dynamic code analysis and tracing", "Joe Speed, Erik", "Ralph, Jan", "Dejan", "Andrei", "Carlos", "Martin", "\u2026", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", " - would you be able to join the meetings for this group?", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros2-real-time-working-group-online-meeting-3-oct-2-2019-7am-pdt-utc-7/10741"}
,{"title": "Introducing the Robot Vulnerability Database", "thread_contents": ["Hello ROSers,", "As part of our commitment with security, ", " is glad to introduce the ", ", a community-contributed list of robot vulnerabilities and weaknesses.", "This effort aligns with Alias\u2019 mission to ", " and is the first public step we take towards implementing it. Briefly, we share the belief that vulnerability disclosure is a two-way street where both vendors and researchers, must act responsibly.  We thereby adhere to a ", " (read more about ", ") while other flaws such as simple bugs or weaknesses could be filed at any point in time. We notify vendors of vulnerabilities immediately, cooperate with them and favour a coordinated  disclosure where ", ", or sooner if the vendor releases a fix.", "This policy is strongly in line with our desire to improve the robotics industry response times to security bugs, but also results in softer landings for bugs marginally over deadline. According to ", ", most vendors are ignoring security flaws completely. Similar to us, we call on all security researchers to adopt disclosure deadlines in some form, and feel free to use our policy verbatim (we\u2019ve actually done so, from ", ") if you find our record and reasoning compelling. Creating pressure towards more reasonably-timed fixes will result in smaller windows of opportunity for blackhats to abuse vulnerabilities. Given the direct physical connection with the world that robots have,  in our opinion, vulnerability disclosure policies such as ours result in greater security in robotics and an overall improved safety. A security-first approach is a must to ensure safe robotic operations.", "The ", " is an attempt to register and record robot security bugs including both weaknesses and vulnerabilities (refer to ", ") The current content has been built over the past months and includes at the time of writing more than ", ":", "As contributors of ROS and ROS 2, we have create a particular section for ROS (currently only highlighting ROS 2 flaws) available ", ".  We have committed resources to maintain this list and process flaws while reporting about the status of vulnerabilities at the corresponding ROS 2 Security WG meetings. We invite everyone in the community to contribute and help processing security flaws. Currently and as recorded by our team at RVD, ", ":", "Over the coming months we expect to include several ROS and ROS 2 packages in our pseudo-automatic robot security pipelines and collaborate with maintainers while recording and addressing security vulnerabilities and weaknesses", "We\u2019d like to acknowledge and credit the support we received from the ROSin project which partially enabled the development of this work. In particular, RVD will be used to report the findings of ROSIN RedROS2-I and RedROS2-II FTPs, funded by the European Union\u2019s Horizon 2020 research and innovation programme under the project ROSIN with the grant agreement No 732287.", "Finally, a small disclaimer: ", "BTW, for those interested in learning more about our work but also about security in robotics, overall, we invite you all to attend the ", " that will happen within ROSCon 2019 in Macau!", "Do ", " rules that create world writable devices count as a vulnerability or a weakness?", "Hello ", ",", "Do ", " rules that create world writable devices count as a vulnerability or a weakness?", "IMHO, provided those rules were defaults and/or could be forced into a robot or robot component (remotely or even locally, that would just affect the severity) and take advantage of those devices (as part of an exploit), then according to our ", ",  it could be considered a vulnerability. In that case you should be able to provide it with a score for its severity using ", ". See the discusion ", " for a bit more of context in vulnerability/weakness.", "Our goal with existing reported (and future) flaws is to provide means for reproducing and validating its exploitability. We\u2019ve been prototyping for a while (refer to our early ", " approach) and are currently working on a prototype based in Docker. We hope to release is shortly. The idea is that each researcher reporting a flaw should provide a docker-based image that allows reproducing the flaw, arguing about it and ultimately, facilitating its mitigation.", "Interesting undertaking; we clearly need more attention paid to security issues in robotics.", "To help better understand the goal and how the process works, I have some questions:", "Why do we need a robot-specific database for cybersecurity vulnerabilities? Is there a shortcoming in the widely used ", " system? A ", " in their database shows more than a few entries that would seem to be of the type that you\u2019re proposing to keep track of.", "If disclosure of vulnerabilities is meant to be delayed for up to 90 days, but the community submits new vulnerabilities to RVD as public github issues, then how does the delayed disclosure work?", "When you notify a vendor of a vulnerability, does an email address (e.g., security@) suffice, or do you need require other communication mechanism?", "ROS 2 presents 236 security weaknesses", "I have a (maybe stupid) question: is it common practice for for these kind of vulnerability statistics to include test-only code? While problem in test code should certainly be addressed it looks to me that they aren\u2019t really a defect in the shipped software (since they are not part of the packaged software after the build).", "Another note regarding the published numbers here: the public GitHub tickets seems to contain ", " redundancy. The very same defect is being ticketed up to one hundred times just since the functionality / API is used in numerous packages - for each a separate ticket is created. I would certainly consider that to be only a single vulnerability / flaw (even though it affects many packages).", "Both of the above seems to \u201cinflate\u201d the reported numbers significantly therefore I would suggest to reconsider how to account for those in statistics like this. I would also be interested in the actual (non-inflated) numbers to see where we are roughly at.", "Why do we need a robot-specific database for cybersecurity vulnerabilities? Is there a shortcoming in the widely used ", " system? A ", " in their database shows more than a few entries that would seem to be of the type that you\u2019re proposing to keep track of.", "CVEs are managed by ", ", organizations authorized to assign CVE IDs. ", " (see Requirements at ", "). We are taking (what we think are) the right steps to become a CNA and will soon start submitting CVEs (and provide CVE IDs to reported vulnerabilities within RVD). By no means Alias Robotics intends to ", ", we aim to empower it.", "That said, while we look up to the work that Mitre and many other partners started within CVE, over the past year or so, Alias Robotics identified several limitations and started building RVD accordingly (a robot-", " database of vulnerabilities) and strong barriers to change things. Without getting into an extremely verbose reasoning on the things we\u2019ve tried (and failed) and would like to see improved within CVE for robotics, see below some of the aspects we dislike and consider critical to move forward in securing robots and their components:", ": while ", " is complete right about the fact that the current CVE List provides results when searching for ", " (43 CVE entries), ", " (13 CVE entries) and even the more generic (and misleading) query ", " ( 892 CVE entries), a closer look into results (at least to us) into realizing that finding ROS-related flaws is a challenge. Contributing to categorizing this information better is something we\u2019ve committed to but don\u2019t believe will happen in the short term given the complexity/limitations of how the CVE List works (and where robot-related vulnerabilities are the minority, still) ", ". There\u2019s a lot of work to do on this still and we have internal tickets for it. We plan to separate the existing template for reports into two (weakness and vulnerability, facilitating escalation from weakness to vulnerabilities). We also hope to automate the process of reviewing flaws by using parsers that automatically and periodically review all tickets and report/tag those that are malformed.", ": Let\u2019s take ", " (and related sub-reports within the entry) as an example, for a security researcher to reproduce this flaw and provide a mitigation or simply patch it temporarily in their shop-floor, more information would be required. The intrinsic system integration of the robotics field demands for additional bits of information. Examples include a well defined and appropriate severity (to priorize flaws), a reproducible environment and instructions (if feasible) and likely (though this is a personal feeling), a channel for an open discussion where other researchers might triage/contribute/discard the flaw itself (you will find that most robot-related flaws within CVE List have barely been triaged). ", ".", ": Working with robots is very time consuming. From my experience, anyone that has built a robot with ROS understands the pain of rebuilding workspaces across platforms. This is not a criticism though, it\u2019s likely an inherent characteristic of the complexity of the field and the tradeoff of the modularity of ROS. Mitigating a vulnerability or a weakness requires one to first reproduce the flaw. This can be ", " time consuming. Not so much providing the fix itself but ensuring that your environment is appropriate. ", ". We\u2019re still working on it and hope to make it available to everyone very soon.", ": CVE uses CVSS to report on the severity of vulnerabilities. As we discussed and published a while ago, CVSS has strong limitations when applied to robotics. Simply put, it fails to capture the interaction that robots may have with their environments and humans. This is critical when considering the severity of a flaw and has been discussed repeatedly in the security community. We\u2019ve been thinking about all these aspects for ", ", ", ": while some might disagree, from our iterations we found that the process with CVE is somewhat slow. From our research we found that most robots and robots explored nowadays (specially industrial robots!) are highly vulnerable. We believe that a more dynamic path would facilitate mitigating many of these vulnerabilities and accomplish our mission of ", ". ", " (such as checking prior tickets and invalidate it if repeated, tag it as malformed and request for more information, etc.)", "Alias Robotics has committed resources to all the things listed above but of course, support and contributions are more than welcome. We hope the ", " and its members can find a way to support us in this endeavour. Of course, contributing to the CVE List is something we all should do but in our opinion, that falls short.", "From our humble experience, we don\u2019t see CVE changing several of these aspects (in a somewhat acceptable timeframe). This to us, justifies the launch of RVD. We hope to prove with RVD that our statements regarding robotics are somewhat correct and that more resources should be allocated to it. Hopefully this will provide a much stronger argument to Mitre and other parties within CVE.", "\nThe ultimate reason why we decided to launch RVD is because we hope to demonstrate that some of these features are worth integrating into the CVE List.", "If disclosure of vulnerabilities is meant to be delayed for up to 90 days, but the community submits new vulnerabilities to RVD as public github issues, then how does the delayed disclosure work?", "The disclosure policy applies to Alias Robotics and our engineers. We hope to inspire the community with this policy. It\u2019d be great if other groups and individuals were to adopt it as well but we can\u2019t enforce it.", "Anyone can literally jump into the wild and publish vulnerabilities or even worse, sell them in dark markets (rather common from what we\u2019re observing lately). RVD provides a channel to do it responsibly. An approach could be to list the flaw as a weakness (according to our classification, vulnerabilities are a more elevated \u201cdegree\u201d but all vulnerabilities are weaknesses) and then reach out the vendor/maintainer privately providing more information about the flaw and offer support for its mitigation. Eventually, either after 90 days or after a fix has been shipped, the weakness ticket could be enhanced and the complete exploit could be disclosed turning the weakness into a vulnerability.", "A particular example of this is ", ". ", " and other folks have done an amazing job characterizing that and even disclosing a mechanims to reproduce it. The RVD ticket remains as a weakness because we still haven\u2019t found time to reproduce it and provide a mitigation for it. Once we do so, we will elevate it to a vulnerability.", "When you notify a vendor of a vulnerability, does an email address (e.g., security@) suffice, or do you need require other communication mechanism?", "The e-mail ", " is what we are using so far. We haven\u2019t identified the need of additional communication channels for now.", "That was a bit long, sorry ", " !", "have a (maybe stupid) question: is it common practice for for these kind of vulnerability statistics to include test-only code? While problem in test code should certainly be addressed it looks to me that they aren\u2019t really a defect in the shipped software (since they are not part of the packaged software after the build).", "I don\u2019t think this is stupid at all, you\u2019re right. Without saying that we discard these issues (having good flawless tests is relevant), most of the flaws affecting tests that we\u2019ve processed affect underlying software layers (and not explicitly the test code itself). Also, testing on test-only code gives a first valuable intuition. Of course, use-case specific tests are more appropriate but I doubt vendors or integrators would be willing to open source these up (and if they do we\u2019ll do our best to pick them up!)", "I haven\u2019t processed all the RVD tickets but from the intuition acquired building it, I\u2019d say that currently most of the flaws at RVD do report about these underlying defects. We have limited bandwidth and try to focus on what\u2019s more critical.", "Another note regarding the published numbers here: the public GitHub tickets seems to contain ", " redundancy. The very same defect is being ticketed up to one hundred times just since the functionality / API is used in numerous packages - for each a separate ticket is created. I would certainly consider that to be only a single vulnerability / flaw (even though it affects many packages).", "True. This can be further seen with a closer look at the ", ", mitigating a flaw closed several tickets. We\u2019re working on this. As mentioned in my previous comment above, we\u2019ve got some internal tasks already allocated to do this automatically parsing syntactically tickets daily and reaching a compromise. We don\u2019t have a solution ready unfortunately but it\u2019s coming.", "Both of the above seems to \u201cinflate\u201d the reported numbers significantly therefore I would suggest to reconsider how to account for those in statistics like this. I would also be interested in the actual (non-inflated) numbers to see where we are roughly at.", "All right, noted. We\u2019re slowly building up though and have already filtered out quite a bit. Note that the current tickets represent only a very small subset of the ROS 2 packages (ROS core and navigation2 mostly, we disabled the rest for now) with a ", ". Our security pipelines include several static and dynamic tests. Including the autogenerated reports from static testing tools will increase the current number of flaws by an order of magnitude at least (which again, would be hard to interpret).", "Any advice or disagreements (reasoned please) would be very helpful but the general intuition we\u2019re trying to develop is:", "A relevant number to obtain a quick intuition for the insecurity of ROS 2 would be the number of vulnerabilities open (not mitigated). Does this make sense to you ", "? Also, would it help to point to this thread of conversation from the RVD README.md file for further intuition?", "Happy to join the great (and very relevant) discussion points on this thread. Just sharing some thoughts:", "I may add, that conversely to what ", " was stating, CVE covers a yet somehow ", " of Robot-specific vulnerabilities. Very little commitment has been shown so far both by security researcher and robot manufacturers at least when it comes to reporting CVE\u2019s and there is vast amounts of work to be done. RVD is an attempt to systematize this workflow, which complements and feeds vulnerability records maintained by the competent authorities and serves as supporting documentation.", "I\u2019d like to share as well additional challenges we faced ourselves in Alias Robotics when digging into the actual records in vulnerabilities. For example, when we type \u201crobot\u201d in the CVE browser, most of the references will refer back to ROBOT (Return Of Bleichenbacher\u2019s Oracle Threat) which as 19-year old vuln on the RSA encryption which in most cases, does not necessarily apply to a robotic system (won\u2019t in all cases I\u2019ve inspected). Making an emphasis in the fact that we report actual \u201crobot - specific vulnerabilities\u201d is and will be an additional challenge to segregate from other more \u201cIT related\u201d flaws, as ", " points out.", "Similarly, I do believe that ROS2 adoption can greatly benefit from the transparency in the security workflow proposed within RVD.  Weaknesses can be separately inspected and mitigations adopted, all in a trackable and reproducible manner, so ROS2 resources and be kept up to date and secure ", " when used. Of course, there is tons of work to be done still and community contributions will be super-welcome!", "I think this is a very important and highly needed initiative. The potential consequences of an insecure robot are very concerning.", "\nI support the idea of a robot-specific collection but I also agree that it needs to be well maintained.", "However, I think it es even more important to raise the visibility of such a platform. Otherwise ist usefulness will be very limited. OEMs, System Integrators and researchers alike should be aware of this and ideally actively taking part in the process.", "\nI absolutely welcome that Alias is taking the lead here, but elevating this initiative to a broader support by other players would be important. All the issues discussed above (90-days deadlines, scoring, \u2026) could be agreed-upon rules. What are your plans for this and what would be options?", "In any case, we will also actively contribute to RVD in the future.", " ", " Thanks for the detailed rationale.", "If RVD in intended to act as a more responsive and more detailed front-end to CVE, then the concern I\u2019m describing below can be mostly ignored. In that case maybe we can eventually team up with MITRE to improve the CVE feature set based on what our community finds useful in RVD.", "I\u2019m concerned that we might be claiming ", " by saying that robots need their own security flaw scoring and reporting systems. Robots are complex, sure, but there are plenty of physical, actuated things in the world that are controlled by software that might contain vulnerabilities. Are we following the example of other domains that have their own scoring and reporting systems or are we striking out on our own here? What do organizations working in automotive, building infrastructure, factory equipment, medical devices, or other \u201ccyber-physical\u201d fields do?", "Regarding the poor search results available for robot/ROS in CVE today: can that be attributed to the fact that approximately nobody is yet reporting flaws in these systems anywhere? Presumably once we get the community to consistently report their findings, the CVE database would come to contain much more useful information.", "To be clear: I\u2019m very enthusiastic about finding, reporting, and mitigating security flaws! But after 20 years of personally arguing that robots are special and so we need our own X (for many values of X), then living with the resulting maintenance burdens, I\u2019m also eager to reuse existing systems and approaches wherever possible.", "I still have not seen a good reason why we need to strike out on our own. I would rather leverage the work of the NVD and MITRE so people can reuse existing tooling, process and procedures.", "I would say the NVD is lacking in robotics specific CVEs because people have not submitted issues. We have opened 3 CVEs with MITRE this year for ROS packages:", "ROS is just packages on top of an operating system, it would be like Apache standing up a new vuln database just for Apache  projects instead of using MITRE.", "Cheers,", "\n-Joe", "I\u2019m concerned that we might be claiming ", " by saying that robots need their own security flaw scoring and reporting systems.", "Slightly off-topic, but: this is something me and my colleagues ", " and ", " also started wondering. This will probably also come up in our ROSCon presentation (", "), but I just wanted to add that at this point we\u2019re not sure whether CVE is sufficient for robot related bugs/vulnerabilities or whether issues in robot software are actually ", " different that they should get their own classification.", "I guess I wonder why ROS is special and a CVE would not be sufficient to handle a security issue? I mean from a design perspective it sends messages over the network and runs on Linux(for the most part). How is this different from an issue with MQTT?", "This seems like a bit of vanity project, ", " .", "I\u2019d rather we leverage the work and efforts of MITRE.", "Cheers.", "\n-Joe", "Regarding the poor search results available for robot/ROS in CVE today: can that be attributed to the fact that approximately nobody is yet reporting flaws in these systems anywhere? Presumably once we get the community to consistently report their findings, the CVE database would come to contain much more useful information.", "I want to echo this point. There\u2019s actually a PR aspect of this to consider: mature products have CVEs. It\u2019s part of life these days. Security is only recently becoming more of a concern in ROS. As that grows, so will the CVEs, and the perceived maturity of ROS 2. That\u2019s one of the reasons we\u2019ve been submitting them!", "If RVD in intended to act as a more responsive and more detailed front-end to CVE", "I think this is a good way to put it. We certainly tried our best to avoid reiventing the wheel. Our intention is to get aligned as soon as possible with MITRE and the CVE List. Becoming a CNA will help  voice out our opinions (to some extend) and we hope to remain constructive on what needs to change to facilitate securing robots and robot components. RVD is a fast-track we\u2019re taking.", "Our experience (coming from a robotics background and) having tried these tools for a period of time is that they\u2019re not sufficient. I\u2019d be interested in other roboticists from the community sharing their views as well.", "When it comes to scoring mechanisms for the severity of vulnerabilities, ", " (", ") was built by researching what other (robotics) related areas were demanding and wasn\u2019t being met. [", "] or [", "] are among the ones cited while building it. The white paper above discusses it in more detail and proposes a scoring mechanism that takes in consideration aspects that directly apply to self-driving cars and other similar autonomous devices.", "Any help interfacing with MITRE will certainly be very helpful!", "I would say the NVD is lacking in robotics specific CVEs because people have not submitted issues. We have opened 3 CVEs with MITRE this year for ROS packages:", "I applaud this action. This is great and we certainly encourage everyone involved in security to follow a similar approach and commit resources to file reports in the CVE List. As pointed out above, we certainlly will. The big question we asked ourselves when designing RVD was, \u201cAs a roboticist/security researcher, what do I need and find more useful to ", " flaw A in a robot?\u201d  (ROS  specifically, here)\". The core is mitigation.", "I did a quick search on the first ID (", ") you listed above but it\u2019s undisclosed. My guess is that the same patterns critized above about reports in the CVE List are in those reports (or maybe not and I\u2019d be gladly surprised!).", "ROS is just packages on top of an operating system, it would be like Apache standing up a new vuln database just for Apache projects instead of using MITRE.", "This is not what\u2019s being proposed here, at least not within RVD. It\u2019s implicit on its name \u201cRobot Vulnerability Database\u201d. It\u2019s not ROS-specific, it\u2019s for robots. You may claim the same and that robots are a sub-class of hardware which doesn\u2019t deserve its own treatment. Well, I would then object and indicate that according to several sources CVE has currently serious issues capturing vulnerabilities that affect to hardware.", "\nOne only needs to parse the CVE List, compare the density (of hardware vs software reports this year) and the \u201cvalue\u201d on the content of these reports to draw some conclusions.", "One point I\u2019d like to make is that ", ". This is not the feeling we get with the CVE List. We advocate to bring these mechanisms to the CVE List. When reading the CVE List, we felt that many reports where terrible and didn\u2019t help at all reproducing the flaw.", "This seems like a bit of vanity project, ", " .", "Nobody is trying to replace CVE ", ", see reasoning above.", "Are we following the example of other domains that have their own scoring and reporting systems or are we striking out on our own here? What do organizations working in automotive, building infrastructure, factory equipment, medical devices, or other \u201ccyber-physical\u201d fields do?", "Consider ", ", particularly their ", ".  Each advisory may contain multiple vulnerabilities but each vuln links to a CVE.  Also important to note that each vuln has a standard CVSS score to support prioritizing.  Both CVE and CVSS are mature, communicate well, are deeply integrated into vuln management tools.", "most of the flaws affecting tests that we\u2019ve processed affect underlying software layers", "I think I have seen several entries which are only identifying flaws in test code, e.g. using the public API incorrectly / insufficiently. So in these cases the defect is not in the used code but in the test itself.", "Anyway my suggestion would be that it would be helpful if those would at least be moved / accounted for in a separate category to draw a more precise picture how many problems actually affect the code used by applications.", "But after 20 years of personally arguing that robots are special and so we need our own X (for many values of X), then living with the resulting maintenance burdens", "This is my daily life right now\u2026 I really cannot recommend it!", "I may have missed this, but how does the RVD propose to avoid:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "there will be a significant amount of weaknesses reported for ROS 2, several referring to quality bugs (opposed to security ones)", "flaws that are exploitable will be listed as vulnerabilities (note that we don\u2019t have yet a single vulnerability for ROS 2 (neither closed ones, which to us means, mitigated))", "CVE-2019-13445 - potential integer overflow", "CVE-2019-13566 - potential string overflow", "CVE-2019-13465 - potential iterator cause buffer overflow", "CVE-2019-13445 - potential integer overflow", "CVE-2019-13566 - potential string overflow", "CVE-2019-13465 - potential iterator cause buffer overflow", "Duplication of reports between CVEs and the RVD?", "Robotics engineers needing to watch two databases to get all possible vulnerabilities?"], "url": "https://discourse.ros.org/t/introducing-the-robot-vulnerability-database/11105"}
,{"title": "ROS 2 Real-time Working Group Online Meeting 7 - Dec 11, 2019, 7AM PDT (UTC-8)", "thread_contents": ["RTWG folks, here is an agenda for our next meeting. Please add your topics if you have them.", "Zoom coordinates: TBD.", "More information from the meeting recording:", "I\u2019ll let ", " add more details on Wednesday!", "Hi Dejan,", "Both Ishu and I are busy/not in the office tomorrow, so we won\u2019t be able to attend this meeting.", "To address points 5.1-5.3:", "\nOur reasoning is as follows: We are very excited that our static_executor has gotten as much attention as it did (this was very unexpected for us) and we hope some of our work will end up in the core of ROS2 (this would be amazing!), but we\u2019ve always seen the static-executor more as a POC w.r.t. \u201cjust\u201d CPU utilization. The executor in ROS2 has some more flaws (i.e. scheduling as highlighted in the paper by Tobias) and our static-executor POC is build on top of legacy that blocks/prevents neat minimal code and further other improvements.", "It is difficult to rework rclcpp, rcl and rmw with everything sitting on top of it and equally difficult from the other perspective to keep up with API changes. We want to see what design OSRF/William/the community can come up with and what changes to ROS2 will (need to) happen to realize that design.", "With the limited manpower we have, we decided the nicest thing we could do was to wrap the static-executor into a separate library for both dashing and eloquent users to use. When we get closer to the F release and the design has been settled on, then we are definitely willing to contribute and help port the static-executor if desired. However, keeping up our PR with master in (what is expected to be) a quite tumultuous time w.r.t. rclcpp, rcl and rmw, is not really possible (especially since both Ishu and I are on different projects atm that take up a lot of our time).", "My apologies for the long explanation, but hopefully this shines a light on our point of view. We hope for your understanding and look forward to the design discussions and of course the final design for F turtle ", "Zoom coordinates: TBD.", "is that available now?", "At all, sorry for being late but here are the meeting coordinates:", "Topic: RTWG Meeting 7", "\nTime: Dec 11, 2019 07:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "Meeting ID: 910 363 819", "\nPassword: 935893", "One tap mobile", "\n+16699006833,910363819# US (San Jose)", "\n+19294362866,910363819# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 910 363 819", "\nFind your local number: ", "\n", "about ", " only to connect specific device/robot, yesterday you mentioned some new feature is submitted by ", "  to DDSI-RTPS, right? is it already ready on ", "?", "could you enlighten me a little bit more? I\u2019d like to catch up this.", "Best", "Hi Tomoya, Yes, Erik ", " added domainTag to ", " and implemented iRobot\u2019s use case in Eclipse Cyclone DDS:", "Find Roomba by serial # among ~1,000 robots on the network", "With ", " things talk if domain and the tag (e.g. robot serial #) are matched.", "Happy to chat about that and other ", " contributions to the ROS community.", "\u201cEclipse Cyclone DDS makes ROS 2 Easier, Smaller, Faster\u201d talk today at ROS-Industrial Stuttgart RICEU2019 is a good overview of the ", " ", " and interesting use cases from the community ", "\n", "thanks, will look into it.", "Meeting minutes (thx ", " for help):", "Video recordings: ", " Jan 8 2020 (skipping the one on Dec 25 2019).", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Presentation: SLX ROS Performance Testing Platform by Silexica", "Discourse post on how to measure performance correctly\n", "Jaime\u2019s talk transcribed", "\n", "LET executor ", " => ", " ", " ", "\n", "Architecture of single-process, real-time rmw => ", "\n", "\n", " would now be a good time?", "\n", "static_executor recap:\n", "William suggests to change executor design in ROS 2 F: ", "\n", "Martin agrees and offers the current version as an ", " library", "After the changes in ROS 2 F, we will port some parts from ", " to rclcpp?", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Discourse post on how to measure performance correctly\n", "Jaime\u2019s talk transcribed", "\n", "eProsima published latency tests results, looking at the median value of multiple runs", "using a clone of the system and same configuration as the one used by their competition", "with open source code for reproducibility", "Presentation: SLX ROS Performance Testing Platform ", "  by Silexica/Ben\n", "\n", " Ask Amazon if they want to host HW for performance testing platform (on ", ")", "Ben to give regular updates in the RTWG", "\n", "LET executor\n", "\n", " comments on the rcl PR (", ") will be addressed and then the PR will be re-directed to rclc (", ")", "Bosch will also provide the documentation and the use case description for LET executor", "William is OK with the full restructure of the rclc project", "\n", "Architecture of single-process, real-time rmw\n", "Erik\u2019s comment on memory allocation for the data path in rmw_cyclconedds: ", "\n", "\n", "  Members of the RTWG should provide feedback", "Doing it from scratch vs. building on rmw_cyclonedds => 2nd option is preferred", "Erik thinks it makes sense to implement the zero-copy interface (from Bosch) first", "ROS 2 F => Erik thinks that we should start making use of DDS keys\n", "\n", " take this request to ROS 2 TSC", "\n", "\n", "static_executor work to be done for ROS 2 F:\n", "be able to use more than one executor per node.", "Implement Waitset similar to the one at Apex => ", " to work with William", "Interface clean up around the executor", "\n", "ROS 2 F Roadmap will be finalized early 2020\n", "Input to the roadmap: brainstorming of ROS 2 team@OSRF & others", "\n", " provide your input for the ROS 2 F roadmap", "\n", "DDS direct access (", ")\n", "There exist 2 different solutions/approaches: DDS configuration vs accessing DDS APIs", "Borja\u2019s use case in microROS => configuring vendor specific implementation", "William => 3 ways to expose more DDS features:\n", "out of bound for configuration of the underlying DDS implementation", "programmatically (e.g. rmw_init(payload) which has been done at Apex, but it is not portable across DDS vendors,", "ROS way with lots of abstraction and which must be portable", "AI ", " ", ": provide more concrete set of requirements for this use case", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-2-real-time-working-group-online-meeting-7-dec-11-2019-7am-pdt-utc-8/11863"}
,{"title": "ARIAC code release updates", "thread_contents": ["The software for competing in ARIAC 2018 has been released.", "See ", " for competition documentation, installation instructions, and tutorials for working with the software.", "New ARIAC software release (excerpt from ", "):", "First qualifier has been released (excerpt from ", "):", " Submissions for the first qualifier will be uploaded via secure online workspaces. All registered teams must contact ", " to have their workspace prepared in advance of when they intend to submit. If you are planning a submission for the first qualifier and do not yet have a secure workspace, you must contact ", " immediately or you risk missing the submission deadline.", "After the first qualification task closes the ", " topic will become classified as a cheat.", " The IIWA14 will be used in all future trials; the UR10 will no longer be used. The IIWA14 presents the same ROS interface but ", ". This was announced as an upcoming change on 17 February 2018.", " The workcell environment, particularly the shipping container, shelving and the conveyor belt, has been updated to accommodate the working area of the IIWA14. ", " The dimensions of the shipping boxes, storage bins, and products have not been modified.", "The controller has been updated to avoid an issue with joint 7 getting stuck, and to relax tolerances on the trajectory controller (", ").", "To test out the proposed changes, add the ARIAC pre-release repository with:", "and then run ", ". Your ARIAC version will be updated to 2.1.5, which will become the standard version if no regressions are reported. Please report regressions on the issue linked above.", "When you are finished testing we suggest you remove the ", " added in the above command.", "The software for competing in ARIAC 2019 (", ") has been released!", "Please see ", " for instructions as they have changed, especially if you have installed a prerelease. This version is feature complete. Follow this thread to get notified of future bug fix releases.", "Other links", "The software for competing in ARIAC 2019 (", ") has been released!", "\nPlease see the ", " and ", " for more information.", "This release contains the trial config files for the Qualifier.", "\nPlease see the ", " to learn how qualification works and the ", " to learn how to practice on the Part A trial configs.", "\nQualifier submissions will be accepted until ", ".", "Other links", "The end date of the 14th conflicts with those on the ", " site (", "):", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " See ", " for how to launch the trials. Summary of new trial config files (more details in the files themselves):\n", "\n", ": high priority order interruption at a \u201cconvenient\u201d time.", "\n", ": high priority order interruption at an \u201cinconvenient\u201d time.", "\n", ": order requiring products to be flipped (", ").", "\n", ": order requiring more products than available for use.", "\n", "\n", " See ", " for details.", "\n", " The arm can no longer pass through the support frame around the storage bins. This was advised as an upcoming change on 17 February.", "\n", " In response to the additional workcell collisions, the logical camera range has been reduced.", "\n", " This would happen if the ", " service was called before the simulation had finished loading.", "\n", " See ", " for details.", "\n", " See ", " for how to enable/playback simulation state logging for debug purposes.", "\n", " ", " now covers rules/scoring metrics in addition to software changes.", "\n", " Fixes for the simulation state logging (low RTF during recording, no arm inserted during playback) will be resolved in the next Gazebo release.", "\n", " The ", " product\u2019s yaw is not distinguishable with perception sensors, but its yaw is evaluated by the scoring algorithm. This will be corrected for, either by making the yaw perceivable or by having the scoring algorithm ignore the yaw.", "\n", " In particular, info on the storage bins, quality control sensors, potential products, and potential order count. See ", " for the added details.", "\n", " ", "/competition specifications have been updated to clarify that sensors can only be placed in static locations around the environment, they cannot be attached to the arm.", "\n", " Docker images used for the ", " are currently using gazebo7.7 to avoid state log recording issues, but will be swapped to the next gazebo version when it is released (within the next week).", "\n", " A fix for the simulation state log playback freezing with gazebo8.3 will be resolved in the next Gazebo release. This impacts teams looking to playback state logs from the automated evaluation setup. See ", " for how to install gazebo8.2 as a temporary fix.", "\n", " After the first qualification task closes the ", " service will become classified as a cheat. Teams will need to use sensors to infer the products available in storage bins.", "\n", " In ariac2.0.4, models in the environment of a particular type have sequential IDs. The naming system will be modified so that model IDs are randomized. As a result, logical cameras/quality control sensors will not publish TF frames, which include model IDs, correlated to the number of models in the environment.", "\n", " In ariac2.0.4, the conveyor can be controlled before the competition has been started. This functionality will be removed and must not be exploited by teams.", "\n", " For all future trial config files released (including qual1b), products in the storage bins will no longer have sequential IDs.", "\n", " Fixed misalignment of the vertical bars in the support frame around the storage bins.", "\n", " It is now asymmetric on top and bottom. Pulleys (and all other products) will always start un-flipped in the storage bins.", "\n", " By default the UR10 has joint limits of ", "; there is now an option to use joint limits of ", ", which provides better controller performance ", ". It can be enabled by adding ", " to your team\u2019s config file ", ". Participants that do not wish to enable this option do not need to make any changes to their system. The use of this option is permitted in submissions for the first qualifier.", "\n", " The ", " Docker image used in ", " has been updated to use gazebo8.4 now that the logging issues have been resolved.", "\n", " Teams must have had a secure workspace created by competition controllers before they can upload their submissions for ", ".", "\n", " See ", ". Note that these may change between rounds of the competition.", "\n", " Clarified that only automated evaluation metrics will be used for ", " and that any participants found violating submission guidelines (e.g. by starting the conveyor belt before starting the competition) will not be permitted to qualify for the Finals.", "\n", " ", ", ", ", ", ", ", ", ", " topics/services documentation added/updated in ", ".\n", "Clarified that requesting that the drone deliver any unknown shipment type will cause the drone to remove the box without scoring it.", "\n", "\n", "\n", "\n", "\n", " Re-runs for ", " will be permitted only in the event of simulation bugs such as reported issues ", " and ", ", at the discretion of the competition controllers. Any issues caused by competitors\u2019 code does not warrant a re-run.", "\n", " ", " has been updated to clarify that simulation state logging will be enabled in the first qualifier trials via the ", " setting in the the trial config file. If the config file that you are testing with in the automated evaluation setup does not have logging explicitly enabled ", ", please add that line to the config file to enable logging.", "\n", "\n", "\n", "\n", "\n", " Tutorials involving control of the robot arm have been updated for the switch to the IIWA14.", "\n", " Read/use ", " for practice.", "\n", " Read/use ", " for practice.", "\n", " The config file that was used for evaluating Part B of the first qualifier is in ", ". Congratulations to the teams that qualified for the Finals.", "\n", "\n", "The ", " service and the ", " topic have been re-classified as cheats.", "The quality control sensors now publish anonymized model names instead of the types of the faulty products that are detected. As a side effect, the model numbering scheme has been updated; any randomized faulty product IDs in custom-defined trial configs will need to be updated.", "\n", "\n", " Use of the devel space is now supported for users building from source (contributed by ", ").", "\n", " The ", " now highlights that sample config files are available for practicing with.", "\n", " Read/use ", " for practice. Note that re-connecting to some sensors during development will cause them to resume publishing data, but this functionality is blocked in the automated evaluation setup.", "\n", " This is the break beam positioned at the end of the conveyor belt by default.", "\n", " Service calls to set the conveyor belt power before the competition has been started will fail.", "\n", " The ", " now detail that sensors can be placed in any free space in the workcell, they do ", " need to be mounted such that they are touching the conveyor belt/support frame of the storage bin. Sensors must be used in a realistic manner and must not exploit any simulation technicalities such as the logical camera seeing through obstructions.", "\n", " See ", " for details. Teams that did not participate in the first qualification task, and teams that participated but did not qualify, are still eligible to participate in the second qualification round.", "\n", " ", ", which would occur occasionally for some participants, should now be resolved.", "\n", " The external walls of shipping boxes have been widened to make ", ", where boxes would fall into the conveyor belt, less likely to occur. The internal dimensions of the boxes are unchanged.", "\n", " The ", " have been updated to highlight that Gazebo 8.4 is required to avoid issues with state logging recording/playback. ", "\n", "\n", " ", " has been added, which summarizes the released agility challenges, including additional details on the \u201csensor blackout\u201d challenge.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", " The arm simulation model/controller has been updated to avoid an issue with joint 7 getting stuck/causing instability, and to relax tolerances on the trajectory controller (", "). ", "\n", "\n", " The ", " page now clarifies that products must be placed onto the base of the shipping box to be counted for scoring, not on top of other products.", "\n", " Two \u201cpre-release\u201d versions of 2.1.5 were released to users for testing prior to yesterday\u2019s ariac2.1.5 release to improve IIWA14 controller performance: 2.1.5~pre1 and 2.1.5~pre2. The version released yesterday was 2.1.5~pre2; ariac2.1.6 rolls back to 2.1.5~pre1 based on feedback from participants that it provides better arm performance.", "\n", " As mentioned in ", ", 500 sim seconds will be used for qual2b. The ", " now clarify that this information is not broadcast by the ARIAC server. For the Finals, as with the Qualifiers, the time limit will be set as a fixed value for all trials, which teams will know in advance. There are no time limits for individual orders.", "\n", " The config file that was used for evaluating Part B of the second qualifier is in ", ".", "\n", " The simulation model of the conveyor belt has been updated to  make ", ", where boxes fall into the conveyor belt, less likely to occur. There should be no other observable impact to users.", "\n", " The ", " has been released, including details on what to expect in the Finals, dry-run testing on the competition machines, and the schedule for the process.", "\n", " Structure of the ", " directory output by the automated evaluation setup is now described ", ".", "\n", " The config files used for the Finals of the competition are in the ", " directory. ", ". ", "\n", "\n", " (This change was included in the version of ARIAC used for evaluating the Finals).", "\n", " Closing/collection of shipping boxes during log playback now works correctly during state log playback.", "Registration: ", "\n", "Main NIST ARIAC Website (General information): ", "\n", "Competition rules: ", "\n", "Software documentation: ", "\n", "Support Email: ", "\n", "Forum for discussion: ", "\n", "Register to compete: ", "\n", "Main NIST ARIAC Website (General information): ", "\n", "Competition rules: ", "\n", "Software documentation: ", "\n", "Support Email: ", "\n", "Forum for discussion: ", "\n"], "url": "https://discourse.ros.org/t/ariac-code-release-updates/4009"}
,{"title": "Integration testing in ROS (2)", "thread_contents": ["We are currently working on a library of algorithms for autonomous (", ") driving based on ROS 2.", "For this library we detailed how we do unit and static analysis testing: ", " and are quite happy with it.", "What we would like to improve though is an integration testing. As a first step we consolidated (", ") and documented how integration testing is being currently done for ROS 2: ", ".", "However there are several problems with ", " described integration_test framework:", "To improve upon above we would like to propose the following improvements", "Secondly, we would also like to propose and get your thoughts on the additional types of unit and integration tests to be written.", "Fault injection tests", "\nThese tests aim at increasing the code coverage by introducing fault into the code path, in particular error handling code path which are rarely executed in normal tests.", "There are three potential places where faults can be injected in runtime:", "If fault injection test is adopted, testing code must be removed in release. Otherwise it could be utilized to perform attack.", "Random input tests", "\nROS 2 nodes are tested against independent random input data. The output, if it exists, doesn\u2019t have to be meaningful as long as the program handles it correctly. If an unexpected exception arises then it means there\u2019s a fault in the program. Random input tests are also used to avoid biased testing.", "Chaos tests", "\nIn distributed systems, chaos tests are introduced to test the system\u2019s capability of withstanding turbulent conditions in production. It\u2019s could be both hardware or software based test. It works best on systems with redundancy.", "Property based-tests", "\nAlso named as QuickCheck. Property is here defined as a ", " (", "). With property-based tests, developers don\u2019t specify the test samples. Instead they write the rule of test and tools will generate the test samples automatically and randomly.", "Example:", "\n", "Very good article about ", " in Go.", "Mutation tests", "\nCertain statements of code are changed to check if the test can find the error. This can simulate typical coding mistakes like wrong operator or variable name.", "Types of mutation tests:", "Good resource: ", "\nA C++ mutation framework: ", "I do not have experience with ROS 2, but your points regarding integration testing sound valid.", "Do above tests make sense?", "Yes, at least for the major part. I have been working on random testing and property-based testing myself, at the node/integration level, although for ROS 1.", "I had this idea of expanding mutation testing in ROS, in which the ROS primitives could be mutated themselves in a meaningful way (i.e. redirecting topics, messing with queue sizes, changing callback functions to a simple ", " or ", "), although I am not sure how useful that would be in practice.", "Is it possible to pack them all into one single framework?", "Is there any advantage to packing all these different kinds of tests into a single framework? It could prove hard to do so in a way that is intuitive for the user.", " could you provide some input here since you are currently doing integration tests for navigation stack?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The tested components cannot be started in a deterministic sequence since ", " is being used", "The framework lacks the ability of an orchestrated startup and coordinating of different components", "There is many flaky tests and it is hard to attribute flakiness to either bad tests, unreliable testing framework or CI system that does not provide guarantees.", "Move integration_test framework to roslaunch2. This should take care of the deterministic startup and state transitioning within the nodes.", "Determine whether a test needs certain guarantees (e.g. timing). In yes it should then probably run on the dedicated hardware, and if not, a cloud CI like ", " is OK. This should eliminate the concept of flaky tests. Tests should either be passing or failing.", "Add more automated debugging tools to the framework, eg., tshark for network packets capturing, perf, memory tools, valgrind and other tools for profiling.", "\n", "\n", "\n", "Data source, where the data is collected. Eg. simulated sensor failure, corrupted data or duplicate data.", "Communication. Eg. UDP packets get lost, duplicated or order of arrival are reversed.", "System hardware, eg. memory data corruption, unstable time source or memory allocation failure.", "\n", "\n", "\n", "\n", "\n", "\n", "Start the whole stack and define a \u201csteady state\u201d as normal behavior.", "Introduce some real world possible failures like disk full, power outage or network going down.", "Test if the services of other components can be uninterrupted or switched to redundancy.", "\nThe harder it is to disrupt the steady state, the more confident we are at the robustness of the system. If weakness is uncovered, now we have a concrete target to improve.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Statement mutation:", "\nCut or paste some lines of code. Most likely it wouldn\u2019t compile. This is highly manual rather than automated.", "Value mutation:", "\nValues of primary parameters are modified.", "Decision Mutation", "\nControl flow is reversed.", "\n", "\n", "Do above tests make sense?", "Are any particular flavors of tests missing?", "Is it possible to pack them all into one single framework (we struggle with this thought honestly)?", "What other use case do you have that need better integration and unit testing?", "Are there other integration testing framework from non-robotics world worth to inspect?"], "url": "https://discourse.ros.org/t/integration-testing-in-ros-2/6859"}
,{"title": "[TB3] ROS 2 Dashing Release", "thread_contents": ["Hi ", "We are happy to announce TurtleBot3 ROS 2 Dashing Release.", "This updates includes", "When you visit our ", ", you can find step by step from installation to launch tele-operation, cartographer and navigation2  ", "If you have any issue or questions, please feel free to get new ticket on github issue page.", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "We specially thanks to OpenRobotics Memeber,  every ", " developer and ROS community,", "\n", " at ", " , ", ", ", ", ", ",  ", " at ", ", ", " at ", " and My colleague ", " and ", " at ", ".", "Great stuff, thank you very much!", "Great, many thanks. ", "\nI will have to try working with Jetson Nano.", "Here\u2019s video\u2019s about Cartographer and Navigation2 ", "[ROS 2 Dashing Diademata Cartographer]", "[ROS 2 Dashing Diademata Navigation2]", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["OpenCR(Embedded Board) communicate with ", " by ", "\n(For more detailed, please refer to attached picture below)", "\n", " package was ported to ROS 2 Dashing", "Added some services (/sound, /motor_power, /reset)", "Added some parameters", "Applied message filter to calculate ", " topic"], "url": "https://discourse.ros.org/t/tb3-ros-2-dashing-release/10364"}
,{"title": "[ISO] TurtleBot Kinetic ISO - Beta Testers needed!", "thread_contents": ["After significant work refactoring our build systems for maintainability, we are pleased to announce that ", " is now available for beta testing on x86_64.", "The Intel RealSense driver ", " with 16.04.5 or later", "The ROS repository is pre-configured in ", " and most commonly used ROS packages should be pre-installed.", "The ROS environment  is pre-configured", "\n", " runs ", "ISO defaults to KHA1 Loadout in ", "Manufacturers will be able to have custom ISOs to install a loadout matching the  equipped hardware. ", " rules for these devices are preconfigured.", "Updates to 18.04 are currently disabled pending work on Melodic.", "Please submit pull  requests against ", "Did we miss installing your favorite package?", "Is this a priority for anyone?", "How much effort should we spend adding things like the rqt tools to the launcher?", "\nShould we prioritize Melodic / ROS2 support?", "Prior to ", " we built a fairly robust ", " for upstart.", "\nSystem bootup automation still needs to be  refactored for launching", "ISO adds the user to  group  dialout", "\nCan we stop setting ", " permissions for devices?", " we talked NetworkManager into helping to configure ROS networking. This needs to be updated and tested with Xenial. The current thinking is  to move  this into a pure python package, but depending  on interest this work could be delayed to Melodic.", "Please file ISO  bug reports ", "After a Kinetic release we hope to quickly produce a Melodic release followed by some sort of ROS2 release", "The TurtleBot Kinetic ISO has been sponsored in part by ", "It now ships with automatic startup and network auto-configuration", "\n", "Systemd Robot Initialization. Contribute to LucidOne/robot_systemd development by creating an account on GitHub.", "\n", "\n", "ROS Network Autoconfiguration. Contribute to LucidOne/network_autoconfig development by creating an account on GitHub.", "\n", "Uploaded!", "This release includes a basic GUI for ", "\n", "ROS Robot Status Indicator. Contribute to LucidOne/robot_indicator development by creating an account on GitHub.", "\n", "Updated ", " to support GUI for launching new systemd roslaunch units", "\nHoping for a release this week before starting on Melodic support.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Kobuki Mobile Base", "Hexagonal Plate Stack", "Orbec Astra 3D Camera", "Laptop Battery /sys/class/power_supply/BAT1", "We are planning to use an indicator like the volume control in the top right", "\nGUI to enable and disable startup", "\n", " + ", " launched by default", "\nEnables RViz to display pointclouds after first boot without opening a terminal", "ROS Startup", "\nAs ", " user?", "\nAs user ", "?", "\nWhat happens If the initial user is not named ", "?"], "url": "https://discourse.ros.org/t/iso-turtlebot-kinetic-iso-beta-testers-needed/11095"}
,{"title": "TurtleBot3 software and firmware update and 'waffle_pi'", "thread_contents": ["Hello everyone ", "I announce that TurtleBot3 is huge updated!!!", "\nThis update considered many issues and requests from users. We are sincerely thankful to them.", "\nMore interest makes more progress. If you have any issues or suggestions, please feel free to get ", "Existing users need to download new version in master branch of turtlebot3 and turtlebot3_msgs", "Direct download in repository ", ", ", "or using command line", " sudo rm -rf turtlebot3/", "\n$ git clone ", " sudo rm -rf turtlebot3_msgs/", "\n$ git clone ", "Open Arduino -> Toos -> Board: -> Board Manager\u2026 -> Update (v 1.0.15)", "Best regards!", "\nDarby", "I assume the Waffle Pi is a lower-cost waffle with a Raspberry Pi instead of the Joule. This is great news!", "Is there any progress on a Burger or Waffle version that comes with no sensors or compute boards, for those of us who have all that stuff already?", "Hi ", " thanks alot for the update.", "\ni have a question\u2026 Do we need to update the OpenCr board? and How? i get \u201cChecksum does not match\u201d error . thanks alot\u2026 ", "Hello ", "You can follow below instruction", "or you can find more detail in ", "Thanks", "\nDarby", "I am interested in using the new software update on my TB3 and thank you for these instructions. Are there also updates planned or available to the documentation to describe the ROS commands necessary to show the new Publisher /diagnostic, /battery_state information and to connect a speaker to the RasPi3 or OpenCR to hear the generated sounds? Ross", "Hello ", " ", "We have a plan to be compatible with ROS2 though we don\u2019t have any details.", "\nWe provide a ", " and it will be added more information soon for these software updates.", "\nAdditionally, you don\u2019t need to connect separate speaker due to OpenCR already has it ", "Thanks", "\nDarby", "Darby, Following your good instructions and compiling the revised catkin_ws/src, I easily updated the ROS SBC TB3 and OpenCR software and very pleased that it all works well, including \u201crostopic echo /diagnostics\u201d node that is very informative. I look forward to the new wiki to learn how to connect the new /sound Subscription that roswtf reports as unconnected. I note there is a reference to a Adafruit display driver in the TB3 github-Is that a future additional feature? A small request is make available the pdf version of the previous wiki.Thank you and your colleagues for their hard work and producing an excellent, fun and educational robot :).", "I\u2019m still getting a checksum mismatch. I upgraded to OpenCR 1.0.15 and also tried 1.0.16. I upgraded with turtlebot3.git and on turtlebot3_msg.git on both my RemotePC and TurtleBot.", "This is the execution", "\n<", "\n$ roslaunch turtlebot3_bringup turtlebot3_core.launch", "\n\u2026 logging to /home/eepp/.ros/log/5a7d18a4-0545-11e8-9b6b-080027c0cb1e/roslaun", "\nch-orras-3438.log", "\nChecking log directory for disk usage. This may take awhile.", "\nPress Ctrl-C to interrupt", "\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS", "NODES", "ROS_MASTER_URI=http://10.0.0.159:11311", "process[turtlebot3_core-1]: started with pid [3447]", "\n[INFO] [1517266129.977476]: ROS Serial Python Node", "\n[INFO] [1517266130.051262]: Connecting to /dev/ttyACM0 at 115200 baud", "\n[ERROR] [1517266132.304481]: Creation of publisher failed: Checksum does not ma", "\ntch: 427f77f85da38bc1aa3f65ffb673c94c,d537ed7b8d95065b6c83830430b93911", "\n[INFO] [1517266132.362502]: Note: publish buffer size is 1024 bytes", "\n\u2026", "\n/>", "Nice! we are preparing updated wiki including your requests.", "\nThanks you for your interest ", "Hello ", " ", "Have you set network config??", "\nYou can check how to config network btw RemotePC and TB3 on ", "Best regards", "\nDarby", "ROS guys ", "Thank you for your interest on TB3 ! (I am happy as if i get a sweet coffee)", "\nBut ", " is not proper page to create issue ", "If you have any question for TB3, please use Github ", " or ", ".", "\nYou can meet me in there ", "Thanks", "\nDarby", "Darby,", "Thanks for your quick response. This is how I set it up", "Remote PC", "$ ifconfig", "\nenp0s3    Link encap:Ethernet  HWaddr 08:00:27:c0:cb:1e", "\ninet addr:10.0.0.159  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.159", "\nexport PATH=$PATH:$HOME/tools/arduino-1.8.5", "turtlebot3", "$ ifconfig", "\nwlp1s0    Link encap:Ethernet  HWaddr a0:c5:89:4b:3a:e3", "\ninet addr:10.0.0.158  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.158", "\nexport TURTLEBOT3_MODEL=waffle", "Ed", "I followed the instructions in 7.1.5  for Porting OpenCR1.0 to Arduino IDE. I want to reason through my issue. I\u2019m not familiar with Arduino checksum generation. I connected my OpenCR to my Remote PC. I ran the Arduino IDE from there and loaded the bootloader. So I assume I can load the OpenCR from any machine as long as I have the right bootloader version. Where and how are the checksums being generated and compared? Will I have the same bootloader checksum as everyone else in the world? If that is correct, someone should be able to tell me if I have the correct one and tell me which side is incorrect. If not correct, does the checksum depend on my hardware and/or software.", "ed", "Hi Darby", "Just wanted to confirm when you pointed to this article for updating, you meant: ", "Is it possible to update the board- manager as defined in the wiki using dfu-util?", "Or you meant some other steps such as: ", "Can you please confirm?", "Thanks", "\nSandip", "Hi ", " ", "You don\u2019t need to enter dfu mode.", "\nIf you have used OpenCR, you just update it using ", ".", "Please follow below link, it will help you", "Thanks", "\nDarby", "Thanks Darby for the reply,", "\nI updated the Board Manager on Arduino IDE to 1.0.15 (also tried 1.0.16) --> the tried updating the Bootloader using Arduino IDE (but failed) with the following error message:", "I made sure the", "lsusb", "is showing the STMicroelectronics (NOTE: There is no DFU mode written as you mention in the emanual - but this entry in lsusb starts showing up only after DFU mode triggered on using Boot + Reset button)", "Can you let me know some tips to go ahead with debugging?", "Thanks", "\nSandip", "Hi ", ",", "This is a space for discussion, so the question is not appropriate.", "\nI will continue on the issue page of the link below.", "\n", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "\n", "Thanks!", "Exactly same Checksum error here - before I start digging\u2026was it solved? any ideas? Tnx Michael", "BIG SORRY - my fault\u2026didn\u00b4t read the error messages after uploading correctly\u2026no issue anymore!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["turtlebot3_controller - include RC100(for remote control) library", "turtlebot3_diagnosis - include diagnostic functions", "turtlebot3_motor_driver - include DYNAMIXEL SDK", "turtlebot3_sensor - include functions for IMU, battery, magnetic filed and analog Input", "/version_info - Contains the hardware, firmware and software information", "/battery_state - Contains battery voltage and status", "/magnetic_field - Contains magnetic field information", "/diagnostic - Contains self diagnostic information", "/sound - Output beep sound", "/motor_power - Dynamixel torque on/off", "/reset - Reset odometry and re-calibration IMU", "add Sound.msg", "Simple command makes USB setup", "It shows state of IMU, motor, lidar, battery, button and version information", "Add Waffle PI", "Now, we are preparing new version of TurtleBot3 called ", ".  Meet Waffle PI in ", " before you get an it.", "Software (v 1.0.0)", "Firmware", "I did a roscore on my RemotePC", "a ssh into the TurtleBot", "did a \u201croslaunch turtlebot3_bringup turtlebot3_core.launch\u201d on the TurtleBot", "/rosdistro: kinetic", "/rosversion: 1.12.12", "/turtlebot3_core/baud: 115200", "/turtlebot3_core/port: /dev/ttyACM0"], "url": "https://discourse.ros.org/t/turtlebot3-software-and-firmware-update-and-waffle-pi/3729"}
,{"title": "[CFP] ACM/IEEE IPSN 2019 in CPSWeek", "thread_contents": ["The 18th ACM/IEEE The International Conference on Information Processing in Sensor Networks (IPSN)", "===================================================================", "\nApril 16-18, 2019, Montreal, Canada", "IPSN\u201919 is part of CPS-IoT Week 2019, co-located with HSCC, ICCPS, CPS-IoT, and RTAS.", "===================================================================", "The International Conference on Information Processing in Sensor Networks (IPSN) is a leading annual forum on research in networked sensing and control, broadly defined. IPSN brings together researchers from academia, industry, and government to present and discuss recent advances in both theoretical and experimental research. Its scope includes signal and image processing, information and coding theory, databases and information management, distributed algorithms, networks and protocols, wireless communications, collaborative objects and the Internet of Things, machine learning, mobile and social sensing, and embedded systems design. Of special interest are contributions at the confluence of multiple of these areas.", "Like prior years, IPSN 2019 will continue its co-location with CPS-IoT WEEK, the premier venue for research and development of Cyberphysical Systems, and its strong focus on algorithms, theory, and systems for information processing using networks of embedded, human-in-the-loop, or social sensors, as well as new hardware and software platforms, design methods, architectures, modelling, implementation, evaluation, deployment experiences, and tools for networked embedded sensor systems and the Internet of Things. Topics of interest include, but are not limited to:", "In addition to IPSN\u2019s traditional focus, IPSN 2019 will place a particular emphasis on embedded machine learning and computer vision, with a special track centered on these topics. In this case, topics of interest include but are not limited to:", "\nComing soon.", "\nAs in previous years, IPSN will also elicit a review and response process. After the initial review, the program committee may request a short response from selected papers for additional clarifications. The authors will get a short time window of roughly 24 hours to respond. Participation in the response process is not mandatory.", "\nThe two conferences are collaborating to create new synergies, also exploiting their CPSWEEK bi-annual co-location. Prospective authors are thus strongly encouraged to consider these guidelines:", "The TPC chairs of both conferences may agree to move a submitted paper from one to the other if they see a better fit, subject to the authors\u2019 consent. The final decision about where to submit, and therefore what program committee will review the work, ultimately rests with the authors.", "\nPaper Abstract Registration: October 10th, 2018 11:59pm AoE(UTC-12)", "\nPaper Submission Deadline: (Firm*): October, 17th 2018 11:59pm AoE(UTC-12)", "\nAuthors\u2019 rebuttal period December 17th - 19th, 2018", "\nAcceptance Notification: January 16th, 2019", "\nCamera-Ready Deadline: TBD", "\nGeneral Chair:", "\nRasit Eskicioglu (University of Manitoba)", "Technical Committee Chairs:", "\nLuca Mottola (Politecnico di Milan and RISE SICS)", "\nBodhi Priyantha (Microsoft Research)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Sensor data storage, retrieval, processing", "Streaming sensor system tasking and operation", "Coding, compression, and information theory", "Theoretical foundation and fundamental bounds", "Network and system architectures and protocols", "IoT gateway platform architecture and services", "Outdoor, wide-area sensing systems", "Location, time, and other network services", "Programming models, languages, and systems", "Programming models for IoT ensembles", "Modeling, simulation, and measurement tools", "Operating systems and runtime environments", "Applications in health, wellness & sustainability", "Applications in smart cities and urban health", "Experiences, challenges, comparisons of platforms", "Discovery, coordination, and use of IoT services", "Security and privacy in heterogeneous systems", "IoT reliability, adaptability, and dependability", "Technical assessment of emerging IoT standards", "Wearable systems and data processing algorithms", "Sensor-enabled drone platforms and algorithms", "Machine learning and deep learning on sensor data", "New hardware and system design to enable machine learning on sensor data", "Novel embedded machine learning algorithms", "Data related issues, such as methods, tools, and analysis", "Computer vision for resource-constrained and mobile platforms", "Contributions of embedded nature and applying to network segments from the IoT gateway to field devices should be submitted to IPSN. Examples include localization, low-power wireless networking, and embedded data processing.", "Contributions with an end-to-end perspective or applying to network segments from the IoT gateway to the cloud should be submitted to IoTDI. Examples include cloud data processing, edge computing, and systems covering multiple network segments."], "url": "https://discourse.ros.org/t/cfp-acm-ieee-ipsn-2019-in-cpsweek/6240"}
,{"title": "Experiences with Distributed ROS", "thread_contents": ["I am curious to know of any experience people have had running distributed ROS in all shapes and sizes. Have you worked with any of the following solutions (or anything else?), and how did it go? What thoughts or advice can you share?", "As for my personal interest, I have been doing some work with distributed ROS and UAV, and while it is all going well on the ground side, I\u2019m having some severe spikes in latency going through a cheaper wireless router. I\u2019m specifically interested if anyone has any ideas about reliable low-latency wireless links that are easy to interface with ROS.", "The most interesting setup I used was:", "This resulted in a ca. 100ms ping time. With this, I could teleop the robot\u2019s base and arms using the on-board Kinect, so OK data rate as well.", "Lessons learned:", "So I have worked with some rather large distributed systems and best practices are rather hard to say without understanding your topology.", "I have had the best success with routed architecture. Each node master then uses a static packet structure to communicate to the higher level controller (mainly due to the radio link).", "Your spikes are more than likely caused by the inefficiencies of your packet size vs your radio\u2019s frame size. For a test run iperf across your radio link at different packets size. You should be able to see what that most efficient packet sizes are.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Static ethernet network with some kind of switch", "Dynamic ethernet network with some kind of router", "WiFi and ethernet through a router", "WiFi-Only network with a common base station / access point", "WiFi Ad-Hoc network", "Some kind of mixture of anything else?", "a mobile robot with 2 PCs. One of these ran a OpenVPN server", "wired to a router", "that router bridged to a WLAN with multiple access points", "the care home\u2019s (in which the robot worked) network was configured to port-forward to the OpenVPN server", "A control station some 60-70km away VPNed into the robot.", "Tuning a multi-access point WLAN, where the robot roams between APs, is some work. Contrary to my intuition at the time, is that you do ", " want to have each AP at a high power but instead low. That way, the robot switches to the now-closest AP faster, yielding a better connection. WiFi is designed to stay \u2018attached\u2019 to the current AP as long as it gets a signal, even though it may be weak. Dropping a low signal early is better in that case.", "Running the VPN server on the robot is not the way to go\u2026 The server should be on a public IP, the robot behind a firewall."], "url": "https://discourse.ros.org/t/experiences-with-distributed-ros/1768"}
,{"title": "CIS ToF Camera Sensor ROS Driver Package Released", "thread_contents": ["Tokyo Opensource Robotics Kyokai Association (TORK) released a new ROS package \u201ccis_camera\u201d (", ").", "This ROS package is a driver package for ", " ( ", " ) ToF (Time of Flight) camera sensor DCF-RGBD1.", "DCC-RGBD1 is a small ToF camera sensor (development kit) that can acquire a wide range of depth images.", "In this package, in addition to the ROS driver for the CIS ToF camera sensor, a sample program of point cloud processing for noise removal, plane detection / removal, an object point cloud extraction and calculating the object frame position. And a launch file for 3D drawing of these processing results in RViz is included.", "Please refer to a documentation in GitHub for usage.", "If you have problems with this package, please report them on GitHub Issues.", "For inquiries about CIS ToF camera sensor hardware, please contact the following:", "For hardware inquiries: Sales Representative, CIS Corporation", "\nEmail address: ", "\nPhone number: +81-(0)42-664-5568", "That\u2019s a lot of work, but it appears to be a solid package. What\u2019s the retail price of the camera module? Would you happen to know of any North American or European distributors?", "Thanks for your comment!", "I have heard the ToF camera is at the mass production development stage.", "\nSo the price is not yet decided and the sales channel is probably under consideration too.", "Please send an Email to CIS Corporation, you will get information on how to obtain  the ToF camera in the present stage.", "Thank you for your inquiry on CIS TOF camera. We are at the very final stage of product validation, and should be able to launch the product soon. We will initially make this product available at amazon.co.jp. Thanks for your patience!", "RGB-D camera from CIS is finally on sale, available now at amazon.co.jp.", "\nIf you are overseas, please write to us directly at: ec-sales@ciscorp.co.jp", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["High accuracy depth images can be acquired in the range of 15cm to 5m.", "Small size H: 50mm \u00d7 W: 55mm \u00d7 D: 35mm (excluding protrusions)", "Simultaneous acquisition of RGB (QVGA) and Depth / IR (VGA) images", "Interface : USB 3.0 (USB 3.0 micro B connector installed: USB power supply is not supported)", "For indoor use", "GitHub Site ( Including a Quick Start guide )\n", "\n", "GitHub Document\n", "\n", "GitHub Document ( PDF )\n", "\n", "GitHub Issues\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/cis-tof-camera-sensor-ros-driver-package-released/11989"}
,{"title": "New Packages for Kinetic 2019-08-09", "thread_contents": ["We\u2019re happy to announce 15 new packages and 131 updated packages for Kinetic this week. For full details please see below.", "Thank you to all the maintainers and contributors who helped make these updates possible!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Hi, I\u2019m having issue installing ros after this update. I\u2019m on ubuntu 16.04.", "When I try to apt install ros-kinetic-ros-core after using apt update it attempts to fetch python-rospkg 1.1.9 and python-rospkg-modules 1.1.9. Neither of those exist after this update so it just gives me an error about not being able to find them. I\u2019d expect apt update to cause it to search for 1.1.10, but that\u2019s not happening and I\u2019m confused as to how to fix this. I have a file in my /etc/apt/sources.list.d for ros called ros-latest.list and it\u2019s contents are,", "deb ", " xenial main", "I\u2019ve also mentioned my issue on answers.ros here, ", ", if you think that\u2019s a better place to answer.", "You\u2019re correct that ROS Answers is the correct place for your question following our ", ". Your issues with downloading rospkg via apt are not related to this announcement. And double posting it to multiple forums ends up cluttering everyones inbox.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-distance-map: 0.1.0-1", "ros-kinetic-distance-map-core: 0.1.0-1", "\n", ": 0.1.0-1", "\n", ": 0.1.0-1", "ros-kinetic-distance-map-node: 0.1.0-1", "\n", ": 0.1.0-1", "ros-kinetic-distance-map-rviz: 0.1.0-1", "ros-kinetic-distance-map-tools: 0.1.0-1", "ros-kinetic-drone-wrapper: 1.0.0-1", "\n", ": 0.0.3-1", "\n", ": 1.0.0-1", "ros-kinetic-rqt-drone-teleop: 1.0.0-1", "ros-kinetic-seed-smartactuator-sdk: 0.0.3-1", "\n", ": 0.1.0-1", "\n", ": 0.1.0-1", "ros-kinetic-actionlib-enhanced: 0.0.6-1 -> 0.0.9-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "ros-kinetic-cob-base-controller-utils: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-bms-driver: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-cartesian-controller: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-control-mode-adapter: 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-control-msgs: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "ros-kinetic-cob-default-robot-behavior: 0.6.11-0 -> 0.7.0-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "ros-kinetic-cob-docker-control: 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-elmo-homing: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-frame-tracker: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.7.2-0 -> 0.7.3-1", "\n", ": 0.7.2-0 -> 0.7.3-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.5-0 -> 0.6.6-1", "ros-kinetic-cob-hand-bridge: 0.6.5-0 -> 0.6.6-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "ros-kinetic-cob-helper-tools: 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-model-identifier: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "ros-kinetic-cob-moveit-config: 0.6.11-0 -> 0.7.0-1", "ros-kinetic-cob-msgs: 0.6.12-1 -> 0.7.0-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-object-detection-visualizer: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-obstacle-distance: 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-omni-drive-controller: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-phidget-em-state: 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-phidget-power-state: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-reflector-referencing: 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "ros-kinetic-cob-safety-controller: 0.6.7-0 -> 0.6.8-1", "ros-kinetic-cob-scan-unifier: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-tricycle-controller: 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-twist-controller: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-generic-throttle: 0.6.12-1 -> 0.6.14-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 1.1.2-1 -> 1.1.3-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "ros-kinetic-ipa-3d-fov-visualization: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-jderobot-assets: 0.0.1-1 -> 0.0.2-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "ros-kinetic-laser-scan-densifier: 0.6.14-1 -> 0.7.0-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "ros-kinetic-rc-genicam-api: 2.2.0-1 -> 2.2.2-1", "\n", ": 1.0.0-3 -> 1.0.1-1", "\n", ": 0.17.6-0 -> 0.19.3-1", "ros-kinetic-rtabmap-ros: 0.17.6-0 -> 0.19.3-1", "ros-kinetic-service-tools: 0.6.12-1 -> 0.6.14-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.0.1-0 -> 1.0.2-1", "AWS RoboMaker", "Adi Singh", "Alexander Bubeck", "Benjamin Maidel", "Brian Bingham", "Fabrice Poirier", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florenz Graf", "Florian Weisshardt", "Jannik Abbenseth", "Jeremie Deray", "Joshua Hampp", "Mathieu Labbe", "Matthias Gruhler", "Nikhil Khedekar", "Paul Bovbel", "Richard Bormann", "Sean Hackett", "Tony Baltovski", "Yasuto Shiigi"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2019-08-09/10240"}
,{"title": "ROS 2 Dashing Diademata Released!", "thread_contents": ["We\u2019re happy to announce the ROS 2 release Dashing Diademata!", "We\u2019re especially excited to let you know that Dashing Diademata is the first long(er)-term support (LTS) release for ROS 2. After several years of development, and following a big boost in productivity over the past half year from new contributors, including the ", ", we\u2019ve reached a level of maturity with ROS 2 such that we\u2019re extending the support period for Dashing to be two years, through May 2021.", "So whether you\u2019re looking for a platform on which to build a new application, or planning to migrate an existing ROS 1 system, Dashing should be your starting point. Over the coming two years, we\u2019ll be providing patches for Dashing. While we can\u2019t guarantee API compatibility between ROS distributions, for the updates to Dashing we aim to maintain API and ABI stability. This matches what we\u2019ve done in the past with ROS 1 LTS distributions.", "To get an idea of what\u2019s in this release and how to update existing code from ROS 2 Crystal, be sure to read the ", ".", "Here are a few features and improvements we would like to highlight in this release:", "We\u2019re looking forward to getting your ", " and ", ", and to hearing about your new applications based on Dashing! If you have demonstrations of Dashing from your own work that you can share, feel free to post in this thread.", "We also invite you to ", " in Dashing! A huge thanks to all those who\u2019ve already participated in our ", ".", "And finally the name of the next ROS 2 release scheduled for November 2019 will be:", "Your friendly ROS 2 Team", "P.S. Show your color and get a dashing ", ".", "Congratulations for this milestone! ", " would like to thank everyone in the community and specially the folks from Open Robotics who took part into this release and helped putting it together.", "It\u2019s been about 5 years contributing to ROS 2 for some of us here at Acutronic. This LTS release makes us specially proud of what\u2019s been achieved. Our latest contributions are summarized in ", ". We\u2019d like to celebrate this launch by sharing with you some of the demos we\u2019ve been putting together over the last period. All of them based in ROS 2 and powered by ", ", which uses ROS 2 native hardware. Enjoy!", "Here we present the demonstration of a sensorless collision detection system for the MARA modular robot using some of the moveit_core submodules of MoveIt 2. The whole system is based in ROS 2 and has been tested using the Dashing Diademata pre-release while leveraging the real-time capabilities that our team is developing as part of the ", " communication bus for robots.", "For more on this, refer to ", ".", "We present the first demonstrator of the capabilities of MoveIt 2 by showing how to plan to a joint-space goal and how to reproduce it with Dashing. Refer to ", " for the alpha release of MoveIt 2.", "Read more in ", ".", "Often researchers of AI benchmarking formal methods vs sub-symbolic ones find hurdles when it comes to the robot interfaces (real vs simulated). Here we demonstrate how through our contributions (", " framework and ", " toolkit), the transfer of a learned policy from simulation to a robot has become easier. The video shows how we can replicate the behavior demonstrated in simulation accurately using a real robot and with the same ROS 2-powered interfaces.", "Get to know more about RL in ROS 2 ", ".", "Getting different robots to coordinate together precisely is one of the challenges of system integration. This becomes specially tedious when each robot uses proprietary interfaces. A complete ", ". We demonstrate here how 5 robots running ROS 2 natively  are controlled with a single computer achieving millisecond-level precision, while the network is being challenged with simulated depth sensors. Through ROS 2, the H-ROS robot bus provides real-time and synchronization allowing to control simultaneously 30 (5x robots, 3x 2DoF joints each) joints.", "Read more in ", ".", "Synchronization and repeatability are essential for industrial robots to be reliable. These MARA modular arms coordinate precisely thanks to the ", " which empowered by ROS 2 is able to provide sub-microsecond and distributed synchronization.", "Read more in ", ".", "We look forward towards what ROS 2 is bringing to the overall robotics ecosystem and hope to continue contributing. Cheers for Dashing release and cheers for ROS 2!", "Congratulations on the LTS release! Thank you very much to all contributors and to the team at Open Robotics!", " uses the ROS 2 stack to bridge the gap between powerful microprocessors and embedded microcontrollers (MCU). The two major goals:", "Due to the well-designed abstractions in the ROS 2 stack, the rmw and rcl layers may be basically used unchanged on MCUs. On the middleware level, the upcoming DDS-XRCE standard allows the communication from and with MCUs at only a few tens of kilobytes of RAM.", "Although micro-ROS started as a joint endeavor by five companies/institutions ", ", ", ", ", ", ", ", and ", " in the context of a European project, we strive to incorporate the ROS community as early as possible. Join the ", " to learn more!", "As a first community use-case, we brought micro-ROS to an STM32 F4 and created a tiny demo using the Kobuki Turtlebot 2 with it.", "Check it out at ", ".", "          ", "Further use-cases with a modular manipulator, a lawnmower robot, integration of a drone autopilot, and robot operating in smart warehouse will be developed and demonstrated in the next 18 months.", "We have also prepared another small demonstration. In this case, we show a ", ".", "\nCheck the next video to see how it works:", "          ", "Congratulations on the release! Thank you to Open Robotics for their continued work on coordinating all the ROS2 efforts.", "LG Electronics has been building ", ", an autonomous vehicle simulator that is compatible with ROS2. Our simulator is Unity-based and features photorealistic environments, high-performance sensors, and a Python API to control non-ego vehicles, objects, and configurations. Through our use of ros2-web-bridge, anyone can connect their ROS2-based autonomous driving (AD) stack to our tool to test and speed up development. The simulator provides sensor input to the ROS2-based AD stack, and the AD stack provides control commands back to the simulator.", "One way we have been using LGSVL Simulator is for deep learning training. Here we show a video of a lane following test run after training a deep learning model in the LGSVL Simulator. ROS2 is used to run a simplified autonomous driving stack that publishes steering and throttle commands to the vehicle inside the simulator, taking only images from the camera inside the virtual vehicle as input. The end-to-end lane following model was trained in various environmental conditions within the simulator, and is robust to weather, time-of-day, and visibility conditions.", "If you\u2019re interested in getting started with machine learning research using ROS2, the full documentation and guide to this project have been posted ", ", and you can see the code and trained model on ", ".", "We also have a tutorial on getting started with ", " with LGSVL Simulator. Thanks!", "Nice! Is it working faster than with ROS1 bridge?", "Congratulations to the ROS community and to ", " on reaching this new milestone!", "Here at Apex.AI we are pretty excited about this release and wrote this ", " to celebrate.", "As noted in the Blog post, ", " and ", " have now contributed a ROS 2 based 3D perception library for automotive applications to Autoware.Auto, which is shown in this short video.", "For anyone who does not know what we do, in brief, ", " is building ", ", which is API-compatible to ROS 2, runs in hard real-time and is being certified to the highest level of the automotive safety norm ISO 26262 (ASIL-D).", "In case anyone needs some advice with transitioning from ROS 1 to ROS 2, we previously also wrote this detailed Blog post describing ", ".", "Congratulations to Open Robotics on hitting this big milestone. Here at ", " (", ") we are excited to announce our new series of demos that we are creating in conjunction with ", " and ", " to showcase ROS 2 running on industrial-grade, reliable hardware. Our robots were originally designed for SWAT teams so they are built to be really rugged! We have over 10 years of experience in fielding reliable robots and we are excited to show the world what reliable hardware can do when you have reliable software backing it.", "Our demo series will include how to setup an use common packages in ROS 2 such as Google Cartographer, AMCL, RVIZ, and Gazebo. We will be contributing to the sensor drivers needed for these demos to ensure they are reliable for all to use, and lastly we will be hooking this all up to AWS RoboMaker for a professional workflow, professional-grade security and a reliable back-end. Our first full write-up will be released on June 30th.", "\nVisit our website to checkout more about their features and specifications.", "\n", "Cool project!", "I have a working example for publishing and subscribing directly to ROS2 (crystal) from Unity using a custom C# rcl client library and message generator. This approach should have significantly better performance than rosbridge.", "Contribute to DynoRobotics/unity_ros2 development by creating an account on GitHub.", "We\u2019re all excited about Dashing Diademata!", "btw, the ", " logo gets cropped by social media because the image is 718x1000 (portrait). Social media (twitter, linkedin, et al.) need something between square and landscape. So poor Dashing keeps getting his head lopped off", "\n", "Arghh! \u2026 almost, needs to be more landscape ", "\n", " I roughly measured ratio from your LinkedIn screenshot and resized width of canvas of your file according to it:", "\nHere\u2019s a file for sharing: ", "\n(edit: Tested with LinkedIn and Twitter, updated the file)", " is very excited about ROS 2 dashing! We\u2019ve been working with ROS 2 since its inception. Dashing is supported in ADLINK ", "), ", " & ", " used in AVs/industrial/military, and the small & fast ", "\u2019s ", " ", " to which we contribute. Here\u2019s ADLINK ROS 2 dashing demo from IEEE ROS Summit last month with ", "\n", "Here\u2019s more from last month\u2019s ", " put together with the help of many. You\u2019ll notice familiar faces including ", ", also ROS 2 dashing demos. 600 developers & roboticists made it \u2026 the biggest ROS 2 event ever?", "\n", "We are happy to announce TurtleBot3 ROS 2 Dashing Release.", "This updates includes", "For more detailed, please refer to this ", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " are now the recommended way to write your node. They can be used standalone as well as being composed within a process and both ways are fully support from ", " files.", "The ", " (C++ only) has been improved - both in terms of latency as well as minimizing copies.", "The Python client library has been updated to match most of the C++ equivalent and some important bug fixes and improvements have landed related to memory usage and performance.", "Parameters are now a complete alternative to ", " from ROS 1 including constraints like ranges or being read-only.", "By relying on (a subset of) ", " for the message generation pipeline it is now possible to use ", " files (beside ", " / ", " / ", " files). This change comes with support for optional UTF-8 encoding for ordinary strings as well as UTF-16 encoded multi-byte string.", "Command line tools related to ", " and ", ".", "Support for Deadline, Lifespan & Liveliness QoS", "MoveIt 2.0 ", "\n", "\n", " as Tier 3 supported platform", "Integrate the different types of computing platforms seamlessly", "Ease the portability of ROS code to microcontrollers", "OpenCR(Embedded Board) communicate with ", " by ", "\n(For more detailed, please refer to attached picture below)", "\n", " package was ported to ROS 2 Dashing", "Added some services (/sound, /motor_power, /reset)", "Added some parameters", "Applied message filter to calculate ", " topic"], "url": "https://discourse.ros.org/t/ros-2-dashing-diademata-released/9365"}
,{"title": "New packages for Melodic 2019-08-14", "thread_contents": ["We\u2019re happy to announce the next update for ROS Melodic. We have 121 new packages as well as 66 updated packages.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 0.6.14-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "ros-melodic-cob-base-controller-utils: 0.8.0-1", "\n", ": 0.7.0-1", "\n", ": 0.8.0-1", "ros-melodic-cob-bms-driver: 0.7.0-1", "\n", ": 0.6.12-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "ros-melodic-cob-cartesian-controller: 0.8.0-1", "\n", ": 0.8.0-1", "\n", ": 0.6.14-1", "\n", ": 0.6.14-1", "\n", ": 0.8.0-1", "ros-melodic-cob-control-mode-adapter: 0.8.0-1", "ros-melodic-cob-control-msgs: 0.8.0-1", "\n", ": 0.6.14-1", "ros-melodic-cob-docker-control: 0.6.8-1", "\n", ": 0.7.0-1", "ros-melodic-cob-elmo-homing: 0.7.0-1", "\n", ": 0.6.13-1", "\n", ": 0.8.0-1", "ros-melodic-cob-frame-tracker: 0.8.0-1", "\n", ": 0.7.3-1", "\n", ": 0.7.3-1", "\n", ": 0.7.0-1", "\n", ": 0.6.6-1", "ros-melodic-cob-hand-bridge: 0.6.6-1", "ros-melodic-cob-helper-tools: 0.6.14-1", "\n", ": 0.6.14-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.7.0-1", "ros-melodic-cob-model-identifier: 0.8.0-1", "\n", ": 0.6.14-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.14-1", "ros-melodic-cob-object-detection-visualizer: 0.6.14-1", "ros-melodic-cob-obstacle-distance: 0.8.0-1", "ros-melodic-cob-omni-drive-controller: 0.8.0-1", "\n", ": 0.6.14-1", "\n", ": 0.6.14-1", "ros-melodic-cob-phidget-em-state: 0.7.0-1", "ros-melodic-cob-phidget-power-state: 0.7.0-1", "\n", ": 0.7.0-1", "ros-melodic-cob-reflector-referencing: 0.6.8-1", "\n", ": 0.7.0-1", "ros-melodic-cob-safety-controller: 0.6.8-1", "ros-melodic-cob-scan-unifier: 0.7.0-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "\n", ": 0.6.8-1", "\n", ": 0.6.12-1", "\n", ": 0.6.14-1", "\n", ": 0.8.0-1", "ros-melodic-cob-tricycle-controller: 0.8.0-1", "ros-melodic-cob-twist-controller: 0.8.0-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "ros-melodic-distance-map: 0.1.0-1", "ros-melodic-distance-map-core: 0.1.0-1", "\n", ": 0.1.0-1", "\n", ": 0.1.0-1", "ros-melodic-distance-map-node: 0.1.0-1", "\n", ": 0.1.0-1", "ros-melodic-distance-map-rviz: 0.1.0-1", "ros-melodic-distance-map-tools: 0.1.0-1", "ros-melodic-generic-throttle: 0.6.14-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "ros-melodic-ipa-3d-fov-visualization: 0.6.14-1", "\n", ": 0.3.2-1", "\n", ": 0.3.0-1", "\n", ": 0.3.0-1", "ros-melodic-jackal-viz: 0.3.2-1", "ros-melodic-laser-scan-densifier: 0.7.0-1", "\n", ": 0.6.13-1", "\n", ": 0.6.13-1", "\n", ": 0.6.13-1", "\n", ": 0.6.13-1", "\n", ": 0.1.0-1", "\n", ": 0.0.3-1", "\n", ": 1.2.0-0", "\n", ": 0.6.13-1", "\n", ": 0.1.1-1", "\n", ": 0.1.0-1", "ros-melodic-ridgeback-gazebo-plugins: 0.1.0-1", "\n", ": 0.1.0-1", "ros-melodic-ridgeback-viz: 0.1.1-1", "\n", ": 1.1.4-1", "ros-melodic-rosparam-handler: 0.1.4-1", "ros-melodic-service-tools: 0.6.14-1", "\n", ": 0.0.1-1", "\n", ": 0.2.0-1", "\n", ": 0.2.0-1", "ros-melodic-warthog-viz: 0.0.1-1", "\n", ": 1.11.13-0 -> 1.12.0-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "ros-melodic-cob-msgs: 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "ros-melodic-fetch-bringup: 0.8.6-0 -> 0.8.7-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.6-0 -> 0.8.7-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "ros-melodic-freight-bringup: 0.8.6-0 -> 0.8.7-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 1.1.2-1 -> 1.1.3-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 2019.7.7-1 -> 2019.8.8-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "ros-melodic-rc-genicam-api: 2.2.0-1 -> 2.2.2-1", "\n", ": 1.14.0-1 -> 1.15.0-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.0.0-0 -> 1.0.1-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "ros-melodic-rosbridge-msgs: 0.11.2-1 -> 0.11.3-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "\n", ": 0.5.8-0 -> 0.5.9-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "ros-melodic-test-mavros: 0.32.0-1 -> 0.32.1-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.0.1-0 -> 1.0.2-1", "AWS RoboMaker", "Aaron Blasdel", "Adi Singh", "Alex Moriarty", "Alexander Bubeck", "Alexander Moriarty", "Bence Magyar", "Benjamin Maidel", "Brian Bingham", "Eric Relson", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florenz Graf", "Florian Weisshardt", "Hans-Joachim Krauch", "Jannik Abbenseth", "Jeremie Deray", "Johannes Meyer", "Joshua Hampp", "Martin Pecka", "Matthias Gruhler", "Michael Carroll", "Mike Purvis", "Paul Bovbel", "RDaneelOlivaw", "ROS Orphaned Package Maintainers", "Richard Bormann", "Russell Toris", "Tony Baltovski", "Vladimir Ermakov"], "url": "https://discourse.ros.org/t/new-packages-for-melodic-2019-08-14/10288"}
,{"title": "Answer ROS Questions Like a Pirate Day 2K18", "thread_contents": ["Avast there!", "\n", "This Wednesday is ", " which our community uses as an arbitrary day to put a focus on resolving issues on ", ", a.k.a. ", ".", "This is a group effort and can be participated in whether you\u2019re a seasoned captain of a ROS stack or a", "\npowder monkey who\u2019s just dipping their proverbial toe into the sea water.", "Here are some ways to help.", "For more on the subject, check out this other thread: ", "Day turned in UTC to Sept. 19th. It\u2019s time, turtles.", " / All questions: 13,259 / 42,211 = 31.4% (UTC 12:08 AM)", "(I feel like what the link above shows is \u201cunresolved\u201d questions (", ").)", "Today the number be down to 12918. Progress, me harties!", "Yarrr\u2026same thing new year. Close your olde questions that are no longer relevant like they were outdated memes.", "Specifically, I be callin out the following ", " users who have more than 30 questions without accepted answers. Avast!", "Kishore Kumar 30", "\nMehdi. 31", "\nlucasw 35", "\nMarkyMark2012 36", "\nS.Yildiz 36", "\nEmilien 37", "\naks 40", "\nNaman 41", "\nAutoCar 43", "\nEdwardNur 45", "\nCerin 49", "\nrnunziata 52", "\nstevemartin 54", "\ndinesh 55", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " - You know what\u2019s fun? Judging people. If there are answers that have been particularly useful to you, even on questions you didn\u2019t ask, go upvote them. Are there burning questions that you also want to know the answer to? Vote for the question. Giving positive reinforcement to the people answering questions can\u2019t hurt.", "\n", " - Fairly straightforward. Look through ", " and find something interesting.", "\n", " -  Go to your user profile and look through the questions you\u2019ve asked. For the ones that are still open, is there an answer you can mark as correct? Or if the question is no longer relevant, consider closing the question altogether.", "\n", " - If you\u2019re an ol\u2019 sea dog and have edit privileges, use your power for good and make sure that people have the correct formatting on their questions."], "url": "https://discourse.ros.org/t/answer-ros-questions-like-a-pirate-day-2k18/6058"}
,{"title": "What do you want to see in an educational ROS platform?", "thread_contents": ["We\u2019ve started developing a table top / ground ROS robot for use in educational environments. We\u2019d like to hear from educators that really want to teach ROS based robotics, but don\u2019t feel they can because currently available robots are either too expensive, too complicated or too [insert your adjective here].", "We\u2019d also like to hear from educators that are teaching ROS robotics but want to broaden what they do, or find alternative platforms.", "If you are an educator who offers courses and would like to do so with ROS can you respond to this thread with the one thing that you\u2019d like to see in a educational platform for ROS. If enough people express interest we will do a couple of group calls so we can hear what people would like and make trade-off decisions. Then we will do our best to build what the group wants with open source repos to go along with it!", "Please respond with the one thing you\u2019d like to see in an ROS robot platform oriented for education.", "David", "P.S. Thanks to Tully and Kat for suggesting this!", "I\u2019m not using ROS in education yet but I\u2019ve been looking for a platform like this a while back. I was looking at this from a hobbyst perspective, so a low price was a must (Turtlebot looks good but it\u2019s a bit too expensive if someone wanted to buy it to get into ROS).", "Some of the platforms I looked at didn\u2019t have wheel encoders but I think it\u2019s a very good to have if you are teaching about odometry.", "Here are some things I was looking for in the robot platform:", "Functionality wise I wanted to be able to build a full ROS stack on the platform, starting with the drivers, through ros_control and ending with navigation and localization. I was hoping that with a platform like this I could guide students through all levels of software design for a robot.", "Sorry it\u2019s not a single thing but I thought some of these observations could be useful for you.", "Perfect educational resource to my mind is github repo with whole bunch of small working examples. Each of them compiles, runs, has reach comments and illustrates particular piece of ROS functionality.", "\nE.g.", "\nexample 1: how to use tf2", "\nexample 2: how to use can_bridge", "\nexample 3: how to use pointcloud and access individual points", "\nexample 4: you name it\u2026", "Awesome list! I agree price is the key thing that is missing here.", "How about if we designed it so that you\u2019d have ROS topics shared between a workstation computer and the robot itself and the heavy lifting was done on the workstation? Do you think that would be workable? This is one strategy to keep costs down for the robot - it also is a great example of the power of ROS.", "This reminds me a bit of the vector_ros: ", ". I can see a value in that for learning some high level concepts.", "The reason I was initially looking at low level concepts is because I think there is a niche in this area. At the time I was looking into it I couldn\u2019t find any tutorials on how to create a robot from scratch with ROS (I remember it took me quite a while to figure out how ros_control works).", "If you were to share the topics with the user\u2019s workstation would a user be able to add their own sensors on the robot? Because to me that is the single best thing about ROS and projects like vector_ros are very interesting but because you are not able to extend the robot the educational value might be a bit limited.", "That is definitely possible and workable, given decent WiFi. ", " is nontrivial I guess for complete noobs, but once that\u2019s taken care of, running code from a workstation removes the need to sync code from workstation to the robot.", "Running hardware interfaces of course needs to happen on the robot.", "Two places too look at are the SV-ROS github, which uses a Neato Botvac as a ROS platform, and the ROS by Example books by Patrick Goebel, available from Lulu. It is possible to build a ROS mock Turtlebot using a Raspberry Pi, and a Botvac that has a \u201cLidar\u201d for $300.", "In my experience with new comers (or in some cases even research labs that have been using ROS for years) is that network issues tend to cause them a lot of grief. I think the idea to have some of the logic run on the workstation is good but that would require the existence of very thorough noob friendly tutorials about how to get your network just right.", "Yes network provisioning is usually messy.", "We made it significantly less messy at Ubiquity Robotics by building PiFi. On bootup it scans all the available networks then boots in to AP mode with a unique network name (something like ubiquityrobotXXXX) where XXXX is the last 4 digits of the MAC address.", "You can connect to the robot via AP mode then when you do it presents a list of available networks and you can elect to connect to one of those or you can just stay in AP mode.", "It works well, although educational institutions some times have problems with new WiFi networks and also don\u2019t always make it easy to connect to the available infrastructure networks. Our solution is pretty slick and I can\u2019t think of a better one - but I am all ears for suggestions.", "I have used turtlebot3 for a while and for what it\u2019s intended for (as an introduction to ROS), it doesn\u2019t really justify the price of it.", "In the case of turtlebot3, the heavy lifting such as SLAM, map navigation is done on a workstation. But, I found this rather limiting.", "Ideally the onboard computer can function as a WiFi hotspot, possibly with a 4G/LTE dongle, so it can connect to internet if it needs to. In this case any computer could connect to such network. This will minimize network infrastructure.", "With waffle-pi, beginners could run the examples, launch the ros nodes and configure it with the available setup. But, that\u2019s pretty much the extent of what they can do. If they want to level up eventually, they will seek for better sensors and onboard computer.", "It\u2019s hard to scale the turtlebot or to upgrade it without replacing the core components (raspberry pi 3 model B, the dynamixel motors, etc.). When I plug the OpenCR to an Intel NUC, it doesn\u2019t immediately work out of the box, without some configuration and re-testing the Arduino code. This is what most beginners are not aware of.", "I also noticed people have used more powerful computers such as the Jetson TX2 to run processing onboard because tracking or depth cameras such as realsense / zed are too heavy for a raspberry pi 3, and yet they are quite popular among robotics researcher.", "Perhaps something like the PULP platform, OpenMV camera is a good alternative considering the costs.", "On the software side, it\u2019s not clear how to migrate to ROS 2 while keeping the software stack to be the same. This is if we still want to run the same SLAM / navigation packages. Though I understand it really depends on whether there\u2019s an upgrade on dependencies.", "In short, I would suggest that there should be a guidance on such \u201cupgrade\u201d path and scalability issues. As probably those who started to learn robotics are here to invest their time and skills in the long run.", "My apologies I\u2019m not too familiar with the tb3 but trying to understand why SLAM (I assume against an rplidar) qualifies as a heavyweight algorithm. What\u2019s being used as a default SLAM algorithm for tb3?", "The default setup (as per documentation) is to run the SLAM algorithm and navigation on a remote computer. Running gmapping (the default) or hector with rpildar A3 on the raspberrypi 3 model B is fine and I have tested that. But, I think cartographer is heavier. The other default option is to run frontier_exploration.", "To build the DWA local planner and map_server on the rpi3 itself requires pcl_ros to be installed as a dependency, which is unnecessary in most cases. So, up until now I never run the DWA local planner + map_server onboard.", "Then, once I setup a realsense T265 as a tracking camera to improve odometry, the realsense nodelets used up about 50% of memory on average. Occasionally the realsense camera manager crashes. If I run SLAM onboard.", "If I just use a 3S LiPo 5000mah battery (it\u2019s already larger than the default). The realsense fisheye camera nodes are not streaming their images. I assume there\u2019s not enough power from OpenCR. Although these camera images are fine if I use a larger capacity battery and higher voltage, such as 6S.", "For sure, I would need a better computer and more battery cells + capacity to run something like RTAB-Map with D435 + T265 for example.", "The impression is the rpi3 seem to only be utilised to run the turtlebot bringup or at most gmapping. I am in the process to at least migrate to rpi4 for its USB3 as realsense cameras worked best with USB3.", "At Stanford, one researcher ran a compute intensive Turtlebot II with extra  18v battery packs for a gaming Nvidia equiped laptop.  Runs up to 1 hour autonomous were obtained.", "But, I think cartographer is heavier.", "For what it is worth, I successfully ran cartographer on a ", " attached to a Turtlebot 2.  The caveat was that I had to run it in 2D mode; in 3D mode, it was too heavyweight.  You can see my short presentation from ROSCon 2017 about it ", ".", "Interesting, thanks for sharing your talk!", " does seem to have a better setup from what I\u2019ve seen. It doesn\u2019t have waffle plates. But, they are not really necessary.", "It comes with ", " camera or ASUS Xtion pro live if it\u2019s bought from clearpath robotics. Also dimension is better, while turtlebot3 require additional plates and plate support.", "I\u2019ve been teaching with Turtlebot 2 for several years now. In terms of hardware, it is hard to beat:", "There are a few downsides:", "I\u2019m at the point where I need to re-equip our robotics classroom, and I\u2019m at a loss. It looks like Turtlebot 3 is the default choice at this point, but it doesn\u2019t have any of the pros I list above.", "So\u2026 to answer your question. The platform I\u2019m looking for is something that looks a lot like TB2, with nice clean ROS and ROS2 packages.", "At Ubiquity Robotics we have been running platforms off RPi for a couple of years. We have a simplified navigation node called move-basic that is suitable for student learning and runs on low powered CPUs.", "I going to suggest a few things to make this discussion more productive.", "Please make it clear if you are an educator or not and if so what your student body looks like. Not all students are the same and it helps us if we understand the different constituencies of users.", "If you have a request or an idea please frame it in terms of the subject matter you want to teach not the hardware. Hardware changes from quarter to quarter; fundamentals more slowly.", "I\u2019m not an educator.", "But however we\u2019d like to consider robotics as an abstraction like how software is. It\u2019s not the same, hardware is also an important subject.", "A junior engineer could quickly drop a robot platform that doesn\u2019t do all the things he/she could see on videos of latest research.", "What they couldn\u2019t see is the efforts, workarounds, to make an algorithm work in a certain environment with certain setup of hardware.", "If I would to educate someone, be it a student or an engineer. I would emphasize:", "The JPL Mars rover is probably also a good reference as an educational platform.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Aforementioned wheel encoders", "IMU", "Onboard computer running ROS (extra points for this to be an option, lot\u2019s of people already have embedded computers)", "Ease of integration of custom sensors (consideration for space, robot payload)", "Some built-in DC-DC converters would be great to power external sensors (3.3, 5 and 12V would be superb)", "The size is good.  It is small enough to be safe and portable, but big enough to operate on human-scale problems like office delivery, tour guide etc.", "RGBD sensors provide a nice bang for the buck.  We can do 2d-slam, 3d-slam, computer vision etc.", "Using a laptop for computation makes it much easier to use in the classroom.  Trying to get networking set up correctly and keep it working for multiple robots is a pain.  It is much easier to just write some code on a laptop and plug it in.", "It would be nice if it were cheaper (though honestly, I don\u2019t think the price is unreasonable).", "The ROS Turtlebot packages are not very easy for novices to make sense of.  TB2 is relatively easy for beginners to use, but it is hard for beginners to modify. It would be nice to see an educational platform that serves as a clear, well-documented, example of how to set up a robot with ROS support.", "As far as I can tell, TB2 is going away. It\u2019s not clear if the Kobuki base is even being manufactured anymore.", "\n", "\n", "\n", "\n", "Environments that a robot should operate. This will cover perception, map, navigation, kinematics (if we\u2019re considering a more complicated movement than 2-wheeled differential drive robot)", "Some mechanical / electronics foundation (enough to get something working)", "OS & networking fundamentals, why Python / C++ as a common programming language. Although anyone can use other languages, but these two are the most common.", "Upgrade / scalability problems", "Algorithms", "Software management", "User Interface", "Security (this would be the advanced subject)"], "url": "https://discourse.ros.org/t/what-do-you-want-to-see-in-an-educational-ros-platform/10958"}
,{"title": "New packages for Melodic 2019-11-14", "thread_contents": ["We\u2019re happy to announce the next update for ROS Melodic. We have 32 new packages as well as 126 updated packages.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-melodic-cob-default-robot-behavior: 0.7.2-1", "\n", ": 0.7.2-1", "\n", ": 0.7.3-1", "\n", ": 0.7.3-1", "\n", ": 0.7.3-1", "\n", ": 0.7.2-1", "ros-melodic-cob-hardware-emulation: 0.8.1-1", "ros-melodic-cob-moveit-config: 0.7.2-1", "ros-melodic-dccomms-ros: 0.0.2-1", "ros-melodic-dccomms-ros-msgs: 0.0.2-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "ros-melodic-network-autoconfig: 0.1.1-1", "ros-melodic-seed-smartactuator-sdk: 0.0.4-1", "ros-melodic-swri-profiler: 0.2.2-1", "ros-melodic-swri-profiler-msgs: 0.2.2-1", "ros-melodic-swri-profiler-tools: 0.2.2-1", "ros-melodic-turtlesim-dash-tutorial: 1.0.0-2", "ros-melodic-ubiquity-motor: 0.10.0-1", "ros-melodic-underwater-sensor-msgs: 1.4.2-1", "ros-melodic-underwater-vehicle-dynamics: 1.4.2-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "ros-melodic-behaviortree-cpp-v3: 3.0.7-0 -> 3.1.0-2", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "ros-melodic-cloudwatch-logs-common: 1.1.0-1 -> 1.1.2-1", "ros-melodic-cloudwatch-metrics-common: 1.1.0-1 -> 1.1.2-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-base-controller-utils: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-bms-driver: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-cartesian-controller: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-control-mode-adapter: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-control-msgs: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-elmo-homing: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-frame-tracker: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-helper-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-model-identifier: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "ros-melodic-cob-msgs: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "ros-melodic-cob-obstacle-distance: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-omni-drive-controller: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-phidget-em-state: 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-phidget-power-state: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-scan-unifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-tricycle-controller: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-twist-controller: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.0.9-0 -> 0.0.11-1", "ros-melodic-dataflow-lite: 1.1.0-1 -> 1.1.2-1", "ros-melodic-dataspeed-ulc: 0.0.4-1 -> 0.0.5-1", "ros-melodic-dataspeed-ulc-can: 0.0.4-1 -> 0.0.5-1", "ros-melodic-dataspeed-ulc-msgs: 0.0.4-1 -> 0.0.5-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.5.1-2 -> 1.6.5-1", "ros-melodic-file-management: 1.1.0-1 -> 1.1.2-1", "ros-melodic-generic-throttle: 0.6.14-1 -> 0.6.15-1", "\n", ": 2.0.1-1 -> 2.0.3-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.8.7-1 -> 1.8.8-1", "ros-melodic-laser-scan-densifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "ros-melodic-linux-networking: 1.0.13-2 -> 1.0.16-1", "\n", ": 2019.10.10-1 -> 2019.11.11-1", "\n", ": 0.5.1-2 -> 0.5.2-1", "\n", ": 0.5.1-2 -> 0.5.2-1", "ros-melodic-mongodb-store-msgs: 0.5.1-2 -> 0.5.2-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "ros-melodic-osg-interactive-markers: 1.0.2-1 -> 1.0.2-2", "ros-melodic-osg-markers: 1.0.2-1 -> 1.0.2-2", "ros-melodic-osg-utils: 1.0.2-1 -> 1.0.2-2", "\n", ": 1.0.0-0 -> 1.1.1-1", "\n", ": 2.3.6-2 -> 2.4.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 1.1.4-1 -> 1.1.6-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 2.0.2-1 -> 2.0.4-1", "\n", ": 0.3.0-1 -> 0.4.2-1", "ros-melodic-rosmon: 2.1.1-1 -> 2.2.1-1", "ros-melodic-rosmon-core: 2.1.1-1 -> 2.2.1-1", "ros-melodic-rosmon-msgs: 2.1.1-1 -> 2.2.1-1", "\n", ": 0.5.0-1 -> 0.5.1-1", "ros-melodic-rqt-rosmon: 2.1.1-1 -> 2.2.1-1", "ros-melodic-service-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.2-1 -> 0.8.3-1", "ros-melodic-um7: 0.0.5-1 -> 0.0.6-1", "\n", ": 0.4.1-1 -> 0.4.1-2", "ros-melodic-uwsim-bullet: 2.82.1-1 -> 2.82.2-1", "ros-melodic-uwsim-osgbullet: 3.0.1-1 -> 3.0.1-3", "ros-melodic-uwsim-osgocean: 1.0.3-1 -> 1.0.4-1", "ros-melodic-uwsim-osgworks: 3.0.3-1 -> 3.0.3-2", "\n", ": 1.0.2-1 -> 1.0.2-2", "AWS RoboMaker", "Alexander Bubeck", "Benjamin Maidel", "Chris Lalancette", "Christoph R\u00f6smann", "Daniel Miller", "Davide Faconti", "Devon Ash", "Diego Centelles", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Florian Weisshardt", "HXR", "Jannik Abbenseth", "Javier Perez", "Johannes Meyer", "Jon Binney", "Joshua Hampp", "Justin Carpentier", "Kevin Hallenbeck", "Marc Hanheide", "Mario Prats", "Martin Pecka", "Mathias L\u00fcdtke", "Matthias Gruhler", "Max Schwarz", "Micho Radovnikovich", "Nick Hawes", "P. J. Reed", "Richard Bormann", "Rohan Agrawal", "Scott K Logan", "Siddhartha Banerjee", "Vladimir Ermakov", "Yasuto Shiigi", "dfaconti"], "url": "https://discourse.ros.org/t/new-packages-for-melodic-2019-11-14/11494"}
,{"title": "New packages for ROS Kinetic Kame 2019-11-15", "thread_contents": ["We\u2019re happy to announce 12 new and 144 updated packages for Kinetic.", "There is one regression of pinocchio due to cmake/pkg-config that\u2019s being actively worked on to be resolved.", "Thank you to all the maintainers and contributors who have helped make these packages available!", "Details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", " can we expect gurobi and coinutils packages to be made available for Kinetic as well?", " I\u2019m not familiar with those ROS packages in this distro or searching any of the other of the ROS distros. If you know of ones, you can reach out to their maintainers to know when and where they\u2019re expecting to make a release.", " they were used as dependencies for ipa_room_segmentation in ROS Indigo", " does not appear to have been released. I found a reference here: ", " and the source code it references doesn\u2019t appear to have either of those packages in it\u2019s dependency list on the ", " branch:", "There are some rosdeps defined related to coinutils now ", ":", "But I\u2019ll return to my previous statement that if you\u2019re interested in having something released into a rosdistro please reach out to the maintainer of the package that you\u2019re looking for. As the release manager I coordinate the releases from maintainers throughout the community. But it\u2019s up to the maintainers to make releases.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 0.7.3-1", "ros-kinetic-cob-hardware-emulation: 0.7.9-1", "ros-kinetic-leuze-description: 1.0.0-2", "ros-kinetic-leuze-msgs: 1.0.0-2", "ros-kinetic-leuze-phidget-driver: 1.0.0-2", "ros-kinetic-network-autoconfig: 0.1.1-1", "ros-kinetic-robot-indicator: 0.1.3-1", "ros-kinetic-robot-systemd: 0.1.2-1", "\n", ": 0.3.0-1", "\n", ": 0.3.0-1", "ros-kinetic-seed-r7-samples: 0.3.0-1", "ros-kinetic-turtlebot-loadout-kha1: 0.1.0-3", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "ros-kinetic-behaviortree-cpp-v3: 3.0.7-0 -> 3.1.0-3", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "ros-kinetic-cloudwatch-logs-common: 1.1.0-2 -> 1.1.2-1", "ros-kinetic-cloudwatch-metrics-common: 1.1.0-2 -> 1.1.2-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-base-controller-utils: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-bms-driver: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-cartesian-controller: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-control-mode-adapter: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-control-msgs: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "ros-kinetic-cob-default-robot-behavior: 0.7.1-1 -> 0.7.2-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-elmo-homing: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-frame-tracker: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "ros-kinetic-cob-helper-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-model-identifier: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "ros-kinetic-cob-moveit-config: 0.7.1-1 -> 0.7.2-1", "ros-kinetic-cob-msgs: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "ros-kinetic-cob-obstacle-distance: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-omni-drive-controller: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-phidget-em-state: 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-phidget-power-state: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "ros-kinetic-cob-scan-unifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-tricycle-controller: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-twist-controller: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.0.9-0 -> 0.0.11-1", "ros-kinetic-dataflow-lite: 1.1.0-2 -> 1.1.2-1", "ros-kinetic-dataspeed-ulc: 0.0.4-1 -> 0.0.5-1", "ros-kinetic-dataspeed-ulc-can: 0.0.4-1 -> 0.0.5-1", "ros-kinetic-dataspeed-ulc-msgs: 0.0.4-1 -> 0.0.5-1", "\n", ": 1.5.1-1 -> 1.6.7-1", "ros-kinetic-file-management: 1.1.0-2 -> 1.1.2-1", "ros-kinetic-generic-throttle: 0.6.14-1 -> 0.6.15-1", "\n", ": 2.0.1-1 -> 2.0.3-1", "ros-kinetic-laser-scan-densifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2019.10.10-1 -> 2019.11.11-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 1.0.0-0 -> 1.1.1-1", "\n", ": 2.3.6-2 -> 2.4.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 2.0.2-1 -> 2.0.4-1", "\n", ": 0.3.0-1 -> 0.4.1-1", "\n", ": 0.5.0-1 -> 0.5.1-1", "ros-kinetic-seed-r7-bringup: 0.2.0-2 -> 0.3.0-1", "ros-kinetic-seed-r7-description: 0.2.0-2 -> 0.3.0-1", "ros-kinetic-seed-r7-navigation: 0.2.0-2 -> 0.3.0-1", "ros-kinetic-seed-r7-robot-interface: 0.2.0-2 -> 0.3.0-1", "\n", ": 0.2.0-2 -> 0.3.0-1", "ros-kinetic-service-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-nodelet: 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-profiler: 0.1.0-0 -> 0.2.2-2", "ros-kinetic-swri-profiler-msgs: 0.1.0-0 -> 0.2.2-2", "ros-kinetic-swri-profiler-tools: 0.1.0-0 -> 0.2.2-2", "ros-kinetic-swri-roscpp: 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-rospy: 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-route-util: 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.3.1-1 -> 0.3.1-2", "ros-kinetic-test-mavros: 0.33.0-1 -> 0.33.3-1", "ros-kinetic-ubiquity-motor: 0.9.0-0 -> 0.10.0-1", "ros-kinetic-um7: 0.0.5-1 -> 0.0.6-1", "AWS RoboMaker", "Alexander Bubeck", "Benjamin Maidel", "Christoph R\u00f6smann", "Daniel Miller", "Davide Faconti", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Florian Weisshardt", "HXR", "Jannik Abbenseth", "Jordy van Appeven", "Joshua Hampp", "Justin Carpentier", "Kevin Hallenbeck", "Kris Kozak", "Ludovic Delval", "Marc Alban", "Mathias L\u00fcdtke", "Matthias Gruhler", "Micho Radovnikovich", "Nick Rotella", "P. J. Reed", "Richard Bormann", "Rohan Agrawal", "Scott K Logan", "Vladimir Ermakov", "Yasuto Shiigi", "dfaconti", "hi.kondo", "turtlebot", "\t<buildtool_depend>catkin</buildtool_depend>", "\t", "\t<depend>actionlib</depend>", "\t<depend>cv_bridge</depend>", "\t<depend>dynamic_reconfigure</depend>", "\t<depend>ipa_building_msgs</depend>", "\t<depend>libdlib</depend>", "\t<depend>libopencv-dev</depend>", "\t<depend>nav_msgs</depend>", "\t<depend>opengm</depend>", "\t<depend>roscpp</depend>", "\t<depend>roslib</depend>", "\t<depend>sensor_msgs</depend>"], "url": "https://discourse.ros.org/t/new-packages-for-ros-kinetic-kame-2019-11-15/11527"}
,{"title": "New Packages for Indigo Jade and Kinetic 2016-10-24", "thread_contents": ["We have a simultaneous sync of Indigo Jade and Kinetic packages. This includes close to 100 new packages as well as several hundred updated packages. Please see the full details below. And as always thank you to everyone who has contributed to the many updates included!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "#", " to kinetic", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-aubo-driver: 0.2.1-0", "ros-indigo-aubo-trajectory: 0.2.1-0", "ros-indigo-cob-docker-control: 0.6.5-0", "ros-indigo-cob-hand: 0.6.1-0", "ros-indigo-cob-hand-bridge: 0.6.1-0", "ros-indigo-cob-phidget-em-state: 0.6.8-0", "ros-indigo-cob-phidget-power-state: 0.6.8-0", "ros-indigo-cob-reflector-referencing: 0.6.5-0", "ros-indigo-cv-detection: 0.0.2-0", "ros-indigo-grid-map-pcl: 1.4.1-0", "ros-indigo-ihmc-msgs: 0.8.0-6", "ros-indigo-ihmc-ros-common: 0.8.0-6", "ros-indigo-ihmc-ros-control: 0.5.0-1", "ros-indigo-ihmc-ros-core: 0.8.0-6", "ros-indigo-ihmc-ros-diagnostics: 0.8.0-1", "ros-indigo-ihmc-ros-java-adapter: 0.8.0-6", "ros-indigo-imagezero: 0.2.3-0", "ros-indigo-imagezero-image-transport: 0.2.3-0", "ros-indigo-imagezero-ros: 0.2.3-0", "ros-indigo-libconcorde-tsp-solver: 0.6.7-0", "ros-indigo-libdlib: 0.6.9-0", "ros-indigo-libopengm: 0.6.9-0", "ros-indigo-lpg-planner: 2.0.17-0", "ros-indigo-mrpt-rbpf-slam: 0.1.3-0", "ros-indigo-nao-dcm-bringup: 0.0.2-0", "ros-indigo-reapp-description: 0.1.1-0", "ros-indigo-reapp-msgs: 0.1.1-0", "ros-indigo-ros-type-introspection: 0.3.1-0", "ros-indigo-rwt-image-view: 0.0.3-1", "ros-indigo-rwt-moveit: 0.0.3-1", "ros-indigo-rwt-plot: 0.0.3-1", "ros-indigo-rwt-speech-recognition: 0.0.3-1", "ros-indigo-rwt-utils-3rdparty: 0.0.3-1", "ros-indigo-sick-visionary-t-driver: 0.0.3-1", "ros-indigo-slic: 2.0.17-0", "ros-indigo-visualization-rwt: 0.0.3-1", "ros-indigo-warthog-control: 0.0.1-0", "ros-indigo-warthog-description: 0.0.1-0", "ros-indigo-warthog-desktop: 0.0.1-0", "ros-indigo-warthog-msgs: 0.0.1-0", "ros-indigo-warthog-viz: 0.0.1-0", "ros-indigo-ar-track-alvar: 0.5.3-0 -> 0.5.4-0", "ros-indigo-aruco: 0.1.0-0 -> 0.2.0-0", "ros-indigo-aruco-msgs: 0.1.0-0 -> 0.2.0-0", "ros-indigo-aruco-ros: 0.1.0-0 -> 0.2.0-0", "ros-indigo-assimp-devel: 2.0.14-0 -> 2.0.17-0", "ros-indigo-aubo-description: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-gazebo: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-i5-moveit-config: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-kinematics: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-msgs: 0.1.1-3 -> 0.2.1-0", "ros-indigo-bayesian-belief-networks: 2.0.14-0 -> 2.0.17-0", "ros-indigo-camera-calibration-parsers: 1.11.10-0 -> 1.11.11-0", "ros-indigo-camera-info-manager: 1.11.10-0 -> 1.11.11-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-base-drive-chain: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-base-velocity-smoother: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-bms-driver: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-bringup: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-bringup-sim: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-calibration-data: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-cam3d-throttle: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-camera-sensors: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-canopen-motor: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-cartesian-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-collision-velocity-filter: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-command-gui: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-command-tools: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-common: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-control: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-control-mode-adapter: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-control-msgs: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-controller-configuration-gazebo: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-dashboard: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-default-env-config: 0.6.3-0 -> 0.6.4-0", "ros-indigo-cob-default-robot-behavior: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-robot-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-description: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-driver: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-elmo-homing: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-environments: 0.6.3-0 -> 0.6.4-0", "ros-indigo-cob-extern: 0.6.4-0 -> 0.6.9-0", "ros-indigo-cob-footprint-observer: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-frame-tracker: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-gazebo: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-objects: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-worlds: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-generic-can: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-hardware-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-head-axis: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-image-flip: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-interactive-teleop: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-lbr: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-light: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-mimic: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-model-identifier: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-monitoring: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-msgs: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-object-detection-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-object-detection-visualizer: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-obstacle-distance: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-omni-drive-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-perception-common: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-perception-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-phidgets: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-relayboard: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-robots: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-safety-controller: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-scan-unifier: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-script-server: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-sick-lms1xx: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-sick-s300: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-simulation: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-sound: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-srvs: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-substitute: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-teleop: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-trajectory-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-twist-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-undercarriage-ctrl-node: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-utilities: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-vision-utils: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-voltage-control: 0.6.7-0 -> 0.6.8-0", "ros-indigo-collada-urdf-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-indigo-compressed-depth-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-compressed-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-concert-conductor: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-master: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-concert-schedulers: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-link-graph: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-manager: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-concert-service-utilities: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-software-farmer: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-utilities: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-workflow-engine-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-cv-bridge: 1.11.13-0 -> 1.11.14-0", "ros-indigo-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-downward: 2.0.14-0 -> 2.0.17-0", "ros-indigo-eus-assimp: 0.2.4-0 -> 0.2.5-0", "ros-indigo-euscollada: 0.2.4-0 -> 0.2.5-0", "ros-indigo-eusurdf: 0.2.4-0 -> 0.2.5-0", "ros-indigo-ff: 2.0.14-0 -> 2.0.17-0", "ros-indigo-ffha: 2.0.14-0 -> 2.0.17-0", "ros-indigo-frida-driver: 0.6.4-0 -> 0.6.5-0", "ros-indigo-gateway-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-grid-map-core: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-cv: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-filters: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-loader: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-msgs: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-ros: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-visualization: 1.4.0-0 -> 1.4.1-0", "ros-indigo-hironx-calibration: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hironx-moveit-config: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hironx-ros-bridge: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-indigo-husky-base: 0.2.5-0 -> 0.2.6-0", "ros-indigo-husky-bringup: 0.2.5-0 -> 0.2.6-0", "ros-indigo-husky-robot: 0.2.5-0 -> 0.2.6-0", "ros-indigo-image-common: 1.11.10-0 -> 1.11.11-0", "ros-indigo-image-exposure-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-image-geometry: 1.11.13-0 -> 1.11.14-0", "ros-indigo-image-transport: 1.11.10-0 -> 1.11.11-0", "ros-indigo-image-transport-plugins: 1.9.3-0 -> 1.9.5-0", "ros-indigo-imu-monitor: 1.6.16-2 -> 1.6.16-4", "ros-indigo-jsk-3rdparty: 2.0.14-0 -> 2.0.17-0", "ros-indigo-jsk-apc2015-common: 1.5.1-0 -> 2.0.0-0", "ros-indigo-jsk-apc2016-common: 1.5.1-0 -> 2.0.0-0", "ros-indigo-jsk-interactive: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-interactive-marker: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-interactive-test: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-model-tools: 0.2.4-0 -> 0.2.5-0", "ros-indigo-jsk-roseus: 1.5.3-0 -> 1.6.0-0", "ros-indigo-jsk-rqt-plugins: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-rviz-plugins: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-visualization: 1.0.33-0 -> 1.0.34-0", "ros-indigo-julius: 2.0.14-0 -> 2.0.17-0", "ros-indigo-laser-filters-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-indigo-libcmt: 2.0.14-0 -> 2.0.17-0", "ros-indigo-libntcan: 0.6.4-0 -> 0.6.9-0", "ros-indigo-libpcan: 0.6.4-0 -> 0.6.9-0", "ros-indigo-libphidgets: 0.6.4-0 -> 0.6.9-0", "ros-indigo-librealsense: 0.9.2-3 -> 1.11.0-1", "ros-indigo-libsiftfast: 2.0.14-0 -> 2.0.17-0", "ros-indigo-mapviz: 0.0.6-0 -> 0.0.7-0", "ros-indigo-marti-can-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-common-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-data-structures: 0.0.12-0 -> 0.0.13-0", "ros-indigo-marti-nav-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-perception-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-sensor-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-visualization-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-mini-maxwell: 2.0.14-0 -> 2.0.17-0", "ros-indigo-mrpt-ekf-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-indigo-mrpt-icp-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-indigo-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-nextage-description: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-gazebo: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-ik-plugin: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-moveit-config: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-ros-bridge: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nlopt: 2.0.14-0 -> 2.0.17-0", "ros-indigo-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-opt-camera: 2.0.14-0 -> 2.0.17-0", "ros-indigo-parrot-arsdk: 3.9.1-6 -> 3.10.1-0", "ros-indigo-pgm-learner: 2.0.14-0 -> 2.0.17-0", "ros-indigo-pid: 0.0.17-0 -> 0.0.18-0", "ros-indigo-pointgrey-camera-description: 0.12.1-1 -> 0.12.2-0", "ros-indigo-pointgrey-camera-driver: 0.12.1-1 -> 0.12.2-0", "ros-indigo-polled-camera: 1.11.10-0 -> 1.11.11-0", "ros-indigo-pr2-bringup: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-camera-synchronizer: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-computer-monitor: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-controller-configuration: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-ethercat: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-robot: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-run-stop-auto-restart: 1.6.16-2 -> 1.6.16-4", "ros-indigo-prace-common: 0.6.4-0 -> 0.6.5-0", "ros-indigo-prace-gripper-driver: 0.6.4-0 -> 0.6.5-0", "ros-indigo-raw-description: 0.6.5-0 -> 0.6.6-0", "ros-indigo-realsense-camera: 1.4.0-0 -> 1.5.0-0", "ros-indigo-ridgeback-control: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-description: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-msgs: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-navigation: 0.1.7-0 -> 0.1.8-0", "ros-indigo-rocon-app-manager: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-app-manager-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-app-platform: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-app-utilities: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-apps: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-bubble-icons: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-concert: 0.6.11-0 -> 0.6.11-1", "ros-indigo-rocon-console: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-device-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-ebnf: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-gateway: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-gateway-tests: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-gateway-utils: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-hub: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-hub-client: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-icons: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-interaction-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-interactions: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-launch: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-master-info: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-multimaster: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-python-comms: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-redis: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-utils: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-wifi: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-semantic-version: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-service-pair-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-std-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-test: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-tf-reconstructor: 0.6.11-0 -> 0.6.11-1", "ros-indigo-rocon-tools: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-tutorial-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-unreliable-experiments: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-uri: 0.1.23-0 -> 0.1.23-1", "ros-indigo-roseus: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-mongo: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-smach: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-tutorials: 1.5.3-0 -> 1.6.0-0", "ros-indigo-rospatlite: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rosping: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rostwitter: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rtmros-hironx: 1.1.16-0 -> 1.1.17-0", "ros-indigo-rtmros-nextage: 0.7.8-0 -> 0.7.10-0", "ros-indigo-rviz: 1.11.14-0 -> 1.11.15-0", "ros-indigo-scheduler-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-schunk-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-libm5api: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-modular-robotics: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-powercube-chain: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-sdh: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-sdhx: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.7-0 -> 0.6.8-0", "ros-indigo-statistics-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-swri-console-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-geometry-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-image-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-math-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-nodelet: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-opencv-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-prefix-tools: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-roscpp: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-rospy: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-route-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-serial-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-string-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-system-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-transform-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-yaml-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-theora-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-vision-opencv: 1.11.13-0 -> 1.11.14-0", "ros-indigo-voice-text: 2.0.14-0 -> 2.0.17-0", "ros-indigo-web-video-server: 0.0.4-0 -> 0.0.5-0", "ros-indigo-wfov-camera-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-grid-map", "ros-indigo-grid-map-demos", "ros-indigo-tiago-bringup", "ros-indigo-tiago-gazebo", "ros-indigo-tiago-moveit-config", "ros-indigo-tiago-robot", "ros-indigo-tiago-simulation", "Alexander Bubeck", "Alexander Tiderko", "Andy Zelenak", "Bence Magyar", "Benjamin Maidel", "Daniel Stonier", "David Gossow", "Davide Faconti", "Devon Ash", "Dongwook Lee", "Doug Stephen", "Ed Venator", "Edmond DuPont", "Elliot Johnson", "Felix Messmer", "Florian Weisshardt", "Hitoshi Kamada", "IK Fast Plugin Creater", "Isaac I.Y. Saito", "Isaac IY Saito", "Isaac Isao Saito", "Jack O\u2019Quin", "Jan Fischer", "Jesper Smith", "Jihoon Lee", "Jose Luis", "Jose Luis Blanco Claraco", "Joshua Hampp", "Julius Kammerl", "Kei Okada", "Kentaro Wada", "Kris Kozak", "Liuxin", "Mani Monajjemi", "Marc Alban", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Mikael Arguedas", "Mike Purvis", "MoveIt Setup Assistant", "Nadia Hammoudeh Garcia", "Noda Shintaro", "P. J. Reed", "Paul Bovbel", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Rajvi Jingar", "Richard Bormann", "Russell Toris", "Ryohei Ueda", "Scott Niekum", "Sergey Dorodnicov", "TORK", "Takuya Nakaoka", "Thiago de Freitas", "Tianjiang Hu", "Tony Baltovski", "Vincent Rabaud", "Vladislav Tananaev", "Yohei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "k-okada", "liuxin", "ros-jade-grid-map-pcl: 1.4.1-0", "ros-jade-imagezero: 0.2.3-0", "ros-jade-imagezero-image-transport: 0.2.3-0", "ros-jade-imagezero-ros: 0.2.3-0", "ros-jade-lpg-planner: 2.0.17-0", "ros-jade-mrpt-rbpf-slam: 0.1.3-0", "ros-jade-mrpt-slam: 0.1.3-0", "ros-jade-ros-type-introspection: 0.3.1-0", "ros-jade-slic: 2.0.17-0", "ros-jade-swri-console: 0.2.0-0", "ros-jade-ar-track-alvar: 0.5.3-0 -> 0.5.4-0", "ros-jade-assimp-devel: 2.0.14-0 -> 2.0.17-0", "ros-jade-bayesian-belief-networks: 2.0.14-0 -> 2.0.17-0", "ros-jade-camera-calibration-parsers: 1.11.10-0 -> 1.11.11-0", "ros-jade-camera-info-manager: 1.11.10-0 -> 1.11.11-0", "ros-jade-collada-urdf-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-jade-compressed-depth-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-compressed-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-cv-bridge: 1.11.13-0 -> 1.11.14-0", "ros-jade-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-downward: 2.0.14-0 -> 2.0.17-0", "ros-jade-eus-assimp: 0.2.4-0 -> 0.2.5-0", "ros-jade-euscollada: 0.2.4-0 -> 0.2.5-0", "ros-jade-eusurdf: 0.2.4-0 -> 0.2.5-0", "ros-jade-ff: 2.0.14-0 -> 2.0.17-0", "ros-jade-ffha: 2.0.14-0 -> 2.0.17-0", "ros-jade-grid-map: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-core: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-cv: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-demos: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-filters: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-loader: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-msgs: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-ros: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-visualization: 1.4.0-0 -> 1.4.1-0", "ros-jade-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-jade-image-common: 1.11.10-0 -> 1.11.11-0", "ros-jade-image-exposure-msgs: 0.12.1-0 -> 0.12.2-0", "ros-jade-image-geometry: 1.11.13-0 -> 1.11.14-0", "ros-jade-image-transport: 1.11.10-0 -> 1.11.11-0", "ros-jade-image-transport-plugins: 1.9.3-0 -> 1.9.5-0", "ros-jade-jsk-3rdparty: 2.0.14-0 -> 2.0.17-0", "ros-jade-jsk-interactive: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-interactive-marker: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-interactive-test: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-model-tools: 0.2.4-0 -> 0.2.5-0", "ros-jade-jsk-roseus: 1.5.3-0 -> 1.6.0-0", "ros-jade-jsk-rqt-plugins: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-rviz-plugins: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-visualization: 1.0.33-0 -> 1.0.34-0", "ros-jade-julius: 2.0.14-0 -> 2.0.17-0", "ros-jade-libcmt: 2.0.14-0 -> 2.0.17-0", "ros-jade-libsiftfast: 2.0.14-0 -> 2.0.17-0", "ros-jade-marti-can-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-common-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-data-structures: 0.1.5-0 -> 0.1.6-0", "ros-jade-marti-nav-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-perception-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-sensor-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-visualization-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-mini-maxwell: 2.0.14-0 -> 2.0.17-0", "ros-jade-mrpt-ekf-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-jade-mrpt-ekf-slam-3d: 0.1.1-0 -> 0.1.3-0", "ros-jade-mrpt-icp-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-jade-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-nerian-sp1: 1.3.3-0 -> 1.4.0-0", "ros-jade-nlopt: 2.0.14-0 -> 2.0.17-0", "ros-jade-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-opt-camera: 2.0.14-0 -> 2.0.17-0", "ros-jade-parrot-arsdk: 3.9.1-3 -> 3.10.1-0", "ros-jade-pgm-learner: 2.0.14-0 -> 2.0.17-0", "ros-jade-pid: 0.0.17-0 -> 0.0.18-0", "ros-jade-pointgrey-camera-description: 0.12.1-0 -> 0.12.2-0", "ros-jade-pointgrey-camera-driver: 0.12.1-0 -> 0.12.2-0", "ros-jade-polled-camera: 1.11.10-0 -> 1.11.11-0", "ros-jade-roseus: 1.5.3-0 -> 1.6.0-0", "ros-jade-roseus-mongo: 1.5.3-0 -> 1.6.0-0", "ros-jade-roseus-smach: 1.5.3-0 -> 1.6.0-0", "ros-jade-rospatlite: 2.0.14-0 -> 2.0.17-0", "ros-jade-rosping: 2.0.14-0 -> 2.0.17-0", "ros-jade-rostwitter: 2.0.14-0 -> 2.0.17-0", "ros-jade-rviz: 1.11.14-0 -> 1.11.15-0", "ros-jade-statistics-msgs: 0.12.1-0 -> 0.12.2-0", "ros-jade-swri-console-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-geometry-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-image-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-math-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-nodelet: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-opencv-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-prefix-tools: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-roscpp: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-route-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-serial-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-string-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-system-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-transform-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-yaml-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-teb-local-planner: 0.5.1-0 -> 0.5.2-0", "ros-jade-theora-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-vision-opencv: 1.11.13-0 -> 1.11.14-0", "ros-jade-voice-text: 2.0.14-0 -> 2.0.17-0", "ros-jade-wfov-camera-msgs: 0.12.1-0 -> 0.12.2-0", "Alexander Tiderko", "Andy Zelenak", "Christoph R\u00f6smann", "David Gossow", "Davide Faconti", "Edmond DuPont", "Elliot Johnson", "Hitoshi Kamada", "Jack O\u2019Quin", "Jose Luis", "Jose Luis Blanco Claraco", "Julius Kammerl", "Kei Okada", "Konstantin Schauwecker", "Kris Kozak", "Mani Monajjemi", "Marc Alban", "Mike Purvis", "Noda Shintaro", "P. J. Reed", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Ryohei Ueda", "Scott Niekum", "Takuya Nakaoka", "Tony Baltovski", "Vincent Rabaud", "Vladislav Tananaev", "Yohei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "k-okada", "ros-kinetic-dynamic-tf-publisher: 2.1.2-1", "ros-kinetic-grid-map-pcl: 1.4.1-1", "ros-kinetic-image-view2: 2.1.2-1", "ros-kinetic-imagezero: 0.2.3-0", "ros-kinetic-imagezero-image-transport: 0.2.3-0", "ros-kinetic-imagezero-ros: 0.2.3-0", "ros-kinetic-jsk-common: 2.1.2-1", "ros-kinetic-jsk-data: 2.1.2-1", "ros-kinetic-jsk-network-tools: 2.1.2-1", "ros-kinetic-jsk-roseus: 1.6.0-0", "ros-kinetic-jsk-tilt-laser: 2.1.2-1", "ros-kinetic-jsk-tools: 2.1.2-1", "ros-kinetic-jsk-topic-tools: 2.1.2-1", "ros-kinetic-laser-scan-publisher-tutorial: 0.2.3-0", "ros-kinetic-multi-map-server: 2.1.2-1", "ros-kinetic-navigation-stage: 0.2.3-0", "ros-kinetic-navigation-tutorials: 0.2.3-0", "ros-kinetic-octomap-mapping: 0.6.1-0", "ros-kinetic-octomap-server: 0.6.1-0", "ros-kinetic-odometry-publisher-tutorial: 0.2.3-0", "ros-kinetic-point-cloud-publisher-tutorial: 0.2.3-0", "ros-kinetic-robot-setup-tf-tutorial: 0.2.3-0", "ros-kinetic-roomba-stage: 0.2.3-0", "ros-kinetic-ros-type-introspection: 0.3.1-0", "ros-kinetic-roseus: 1.6.0-0", "ros-kinetic-roseus-smach: 1.6.0-0", "ros-kinetic-rqt-wrapper: 0.1.3-0", "ros-kinetic-simple-navigation-goals-tutorial: 0.2.3-0", "ros-kinetic-swri-console: 0.2.0-0", "ros-kinetic-virtual-force-publisher: 2.1.2-1", "ros-kinetic-wts-driver: 1.0.4-0", "ros-kinetic-concert-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-concert-service-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-concert-workflow-engine-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-gateway-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-geometric-shapes: 0.5.1-0 -> 0.5.2-0", "ros-kinetic-grid-map: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-core: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-cv: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-demos: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-filters: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-loader: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-msgs: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-ros: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-visualization: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-kinetic-librealsense: 1.11.0-0 -> 1.11.0-1", "ros-kinetic-marti-can-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-common-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-data-structures: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-marti-nav-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-perception-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-sensor-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-visualization-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-mavlink: 2016.9.9-0 -> 2016.10.10-0", "ros-kinetic-moveit-python: 0.2.17-0 -> 0.2.17-1", "ros-kinetic-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-nerian-sp1: 1.3.3-0 -> 1.4.0-0", "ros-kinetic-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-pid: 0.0.17-0 -> 0.0.18-0", "ros-kinetic-robot-state-publisher: 1.13.2-0 -> 1.13.3-0", "ros-kinetic-rocon-app-manager: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-app-manager-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-app-platform: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-app-utilities: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-apps: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-bubble-icons: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-console: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-device-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-ebnf: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-gateway: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-gateway-tests: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-gateway-utils: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-hub: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-hub-client: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-icons: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-interaction-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-interactions: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-launch: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-master-info: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-multimaster: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-python-comms: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-redis: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-utils: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-wifi: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-semantic-version: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-service-pair-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-std-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-test: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-tools: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-tutorial-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-unreliable-experiments: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-uri: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rtabmap-ros: 0.11.8-0 -> 0.11.8-1", "ros-kinetic-rviz: 1.12.1-0 -> 1.12.3-0", "ros-kinetic-scheduler-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-swri-console-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-geometry-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-image-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-math-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-nodelet: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-opencv-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-prefix-tools: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-roscpp: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-route-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-serial-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-string-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-system-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-transform-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-yaml-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-teb-local-planner: 0.6.3-0 -> 0.6.4-0", "AlexV", "Alexander Tiderko", "Andy Zelenak", "Armin Hornung", "Chittaranjan Srinivas Swaminathan", "Christoph R\u00f6smann", "Daniel Stonier", "David Gossow", "Davide Faconti", "Dongwook Lee", "Edmond DuPont", "Elliot Johnson", "Ioan Sucan", "Jihoon Lee", "Kei Okada", "Konstantin Schauwecker", "Kris Kozak", "Marc Alban", "Mathieu Labbe", "Michael Ferguson", "P. J. Reed", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Ryohei Ueda", "Sergey Dorodnicov", "Vladimir Ermakov", "William Woodall", "YoheiKakiuchi"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-jade-and-kinetic-2016-10-24/722"}
,{"title": "New Packages for Kinetic 2017-07-25", "thread_contents": ["We\u2019re happy to announce another set of packages for Kinetic as well. We have 86 new packages as well as 78 updated packages in this sync.", "Thank you to all the contributors and maintainers who make these packages available to the community.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-cob-3d-mapping-msgs: 0.6.10-0", "ros-kinetic-cob-base-drive-chain: 0.6.10-0", "ros-kinetic-cob-base-velocity-smoother: 0.7.0-0", "ros-kinetic-cob-bms-driver: 0.6.10-0", "ros-kinetic-cob-calibration-data: 0.6.7-0", "ros-kinetic-cob-cam3d-throttle: 0.6.10-0", "ros-kinetic-cob-camera-sensors: 0.6.10-0", "ros-kinetic-cob-canopen-motor: 0.6.10-0", "ros-kinetic-cob-cartesian-controller: 0.7.0-0", "ros-kinetic-cob-collision-velocity-filter: 0.7.0-0", "ros-kinetic-cob-common: 0.6.7-0", "ros-kinetic-cob-control: 0.7.0-0", "ros-kinetic-cob-control-mode-adapter: 0.7.0-0", "ros-kinetic-cob-control-msgs: 0.7.0-0", "ros-kinetic-cob-default-env-config: 0.6.5-0", "ros-kinetic-cob-description: 0.6.7-0", "ros-kinetic-cob-docker-control: 0.6.6-0", "ros-kinetic-cob-driver: 0.6.10-0", "ros-kinetic-cob-elmo-homing: 0.6.10-0", "ros-kinetic-cob-environments: 0.6.5-0", "ros-kinetic-cob-extern: 0.6.11-0", "ros-kinetic-cob-footprint-observer: 0.7.0-0", "ros-kinetic-cob-frame-tracker: 0.7.0-0", "ros-kinetic-cob-gazebo-plugins: 0.7.1-0", "ros-kinetic-cob-gazebo-ros-control: 0.7.1-0", "ros-kinetic-cob-generic-can: 0.6.10-0", "ros-kinetic-cob-hand: 0.6.2-0", "ros-kinetic-cob-hand-bridge: 0.6.2-0", "ros-kinetic-cob-head-axis: 0.6.10-0", "ros-kinetic-cob-image-flip: 0.6.10-0", "ros-kinetic-cob-light: 0.6.10-0", "ros-kinetic-cob-mimic: 0.6.10-0", "ros-kinetic-cob-model-identifier: 0.7.0-0", "ros-kinetic-cob-msgs: 0.6.7-0", "ros-kinetic-cob-object-detection-msgs: 0.6.10-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.10-0", "ros-kinetic-cob-obstacle-distance: 0.7.0-0", "ros-kinetic-cob-omni-drive-controller: 0.7.0-0", "ros-kinetic-cob-perception-common: 0.6.10-0", "ros-kinetic-cob-perception-msgs: 0.6.10-0", "ros-kinetic-cob-phidget-em-state: 0.6.10-0", "ros-kinetic-cob-phidget-power-state: 0.6.10-0", "ros-kinetic-cob-phidgets: 0.6.10-0", "ros-kinetic-cob-reflector-referencing: 0.6.6-0", "ros-kinetic-cob-relayboard: 0.6.10-0", "ros-kinetic-cob-safety-controller: 0.6.6-0", "ros-kinetic-cob-scan-unifier: 0.6.10-0", "ros-kinetic-cob-sick-lms1xx: 0.6.10-0", "ros-kinetic-cob-sick-s300: 0.6.10-0", "ros-kinetic-cob-sound: 0.6.10-0", "ros-kinetic-cob-srvs: 0.6.7-0", "ros-kinetic-cob-substitute: 0.6.6-0", "ros-kinetic-cob-supported-robots: 0.6.7-0", "ros-kinetic-cob-trajectory-controller: 0.7.0-0", "ros-kinetic-cob-twist-controller: 0.7.0-0", "ros-kinetic-cob-undercarriage-ctrl: 0.6.10-0", "ros-kinetic-cob-undercarriage-ctrl-node: 0.7.0-0", "ros-kinetic-cob-utilities: 0.6.10-0", "ros-kinetic-cob-vision-utils: 0.6.10-0", "ros-kinetic-cob-voltage-control: 0.6.10-0", "ros-kinetic-eband-local-planner: 0.3.1-0", "ros-kinetic-follow-waypoints: 0.3.0-2", "ros-kinetic-grid-map-costmap-2d: 1.5.2-0", "ros-kinetic-grid-map-octomap: 1.5.2-0", "ros-kinetic-image-overlay-scale-and-compass: 0.2.1-0", "ros-kinetic-julius-ros: 2.1.4-0", "ros-kinetic-libconcorde-tsp-solver: 0.6.11-0", "ros-kinetic-libdlib: 0.6.11-0", "ros-kinetic-libntcan: 0.6.11-0", "ros-kinetic-libpcan: 0.6.11-0", "ros-kinetic-libphidgets: 0.6.11-0", "ros-kinetic-libqsopt: 0.6.11-0", "ros-kinetic-opengm: 0.6.11-0", "ros-kinetic-raw-description: 0.6.7-0", "ros-kinetic-rostune: 1.0.5-1", "ros-kinetic-safe-teleop-base: 0.0.2-0", "ros-kinetic-safe-teleop-stage: 0.0.2-0", "ros-kinetic-schunk-description: 0.6.9-0", "ros-kinetic-schunk-libm5api: 0.6.9-0", "ros-kinetic-schunk-modular-robotics: 0.6.9-0", "ros-kinetic-schunk-powercube-chain: 0.6.9-0", "ros-kinetic-schunk-sdh: 0.6.9-0", "ros-kinetic-schunk-simulated-tactile-sensors: 0.6.9-0", "ros-kinetic-soem: 1.3.0-0", "ros-kinetic-urdf-geometry-parser: 0.0.2-0", "ros-kinetic-urdf-sim-tutorial: 0.3.0-1", "ros-kinetic-aruco-detect: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-assimp-devel: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-bayesian-belief-networks: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-bin-pose-emulator: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-bin-pose-msgs: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-binpicking-utils: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-checkerboard-detector: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-default-cfg-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-analysis: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-common-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-updater: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-downward: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-ff: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-ffha: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-fiducial-detect: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-lib: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-msgs: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-pose: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-slam: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducials: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-grid-map: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-core: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-cv: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-demos: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-filters: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-loader: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-msgs: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-pcl: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-ros: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-rviz-plugin: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-visualization: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-imagesift: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-3rdparty: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-jsk-pcl-ros: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-pcl-ros-utils: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-perception: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition-msgs: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition-utils: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-julius: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-libcmt: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-libsiftfast: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-lpg-planner: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-magni-bringup: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-demos: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-description: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-nav: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-robot: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-teleop: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-master-discovery-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-master-sync-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-mavlink: 2017.6.6-0 -> 2017.7.7-0", "ros-kinetic-mini-maxwell: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-move-basic: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-multimaster-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-multimaster-msgs-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-multiwii: 2.0.0-0 -> 2.0.1-0", "ros-kinetic-nlopt: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-node-manager-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-opencv-apps: 1.11.15-0 -> 1.12.0-0", "ros-kinetic-opt-camera: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-pgm-learner: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-plotjuggler: 1.1.2-0 -> 1.1.3-0", "ros-kinetic-resized-image-transport: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-rosdiagnostic: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-rosjava-core: 0.3.4-1 -> 0.3.5-0", "ros-kinetic-roslisp: 1.9.20-0 -> 1.9.21-0", "ros-kinetic-rospatlite: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-rosping: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-rqt-tf-tree: 0.5.7-0 -> 0.5.8-0", "ros-kinetic-self-test: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-slic: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-test-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-thormang3-tools: 0.1.0-0 -> 0.1.2-0", "ros-kinetic-urdf-tutorial: 0.2.5-0 -> 0.3.0-1", "ros-kinetic-voice-text: 2.0.20-0 -> 2.1.4-0", "Aaron Blasdel", "Alexander Bubeck", "Alexander Tiderko", "Austin Hendrix", "Benjamin Maidel", "Brice Rebsamen", "Charles DuHadway (maintained by Benjamin Pitzer)", "Christian Rauch", "Damon Kohler", "Daniel Snider", "David V. Lu!!", "Davide Faconti", "Felix Messmer", "Florian Weisshardt", "Frantisek Durovsky", "Georg Bartels", "George Stavrinos", "Guillaume Autran", "Hitoshi Kamada", "Jan Fischer", "Jim Vaughan", "Joshua Hampp", "Kei Okada", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Nadia Hammoudeh Garcia", "Noda Shintaro", "Piyush Khandelwal", "Pyo", "P\u00e9ter Fankhauser", "Richard Bormann", "Rohan Agrawal", "Ruben Smits", "Ryohei Ueda", "Takuya Nakaoka", "Vincent Rousseau", "Vladimir Ermakov", "Yohei Kakiuchi", "Youhei Kakiuchi", "Yuki Furuta", "Yuto Inagaki", "durovsky"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2017-07-25/2307"}
,{"title": "New Packages for Indigo 2017-07-25", "thread_contents": ["We\u2019re happy to announce another set of new packages for Indigo. We have 20 new packages as well as over 200 updates available.", "Full details are below. Thanks to all the contributors and maintainers who have made this possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-cob-helper-tools: 0.6.6-0", "ros-indigo-cob-map-accessibility-analysis: 0.6.5-0", "ros-indigo-cob-supported-robots: 0.6.7-0", "ros-indigo-generic-throttle: 0.6.6-0", "ros-indigo-grid-map-costmap-2d: 1.5.2-0", "ros-indigo-grid-map-octomap: 1.5.2-0", "ros-indigo-jsk-pr2-desktop: 1.1.0-1", "ros-indigo-julius-ros: 2.1.4-0", "ros-indigo-libconcorde-tsp-solver: 0.6.11-0", "ros-indigo-libqsopt: 0.6.11-0", "ros-indigo-moveit-goal-builder: 0.1.0-0", "ros-indigo-opengm: 0.6.11-0", "ros-indigo-parameter-pa: 1.0.0-0", "ros-indigo-rostune: 1.0.5-1", "ros-indigo-safe-teleop-base: 0.0.2-0", "ros-indigo-safe-teleop-pr2: 0.0.2-0", "ros-indigo-safe-teleop-stage: 0.0.2-0", "ros-indigo-service-tools: 0.6.6-0", "ros-indigo-spin-hokuyo: 1.0.0-0", "ros-indigo-urdf-sim-tutorial: 0.3.0-0", "ros-indigo-assimp-devel: 2.0.20-0 -> 2.1.4-0", "ros-indigo-baxtereus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-bayesian-belief-networks: 2.0.20-0 -> 2.1.4-0", "ros-indigo-can-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-402: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-chain-node: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-master: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-motor-node: 0.6.7-0 -> 0.6.8-0", "ros-indigo-checkerboard-detector: 1.1.2-0 -> 1.2.2-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-android: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-msgs: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-resource-server: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-script-server: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-settings: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-base-drive-chain: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-base-velocity-smoother: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-bms-driver: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-calibration-data: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-cam3d-throttle: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-camera-sensors: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-canopen-motor: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-cartesian-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-collision-velocity-filter: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-command-gui: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-command-tools: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-common: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-control: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-control-mode-adapter: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-control-msgs: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-dashboard: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-env-config: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-description: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-docker-control: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-driver: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-elmo-homing: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-environments: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-extern: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-footprint-observer: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-frame-tracker: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-gazebo-plugins: 0.6.4-0 -> 0.6.6-0", "ros-indigo-cob-gazebo-ros-control: 0.6.4-0 -> 0.6.6-0", "ros-indigo-cob-generic-can: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-hand: 0.6.1-0 -> 0.6.2-0", "ros-indigo-cob-hand-bridge: 0.6.1-0 -> 0.6.2-0", "ros-indigo-cob-head-axis: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-image-flip: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-interactive-teleop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-light: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-linear-nav: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-mapping-slam: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-mimic: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-model-identifier: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-monitoring: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-msgs: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-navigation: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-config: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-global: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-local: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-slam: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-object-detection-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-object-detection-visualizer: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-obstacle-distance: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-omni-drive-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-perception-common: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-perception-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidget-em-state: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidget-power-state: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidgets: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-reflector-referencing: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-relayboard: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-safety-controller: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-scan-unifier: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-script-server: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-sick-lms1xx: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-sick-s300: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-sound: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-srvs: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-substitute: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-teleop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-trajectory-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-twist-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-undercarriage-ctrl-node: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-utilities: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-vision-utils: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-voltage-control: 0.6.8-0 -> 0.6.10-0", "ros-indigo-collada-urdf-jsk-patch: 2.0.20-0 -> 2.1.4-0", "ros-indigo-default-cfg-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-analysis: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-common-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-updater: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diff-drive-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-downward: 2.0.20-0 -> 2.1.4-0", "ros-indigo-effort-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-fetcheus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-ff: 2.0.20-0 -> 2.1.4-0", "ros-indigo-ffha: 2.0.20-0 -> 2.1.4-0", "ros-indigo-force-torque-sensor-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-forward-command-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-grid-map: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-core: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-cv: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-demos: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-filters: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-loader: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-msgs: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-pcl: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-ros: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-rviz-plugin: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-visualization: 1.4.2-0 -> 1.5.2-0", "ros-indigo-gripper-action-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-imagesift: 1.1.2-0 -> 1.2.2-0", "ros-indigo-imu-sensor-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-joint-state-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-joint-trajectory-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-jsk-2015-05-baxter-apc: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-201504-miraikan: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-2016-01-baxter-apc: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-3rdparty: 2.0.20-0 -> 2.1.4-0", "ros-indigo-jsk-apc2015-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-apc2016-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-arc2017-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-baxter-desktop: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-baxter-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-baxter-web: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-common-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-fetch-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-footstep-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-gui-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-hark-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-interactive: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-interactive-marker: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-interactive-test: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-nao-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pcl-ros: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pcl-ros-utils: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pepper-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-perception: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pr2-calibration: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pr2-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pr2eus: 0.3.11-0 -> 0.3.13-0", "ros-indigo-jsk-recognition: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-recognition-msgs: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-recognition-utils: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-robot: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-robot-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-robot-utils: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-rqt-plugins: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-rviz-plugins: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-visualization: 2.1.0-0 -> 2.1.2-0", "ros-indigo-julius: 2.0.20-0 -> 2.1.4-0", "ros-indigo-laser-filters-jsk-patch: 2.0.20-0 -> 2.1.4-0", "ros-indigo-libcmt: 2.0.20-0 -> 2.1.4-0", "ros-indigo-libdlib: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libntcan: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libpcan: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libphidgets: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libsiftfast: 2.0.20-0 -> 2.1.4-0", "ros-indigo-lpg-planner: 2.0.20-0 -> 2.1.4-0", "ros-indigo-master-discovery-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-master-sync-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-mini-maxwell: 2.0.20-0 -> 2.1.4-0", "ros-indigo-multimaster-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-multimaster-msgs-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-naoeus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-naoqieus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-nlopt: 2.0.20-0 -> 2.1.4-0", "ros-indigo-node-manager-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-opencv-apps: 1.11.15-0 -> 1.12.0-0", "ros-indigo-opt-camera: 2.0.20-0 -> 2.1.4-0", "ros-indigo-parrot-arsdk: 3.11.0-0 -> 3.12.6-0", "ros-indigo-peppereus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-pgm-learner: 2.0.20-0 -> 2.1.4-0", "ros-indigo-plotjuggler: 1.1.2-0 -> 1.1.3-0", "ros-indigo-posedetection-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-position-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-pr2-base-trajectory-action: 1.0.6-2 -> 1.1.0-1", "ros-indigo-pr2eus: 0.3.11-0 -> 0.3.13-0", "ros-indigo-pr2eus-moveit: 0.3.11-0 -> 0.3.13-0", "ros-indigo-pr2eus-tutorials: 0.3.11-0 -> 0.3.13-0", "ros-indigo-rail-manipulation-msgs: 0.0.9-0 -> 0.0.10-0", "ros-indigo-raw-description: 0.6.6-0 -> 0.6.7-0", "ros-indigo-resized-image-transport: 1.1.2-0 -> 1.2.2-0", "ros-indigo-ros-canopen: 0.6.7-0 -> 0.6.8-0", "ros-indigo-ros-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-rosauth: 0.1.7-0 -> 0.1.7-1", "ros-indigo-rosdiagnostic: 1.9.0-0 -> 1.9.2-0", "ros-indigo-roseus-remote: 1.0.6-2 -> 1.1.0-1", "ros-indigo-roslisp: 1.9.20-0 -> 1.9.21-0", "ros-indigo-rospatlite: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rosping: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rostwitter: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rqt-joint-trajectory-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-rqt-tf-tree: 0.5.7-0 -> 0.5.8-0", "ros-indigo-schunk-description: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-libm5api: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-modular-robotics: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-powercube-chain: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-sdh: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.8-0 -> 0.6.9-0", "ros-indigo-self-test: 1.9.0-0 -> 1.9.2-0", "ros-indigo-slic: 2.0.20-0 -> 2.1.4-0", "ros-indigo-socketcan-bridge: 0.6.7-0 -> 0.6.8-0", "ros-indigo-socketcan-interface: 0.6.7-0 -> 0.6.8-0", "ros-indigo-speech-recognition-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-surface-perception: 0.1.1-0 -> 0.1.3-0", "ros-indigo-test-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-indigo-transform-graph: 0.1.4-0 -> 0.2.1-0", "ros-indigo-urdf-tutorial: 0.2.5-0 -> 0.3.0-0", "ros-indigo-velocity-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-voice-text: 2.0.20-0 -> 2.1.4-0", "ros-indigo-jsk-apc", "ros-indigo-jsk-arc2017-baxter", "Aaron Blasdel", "Adolfo Rodriguez Tsouroukdissian", "Alexander Bubeck", "Alexander Tiderko", "Austin Hendrix", "Bence Magyar", "Benjamin Maidel", "Brice Rebsamen", "Charles DuHadway (maintained by Benjamin Pitzer)", "David Kent", "David V. Lu!!", "Davide Faconti", "Felix Messmer", "Florian Weisshardt", "Georg Bartels", "George Stavrinos", "Guillaume Autran", "Hasegawa Shun", "Hitoshi Kamada", "Jan Fischer", "Joshua Hampp", "Justin Huang", "KazutoMurase", "Kei Okada", "Kentaro Wada", "Mani Monajjemi", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Nadia Hammoudeh Garcia", "Noda Shintaro", "Peter Weissig", "P\u00e9ter Fankhauser", "Richard Bormann", "Russell Toris", "Ryohei Ueda", "Sachin Chitta", "Sarah Bertussi", "Shohei Fujii", "Takuya Nakaoka", "Yohei Kakiuchi", "YoheiKakiuchi", "Youhei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "inagaki", "k-okada"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2017-07-25/2306"}
,{"title": "New Packages for Indigo 2018-01-09", "thread_contents": ["We\u2019re happy to anounce 6 new packages and 168 updated packages for Indigo.", "Thanks as always to all the contributors and maintainers. The list of releases and maintainers is below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-cost-map: 0.3.3-0", "ros-indigo-cost-map-core: 0.3.3-0", "ros-indigo-cost-map-cv: 0.3.3-0", "ros-indigo-cost-map-demos: 0.3.3-0", "ros-indigo-cost-map-ros: 0.3.3-0", "ros-indigo-cost-map-visualisations: 0.3.3-0", "ros-indigo-baldor: 0.1.1-0 -> 0.1.2-0", "ros-indigo-care-o-bot: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-desktop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-robot: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-simulation: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-android: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-msgs: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-resource-server: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-script-server: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-settings: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-base-drive-chain: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-base-velocity-smoother: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-bms-driver: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-bringup: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-bringup-sim: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-calibration-data: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-cam3d-throttle: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-camera-sensors: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-canopen-motor: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-cartesian-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-collision-monitor: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-collision-velocity-filter: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-command-gui: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-command-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-common: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-control: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-control-mode-adapter: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-control-msgs: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-dashboard: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-default-env-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-robot-behavior: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-default-robot-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-docker-control: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-driver: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-elmo-homing: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-environments: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-extern: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-footprint-observer: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-frame-tracker: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-gazebo: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-gazebo-objects: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-gazebo-plugins: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-ros-control: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-worlds: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-generic-can: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-grasp-generation: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-hand: 0.6.2-0 -> 0.6.3-0", "ros-indigo-cob-hand-bridge: 0.6.2-0 -> 0.6.3-0", "ros-indigo-cob-hardware-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-helper-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-image-flip: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-interactive-teleop: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-kinematics: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-light: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-linear-nav: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-lookat-action: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-manipulation: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-map-accessibility-analysis: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-mapping-slam: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-mimic: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-model-identifier: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-monitoring: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-moveit-bringup: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-moveit-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-moveit-interface: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-navigation: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-global: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-local: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-slam: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-object-detection-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-object-detection-visualizer: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-obstacle-distance: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-obstacle-distance-moveit: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-omni-drive-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-perception-common: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-perception-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidget-em-state: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidget-power-state: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidgets: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-pick-place-action: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-reflector-referencing: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-relayboard: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-robots: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-safety-controller: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-scan-unifier: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-script-server: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-sick-lms1xx: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-sick-s300: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-simulation: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-sound: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-srvs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-substitute: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-supported-robots: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-teleop: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-trajectory-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-twist-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-utilities: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-vision-utils: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-voltage-control: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cost-map-msgs: 0.3.2-0 -> 0.3.3-0", "ros-indigo-dynamic-tf-publisher: 2.2.5-0 -> 2.2.6-0", "ros-indigo-gazebo-grasp-plugin: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-state-plugins: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-test-tools: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-world-plugin-loader: 1.0.1-0 -> 1.0.2-0", "ros-indigo-generic-throttle: 0.6.6-0 -> 0.6.7-0", "ros-indigo-grasp-planning-graspit: 1.1.2-0 -> 1.2.0-0", "ros-indigo-grasp-planning-graspit-msgs: 1.1.2-0 -> 1.2.0-0", "ros-indigo-grasp-planning-graspit-ros: 1.1.2-0 -> 1.2.0-0", "ros-indigo-graspit-tools: 1.1.2-0 -> 1.2.0-0", "ros-indigo-image-view2: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jaco-graspit-sample: 1.1.2-0 -> 1.2.0-0", "ros-indigo-jsk-common: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-data: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-network-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-tilt-laser: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-topic-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-libconcorde-tsp-solver: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libdlib: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libntcan: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libpcan: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libphidgets: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libqsopt: 0.6.11-0 -> 0.6.12-0", "ros-indigo-moveit-controller-multidof: 1.0.0-0 -> 1.0.1-0", "ros-indigo-moveit-object-handling: 1.0.0-0 -> 1.0.1-0", "ros-indigo-moveit-planning-helper: 1.0.0-0 -> 1.0.1-0", "ros-indigo-multi-map-server: 2.2.5-0 -> 2.2.6-0", "ros-indigo-opengm: 0.6.11-0 -> 0.6.12-0", "ros-indigo-pcdfilter-pa: 1.1.0-0 -> 1.2.0-0", "ros-indigo-raw-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-description: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-libm5api: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-modular-robotics: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-powercube-chain: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-sdh: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.9-0 -> 0.6.10-0", "ros-indigo-service-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-universal-robot: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-bringup: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-description: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-driver: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-gazebo: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-kinematics: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-msgs: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur10-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur3-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur5-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-urdf-processing-tools: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-transform: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-traverser: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-viewer: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf2graspit: 1.1.2-0 -> 1.2.0-0", "ros-indigo-urdf2inventor: 1.0.1-0 -> 1.0.2-0", "ros-indigo-virtual-force-publisher: 2.2.5-0 -> 2.2.6-0", "ros-indigo-visp: 3.0.1-1 -> 3.1.0-2", "ros-indigo-xpp: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-examples: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-hyq: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-msgs: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-quadrotor: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-states: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-vis: 1.0.3-0 -> 1.0.4-0", "ros-indigo-cob-head-axis", "ros-indigo-cob-undercarriage-ctrl-node", "Alexander Bubeck", "Alexander W. Winkler", "Benjamin Maidel", "Bruno Brito", "Daniel Stonier", "Fabien Spindler", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Felx Messmer", "Florian Weisshardt", "Francisco Suarez-Ruiz", "Jannik Abbenseth", "Jennifer Buehler", "Joshua Hampp", "Kei Okada", "Matthias Gruhler", "Peter Weissig", "Richard Bormann", "Ryohei Ueda", "YoheiKakiuchi"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2018-01-09/3639"}
,{"title": "New Packages for Kinetic 2018-01-09", "thread_contents": ["We\u2019re happy to announce 22 new packages and 175 updated packages. This includes the restoration of the regressed costmap packages from the last sync.", "Thank you to all the maintainers and contributors who have helped make this possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Added Packages", "Is there a way to add hyperlinks to package home pages that would provide some summary what that package is about?", "\nNow I type google search (example: bus-server), go through a few pages that are unrelated, like general ROS \u201cpackage\u201d installation, and finally hit something usable after 6 links that the search returned. It can be easier than that.", "If others already know a short way to find information, please share.", "Cheers", " was bus_server supposed to make it into this sync? I removed it from the kinetic rosdistro after the rename to ", ".", "EDIT: Looks like I forgot to rename the dependency in loki_bringup, ran a new release. Is there anything else needed to ensure proper deletion?", "Is there a way to add hyperlinks to package home pages that would provide some summary what that package is about?", "Now I type google search (example: bus-server), go through a few pages that are unrelated, like general ROS \u201cpackage\u201d installation, and finally hit something usable after 6 links that the search returned. It can be easier than that.", "I totally agree with ", ", I also searched on Google for several packages without direct success.", "The default location to find a package is on the ROS wiki with the package name. So for package ", " which is packaged into the debian package ", " see ", "I opened a ", " as a link in the summary last week. We are targeting to roll it out before the next sync.", "WRT to bus_server as ", " alluded, that was a package ", " but after review was renamed to ", " which is why you can\u2019t find any documentation. There\u2019s an outstanding ticket to ", " but we haven\u2019t had time to implement it.", "The default location to find a package is on the ROS wiki with the package name. So for package cost_map which is packaged into the debian package ros-kinetic-cost-map see ", "Yes, I know, but it is not always available. A fallback to the Github repo would be nice too.", "I opened a PR to embed the declared homepage as a link in the summary last week. We are targeting to roll it out before the next sync.", "Very good news, thanks!", "Yes, I know, but it is not always available. A fallback to the Github repo would be nice too.", "An alternative is to check the ", " automatically generated by the ROS buildfarm. They provide a complete list of all released packages including a link to the source repository as well as a link to the wiki page for each package if it exists.", "\nexample: ", " package", "\n", " provides a link to the ", " and the ", "In the case of packages that don\u2019t have a source of a doc entry (like ", ") the status page provides only a link to the ", ".", "On a related note, there\u2019s an unmerged PR that aims to encourage package releasers to create pages on ROS wiki ", " (I haven\u2019t had time to make an improvement).", "An alternative is to check the status pages automatically generated by the ROS buildfarm. They provide a complete list of all released packages including a link to the source repository as well as a link to the wiki page for each package if it exists.", "Oh, really interesting.", "\nI did not know that page.", "\nThanks!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-bus-server: 0.1.1-0", "ros-kinetic-care-o-bot: 0.6.6-0", "ros-kinetic-care-o-bot-desktop: 0.6.6-0", "ros-kinetic-care-o-bot-robot: 0.6.6-0", "ros-kinetic-care-o-bot-simulation: 0.6.6-0", "ros-kinetic-cob-manipulation: 0.7.1-0", "ros-kinetic-cob-pick-place-action: 0.7.1-0", "ros-kinetic-cost-map: 0.3.3-0", "ros-kinetic-cost-map-core: 0.3.3-0", "ros-kinetic-cost-map-cv: 0.3.3-0", "ros-kinetic-cost-map-demos: 0.3.3-0", "ros-kinetic-cost-map-ros: 0.3.3-0", "ros-kinetic-cost-map-visualisations: 0.3.3-0", "ros-kinetic-darknet-ros-msgs: 1.1.2-0", "ros-kinetic-loki-base-node: 0.2.1-0", "ros-kinetic-loki-bringup: 0.0.1-0", "ros-kinetic-loki-demos: 0.0.1-0", "ros-kinetic-loki-description: 0.0.1-0", "ros-kinetic-loki-nav: 0.0.1-0", "ros-kinetic-loki-robot: 0.0.1-0", "ros-kinetic-loki-teleop: 0.0.1-0", "ros-kinetic-pcdfilter-pa: 1.2.0-0", "ros-kinetic-baldor: 0.1.1-0 -> 0.1.2-0", "ros-kinetic-cob-3d-mapping-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-android: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-msgs: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-resource-server: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-script-server: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-settings: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-base-drive-chain: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-base-velocity-smoother: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-bms-driver: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-bringup: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-bringup-sim: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-calibration-data: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-cam3d-throttle: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-camera-sensors: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-canopen-motor: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-cartesian-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-collision-monitor: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-collision-velocity-filter: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-command-gui: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-command-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-common: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-control: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-control-mode-adapter: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-control-msgs: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-dashboard: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-default-env-config: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-default-robot-behavior: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-default-robot-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-description: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-docker-control: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-driver: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-elmo-homing: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-environments: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-extern: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-footprint-observer: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-frame-tracker: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-gazebo: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-gazebo-objects: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-gazebo-plugins: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-gazebo-ros-control: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-gazebo-worlds: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-generic-can: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-grasp-generation: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-hand: 0.6.2-0 -> 0.6.3-0", "ros-kinetic-cob-hand-bridge: 0.6.2-0 -> 0.6.3-0", "ros-kinetic-cob-hardware-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-helper-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-image-flip: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-interactive-teleop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-light: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-linear-nav: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-lookat-action: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-map-accessibility-analysis: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-mapping-slam: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-mimic: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-model-identifier: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-monitoring: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-moveit-bringup: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-moveit-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-moveit-interface: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-msgs: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-navigation: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-config: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-global: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-local: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-slam: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-object-detection-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-obstacle-distance: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-obstacle-distance-moveit: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-omni-drive-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-perception-common: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-perception-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidget-em-state: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidget-power-state: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidgets: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-reflector-referencing: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-relayboard: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-robots: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-safety-controller: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-scan-unifier: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-script-server: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-sick-lms1xx: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-sick-s300: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-simulation: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-sound: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-srvs: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-substitute: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-supported-robots: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-teleop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-trajectory-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-twist-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-undercarriage-ctrl: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-utilities: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-vision-utils: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-voltage-control: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cost-map-msgs: 0.3.2-0 -> 0.3.3-0", "ros-kinetic-dynamic-tf-publisher: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-generic-throttle: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-geometry2: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-image-view2: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-interactive-marker-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-jsk-common: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-data: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-network-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-tilt-laser: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-topic-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-kobuki: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-auto-docking: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-bumper2pc: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-capabilities: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-controller-tutorial: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-description: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-keyop: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-node: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-random-walker: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-rapps: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-safety-controller: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-testsuite: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-libconcorde-tsp-solver: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libdlib: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libntcan: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libpcan: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libphidgets: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libqsopt: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-librviz-tutorial: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-mavlink: 2017.12.12-0 -> 2018.1.1-0", "ros-kinetic-moveit-visual-tools: 3.3.0-0 -> 3.4.0-0", "ros-kinetic-multi-map-server: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-opengm: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-pose-cov-ops: 0.1.7-0 -> 0.2.0-0", "ros-kinetic-raw-description: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-robot-localization: 2.4.0-0 -> 2.4.2-0", "ros-kinetic-rviz: 1.12.14-0 -> 1.12.15-0", "ros-kinetic-rviz-plugin-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-rviz-python-tutorial: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-schunk-description: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-libm5api: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-modular-robotics: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-powercube-chain: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-sdh: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-simulated-tactile-sensors: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-service-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-tf2: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-bullet: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-eigen: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-geometry-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-kdl: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-py: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-ros: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-sensor-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-tools: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-universal-robot: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-bringup: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-description: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-driver: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-gazebo: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-kinematics: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-msgs: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur10-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur3-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur5-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-virtual-force-publisher: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-visualization-marker-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-visualization-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-xpp: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-examples: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-hyq: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-msgs: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-quadrotor: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-states: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-vis: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-cob-head-axis", "ros-kinetic-cob-undercarriage-ctrl-node", "Alexander Bubeck", "Alexander W. Winkler", "Benjamin Maidel", "Bruno Brito", "D. Hood", "Daniel Stonier", "Dave Coleman", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Felx Messmer", "Florian Weisshardt", "Francisco Suarez-Ruiz", "Jannik Abbenseth", "Jorge Santos Simon", "Jose-Luis Blanco-Claraco", "Joshua Hampp", "Kei Okada", "Koji Terada", "Marcus Liebhardt", "Marko Bjelonic", "Matthias Gruhler", "Peter Weissig", "Richard Bormann", "Rohan Agrawal", "Ryohei Ueda", "Tom Moore", "Tully Foote", "Vincent Rabaud", "Vladimir Ermakov", "Wayne Gramlich", "William Woodall", "YoheiKakiuchi", "Younghun Ju"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2018-01-09/3641"}
,{"title": "Using ROS on Nao", "thread_contents": ["Hi there, I\u2019m investigating using ROS with Nao robots for a project but have a few questions that I can\u2019t seem to find answers for online.", "First, do the official ROS Nao packages support Nao v4 or are v5 required?", "Second, it looks like it is possible to have ROS running directly on the robot itself, but is not the preferred method. How feasible actually is it to have ROS installed and running on the robot? How well does it work? Are there pre-compiled binaries for it or would we need to cross-compile it, as suggested by one of the tutorials?", "Any advice about this would be greatly appreciated!", "//Mike", "Hi ", ",", "Thanks for your question.", "\nHowever ROS Discourse is for news and general interest discussions. ROS Answers", "\n", " provides a forum which can be filtered by tags to", "\nmake sure the relevant people can find and/or answer the question, and not", "\noverload everyone with hundreds of posts. So we recommend users to ask their questions there following our support guidelines:", "\n", ".", "\nFor NAO (or Pepper/Romeo) related question you may also find answers to your questions on the dedicated mailing list: ", "To not leave you empty handed ", " :", "Hope this helps,", "Thanks for the tip about answers, and thanks for the information, very useful!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS Nao packages should work on v4 and v5", "Nao has a very limited processing power so running the ROS machinery on top of the Naoqi one gets challenging. But is you don\u2019t do expensive processing you should be able to run some nodes directly on the robot. Some people give additional computing power to Nao to avoid the remote processing (see the ", " project for example)", "There are no pre-compiled binaries AFAIK, but you can use the OpenNao VM to compile your packages for the target architecture"], "url": "https://discourse.ros.org/t/using-ros-on-nao/4110"}
,{"title": "Roboware for ROS beginner dealing with BLDC controller and ROS melodic", "thread_contents": ["hi guys,", "when i was trying to start, first thing in my mind is like \u201chow ROS handle the motor controller?\u201d coz so far as i know (CMIIW), not all BLDC controller is ROS friendly.", "\nso i can only place my bet for any controller which has protocol like CANopen and RS232. trying to plug the controller directly to PC (like intel NUC as the brain of the robot) as well as cheap LIDAR sensor and hoping it will auto magically work by the time ROS installed.", "\n*idiot mode. i know\u2026 i\u2019m very newbie, with only 2 weeks experience in ROS ", "just wondering if roboware is compatible for the latest ROS melodic version and ubuntu 18.04.", "\nfiguring out to work with roboware designer and studio to deal with a chinese BLDC controller. is it possible?", "\nespecially roboware and BLDC controller from the same city in jinan ", "i see chance in roboware designer for any newbies like me to play with simple autonomous robot using ROS. but yes, are all controllers (BLDC, brushed DC) and cheap sensors compatible with roboware and ROS?", "i saw many great example using state of the art lidar (sick, hokuyo, and even $4K velodyne for sale which is still obviously out of my wallet ", " ) while i have eyes only on $200-300 2D lidar (they have ROS driver but still don\u2019t have any idea, but at least i have hope as they said they have ROS driver ", "  ) and intel realsense for 3D vision (i\u2019m sure this camera will work as i saw it in many threats although i\u2019ve never tried this until today).", "so here is my steps as newbie from scratch to autonomous wheeled robot:", "do you think my steps are correct? or i\u2019m still lost in the jungle and still long way to go?", "\nplease kindly guide me by giving me a clue, even a small clue or direction is really appreciated.", "anyway, thanks for creating such this great tool.", "\ni do really appreciate it.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["install ubuntu and ROS (18.04 ubuntu and ROS melodic? or just keep it with kinetic for now?)", "install roboware", "connect BLDC controller to any usb port or serial port (sure connect the motor to the controller)", "connect lidar to any usb port", "connect intel realsense camera to usb 3.0 port", "get the power/battery on", "and play with roboware for doing small baby steps"], "url": "https://discourse.ros.org/t/roboware-for-ros-beginner-dealing-with-bldc-controller-and-ros-melodic/4915"}
,{"title": "New Packages for Indigo 2018-07-26", "thread_contents": ["We\u2019re happy to announce 20 new packages and 149 updated packages for Indigo Igloo today.", "Thank you to all the maintainers and contributors who have helped make these available. Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-costmap-cspace: 0.2.3-0", "ros-indigo-ibeo-lux: 2.0.0-0", "ros-indigo-joystick-interrupt: 0.2.3-0", "ros-indigo-map-organizer: 0.2.3-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "ros-indigo-neonavigation: 0.2.3-0", "ros-indigo-neonavigation-common: 0.2.3-0", "ros-indigo-neonavigation-launch: 0.2.3-0", "ros-indigo-obj-to-pointcloud: 0.2.3-0", "ros-indigo-planner-cspace: 0.2.3-0", "\n", ": 0.0.4-1", "ros-indigo-pybind11-catkin: 2.2.3-3", "ros-indigo-safety-limiter: 0.2.3-0", "ros-indigo-track-odometry: 0.2.3-0", "ros-indigo-trajectory-tracker: 0.2.3-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-bms-driver: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-cartesian-controller: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-control-mode-adapter: 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-control-msgs: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-indigo-cob-default-robot-behavior: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-elmo-homing: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-frame-tracker: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.3-0 -> 0.6.5-0", "ros-indigo-cob-hand-bridge: 0.6.3-0 -> 0.6.5-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-helper-tools: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-model-identifier: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-indigo-cob-moveit-config: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-msgs: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-object-detection-visualizer: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-obstacle-distance: 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-omni-drive-controller: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-phidget-em-state: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-phidget-power-state: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-scan-unifier: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-twist-controller: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 3.5.4-0 -> 3.6.2-0", "ros-indigo-eus-assimp: 0.3.5-0 -> 0.4.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 9.23.0-0 -> 9.25.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-depth-layer: 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-description: 0.7.13-0 -> 0.7.14-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "ros-indigo-fetch-ikfast-plugin: 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-maps: 0.7.13-0 -> 0.7.14-0", "\n", ": 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-navigation: 0.7.13-0 -> 0.7.14-0", "\n", ": 0.7.13-0 -> 0.7.14-0", "\n", ": 0.1.4-0 -> 0.1.5-0", "ros-indigo-generic-throttle: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 1.11.3-0 -> 1.11.4-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.1.0-0 -> 1.2.0-2", "\n", ": 1.11.14-0 -> 1.11.15-0", "\n", ": 1.11.14-0 -> 1.11.15-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "ros-indigo-novatel-gps-driver: 3.4.0-0 -> 3.5.0-0", "ros-indigo-novatel-gps-msgs: 3.4.0-0 -> 3.5.0-0", "ros-indigo-openni2-camera: 0.3.0-0 -> 0.4.0-0", "ros-indigo-openni2-launch: 0.3.0-0 -> 0.4.0-0", "\n", ": 0.0.24-0 -> 0.0.27-0", "\n", ": 1.6.2-0 -> 1.7.1-1", "\n", ": 0.2.0-0 -> 0.2.1-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-indigo-rc-genicam-api: 1.3.11-0 -> 1.3.12-0", "ros-indigo-robot-controllers: 0.5.3-0 -> 0.5.4-0", "ros-indigo-robot-controllers-interface: 0.5.3-0 -> 0.5.4-0", "ros-indigo-robot-controllers-msgs: 0.5.3-0 -> 0.5.4-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "ros-indigo-roseus-mongo: 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "ros-indigo-service-tools: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "ros-indigo-tf2-eigen: 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 1.1.2-0 -> 1.1.4-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "ros-indigo-xsens-driver: 2.1.0-0 -> 2.2.0-3", "ros-indigo-freight-calibration", "Alexander Bubeck", "Alexander Tiderko", "Alexander W. Winkler", "Andy Zelenak", "Atsushi Watanabe", "AutonomouStuff Software Development Team", "Benjamin Maidel", "Bruno Brito", "Chris Lalancette", "Davide Faconti", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florian Weisshardt", "Francis Colas", "Isaac I. Y. Saito", "Jannik Abbenseth", "Joshua Hampp", "Kei Okada", "Koji Terada", "Martin G\u00fcnther", "Matthias Gruhler", "Michael Ferguson", "P. J. Reed", "Pyo", "Richard Bormann", "Ron Tajima", "Russell Toris", "Sammy Pfeiffer", "Tully Foote", "Vincent Rabaud", "Vladimir Ivan", "William Woodall", "Yohei Kakiuchi", "Yuki Furuta"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2018-07-26/5506"}
,{"title": "New Packages for Kinetic 2018-07-26", "thread_contents": ["We\u2019re happy to announce 35 new packages and 171 updated packages for Kinetic Kame. The full details are below.", "Thank you to the contributors and maintainers who have helped make these packages available to the community.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-arduino-daq: 1.0.1-0", "\n", ": 0.1.2-0", "ros-kinetic-costmap-cspace: 0.2.3-0", "\n", ": 1.3.1-0", "ros-kinetic-ibeo-lux: 2.0.0-0", "ros-kinetic-joystick-interrupt: 0.2.3-0", "ros-kinetic-map-organizer: 0.2.3-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "ros-kinetic-neonavigation: 0.2.3-0", "ros-kinetic-neonavigation-common: 0.2.3-0", "ros-kinetic-neonavigation-launch: 0.2.3-0", "ros-kinetic-obj-to-pointcloud: 0.2.3-0", "\n", ": 0.2.1-0", "ros-kinetic-planner-cspace: 0.2.3-0", "ros-kinetic-pr2-gripper-sensor: 1.0.10-0", "\n", ": 1.0.10-0", "\n", ": 1.0.10-0", "\n", ": 1.0.10-0", "\n", ": 0.2.1-0", "\n", ": 0.2.1-0", "\n", ": 0.2.1-0", "\n", ": 0.0.4-1", "ros-kinetic-pybind11-catkin: 2.2.3-0", "ros-kinetic-roseus-mongo: 1.7.1-0", "\n", ": 0.0.4-0", "\n", ": 0.0.1-1", "ros-kinetic-safety-limiter: 0.2.3-0", "\n", ": 1.3.2-0", "ros-kinetic-track-odometry: 0.2.3-0", "ros-kinetic-trajectory-tracker: 0.2.3-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-bms-driver: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-cartesian-controller: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-control-mode-adapter: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-control-msgs: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-kinetic-cob-default-robot-behavior: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-elmo-homing: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-frame-tracker: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.3-0 -> 0.6.5-0", "ros-kinetic-cob-hand-bridge: 0.6.3-0 -> 0.6.5-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-helper-tools: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-model-identifier: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-kinetic-cob-moveit-config: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-msgs: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-obstacle-distance: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-omni-drive-controller: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-phidget-em-state: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-phidget-power-state: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-scan-unifier: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-twist-controller: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 3.5.4-0 -> 3.6.2-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.2.0-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "ros-kinetic-eus-assimp: 0.3.5-0 -> 0.4.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 9.23.0-0 -> 9.25.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "ros-kinetic-generic-throttle: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 2.0.2-0 -> 2.0.4-0", "\n", ": 1.11.3-0 -> 1.11.4-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.1.0-0 -> 1.2.0-1", "\n", ": 1.12.10-0 -> 1.12.11-0", "\n", ": 1.12.10-0 -> 1.12.11-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 2018.6.6-0 -> 2018.7.18-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.1.24-0 -> 0.1.25-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "ros-kinetic-novatel-gps-driver: 3.4.0-0 -> 3.5.0-0", "ros-kinetic-novatel-gps-msgs: 3.4.0-0 -> 3.5.0-0", "ros-kinetic-openni2-camera: 0.3.0-0 -> 0.4.0-0", "ros-kinetic-openni2-launch: 0.3.0-0 -> 0.4.0-0", "\n", ": 0.0.25-0 -> 0.0.27-0", "\n", ": 1.6.2-0 -> 1.7.1-1", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-kinetic-rc-genicam-api: 1.3.11-0 -> 1.3.12-0", "\n", ": 0.5.5-0 -> 0.6.0-0", "\n", ": 0.5.5-0 -> 0.6.0-0", "ros-kinetic-robotnik-msgs: 0.2.3-0 -> 0.2.4-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 0.4.11-0 -> 0.4.13-0", "\n", ": 1.1.6-0 -> 1.1.7-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "ros-kinetic-service-tools: 0.6.7-0 -> 0.6.9-0", "ros-kinetic-test-mavros: 0.26.0-0 -> 0.26.1-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "ros-kinetic-tf2-eigen: 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 1.2.2-0 -> 1.3.2-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.2-0 -> 1.1.0-0", "\n", ": 1.0.2-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.2-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "ros-kinetic-tuw-airskin-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-gazebo-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-geometry-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-multi-robot-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-nav-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-object-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-vehicle-msgs: 0.0.7-3 -> 0.0.8-1", "\n", ": 1.1.2-0 -> 1.1.4-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "ros-kinetic-xsens-driver: 2.1.0-0 -> 2.2.0-0", "Alexander Bubeck", "Alexander Tiderko", "Alexander W. Winkler", "Andreas ten Pas", "Andy Zelenak", "Angel Soriano", "Atsushi Watanabe", "AutonomouStuff Software Development Team", "Benjamin Binder", "Benjamin Maidel", "Brenden Gibbons", "Bruno Brito", "Chris Lalancette", "Davide Faconti", "Dirk Thomas", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florian Weisshardt", "Francis Colas", "George Todoran", "Isaac I. Y. Saito", "Jack Kilian", "Jannik Abbenseth", "Jose Luis Blanco Claraco", "Jose-Luis Blanco-Claraco", "Joshua Hampp", "Kei Okada", "Koji Terada", "Markus Bader", "Martin G\u00fcnther", "Matthias Gruhler", "Michael Ferguson", "P. J. Reed", "Pilz GmbH and Co. KG", "Pyo", "ROS Orphaned Package Maintainers", "Raphael Hauk", "Richard Bormann", "Ron Tajima", "Sammy Pfeiffer", "Thomas Le M\u00e9zo", "Tully Foote", "Vincent Rabaud", "Vladimir Ermakov", "Vladimir Ivan", "William Woodall", "Yohei Kakiuchi", "Yuki Furuta", "nick fragale"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2018-07-26/5507"}
,{"title": "New Packages for Kinetic 2019-03-25", "thread_contents": ["We\u2019re happy to announce 24 new packages and 155 updated packages in this sync.", "Thank you to all the contributors and maintainers who have made these packages available to the community!", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 1.2.2-0", "\n", ": 0.6.10-0", "ros-kinetic-cob-base-controller-utils: 0.7.3-0", "ros-kinetic-cob-tricycle-controller: 0.7.3-0", "ros-kinetic-ddynamic-reconfigure: 0.1.7-0", "ros-kinetic-dynamic-robot-state-publisher: 1.1.1-0", "ros-kinetic-indoor-localization: 0.1.0-1", "ros-kinetic-ipa-3d-fov-visualization: 0.6.13-0", "ros-kinetic-laser-scan-densifier: 0.6.13-0", "\n", ": 1.0.3-0", "ros-kinetic-ouster-driver: 0.1.6-0", "\n", ": 1.0.2-0", "\n", ": 1.0.2-0", "\n", ": 1.0.2-0", "ros-kinetic-pr2-navigation-apps: 1.0.2-0", "ros-kinetic-rdl-msgs: 1.1.0-0", "ros-kinetic-rdl-ros-tools: 1.1.0-0", "ros-kinetic-static-transform-mux: 1.1.0-0", "ros-kinetic-tf-remapper-cpp: 1.1.1-0", "ros-kinetic-usb-cam-controllers: 0.0.3-0", "ros-kinetic-usb-cam-hardware: 0.0.3-0", "ros-kinetic-usb-cam-hardware-interface: 0.0.3-0", "ros-kinetic-uuv-descriptions: 0.6.10-0", "ros-kinetic-uuv-simulator: 0.6.10-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "ros-kinetic-behaviortree-cpp-v3: 3.0.1-0 -> 3.0.6-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "ros-kinetic-care-o-bot: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-care-o-bot-desktop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-care-o-bot-robot: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-care-o-bot-simulation: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cloudwatch-logs-common: 1.0.0-0 -> 1.0.1-1", "ros-kinetic-cloudwatch-metrics-common: 1.0.0-0 -> 1.0.1-1", "ros-kinetic-cmake-modules: 0.4.1-0 -> 0.4.2-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-bms-driver: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-cartesian-controller: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-control-mode-adapter: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-control-msgs: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-default-robot-behavior: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-elmo-homing: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-frame-tracker: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-helper-tools: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-model-identifier: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-moveit-config: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-msgs: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-obstacle-distance: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-omni-drive-controller: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-phidget-em-state: 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-phidget-power-state: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-scan-unifier: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-twist-controller: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 1.1.3-0 -> 1.1.4-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.1-1 -> 1.2.0-0", "ros-kinetic-generic-throttle: 0.6.9-0 -> 0.6.10-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.12-1", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "ros-kinetic-linux-networking: 1.0.12-0 -> 1.0.14-0", "\n", ": 2019.2.2-0 -> 2019.3.3-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "ros-kinetic-movie-publisher: 1.2.1-0 -> 1.2.2-1", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 2.1.5-0 -> 2.1.7-0", "\n", ": 1.0.12-0 -> 1.0.12-1", "\n", ": 1.0.12-0 -> 1.0.12-1", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-rdl: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-benchmark: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-cmake: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-dynamics: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-urdfreader: 1.0.0-0 -> 1.1.0-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 1.12.16-0 -> 1.12.17-0", "ros-kinetic-service-tools: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.0.14-0 -> 0.0.15-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "ros-kinetic-test-mavros: 0.29.0-0 -> 0.29.2-0", "\n", ": 1.0.0-1 -> 1.0.1-0", "ros-kinetic-uuv-assistants: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-auv-control-allocator: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-control-cascaded-pid: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-control-msgs: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-control-utils: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-ros-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-ros-plugins-msgs: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-worlds: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-sensor-ros-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-sensor-ros-plugins-msgs: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-teleop: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-thruster-manager: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-trajectory-control: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-world-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-world-ros-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-world-ros-plugins-msgs: 0.6.9-0 -> 0.6.10-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "AWS RoboMaker", "Alexander Bubeck", "Alexander Carballo", "Andres Palomino", "Benjamin Maidel", "D. Hood", "Davide Faconti", "Devon Ash", "Elcin Erdogan", "Felipe Garcia Lopez", "Felix Messmer", "Florenz Graf", "Florian Weisshardt", "Hilario Tome", "Jannik Abbenseth", "Joshua Hampp", "Kevin Hallenbeck", "Luiz Ricardo Douat", "Marko Bjelonic", "Martin G\u00fcnther", "Martin Pecka", "Mathias L\u00fcdtke", "Matthias Gruhler", "Micho Radovnikovich", "Musa Morena Marcusso Manhaes", "Richard Bormann", "Ronald Ensing", "Russell Toris", "Vladimir Ermakov", "William Woodall", "jordan", "yoshito"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2019-03-25/8455"}
,{"title": "Announcing stable release of Slam Toolbox", "thread_contents": ["Hi!", "Over the last 2 years or so a pet project of mine is finally ready for prime time and see get some use. Slam Toolbox is a set of tools and capabilities for 2D planar SLAM. This project contains the ability to do most everything any other available SLAM library, both free and paid, and more. This includes:", "Slam Toolbox for 2D mapping and localization in potentially massive maps - SteveMacenski/slam_toolbox", "For running on live production robots, I recommend using the snap: slam-toolbox, it has optimizations in it that make it about 10x faster. You need the deb/source install for the other developer level tools that don\u2019t need to be on the robot (rviz plugins, etc).", "This package has been benchmarked mapping building at 5x+ realtime up to about 30,000 sqft and 3x realtime up to about 60,000 sqft. with the largest area (I\u2019m aware of) used was a 145,000 sq.ft. building in sychronous mode (e.i. processing all scans, regardless of lag), and ", " larger spaces in asynchronous mode.", "I\u2019d love to see what people think. Features of this have been running live on dozens of robots worldwide. I\u2019ll be the first to admit it needs some refactoring, but the capabilities are there. Take a look, star, and I look forward to the issue tickets and feature requests!", "Steve Macenski, Open Source Robotics Engineering Lead @ Samsung Research America", "Awesome! Can you help us understand either qualitatively or quantitatively:", "It looks terrific, and I am excited to try it on a Ubiquity Robotics Magni when our 3-D TOF sensor is ready.", "One last thing you didn\u2019t include a link to the repo. I presume this is the right one: ", "Hi Dave,", "First, thanks for mentioning to include a link! There\u2019s always something missing when you make an announcement\u2026", "It assumes the \u201cvanilla\u201d mobile base setup of a 2D laser scanner. The testing I have done are with SICK TiM, Hokuyo lidars, and RPlidars, but there\u2019s no reason you couldn\u2019t use a lower cost version and a coarser resolution map. But in general a 2D laser scanner broadcasting a sensor_msgs/LaserScan message.", "On the computational side, I haven\u2019t tried it with something too under powered, I think my weakest robot is a 6th gen i5 and I haven\u2019t had a problem. Its a good question if this would work on something like a Raspberry pi, and the answer would be: I don\u2019t know but I\u2019m open to finding out. I haven\u2019t done anything insanely above the ordinary of 2D laser based SLAM, so you should be fine to use this if you are able to use other 2D slam packages on your platform like Karto, Gmapping, Cartographer, etc. You definitely won\u2019t get 145,000 sq.ft. realtime on that type of platform however. That metric was using a 7th gen i7 mobile NUC processor.", "Nice tool, we\u2019ve been looking to work with something like this.  Anyone interested in using an ", " sensor to make it work with the toolbox?  It provides range information to one or more detected objects in it\u2019s field of view.  Detection range is 0.1 to 20m.  Contact me if interested.", "This is awesome work ", ", thanks for sharing!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Ordinary point-and-shoot 2D SLAM mobile robotics folks expect (start, map, save pgm file)", "life-long mapping (start, serialize, wait any time, restart anywhere, continue refining)", "an optimization-based localization mode (start, serialize, restart anywhere in Localization mode, optimization based localizer)", "synchronous and asynchronous modes", "kinematic map merging (with an elastic graph manipulation merging technique in the works)", "plugin-based optimization solvers with a new optimized Google Ceres based plugin", "RVIZ plugin for interating with the tools", "graph manipulation tools in RVIZ to manipulate nodes and connections during mapping", "Map serialization and lossless data storage", "\u2026 more but those are the highlights", "The types of sensor it expects and the types of sensor it accepts", "The sensor quality it expects and accepts", "The typical computer power needed to do things such as 145,000 square feet real time mapping."], "url": "https://discourse.ros.org/t/announcing-stable-release-of-slam-toolbox/9872"}
,{"title": "New Packages for Kinetic 2019-06-15", "thread_contents": ["We\u2019re happy to announce 6 new packages and  177 update packages for Kinetic. There is one known regression which removes 3 packages in this sync. The maintainer is looking at the regression and we expect it to be restored soon.", "Thank you to everyone who has helped make these packages available to the community, including the maintainers and contributors!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "The regressions in abseil_cpp has been resolved thanks to the work of ", " and the ros_type_introspection and plotjuggler have both been restored building on top if it. There was also a new package topic_switch added in the intervening day which is included too.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 2.1.12-1", "ros-kinetic-flir-camera-driver: 0.1.3-0", "\n", ": 2.1.1-1", "\n", ": 2.6.2-1", "\n", ": 2.6.2-1", "ros-kinetic-rostwitter: 2.1.12-1", "ros-kinetic-abseil-cpp: 0.2.3-0 -> 0.4.1-1", "ros-kinetic-actionlib-enhanced: 0.0.4-1 -> 0.0.6-1", "\n", ": 0.1.1-0 -> 0.1.2-1", "ros-kinetic-assimp-devel: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 0.7.14-0 -> 0.7.18-1", "ros-kinetic-cob-base-controller-utils: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-bms-driver: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-cartesian-controller: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-control-mode-adapter: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-control-msgs: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-elmo-homing: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-frame-tracker: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-helper-tools: 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-model-identifier: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "ros-kinetic-cob-obstacle-distance: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-omni-drive-controller: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-phidget-em-state: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-phidget-power-state: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-scan-unifier: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-tricycle-controller: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-twist-controller: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "ros-kinetic-eigen-typekit: 2.9.2-1 -> 2.9.3-1", "\n", ": 9.26.0-0 -> 9.26.0-1", "ros-kinetic-fcl-catkin: 0.5.96-0 -> 0.5.98-1", "ros-kinetic-ff: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "ros-kinetic-generic-throttle: 0.6.11-0 -> 0.6.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.2.1-0 -> 1.2.1-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "ros-kinetic-julius-ros: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.9.2-1 -> 2.9.3-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "ros-kinetic-laser-scan-densifier: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-libcmt: 2.1.11-0 -> 2.1.12-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "ros-kinetic-libsiftfast: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 2019.5.20-1 -> 2019.6.7-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "ros-kinetic-mini-maxwell: 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "ros-kinetic-nlopt: 2.1.11-0 -> 2.1.12-1", "ros-kinetic-novatel-gps-driver: 3.7.0-0 -> 3.8.0-1", "ros-kinetic-novatel-gps-msgs: 3.7.0-0 -> 3.8.0-1", "\n", ": 2.9.0-1 -> 2.9.1-3", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 0.7.0-1 -> 0.7.2-1", "ros-kinetic-pgm-learner: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.0.0-0 -> 2.1.1-1", "\n", ": 2.0.0-0 -> 2.1.1-1", "\n", ": 2.0.0-0 -> 2.1.1-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "ros-kinetic-quaternion-operation: 0.0.1-2 -> 0.0.3-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "ros-kinetic-respeaker-ros: 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 2.4.4-0 -> 2.4.5-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 2.9.2-1 -> 2.9.3-1", "ros-kinetic-service-tools: 0.6.11-0 -> 0.6.12-1", "\n", ": 0.0.16-0 -> 1.3.21-0", "ros-kinetic-slic: 2.1.11-0 -> 2.1.12-1", "ros-kinetic-test-mavros: 0.30.0-1 -> 0.31.0-1", "ros-kinetic-uuv-assistants: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-auv-control-allocator: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-control-cascaded-pid: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-control-msgs: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-control-utils: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-descriptions: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-ros-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-ros-plugins-msgs: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-worlds: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-sensor-ros-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-sensor-ros-plugins-msgs: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-simulator: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-teleop: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-thruster-manager: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-trajectory-control: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-world-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-world-ros-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-world-ros-plugins-msgs: 0.6.10-0 -> 0.6.12-0", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 0.2.0-0 -> 0.2.1-1", "ros-kinetic-topic-switch", "Alessandro Tondo", "Alexander Bubeck", "Benjamin Maidel", "Dirk Thomas", "Fabien Spindler", "Fabrice Poirier", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Florian Weisshardt", "Hitoshi Kamada", "John Hsu", "Jose Luis Rivero", "Joshua Hampp", "Kei Okada", "Luiz Ricardo Douat", "Masaya Kataoka", "Matthias Gruhler", "Michael Hosmar", "Michael Lehning", "Mike Lautman", "Monika Florek-Jasinska", "Musa Morena Marcusso Manhaes", "Noda Shintaro", "Orocos Developers", "P. J. Reed", "Philipp Schillinger", "Richard Bormann", "Robert Haschke", "Russell Toris", "Ryohei Ueda", "Takuya Nakaoka", "Vladimir Ermakov", "Wolfgang Merkt", "Yohei Kakiuchi", "Yuki Furuta", "Yuto Inagaki", "dfaconti", "k-okada", "matsui_hiro", "nakamichi_d", "\n", ": 2.1.10-0", "\n", ": 1.3.3-1", "ros-kinetic-topic-switch: 0.0.1-2", "ros-kinetic-abseil-cpp: 0.4.1-1 -> 0.4.2-3", "Davide Faconti", "Masaya Kataoka"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2019-06-15/9545"}
,{"title": "Hardware requirements discussion", "thread_contents": ["I draft a hardware requirement, let\u2019s discuss in this thread or in hardware working group meeting.", "based on scenario and sensor performance, should involve sensor vendor to discuss.", "Silicon type:", "Board Architecture:", "Reference function design of boards:", "For board hardware interface:", "Type:", "Configuration:", "For MPU side:", "Operation system", "Performance capability", "Autoware should define the dependency of 3rd party software.", "Low level hardware acceleration capability (Optional)", "low level acceleration capabilities are optional, depends on performance benchmark requirements.", "For RT-Core/MCU side:", "Interconnection with MPU and other board", "Operation system", "Feature could be realized inside MCU:", "Autoware do not define which kind of features could be realized by MCU/RT-Core yet.", "Hardware redundancy is not in scope, there are different realizations and we will not choose one. but we believe the interface requirement and RT-Core/MCU requirement will provide the capability to setup user\u2019s own redundancy solution.", "using AutoCore\u2019s PCU as an example to demonstrate hardware design. it\u2019s designed for P1 or P3 usage.", "below is default ethernet configuration of this board.", "Hi,", "Thank you for this draft. Is it for Autoware hardware requirements?", "Do you have any suggestion for hardware supplier for a self driving car? We are planning to buy a car and all the sensors for research at Aalto University. We will use Autoware in this project. I found AutonomousStuff and DataSpeed to provide all the sensors and do the installation and support. Do you have any other suggestion or do you know any other supplier?", "Thanks", " have you checked ", " for the computing? as ", " mentions above", "Yes. I look for a company that can provide all the sensors like ", ".", "Hi, if I may, we at StreetDrone supply fully integrated autonomous vehicles, including sensors, compute and Autoware.AI stack. We can supply full support too and support a range of vehicle form factors.", "\n", "Deploying open, safe, autonomous vehicles in the worlds smartest cities.", "\n", "\nPlease let me know if you\u2019d like to discuss with us- send me a message and I\u2019ll let you know my email.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Camera", "USB/GigE based Camera are not for production purpose, but its easy to use.", "Most smart camera is ADAS module.", "at Linux platform, V4L2 is generic interface to get image.", "Image format maybe different by different sensors, need framework to support transform.", "the best solution/framework for transform image format could be GStreamer or OpenCV.", "transport image/video stream to different computer board over Eth, Gstreamer with hardware encode/decode should be much better solution than using ROS/ROS2 to publish image topic. we need avoid transfer raw image by network.", "Lidar", "Most Lidars are ethernet based with UDP traffic.", "in order to improve network performance, Lidar should connect processing board directly or in a dedicated VLAN/LAN by ethernet switch.", "UDP multicast provide capability of redundancy feature of computing board, but also meet safety requirement of network switch.", "GPS/IMU", "Most GPS provide UART interface. (can use usb-uart for development env.)", "Some IMU use CAN bus (2.0 or FD).", "ROS application expect /imu /fix  and original data of NMEA.", "In many use cases, GPS/IMU are connected to MCU, by using IRQ to process high frequency IMU data.", "Radar/Sonar", "most radar/sonar are connected to MCU.", "Summary the interfaces", "Most demo cars are using UART/USB/1000base-T", "LVDS/CAN should must be supported if the ECU/computer board are targeting real product.", "100Base-T1(TSN) maybe the best solution for highspeed data exchange who is targeting real product. (Tesla use 100base-T1 to interconnect media part and AP)", "MPU with RT-Core", "MPU without RT-Core", "Standalone MCU", "should follow 96auto spec.", "at least one MCU/RT-Core onboard, prefer with lock-step features.", "considering the capability of current SoC in automotive world, defined several type of boards:\n", "P1 which focus on sensor connection and sensor fusion. It could be a kind of gateway or domain controller which has good network capabilities.", "P2 which focus on camera data processing. It could be a kind of domain controller or smart camera which support ADAS features.", "P3/P4 are used for main software stack, including later fusion, planning, decision maker.", "\n", "All these part should be connected by ethernet, 100Base-T1 for automotive or 1000base-T are OK.", "Detail network design for deployment is not in the scope.", "You can combine different function in one board or single chip. it depends on the capability of your chip and board.", "\n", "\n", "UART/USB/1000base-T must be supported for current POC setup.", "100Base-T1 with TSN capability and CAN 2.0/FD must be supported.", "LVDS or onboard CSI camera depends on the capability of image processing including ISP/CV/Encode/DNN.", "Need support PPS input from GNSS.", "Should has solution to synchronize sensors.", "\n", "\n", "\n", "If connectint GPS/IMU to MPU, it need 1-2 UART. And need extra UART console for debugging.", "At least 2 ethernet interface to isolate different LAN.", "\n", "\n", "\n", "Linux kernel with PREEMPT-RT.", "Suggest support Debian/Ubuntu filesystem, Yocto as an option.", "It\u2019s optional to support QNX and Vxworks.", "\n", "\n", "\n", "Should support the performance requirement of PCL if support Lidar data processing.", "Should support the performance requirement of OpenCV if supporting Camera/Image data processing.", "Should support the performance requirement of DNN if supporting Deep learning. ", "\n", "\n", "\n", "\n", "\n", "OpenCL, which can power PCL2.0 and OpenCV3.x", "OpenVX", "Gstreamer encode/decode, support MJPG and H.264/265", "DNN accelerator", "CV accelerator (with OpenCV or OpenVX wrapper)", "Neon for ARM", "OpenMP", "\n", "\n", "\n", "\n", "interconnect MPU by onboard ethernet and/or I2C/UART.", "Onchip RT-Core will use RPC tech provided by Silicon vendor.", "If don\u2019t have ethernet interface/IP stack, should has agent on MPU to export capability on middleware.", "must have capability to monitor MPU\u2019s status.", "\n", "\n", "\n", "FreeRTOS or other RTOS", "Prefer support Micro-ROS", "\n", "\n", "\n", "\n", "global time sync\n", "Use GNSS time info as NTP source.", "Use PPS from GNSS as PTP source.", "Support RTC.", "\n", "time stamp of sensor data provided by driver\n", "time/space synchronize of sensors will be developed by user, provide PPS dispatch feature for sensor usage.", "from device driver side, need to clarify the correct time stamp.", "most sensor data are generated in a time slice, and depends on sensor venders\u2019 realization.", "\n", "calculate latency between senser data is generated to algorithm output.\n", "well designed time synchronize infrastructure and device drivers will help prediction algorithm.", "\n", "it use NXP LS1046A as MPU, provide much connectivity and ARM A72 CPU for generic algorithm.", "it use TMS570 as MCU with opensource FreeRTOS, provide capability of real-time/safety features, and CAN.", "it has onboard 5port switch.", "it has extend capabilities for acceleration by M.2 and miniPCIE."], "url": "https://discourse.ros.org/t/hardware-requirements-discussion/12175"}
,{"title": "Connect two usb2.0 cameras with autoware", "thread_contents": [": ROS Kinetic", "\nubuntu 16.04", "Hi! I encountered a problem when I tried to connect two usb 2.0 cameras with autoware. It seems that autoware always just can recognize /dev/video0, which means autoware just can recognize one camera all the time. Is there someone who knows how to make autoware recognize another camera like /dev/video2 in my case? Autoware still can recognize one camera even after I  deleted the contents of launch file in ros camera driver or the contents of uvc_camera.sh under /Autoware/ros/src/sensing/drivers/camera/usb, which makes me be stuck at the point where I do not know which file I should change so that two cameras can be used at the same time. I would appreciate it a lot if someone can help me figure this out. Please let me know if you need any details I left out here about this problem.", "My ubuntu 16.04 system will show /dev/video0 and /dev/video1 when I just connect one camera to my computer. I guess that it is probably due to the microphone built in my usb camera.", "USB Genric checkbox in Runtime Manager launches uvc_camera.launch under /Autoware/ros/src/util/packages/runtime_manager/scripts.", " Like you mentioned, the current launch script in runtime manager, and the one in the drivers section, are not designed to work with more than one camera.", "\nHowever, you can instead use the launch file provided by the package, and select which device to use", "\n", "\n", "\nFrom a terminal you might execute:", "\n", "\nand select the device you are interested to use.", "\nFinally, you can find more details in the package documentation: ", "\nHope this helps.", "Thank you for your help. It works now after I changed a little bit of the launch file under util directory.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["<node pkg=\"uvc_camera\" type=\"uvc_camera_node\" name=\"uvc_camera\" output=\"screen\">", "  <param name=\"width\" type=\"int\" value=\"320\" />", "  <param name=\"height\" type=\"int\" value=\"240\" />", "  <param name=\"fps\" type=\"int\" value=\"30\" />", "  <param name=\"frame\" type=\"string\" value=\"wide_stereo\" />", "\n", "  <param name=\"auto_focus\" type=\"bool\" value=\"False\" />", "  <param name=\"focus_absolute\" type=\"int\" value=\"0\" />", "  <!-- other supported params: auto_exposure, exposure_absolute, brightness, power_line_frequency -->", "\n", "  <param name=\"device\" type=\"string\" value=\"/dev/video0\" />", "  <param name=\"camera_info_url\" type=\"string\" value=\"file://$(find uvc_camera)/example.yaml\" />", "</node>", "</launch>"], "url": "https://discourse.ros.org/t/connect-two-usb2-0-cameras-with-autoware/8013"}
,{"title": "Technical Steering Committee (TSC) Meeting #10 Minutes", "thread_contents": [" Geoffrey Biggs (Tier IV)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Lee Baldwin (AutonomousStuff)", "Geoffrey Biggs (Tier IV)", "Esteve Fernandez (Apex.AI)", "Kenji Funaoka (Tier IV)", "Brian Holt (Parkopedia)", "Seonman Kim (LGE)", "Dejan Pangercic (Apex.AI)", "Koga", "Nikos Michalakis (TRI-AD)", "Otake (Macnica)", "Stephane Strahm (Kalray)", "Akihiko Tsukuda (eSOL)", "Joshua Whitley (AutonomousStuff)", "Dmitry Zelnkovskiy (LGE)", "Opening remarks and new member introductions", "\n", " Confirmation of previous minutes", "\n", " Action items from previous meeting", "\n", " New logos for the Autoware Foundation and its projects", "\n", " Hiring a software architect", "\n", " Review TSC members list on the AWF website", "\n", " Autoware", "\n", " Map formats", "\n", " Vehicle interfaces", "\n", " ECU/Platform", "\n", " Simulation", "\n", " Working groups participation levels", "Release the safety case example\n", "\n", "Create working group wiki pages\n", "\n", ", ", ", ", ", ", "\n", "\n", "Post the Autoware software architect recruitment notice to any useful forums\n", "AWF board", "\n", "Provide HD Map for AutonomousStuff carpark\n", "\n", ", ", "\n", "\n", "No board members present.", "Minutes approved.", "Release the safety case example\n", "\n", " The partner has added a legal notice which makes them happy to release it, but they used a non-standard language which means anyone who wants to use it may need to get it checked out by a lawyer. Brian is proposing to add CC-BY-SA 4.0 as the license and include their disclaimer of warranty and limitation of liabilities.", "The AWF should make a fuss about releasing them because they are a big potential impact.", "Will discuss offline where to host the examples.", "\n", "Post the Autoware software architect recruitment notice to any useful forums\n", "AWF board", "\n", " Unknown.", "\n", "Create working group wiki pages\n", "\n", ", ", ", ", ", ", ", ", ", ", ",  ", ", ", ",  ", ", ", "\n", "\n", " Only the Autoware WG has a wiki page so far. Other WGs must create their pages urgently.", "\n", "The new logos created for the Autoware Foundation and its projects are now available for use.\n", "AWF logo: ", "\n", "Autoware.AI logo: ", "\n", "Autoware.Auto logo: ", "\n", "\n", " logo: ", "\n", "\n", "The Autoware Foundation is hiring a system architect to lead the design of Autoware.", "Apex.AI has ", ".", "The list of TSC members on the foundation website is out of date.", "If you are not listed, please send your photo and name so we can get you listed.", "Only the primary TSC representative should be listed", "Reminder: The dates for the next release of Autoware.AI have been set by the maintainer team.\n", "\n", "Changes have been made to the governance of the ", " and Autoware.Auto projects\n", "The new approach is based on the Apache Foundation\u2019s approach and aims at having a more open and accessible project governance structure.", "See here for details: ", "\n", "\n", "Autoware.AI updates:\n", "A significant set of new features are being proposed by engineers from Tier IV, including the ability to drive in reverse.", "Some basic safety features are being added to handle the event of NDT matching losing its place. The new features are use of an EKF to detect when the NDT result makes a large jump, and a paramterised limit on the maximum steering angle.", "The submission deadline for MRs is 12 days away.", "\n", "Autoware.Auto updates:\n", "Progress has been slow but is steadily moving forward.\n", "Objection detection is done", "Lidar-based localization super spec\u2019d out ", "\n", "Maps working group very active and we have a map of AS parking lot", "VI interface in Josh\u2019s hands - he will get that done for Lexus vehicle at least", "Path planning and controls - TierIV will get this done", "Simulation - AS parking lot is in LGSVL", "Platform group is also very active and if not there is always ", "\n", "\n", "IMU and GPS drivers will be landing in a few days. These are specific to the xsense NTIG-601 sensor.", "Tier IV has done work to get the ", " in place and allow parts of Autoware.AI to be used from Autoware.Auto. Currently this work is focusing on enabling the localisation stack from Autoware.AI to be used.", "Epics for the first milestone have not seen much tracking, except for the localisation epic being handled by Christopher Ho from Apex.AI.", "Apex.AI proposes that we hold an all-hands-on-deck integration hackathon in the Bay Area in early 2020 (some time by the end of March).\n", "There is interest in attending: Parkopedia will definitely attend and bring two or three people. AS will do their best to attend. Kalray thinks they can send someone. Nikos may be able to send someone.", "The idea is to host it at the Apex.AI and use the AutonomousStuff carpark which is not that far away.", "We still need a vector map of the AS carpark.", "Parkopedia ", " and intends to port this all to Autoware.Auto in their upcoming hackathon in the UK.", "Parkopedia wants the goal of the integration hackathon to be to flesh out all the remaining little bits of the AVP use case (using artifical landmarks, the application-level, etc.).", "\n", "\n", "The Maps Working Group has met 3 times since the last TSC meeting.", "Meetings are organised on Discourse and a regular time of 2pm GMT every second Thursday has been chosen by regular attendees.\n", "\n", "A general architecture for from the perspective of maps has been under intense discussion and represents the 4 ways in which maps are expected to be used:", "\n", "\n", "We expect to make more progress on rationalising the use cases, turning them into requirements and then making a start on the design at the upcoming Autoware Hackathon next week.", "In our last meeting, I was asked to create a survey to send to members of the group about their needs for the Autoware Vehicle Interface layers. I created and distributed the survey but received no responses in the 4 weeks between meetings. In addition, the last meeting was attended by only ", " and myself. After a brief discussion, ", " and I decided that I would distribute the survey to the entire Autoware community. We will discuss how to raise participation in this (and I believe several other) working groups during the TSC call.", "Survey topic: ", "\n", "WG has not been very active in terms of communication and has not managed to hold a meeting since the last TSC.", "Implementation of the Autocore/Kalray reference board BSP is on-going.", "Apex.AI: How is provisioning/deployment done? We want to avoid duplicating how this is done with ROS 2, commercial companies, etc.", "\n", " where they compared Metamoto\u2019s features and LGSVL\u2019s features.", "They hope to have content creation discussions in the next meeting.", "Several working groups do not have much participation and activity.\n", "Notably, Autoware WG, Reference Platform WG, Vehicle Interfaces WG, Simulation WG", "\n", "It is probable that some topics have been split off too early, before there is a critical mass of participants and contributors to power the discussion and work.", "Proposal to improve this is to drop some working groups and roll their topics into one of the two core working groups (Autoware WG and Reference Platform WG).\n", "Vehicle interfaces WG is most likely candidate for this - move its topics to the Autoware WG.", "Simulators working group probably should be, too - move its topics to the Autoware WG.", "\n", "Reference platform WG will not be removed.", "When we start having too much discussion to fit in the Autoware WG meeting, we can split off a WG appropriate to the overflow topics.", "AS agrees with cutting down the number of workin groups, particularly as it will overwhelm people when they see calls for so many working groups.", "Kalray also agrees.", "Working group activities centre: ", "\n", "Shared calendar for working groups: ", "\n", "Please subscribe to the Autoware and Autoware-TSC categories on Discourse, because all discussion for the AWF\u2019s technical activities happens there now.", "Milestones: ", "\n", "All TSC representatives need to take responsibility for driving participation in AWF development activities.", "All FTEs need to be proactive about doing work, rather than waiting for specific instructions. Pick a task from the milestones list and start doing it, and pick a working group and start participating."], "url": "https://discourse.ros.org/t/technical-steering-committee-tsc-meeting-10-minutes/10818"}
,{"title": "Downloading Dependencies", "thread_contents": ["I looked at ", ", but I didn\u2019t see anything about plans for a new rosdep for ament. Based on the ROS 2 binary installations, I figure that the remote dependencies are managed by brew for Mac, Chocolatey for Windows, and apt for Ubuntu. Are there plans to have a generic tool to download these dependencies based on the package.xml like rosdep? Also, are there plans to generate the necessary files for these dependency managers from a package?", "Unfortunately, the previous version of rosdep didn\u2019t handle downloading repositories directly. It seems useful to enable a complete stack source code build like Mike Purvis\u2019 ROSCon 2016 talk: ", ". Building from source enables cross compiling, link time optimization, cross package bug fixes, native architecture optimization, and dependency artifacts on the fly without a binary server.", "Ultimately, it would be nice to have one command to convert a repository/package(s) into artifact(s). Another tool to help get there would be to convert a package or packages into workspace.", "To answer this properly, I need to layout the workflow we usually use and the different tools we use to accomplish this.", "Here\u2019s the steps first (without details):", "This basic formula is how I start working on everything (except maybe redistributing binaries).", "\nYou can see the pattern in our from source instructions on the wiki:", "And now that list again with more notes in between:", "Ok, so that\u2019s the breadth of what we do with some details.", "\nNow I\u2019ll try to answer you directly by quoting your questions:", "I didn\u2019t see anything about plans for a new rosdep for ament.", "Though ", " does have some ROS specific capabilities, at its core is just takes dependency names, like ", " or ", ", and decides what needs to be done on a system to \u201csatisfy\u201d those as dependencies.", "\nThis is a pretty generic \u201cabstraction\u201d over the OS\u2019s package manager, and is not really ROS specific.", "\nAnd so it should work perfectly fine with ROS 2 as-is.", "That being said we have thought about improvements to ", " and dreamed of removing the the ROS specific stuff from it so that it\u2019s more generic and potentially useful to many other projects outside of ROS.", "\nSpecifically ", " worked on something called ", " while interning at OSRF, and it was meant to be the spiritual successor to ", ":", "xylem - A tool for resolving dependencies in a platform agnostic way.", "Despite his really awesome work, I\u2019ve not had the resources to capitalize on it and push for it to replace ", ".", "\nI\u2019ve always hoped we could make use of it in ROS 2, but it\u2019s really an orthogonal issue as ROS 2 could easily use ", " as well, it would just be nice to use ", "'s new hotness ", ".", "\n", " put together some extensive design documentation and the tool is really close to being ready for use.", "\nIf you\u2019re interested in tools like ", " I highly encourage anyone to have a look at his work:", "So, to answer you, we don\u2019t need a new ", " for ", ", but we\u2019d like to have one ", ".", "Based on the ROS 2 binary installations, I figure that the remote dependencies are managed by brew for Mac, Chocolatey for Windows, and apt for Ubuntu. Are there plans to have a generic tool to download these dependencies based on the package.xml like rosdep?", "Yes, we\u2019ll likely use ", ".", "\nWe are currently working on packaging some dependencies like ", " and ", " for chocolatey on Windows, and still sort of feeling out what the best thing to do there is, see:", "These aren\u2019t official, and will likely move, but for those interested in seeing what\u2019s been tried\u2026", "Unfortunately, the previous version of rosdep didn\u2019t handle downloading repositories directly.", "Actually ", " does have this ability, it\u2019s called \u201crosdep source installer\u201d (I think).", "\nThey\u2019re not used much anymore, because they\u2019re terribly difficult to maintain.", "\nThey looked something like this:", "Since then we\u2019ve deliberately chosen to avoid stuff like this and try to only use OS package managers to resolve dependencies.", "\nThe other point is that you can emulate this by creating a cmake package (or catkin/ament package) which does this \u201cfrom source build\u201d for external dependencies.", "\nWe do this some for ROS 2, for example with ", " (cmake based) because we needed it on Windows and didn\u2019t know how to package it yet, see:", "poco_vendor - CMake shim over the poco library: https://github.com/pocoproject/poco", "Basically, if it cannot find the right version of poco, then it downloads and builds it in place.", " also touched on this topic:", " (search for ", ")", "As for cross-compiling, there are entire toolchains for achieving this, like OpenEmbedded, Yocto, and others.", "\nI am not confident that is a role ", " needs to play, but we can certainly make sure to help support those patterns if we can (i.e. not get in the way).", "Ultimately, it would be nice to have one command to convert a repository/package(s) into artifact(s).", " can provide you with Debian packaging files on a per package basis, and the Debian packaging toolchain can provide you with binary artifacts.", "\nThe same is true for any packaging scheme (Fedora, Homebrew, Chocolatey), in principle.", "\nROS Answers on how to do this locally, see:", "Another tool to help get there would be to convert a package or packages into workspace.", "Not sure what you mean by that.", "\nWe have tools that will build lots of local packages into a single destination folder.", "\nWe have people using this ability to wrap up the result and distribute that, see ", "\u2019s presentation.", "\nI think these tools should continue to work for ROS 2 with little to no changes.", "Sorry for the long post, but hopefully it answered your questions.", "Thanks for the detailed article.", "It\u2019s smart to use a package manager instead of building yet another one. I looked at Conan(", ") and vcpckg(", "), but I guess they are C++ specific.", "For clarification, of that last point. I was thinking of a tool/script that takes a repository/package and builds a workspace around it. Basically, create a workspace folder with a src subfolder and move the package into the src folder. That way ament can run shortly after cloning a repository/package in an arbitrary location. It takes some time to understand the workspace structure and to know how to convert arbitrary packages into runnable nodes.", "I was thinking of a tool/script that takes a repository/package and builds a workspace around it.", "There is no tool that automates so much of the process, but there is an issue on ", " to add an option which would help you do it in a few commands:", "Basically it would be like \u201cI have some package(s) in this folder, and some things installed from binaries, please get me the list of things that are missing for me to build these package(s) I already have.\u201d", "It would be a cool contribution to extend the tool in that direction ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Generate a list of things to download, i.e. \u201cwhat do I need to download to build ", "?\u201d", "Download everything based on that list", "Install the remaining dependencies, e.g. non-ROS things like ", " and ", "\n", "Build everything, in the correct order", "Redistribute binaries", "Generate a list of things to download, i.e. \u201cwhat do I need to download to build ", "?\u201d\n", "We use ", " for this and it draws on data in the \u201crosdistro\u201d", "it generates a ", " file based on your query", "it only gets the location of things that are \u201cROS packages\u201d, i.e. things released in the rosdistro", "it does not figure out how to download things like qt or boost", "it is used in ROS 1 and will likely be used more-or-less unchanged in ROS 2", "can also answer \u201cwhat do I need to download to use ", ", minus ", " and its dependencies?\u201d\n", "you might want to do this so you can install ", " and down from binaries", "\n", "\n", "Download everything based on that list\n", "We use ", " for this (some power users use ", ", we use it in ROS 2)", "Given a ", " (or a similar list of things to go get), it can fetch all the source code", "We\u2019ll likely use both in ROS 1 and ROS 2", "\n", "Install the remaining dependencies, e.g. non-ROS things like ", " and ", "\n", "It can also install binaries for \u201cROS things\u201d if you don\u2019t intend to build them locally and there are binaries for them on the target OS.", "In ROS 1 we use ", " for this, it would install remaining dependencies", "In ROS 2 we just have instructions in the ros2 github wiki on what to install, we don\u2019t use ", " yet", "\n", "Build everything, in the correct order\n", "used to help us build software locally\n", "either to do development or", "to build for a new OS/architecture which lacks binaries", "\n", "software might be anything supported by the build tool, like:\n", "catkin or", "ament or", "cmake or", "python with ", " files", "etc\u2026", "\n", "we use mostly catkin in ROS 1 and mostly ament in ROS 2, but that doesn\u2019t really affect this part of the process", "all result in an \u201cinstall-like\u201d folder which can be used locally", "\n", "Redistribute binaries\n", "One option is to use ", " and release on ", " through rosdistro\n", "\n", " converts the contents of ", " to debian (or others like fedora) packaging files", "this happens on a per package basis (no bundling)", "\n", " uses these files to build binaries and upload them to ", "\n", "\n", "Another option is something like what ", " presented at ROSCon, which builds lots of things at once and bundles the result", "Both options should work for ROS 2, we\u2019re currently looking at any changes that would need to be made for ", " to work on ROS 2 packages (shouldn\u2019t be much)", "\n"], "url": "https://discourse.ros.org/t/downloading-dependencies/1489"}
,{"title": "IPC in ros2", "thread_contents": ["Does ros2 use inter-process communication when we use it in  linux OS ? What about in case of bare metal micro-controllers where there is no OS?", "In the future, please set the category to \u201cNext Generation ROS\u201d when discussing ROS 2.", "Does ros2 use inter-process communication when we use it in  linux OS ?", "First, I\u2019ll assume you mean \u201cdoes ROS 2 use any operating system IPC mechanisms between nodes on the same machine, rather than sending data over UDP?\u201d, because the term \u201cinter-process communication\u201d only means communication between two processes, but does not imply what the mechanism is (i.e. UDP or TCP can be IPC).", "It depends on the middleware implementation you are using. Currently the default of Fast-RTPS does not use anything other than UDP single cast or multicast. It could, RTPS allows for this in the protocol. I believe some of the proprietary implementations provide a shared-memory based IPC mechanism when on the same machine. Fast-RTPS will hopefully support this in the future.", "What about in case of bare metal micro-controllers where there is no OS?", "There have been some experiments which use RTPS on a OS-less microcontroller to communicate with ROS 2 on a desktop and some other experiments to get our ROS 2 API compiling for microcontrollers, but there is no out of the box support for OS-less ROS 2 at this time. We did enough work to convince ourselves that is possible, but haven\u2019t followed through with sustained support because we didn\u2019t have time, though we would love to do that in the future.", "In that case though, with no OS, you probably would not be using processes so the idea of \u201cInter-", " communication\u201d doesn\u2019t really make sense. You would be using something more like our \u201c", "-process communication\u201d, which can avoid sending the message data over the network (UDP in most cases).", "RTI\u2019s DDS does use shared memory internally when possible, I think.", "Hi sagniknitr,", "Fast RTPS will support shared memory in future releases, is in our roadmap. Regarding microcontrollers we are developing a lightweight version of Fast RTPS based on an OMG future standard for constrained resources devices. Of course it is not a full DDS/RTPS implementation, but a reduced API and different protocol.", "You can see the first prototypes in the ", " use case, and we are working in an release now. This will let you to use ROS2 in micro-controlllers.", "If you want more details, please contact me.", "Jamie,", "Thanks for the update about how Fast RTPS will be evolving.  A few questions, is the prototype with Dronecode you\u2019re referring to the work you did on the ", "?  Additionally, can you point to any public links regarding this new OMG standard for constrained resources? I know that the ", " is publicly available, is a draft of the document you\u2019re referring to available?", "Thanks", "\n- Eddy", "The DDS XRCE (\u201ceXtremely Resource Constrained Environments\u201d because all OMG specifications need a cool acronym) specification isn\u2019t even close to finished yet. The ", " but you have to be an OMG member to get the working documents, unless someone involved is willing to provide them. When the specification is adopted, that becomes publicly available.", "As seems to happen a lot with the DDS specs these days, the RTI/eProsima/Twin Oaks camp and the PrismTech camp have produced their own versions of the XRCE specification and are struggling to reconcile them into a single specification that the OMG can adopt. I won\u2019t wade into the debate over which is better, but I\u2019ll let you draw your own conclusions from the fact that one of the two camps only has one company it it. ", " I hope it\u2019s not going to be the RPC for DDS spec all over again; the arguments over that could be heard in other meeting rooms. But it seems likely that it will be a year and a half or more before the final specification is released.", "Thanks for the informative reply ", ", I had no idea the work was under way.", "I attended the OMG meeting last week, so I took the opportunity to hear what PrismTech and RTI/TwinOaks/eProsima had to say about their competing submissions. The following are the notes I took during their presentations. Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "Prismtech presentation:", "RTI/eProsima/TwinOaks presentation:", "My summary:", "Thanks for the summary ", "!  I\u2019m very interested in how the work progresses", "Thanks Geoff, great stuff!", "Hello ", ",", "First of all I\u2019d like to point out that I am one of the author of PrismTech\u2019s/ADLINK XRCE proposal, thus you may think I am giving you my perspective, in that case I urge you to read original documents to see how what I am providing here are facts. That said, let me make a few rectifications to some of the points above.", "You say:", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure ", " You can find the result of our analysts here ", " but you are welcome to derive them by reading both specs.", "This is also partially correct. We had asked to present what would have been our final proposal in Bruxelles to give a chance to the task-force to digest and one more opportunity to the competing team to join. This has nothing to do with the Vote-to-Vote and the fact that the task-force did not decide to allow the Vote-to-Vote procedure. The OMG has very complex rules\u2026 I know that can seam strange, and they still surprise me after having spent more than 10 years dealing with it.", "The RFP (which I wrote) asks for a protocol not for an API.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "\nAdditionally, you may argue that this may add some complexity in the message parsing \u2013 but again \u2013 I think that checking a flag and deciding wether some field is going to be present or not does not belong to the realm of hard. It is also worth  pointing out that some flags are just informative and don\u2019t require any kind of branching in the decoding. Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes ", " And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple ", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples. Our proposal currently has 4 compared to that of RTI/TwinOaks/eProsima which has 16 bytes. We have had our implementation run on a Makeblock robot using BLE with an MTU of 20 bytes.  You can check the comparison on this deck ", " and should wonder why the competing proposal did not provide a proper analysis of the wire overhead. Additionally when leveraging batching we have a wire overhead of (3+n)/n where n is the number of samples being batched.", "Some other thing worth point out is that the protocol allows for data to be pushed, pulled or periodically pushed or pulled. The write protocol also allow to pace the streaming of data that results from a remote query. This is extremely important when dealing with resource constrained nodes that need to consume data little by little.", "I did not hear RTI saying:", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those. The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP. The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "I\u2019d like to understand why you say this:", "What do you think is our submission cutting out? We are wire efficient yes, extremely wire efficient but at the same tine we support:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions. I\u2019ll also be more than happy to answer any question you may have about our protocol.", "A+", "Dear ", ",", "You are being to nice to PrismTech as everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises. I think it is best for people to look with their eyes at what the two submission can do and make their own decisions.", "On my side I like debates, I like inquisitive minds and I like hard questions. Thus I\u2019ll be more than happy to answer any question on the XRCE protocol proposed by the PrismTech/ADLINK team explain why it is better than RTI proposal\u2026 And actually it is better than RTPS.", "Thus, please let\u2019s start the open debate to dissect the reasons why our proposal is the one which should be voted and by far the better one.", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being ", "A+", "Thanks ", " for jumping into the community in such an energetic manner. I think we all appreciate having one of the authors of the proposals providing feedback. That said:", "Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "which answers:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I think I\u2019ll stop here\u2026 for the time being ", " .", "Hi Angelo (", "),", "Coordinate different companies to get a common view on a complex matter is always hard, and in this case, there are multiple design options leading to different tradeoffs.", "Our submission (RTI, Twin Oaks & eProsima) already aligns the views of three different DDS vendors, and sure we will try to incorporate ideas of your submission.", "Our submission tries to accomplish the following:", "We coded a PoC of our submission, and during the presentation, you asked about numbers. At that point, with no optimizations at all, and in debug mode we answered 43Kb. I asked my team to optimize a little bit the code, and here are the numbers we have now for the client:", "Total Memory Use: 8 Kb", "But we could squeeze that even a little more. We are releasing this as Open Source (Apache 2) so anyone can review the results.", "But again, we are not aiming to be as small as possible. We are covering all the requisites of the DDS-XRCE RFP, and testing our solution in what we consider typical microcontrollers today.", "Regarding the process at the OMG meeting, we (RTI, Twin Oaks & eProsima) didn\u2019t want to confront both specifications and choose one of them, but have the time to incorporate ideas from your submission, and that is why now we have an extended deadline.", "Let me join in the fray. I\u2019m the other author of the PrismTech submission and the one who built our tiny prototype. I wasn\u2019t present at the OMG meetings, and I won\u2019t waste any words on what may or may not have happened there.", "Firstly, I don\u2019t think a contest of bytes of RAM adds real value to the discussion, although of course it is an honourable contest in itself ", " I am surprised that you, ", ", had never even had a proper look at the memory use of your implementation given the purpose of the exercise in the first place, but if it is 8kB now then it is much better already \u2014 if still 7kB overweight ", " In any case, memory use is determined more by the implementation than by the protocol messages.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already. Yet even that is not of such great interest to me in this discussion.", "What really matters in my opinion is a difference in philosophy. The two proposals suggest very different views of what one would ideally want to accomplish.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that. In a sense, it is just a hand-crafted alternative to CORBA with a lower overhead. (Simply using CORBA actually \u201cjust works\u201d if the DDS implementation is faithful to the IDL interface mappings, even if it is ugly.) To me, this route is a pragmatic way of going about satisfying the RFP, but at the same time, an uninteresting one. (Sorry ", " and others \u2026)", "We chose to design a compact protocol that can support what amounts to performing DDS operations remotely, but doesn\u2019t limit itself to it. Instead, it also supports a DDS-like peer-to-peer network with vastly lower overhead, and, in many ways a level of flexibility in specifying what data is of interest (through URIs and selections) that DDS doesn\u2019t natively support.", "All of that would be of little value if it doesn\u2019t perform well or doesn\u2019t scale well. Just like the code size and memory use are mostly determined by the implementation, so is maximum sustainable performance more determined by implementation than by the details of the protocol headers. Size-wise, my prototype can run as a client on an Arduino Uno (8-bit CPU, 2kB RAM). A small test application using the same implementation configured as a peer easily sends ~700k 8-byte msgs/s from one RPi3 to 3 others (CPU is << 100%, network load ~75% of Fast Ethernet, so I really should investigate why it isn\u2019t faster), and goes another order of magnitude faster when run over local loopback on my MBP. That\u2019s better than your typically DDSI implementation. Is this relevant? That depends on whether you have high rate, tiny messages \u2026", "Now my test application doesn\u2019t implement all of DDS \u2014 not even close \u2014 and this is another significant reason why it can do this with only a few kB of code and RAM. At the same time, this is, I believe, where it gets interesting for ROS2.", "As ROS2 has its own middleware abstraction layer that uses only a fraction of the DDS feature set, putting ROS2 directly on our protocol would get you a smaller and a faster system. Smaller and faster usually allows doing more interesting things, even if I can\u2019t say what exactly those interesting things will be.", "Disclaimer: I can\u2019t do run ROS2 over it today, there\u2019s more work to be done on my prototype before it supports all that is required. And I wish none of you would have to take my word for the data I mentioned, but that is not something that is in my power to solve today.", "It looks like I started something of a minor storm moments before starting a holiday\u2026", "I\u2019m thankful that the DDS vendors, PrismTech, RTI, Twin Oaks and eProsima, are all engaged enough in the ROS community to be present on the Discourse board. It is encouraging to future adopters of ROS 2.", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure", "I wasn\u2019t implying religious believe. It\u2019s an simply expression to describe someone making an assertion. ", " I fully agree that PrismTech\u2019s submission has much smaller messages than the RTI/Twin Oaks/eProsima submission based on the two presentations alone.", "The RFP (which I wrote) asks for a protocol not for an API.", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "It is quite common to use flags. TCP is full of them. My concern is that the protocol is, in my opinion, undoubtedly complex and, based solely on the presentations, more complex than the other submission. Complexity and size are often a balance and in this situation we appear to have one submission at each end of the balance.", "Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes", "With the sorts of overhead you are achieving, I\u2019m not surprised performance is amazing. I\u2019d still like to see numbers, though. ", "And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple", "As was stated elsewhere in this thread, eProsima\u2019s implementation wasn\u2019t optimised. Since you said you are trying to bring yours to market already and eProsima claimed theirs was a tech demo, I\u2019m not surprised yours is more optimised and thus smaller. Of course, the numbers are definitely in your favour for RAM usage. But I\u2019m curious how much program memory each implementation requires, too. This is often the limiting factor on embedded microprocessors rather than the RAM usage.", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP.", "In that case I am very interested in seeing what these AB members wrote.", "The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "What do you think is our submission cutting out?", "\u201cCutting down\u201d, not \u201ccutting out\u201d. I meant that you are trying to reduce the size of the messages on the wire as much as possible, at the expense of possibly needing more complex parsing code. I didn\u2019t mean to say that you are cutting out features. It was clear from the presentation that PrismTech supports more features and has more flexibility than the other submission. But there are trade-offs involved.", "Dynamic Discovery (RTI does not, please read their spec!)", "\nPeer to Peer Communication (RTI does not)", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Non IP Transports (RTI does not)", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "Generalised Queries (RTI only supports DDS-like queries)", "Well, it is ", "-XRCE, is it not?", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions.", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises", "While this is true, we are operating at a standardisation organisation, not a rubber stamp provider. There are interested parties beyond just the implementers. We would prefer not to just hold a vote on which submission to go forward with. We would prefer the submitters to actually work together and produce a single submission that combines the best of both without any technological compromises (yes, I\u2019m aware how hard that is).", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being", "Or, you could provide the answers to those questions, along with the equivalent answers for your submission, so we can see and compare the data to back up your claims.", "Our submission tries to accomplish the following:", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "\nNeither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use.", "This is the strongest impression I got from the presentation, as I said in my own notes. The data model is similar to DDS, which makes adoption by existing DDS users easier, and the ability to implement the protocol in a relatively simple piece of code (which makes it easier to verify and certify) was considered as important as saving bytes on the wire. I\u2019m not sure where the correct balance is between these two requirements, but the PrismTech implementation really gave the impression of being at one extreme.", "testing our solution in what we consider typical microcontrollers today", "This is something that I think is really relevant but that PrismTech have not addressed at all, and RTI/Twin Oaks/eProsima have not addressed enough. What are the typical microcontrollers in use today? What are the target environments for this protocol to be used in?", "The RFP explicitly says this:", "From a high level perspective, the key requirements that a DDS-XRCE implementation has to satisfy are (1) extremely low footprint \u2013 addressing targets with less than 100 Kbytes of RAM, (2) extremely efficient wire protocol \u2013 inducing a protocol overhead of just a few tens of byte over the user data, and (3) support devices that undergo aggressive sleep cycles.", "Both submissions fit within both the RAM usage (with much room to spare) and the protocol overhead.", "More specifically, the actual mandatory requirement is:", "6.5.3. DDS-XRCE protocol overhead, when sending or receiving user data, shall not exceed 24 bytes per packet.", "Let M be the size of the DDS-XRCE message sent out, and U the size of the user data, then M-U <= 24 bytes.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already.", "Again, this should have been in the RFP if it is so important. That would have saved a lot of trouble. All we got was an evaluation criteria:", "Protocol overhead shall be considered when evaluating submissions. Submissions with smaller overhead are preferable.", "This is a miserably small set of criteria for a complex design space. Even you, ", ", say that protocol overhead is not as important as the design philosophy.", "Which I agree is fundamentally different between the two proposals, and that this is where the root cause lies in the failure to reconcile them.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that.", "The RFP not-so-subtly pushes submitters in this direction. You cannot fault them for taking it at face value.", "I will read both submissions and when I do, I will report back with more technically-informed comments.", "Hello ", ",", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "And you see this as a positive aspect? Our model is simpler and more user friendly. For instance, how many people can digest DDS partitions? That said, we have a well defined mapping between XRCE resources and DDS topics.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "Are you an OMG member? If so I\u2019ll forward you the reviews. Both submissions went to the AB and the reviews were posted both on ", " and ", ". If you have access to those mailing list you\u2019ll be able to see them. I also recommend you take a look at those.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "It was impossible as the other vendors did not want to agree on such a low bound. The 24 bytes was the least we could agree on. This is why there is an evaluation on wire-efficiency. This matters were discussed at length, but again I don\u2019t think you attended those meetings, thus you are missing part of the history and the context. In any case, all of those documents are on the OMG archives, thus if of interest you to reconstruct it. Just search for presentation I did on XRCE for almost a year. starting from 2015!", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "Yes, that is correct as it is since the very beginning that we are trying to do a joint submission. They\u2019ve refused with futile arguments \u2013 if you ask me. We have put lots of effort to trying to join but that has not been corresponded. A pity that you were not in the Bruxelles meeting, otherwise you would have had a taste of it.", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Again, you did not attend the end-less arguments we had during the RFP drafting. RTI does not want peer-to-peer in XRCE because they fear it could become as substitute for DDSI-RTPS. Again, this is not something I am inferring, but something that was openly debated during the RFP drafting. We don\u2019t have any issue with that as we think that having a more efficient protocol than DDSI-RTPS for some use cases would be extremely useful.", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "For me that disqualifies completely the submission as in LowPAN environments nobody can afford to use TCP/IP\u2026", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "I am glad that this helped clarifying the situation, please don\u2019t hesitate to ask any other question. Concerning the code availability we are working on that. I\u2019ll keep you posted.", "A+", "Dear All,", "I wanted to let you know that we have just released under Apache 2 a peer-to-peer implementation of our zenoh protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating zenoh as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", "N.B. For those of you that are familiar with XRCE, zenoh is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as zenoh.", "A+", "Kydos,", "Thank you for publishing the Zeno-He library so we can all begin to interact with it.  It is especially useful for the ROS 2 user community to be aware of the effort since it implements the ATLab XRCE proposal.", "I know I would be extremely interested in someone benchmarking Zeno and the proposed epromisa XRCE implementation, and perhaps can find time to do that.", "This is really appreciated ", ", thanks for making this available.", "At present it\u2019s unlicensed code though (", ").", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They believe that their proposal is more efficient in the wire protocol (trying to save every single byte possible), and supports brokered as well as peer-to-peer communication.", "They have invited the competing submitters to join the PrismTech submission.", "PrismTech believes that they have presented their final submission in Brussels (June) and so were expecting to vote for acceptance in New Orleans (September), but because there are still two competing submissions, and no final submission document was received (only a presentation), this is not possible.", "PrismTech\u2019s reasons for going forward with their own submission:\n", "XRCE tries to target the most wire/power/memory efficient protocol, targeting not just IP infrastructures but a whole range of infrastructures.", "Their submission provides reliability and fragmentation.", "Their current prototype runs on an 8-bit microprocessor with 1 KB of RAM and has a wire overhead of 4 bytes for data samples.", "They got a review from the AB which was favourable (only editorial comments), and they believe that the AB review shows that they satisfy all the mandatory requirements.", "\n", "XRCE applications can be brokered into a DDS data space, or they can discover one another and communicate P2P.", "Their submission is only about the protocol, it does not say anything about the API.", "A DDS-XRCE Agent running on permanently connected hardware provides the access to the rest of the (non-XRCE) DDS domain.", "XRCE provides a data space abstraction in which applictions can read and write data autonomously and asynchronously.", "Data read and written by XRCE applications is associated with one or more resources identified by a URI.\n", "An XRCE resource is a closed description for a set of named values.", "Resource URIs allow for wildcards, which means that more than one resource can be targeted in one definition, which is useful for capturing collections of sub-namespaces.", "A resource with a cardinality of one is called a Trivial Resource.", "Resources also have properties, which are used to attach QoS settings to them, such as \u201cthis resource is transient\u201d or \u201cthis resource is reliable\u201d.", "An XRCE selection is the conjunction of a resource and a predicate over the resource content and properties, used to filter a selection. For example, finding all lights with a luminosity greater than zero. (It seems to provide functionality similar to a very basic subset of SQL.)", "\n", "There is a mapping from XRCE resources to DDS topics.", "Resource serialisation uses the same format as DDS.", "All XRCE messages are a single-byte header and a body.\n", "There are flags in the header for reliability and synchronicity.", "Messages can be \u201cdecorated\u201d by prefixing them with a one-byte pre-header, which determines some of the properties for the following header byte, such as fragmentation.", "The protocol is little-endian.", "Variable-length encoding is used to save space.", "\n", "For addressing, the source address for each message is assumed to be a unique address of the sender.", "The protocol is modular, with a core profile (required for any communication), an optional query profile, and an optional discovery profile.\n", "e.g. If using a communications system like Bluetooth, which already has discovery, the XRCE discovery profile can be removed, saving some space.", "Discovery happens by scouting (sending a scout message to ask for types of nodes to reply, i.e. broker nodes or durability services or peers or clients).", "Other nodes reply to a scout with a HELLO message.", "\n", "After discovery, two nodes need to open a session, which enables publishing and subscribing between those two nodes.\n", "Authentication data is included in the session open message.", "Locator information, telling the other node how it can be reached, is included in the open message.", "The receiving node replies with an accept message or a reject message.", "There is an FSM describing the states a session goes through.", "\n", "Resources and selections are uniquely identified by numerical IDs to save space on the wire.", "A declare message is sent to declare what resources, selections, etc. a node has or will publish or subscribe to.", "Subscriptions can be push, pull, periodic pull or periodic push.", "All data messages and declarations are transported over a conduit (for the session), which is a pair of a reliable and a best-effort channel. Multiple conduits may be used in parallel to avoid head-of-line blocking and allow multicasting.", "One decorator is used to select the conduit for the next message.\n", "It is possible to make this decorator one byte instead of two if the number of conduits is less than five.", "\n", "There is a sync message available to set the next sequence number to expect (e.g. for when not starting at zero).", "The ACKNACK message can acknowledge multiple messages at once, usually up to the given sequence number. It has the option to optionally request retransmission of one or more messages after that point.", "There is a one-shot write function in the protocol, which can do a write of a resource without needing to do any prior registration or discovery.", "This proposal appears to be a very complex protocol with a lot of options in message header structures. An implementation would have a lot of choice points during the decoding of a stream of messages. The \u201cdecorator\u201d idea especially may save bytes on the wire (and in message construction buffers) in some cases, but it complicates the protocol implementation.", "PrismTech are already trying to bring their submission to market as a product.", "This proposal also uses an XRCE agent to provide access from DDS-XRCE nodes and the DDS domain.", "An important use case for them is that devices will tend to mostly sleep, and only wake up occassionally to do some processing, and send and receive data. This means that two devices may never be awake at the same time. This is why they have the agent, which is permanently present and provides a way for XRCE nodes to communicate with each other.", "They say that their proposal focuses not just on the XRCE protocol, but also on the interaction between the XRCE protocol and the DDS domain.", "It is possible for an XRCE agent to behave as an XRCE client to another agent, allowing for a hierarchical structure of agents and clients.", "Their proposal is based on the web-enabled DDS specification, with a DDS-XRCE object model in the agent that has a one-to-one mapping to the DDS data model.", "eProsima have put up a demo on Youtube: ", "\n", "Their demo uses 43 KB of RAM (much bigger than the PrismTech proposal, which can fit in 1 KB).", "The XRCE Agent object model is very similar to the DDS object model, making the mapping very simple.", "XRCE objects are modeled as resources that are addressable by their name and have CRUD operations.\n", "Each resource has a name to address it within the agent and a context; a representation describing the resource; and an ID.", "Resources can be represented as a name, an XML description, or a binary representation.", "\n", "Authentication capability is built into the proposal.", "Types are typically pre-defined profiles in the XRCE Agent.\n", "It is possible to transmit types as binary or XML representations.", "\n", "Message structure:\n", "A message header is either 4 or 8 bytes, depending on if a clientKey is used.", "There is a sub-message header, which is another 8 bytes.", "It is possible to send data in a sequence, meaning the header only needs to be sent once for a bunch of samples.", "\n", "The transport can be message-oriented or packet-oriented.", "CDR and DDS-XML representations are reused.", "Message overhead is typically 12 bytes.\n", "They consider that further reduction in overhead would increase complexity and reduce robustness. e.g. They do not need different code paths for multiple sessions, re-connections, handling variable-length encoding.", "They say it compares favourably to TCP/IP overhead (40 bytes).", "They think they could save 6 bytes from their message header, but the reduction is only 6% in the context of total overhead (when considering the use case of using TCP/IP as the underlying transport protocol) and so is not worth it in the face of increased complexity.", "\n", "They also received 3 AB reviews, which they claim were supportive and did not find any non-editorial problems.", "The submissions are very different. PrismTech is aiming for cutting down the bytes used by the protocol to the absolute minimum at the expense of all else. RTI/eProsima/TwinOaks are favouring some overhead in order to achieve a simpler and more robust protocol and implementation.", "PrismTech\u2019s protocol is overly complex with too many choices during the decoding of a message.", "The extra overhead of the RTI/eProsima/TwinOaks submission is not likely to be a problem in the majority of embedded micro-processors used these days (although I\u2019m not sure how many 8-byte, 1KB-of-RAM micros there are in use in new products). However, their consideration that TCP/IP is going to be the most common transport may not be accurate.", "Dynamic Discovery (RTI does not, please read their spec!)", "Peer to Peer Communication (RTI does not)", "Client to Broker Communication", "Non IP Transports (RTI does not)", "Generalised Queries (RTI only supports DDS-like queries)", "Push/Pull/Periodic-Pull and Periodic-Pull Readers", "Unicast and Multicast communication \u2013 for both client to broker and peer-to-peer", "Durability", "The view ", " provides is a) unbiased, b) the view of a roboticist (that\u2019s what this community is about ", " ! )  and c) based on the information someone got from hearing \u201cyour presentation\u201d. Note the following:", "I believe we all appreciate technical argumentation and slides. I love slides. But what I love even more is code and things that I can reproduce. How can I verify your arguments through experimental results? Is there any open code that supports your arguments? More than bashing around, i think it will do a lot of good to facilitate implementations that others can reproduce in common platforms. Even early stages will do. You might find that you could even get some support (and feedback!) before launching it officially and furthermore, that\u2019ll definitely convince a lot of people on why your approach is better.", "Last but not least, I really hope that the passion you\u2019ve shown answering this thread is shown by supporting and making OpenSplice better.", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "Neither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use."], "url": "https://discourse.ros.org/t/ipc-in-ros2/2619"}
,{"title": "ROS2 Logging", "thread_contents": ["Hello all,", "My team at Amazon is looking at extending the existing logging functionality in ROS2. I\u2019ve described below what our initial plans are for logging and the justifications. I welcome any feedback on the plan.", "Currently in ROS2 most of the logging implementation exists in the rcutils library. The interface into it is a set of macros that get generated during the build and a few functions in rcutils that back them. A string representing the logger name is used as input to these functions to identify which logger is being used/modified. Each implementation of the RCL then provides its own logging interface. In the RCLCPP library, for example, it is again a set of generated macros that are backed by the rcutils macros. When a logger is created in one of the language RCL libraries, it does not call down into the rcutils library to initialize any state for that logger.", "Loggers can be created in association with a node or on their own. Loggers associated with a node are identical to loggers not associated with the node except that the name of the logger is automatically set based on the associated node\u2019s name/namespace. Logs can be created in a hierarchy based on their name. The hierarchy can be used to adjust the severity level at which a logger operates.", "The logging functionality in the rcutils library currently allows for only a single output function  to be set for all logs. It defines a typedef for the output function header and allows the output function to be changed by calling a setter function with a pointer to a new output function. The rcutils library also includes one output function implementation which sends logs to stdout. The output function cannot be changed for different loggers in the hierarchy. Every logger uses the same output function.", "These are the changes my team is looking to implement.", "Increase the number of output functions that can be set in the rcutils layer", "I would suggest to keep a single output function in the core API (simpler, less memory management required). To enable multiple handlers you could create a new function which is capable to dispatch the call to N other functions.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "Yes, the proposed approach should be able to support fluentd. From what I can see, the easiest way to hook fluentd into this proposal would be to use a ", " input to get the log events sent to to the log files. If more advanced features are needed you could always recompile the nodes and swap out the shared logger library with something that hooks directly into fluentd.", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Yes, that should be possible. Since the shared library is linked at runtime it should be possible to just link in a different implementation. I\u2019ll need to research a little more to see what the most convenient way to do this is. It may still require you to recompile a shared package and source it in your workspace. If other people have thoughts on an easy way to do this I\u2019d be interested in hearing.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "(This is Forrest from the same team in Amazon) Yes our goal is to have an interface so that different types of sinks can be implemented and hooked.", "That said, I would suggest the sink should be fluentd agnostic even if you want to use fluentd to consume the data \u2013 or perhaps I misunderstood you in this case\u2026", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Technically this is doable, but we have to be very careful because IMHO logging mechanism should be straightforward to achieve the best robustness possible. Is there a specific use case in your mind that needs such capability?", "I\u2019m thinking about how nodes are typically installed from binaries. If I install nodes from binaries but want to use a different logging daemon to the original node developer, then I lose the ability to install from binaries.", "My understanding is that even when you install Nodes from binaries that ROS dynamically links them from your sourced workspace when they run. That should allow a developer to use some environment variables or some other mechanism to swap out the logger for any predefined ones at runtime or replace the shared library logger by providing their own with the same interface to be linked in instead. I don\u2019t know off hand how difficult or easy that is to do, but it is all possible.", "As noted in a ", ", ros1 logging doesn\u2019t support unicode strings, so it would be nice to support that in ros2.", "We\u2019ve created the first set of pull requests for the new logging features (links below). Our next steps are to work on adding rosout topic capability. As part of that we are planning on porting the Log message from ROS 1.", "For the new ROS 2 message I would like to remove the list of topics the node is publishing on that is in each log message as I don\u2019t think it makes sense to include that with every logged line. I\u2019ll also be adjusting the verbosity level constants to match the values defined in the rcutils package. The final change I was looking at for this message was to move it into the rcl_interfaces package instead of keeping it in a rosgraph_msgs package.", "Links to pull requests below.", "rclcpp: ", "\nrcl: ", "\nrcutils: ", "\n", "A logging implementation for ros2 that using log4cxx.  - ros2/rcl_logging", "\n", "Edit (Nov 14th, 2018 9:45am PST) - Changed the package we\u2019re planning on moving Log message into from common_interfaces/diagnostic_msgs to rcl_interfaces to avoid adding another dependency in rcl.", "Another pull request for the Log message definition.", "The initial implementation of the rosout features is code complete as well and available in the branches listed below.  I\u2019m going to wait to start the pull request for the rosout changes until the existing pull request is complete.", "Common C functions and data structures used in ROS 2 - nburek/rcutils", "\n", "\n", "Library to support implementation of language specific ROS Client Libraries. - nburek/rcl", "\n", "Please go ahead any missing PRs asap (and add a note in it what it is blocked on). We are very close to the API freeze and can\u2019t consider what is not visible anywhere.", "I\u2019ve gone ahead and created pull requests for those features, though I could not find a way to decouple them from the earlier commits being reviewed in the existing pull requests. Once those PRs are done I can rebase these so only the new commits show in the rosout PRs.", "\n", "\n", "\n", "There was some interesting development in rosconsole from ", " to use pluginlib to support different logging sinks, particuilarly to support journald (", "). I wonder if you could consider a similar approach for logging in ROS2, which would provide a familiar mechanism for swapping logging sinks without recompilation.", " is your desire to be able to actually develop custom logging sinks (a reason for using pluginlib), or are you simply suggesting that ROS2 should be able to log to a number of backends which should be configurable at runtime (not necessarily requiring pluginlib)?", "I have a pretty limited number of sinks in mind (stdout, /rosout, journald), but given the fluentd suggestion above (and that ROS2 is cross platform), being able to develop a sink plugin outside of the rcl source tree via pluginlib seems valuable.", "Of course, you can always just write /rosout to journald via an intermediate node, but this can be expensive when intra-process transport is not available.", "it would be very nice if we can choose the backend logging system, cz some vendors have their own specific logging system once it comes to the embedded system.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Increase the number of output functions that can be set in the rcutils layer. We will add new functions to add and remove functions from a list of output functions in the rcutils library, but will not add this capability to the RCL interface. For the initial implementation, we will not add the ability to change the output functions at different logger hierarchies.", "Write a new log output function in the rcutils library that will forward all logs from a logger associated with a Node to a ", " topic on that node.", "Define an interface for a shared library that wraps more power open source logging libraries such as log4cxx. The interface will include initialize/teardown functions, call throughs for when logging metadata changes (such as log levels and formats), and functions to output logs from the rcutils library. We will also define a way of passing through a configuration file location so that the shared library implementation can use the standard config format defined by the library that backs.", "Write a new log output function in rcutils that will call out to a shared library that implements the above interface.", "Write an implementation of that shared library that wraps the log4cxx library. This implementation will include a default configuration that will send all logs to a file.", "Modify the rcutils library so that when it initializes it will hook up the three output functions that now exist in the rcutils (stdout, rosout, and shared library). These will each be able to be individually enabled/disabled via an environment variable.", "Why not add the new interfaces for adding log output functions to the RCL layer so that people implementing their own nodes can hook up additional loggers?\n", "If someone really wants to do this it will be possible by the RCL handle for their node and directly using the rcutils library. The reason for not adding to the RCL interface to make it easy to do in every language is that we do not want to encourage node developers to add their own custom logging in this way. It is better to conform to relying on only the outputs in the rcl/rcutils libraries so that your node can be more easily integrated into other applications. If an application developer wishes to use your node in their application, but wants total control over how logging is handled, it is a better experience for them to only need to adjust the standard output mechanisms natively provided by ROS 2.", "\n", "Why not also add the ability to set different output functions for different logger hierarchies?\n", "We are not providing this as part of the native rcutils library as a simplifier for our initial work. This functionality would be available as part of the shared library output that we are proposing. So anyone who really wanted that level of control could get it for both stdout and file logging by adjusting the configuration file for the shared library logger.", "\n", "Why not send all logs to the rosout topic instead of only those sent to loggers associated with nodes?\n", "Because of the way ROS 2 associates DDS concepts with ROS Nodes, there didn\u2019t seem to be a clean way to setup a general topic on a process that wasn\u2019t associated with an existing Node. Since a process can also contain multiple ROS Nodes, we didn\u2019t think it would be good design to just pick one to have everything published to.", "\n", "Why the shared library?\n", "We went with the shared library approach in order to provide a way to hook into an existing logging library without tying ROS 2 to only using that library. Wrapping an existing logging library will give us the ability to have features such as file logging, log file rotation, and a hierarchy of output sinks without the need to implement all of that in the ROS codebase.", "\n", "Why keep the existing stdout logger implementation in rcutils instead of relying on the shared library to provide the stdout implementation, since most major logging libraries already have those created.\n", "We decided to keep the existing stdout output function because it provides a standard in the cases where someone does not want to rely on the shared library logger. Since you can enable/disable any of the three natively provided log output functions it is easy for a user who wants more control to disable the ROS provided stdout and rely on the", "\n", "Since the shared library will be used in the rcutils layer will it integrate with the custom allocators that the rcl/rcutiles use?\n", "No, we are not planning to provide a hook into the custom allocator for the initial implementation. This could be added later, however, most of the open source logging implementations that are well supported and provide rich features do not provide interfaces for providing your own allocator. We did not want to pass the allocator into the shared library when the shared library wouldn\u2019t actually be able to use it. In the case where someone needs the control over allocation they would need to recompile anyways and at that point could swap out the shared library implementation with a logger of their choosing that provides more control over memory allocation.", "\n"], "url": "https://discourse.ros.org/t/ros2-logging/6469"}
,{"title": "Slightly off-topic: two tools for making Fanuc development a little easier", "thread_contents": ["For some time now I\u2019ve worked on two tools that may make development of Karel programs for Fanuc controllers a little easier. As there might be developers here who work with Fanuc robots \u2013 outside of ROS(-Industrial) \u2013 and may be interested in this, I thought I\u2019d post a link to them in this Discourse category.", "The first is ", ". It\u2019s a wrapper around the regular ", " (the Karel compiler). The wrapper is not a compiler itself, but adds some functionality which I found missing from ", ":", "and some other minor conveniences.", "It does not integrate with Roboguide, but is intended to be used from the command line.", "Manual command line invocation gets tedious pretty quickly, so the second tool tries to provide a solution for that: ", ". This is a CMake like \u201cmakefile generator\u201d that supports a workspace-style workflow. It introduces a package based approach where packages have manifests, export include files (headers) and buildable targets which ", " consumes and generates a single top-level ", " file for. Building an entire workspace requires only ", " to be invoked. Dependency tracking and build ordering (for generated code fi) is built-in.", "Together with ", ", this enables a more modern Karel development workflow where reusable libraries can be created and some form of redistributable package management can be integrated.", "An example workspace may be found here: ", ".", "Right now only Karel is supported, but a member of the community is looking into adding TP support (ie: ", ") to ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["preprocessor support (based on GPP): include guards, macros, defines, conditional compilation and inclusion, etc", "gcc style dependency tracking (ie: ", " support)", "support for multiple include directories"], "url": "https://discourse.ros.org/t/slightly-off-topic-two-tools-for-making-fanuc-development-a-little-easier/10717"}
,{"title": "Support the PLY geometry format in URDF?", "thread_contents": ["Hello! I posted over ", ", but this place seems to be a bit more active.", "At the moment, the URDF spec only seems to mention Collada / DAE and STL file formats for the mesh node. STL, however, doesn\u2019t support a very robust set of data and Collada is pretty inefficient in terms of file size and pretty complicated to export. What are peoples thoughts on including something like the PLY format as an option for a mesh node file format?", "Thoughts? Thanks!", "\nGarrett", "Without being familiar with 3D modle formats, I\u2019d say someone would need to give the point of view from the libraries that use these files. I\u2019m thinking of:", "If they already have support for it, sounds great! If they don\u2019t, a newer version of URDF should at the same time support these big players I guess.", "Sounds reasonable. I\u2019m less familiar with the ROS codebase, myself, but I\u2019ve noticed some references to ", " around the ROS repos, and ", " (in the bullet physics repo?) has discussions around gazebo using assimp to load models, as well.", "If that\u2019s true, then this may be as simple as extending the spec to include another model format. I\u2019d almost suggest just allowing anything that assimp can load (if it\u2019s actually that easy), but I actually like the constrained list of formats because that makes it easier to consume the format on other platforms (we\u2019re visualizing these models using THREE.js and javascript, as well). To me, all that\u2019s missing is a simple to export, terse, binary geometry format.", "Thanks!", "\nGarrett", "It will not help to improve the state-of-the-art wrt supported features, but in some testing I did a few months ago it turned out that you can use just about anything that Assimp supports for meshes in urdfs. In the end it all gets converted to vertices & faces anyway, so as long as Assimp can load your mesh it should work.", "IIRC I even used ", " at some point.", "Note that this is really just about meshes, so none of the \u2018advanced\u2019 features formats like Collada and FBX support are used (bones, skins, kinematics, simulation properties, etc).", "I did not include Gazebo in these tests, so that may be different.", "Collada is pretty inefficient in terms of file size", "Collada has a variant where the whole file is compressed using gzip, the extension changes to ", " and it typically results in 90% reduction in file size. It supports all of the things you mentioned, but it is definitely not meant to be human readable and is not straightforward to export to without a library doing it for you.", "in some testing I did a few months ago it turned out that you can use just about anything that Assimp supports for meshes in urdfs", "Great! That\u2019s more or less what I was expecting. So it sounds like this is more a matter of documentation than anything else. Unless there\u2019s a serious reason not to expand the documented mesh formats, this could be as simple as just adding an extension to the list on the URDF wiki page!", "Collada has a variant where the whole file is compressed using gzip, the extension changes to .zae and it typically results in 90% reduction in file size.", "I\u2019d heard of ", " before, but it also wasn\u2019t listed on the URDF wiki as a suitable format. I\u2019d say it\u2019s worth adding that, as well, if it\u2019s considered a valid format. In a quick test, it looks like PLY is able to get down a bit lower than the zipped collada file, but I understand what you\u2019re getting at.", "The reason I\u2019m asking for documentation updates is because, like I said, we\u2019re looking into generating URDF models and visualizing them in the browser and other platforms outside of ROS because it\u2019s a nice, documented, and standard format that\u2019s easy to parse and export. I feel that the density, robustness, and simplicity of use of the PLY format scratches that last itch that isn\u2019t afforded by the other two mesh formats. I\u2019m open to other format suggestions that scratch that, too, though!", "Overall I\u2019m pretty happy with URDF \u2013 I\u2019m just trying to help the standard grow in a couple ways that accomodate my use cases! Especially if there\u2019s not a lot of work investment to make it happen.", "Thanks for your feedback!", "\nGarrett", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Supports a binary format type, which enables to smaller file sizes.", "Optionally supports texture coordinates so the ", " tag can be used for a surface texture.", "Optionally supports normals and vertex colors (and any other custom vertex attribute) for other color and shading.", "Extremely simple to write an importer / exporter for.", "\n", ". Haven\u2019t found much.", "\n", ". Their tutorial just talks about exporting to .dae/", "Collision computation libraries like ", " which I believe is used in MoveIt."], "url": "https://discourse.ros.org/t/support-the-ply-geometry-format-in-urdf/4325"}
,{"title": "Using NUC with Kobuki", "thread_contents": ["Hi,", "It seems to becoming harder to find netbook size laptops to use with the Kobuki base and so I\u2019m looking into using the Intel NUC (e.g. NUC6i5SYH). I want fully autonomous operation where the Kobuki can dock and charge its batteries and the PC, so I wondered if anyone was aware of a power solution e.g. external battery that can connect to the Kobuki 19v output AND will allow the NUC to get a reading of the battery level e.g. via USB. Alternatively, is it possible to use a dc-dc converter with one of the other Kobuki power outputs (e.g. 12v 5A) to power the NUC?", "Or if anyone knows of a good choice of small laptop to use then that might be preferable.", "Thanks,", "\nLee.", "Hi,", "Have a look at the ", ", it\u2019s a DC-DC converter as well as UPS (6-30V input, 6-24V output). It supports multiple battery chemistries (e.g. Li-Po, SLA) and has an adjustable DC output.", "It has a USB interface, and Linux drivers, supporting ", ".", "I\u2019ve been using this device on my robot, I wrote a very simple Python ROS interface for this, which just publishes the charging state and individual Li-Po cell voltages. I\u2019ll upload this script and link it on this thread when I get the chance (I\u2019m currently away from my robot).", "Alex", "Thanks for the response. After posting I did end up looking at that and it looks like a good solution. Good to know that you have it up and running and that the Linux drivers work etc.", "I\u2019m thinking of using 6 of these batteries:", "\n", "7926", "\n", "\nwhich based on my very quick calculations should drive a NUC for about 6 hours (assuming NUC consumes 15W)?", "I asked the OpenUPS company if they could recommend any enclosures - did you find one that works well?", "Hi,", "we have a NUCi7 on our Turtlebot. We ordered an i5 but got an i7 due to", "\nsome issue while ordering. The i5 had a notebook powerbank with it which", "\nis not powerful enough for an i7 (it lasts about 30 minutes). Thus we", "\nare now testing a custom LiIon battery. It seems to be able to run the", "\ni7 for about 4-5 hours now but we are still working on a battery status", "\nintegration. Additionally, we need about 4-6A for charging the robot", "\nnow. From the internal wiring, this should not be a problem, but the", "\nwall charger only gives us 3.2A. So we also still need to get a powerful", "\nwall charger (right now using adjustable lab power supplies).", "Once the battery runs well, I can send you our setup. The OpenUPS looks", "\nvery nice as well, especially the already integrated battery status and", "\ncharging state. If you deploy it to your Turtlebot, could you give", "\nfeedback about it?", "Best,", "\nLasse", "Hi!", "We are also using Turtlebot (version 1) with an i7 NUC which actually requires 19V / 65 W. So far, we didn\u2019t use any DC/DC converters or something similar, but instead, we powered the NUC directly from the 12V / 5A output port on the Kobuki.", "This is certainly not ideal, and we experienced the NUC shutting down because of a \u201cprocessor thermal trip\u201d after some time (sometimes only 30 mins when the Kobuki battery would still last for long). First we thought it\u2019s a problem with the NUC (old thermal paste or something), but later we realized that the output voltage from the Kobuki is monotonically decreasing and will be too low at some point. This seems to have caused the overheating problems then.", "Either ", " or ", " seem like good solutions for this problem, but I\u2019m not very experienced with DC-DC converters or UPS and I\u2019m glad for any advice.", "Can anyone tell me if this would be a good buy for more reliable mobile NUC operation?", "We had a similar problem and solved it by purchasing a power bank. It charges from the kobuki base, provides 19V to the NUC and does not kill the Kobuki battery. We use one of the 19V connectors on the back of the Kobuki and make an adapter to charge the powerbank when the robot is on the charger.", "This is the one we used: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-nuc-with-kobuki/489"}
,{"title": "Don't miss EU funding for ROS developments via ROSIN FTPs - Next cut-off dates: June 14 & Sept 13, 2019", "thread_contents": ["The EU H2020 ", " has the goal to advance open-source robot software for industry and the robotics community as a whole. One of the main activities of the project is a grant program with a total amount of ", " for Focused Technical Projects (FTPs) on ROS software development.", "Sofar, we have funded almost 40 FTP projects. Many (intermediate) results can be found here:", "\n", " Do you have a good idea for a ROS/ROS-Industrial related project and want to work on ROS(-I) software components, documentation, standardisation or a related topic?", " Are you or your company located within the European Union (or any of the ", ")?", " Then submit an FTP proposal and apply for a ROSIN grant by following the steps outlined in the ", ".", "FTP proposals may be submitted all year long but are evaluated 3-4 times a year.", "The next cut-off date are: ", " .", "Proposals are short (a few pages) and concise. Applicants are guided through the process by an application wizard and a guide is provided.", "To submit your FTP, please visit ", ".", "If you have any questions about the process, whether your idea or project would qualify, send me or one of my colleagues a message either through ROS Discourse or by email (see the ", " on the site for addresses).", " Too much text and no video?", "\n", " Watch our project coordinator explaining ROSIN project and it\u2019s funding opportunities at last year\u2019s ROSCon in less than 200 seconds", "\n          ", "\n", "or follow the 30 minutes presentation from ROS Industrial Conference (followed by four 15 minutes presentations of FTPs already funded by ROSIN):", "\n            ", "\n", "As a result of discussions at ROSCon2018, ROS-Industrial Conference 2018 and exchange of ideas in early 2019, we would like to solicit \u201c", "\u201d (or pre-baked FTPs) for upcoming ROSIN FTP calls in June and September. These suggestions shall give a focus on the topics we fund with our EU funding, but is not meant to not limit the scale and scope of future FTPs. Please check (upcoming) ", " from more than 38 ROSIN FTPs, to see what projects are in the pipeline or have been finished.", "\nWhich manipulator is suitable for a specific task? What is the ideal location of an object (e.g. to be polished) within a workcell for a given manipulator? Compare different solutions (in terms of computation time, energy consumption) for a specified task. Currently within MoveIt! it is possible to benchmark OMPL planners only ", "\nCurrently, mobile bases are approximated with floating joints within MoveIt! and it is not possible to execute these trajectories. Only other possibility is to plan for the arm (MoveIt!) and the mobile platform (navigation stack) separately. ", "\nAlthough FTP in similar direction has been approved (Move-RT): ", "\n(MoveIt!'s transition to ROS 2.0)", "\nMake MoveIt\u2019s interfaces more generic which will simplify integration of standalone components (kinematics and collision checking). and also external planners (Descartes) and optimizers (TrajOpt).", "\nKinematicsBase API", "\n", "\n", "\nCollision checking API", "\n", "\nAllow for \u201cpartial\u201d SRDF definitions to use arms with a combination of end-effectors / mount objects (tables, mobile base), similar to XACROs? Have a unified robot description (URDF, SDF, SRDF, COLLADA). Multiple formats introduces repetition of link names, etc.", "\nTiming and memory analysis tools for ROS nodes.", "\nAlthough some tools have been mentioned in the link: ", " ,", "\nit may not be the best solution for multi threaded, multiprocess applications", "\nEspecially, (most commonly used) Valgrind seems to be mainly for single-threaded applications and runs on VM, which distorts the actual results.", "\nA tool by OSRF (", ") has not been updated since 2014.", "\nHardware-in-the-loop / model-in-the-loop testing", "\nPackage versioning (possibly a REP)", "Kind reminder for the European ROS Community.", "\nOur next cut-off date is approaching.", "\nWe will be happy to evaluate all FTP proposals that are submitted before ", " end-of-day.", "\nNext cut-off is then on September 13, 2019 .", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/dont-miss-eu-funding-for-ros-developments-via-rosin-ftps-next-cut-off-dates-june-14-sept-13-2019/8999"}
,{"title": "Re: Indigo on Raspberry PI 3", "thread_contents": ["Hi,", "Continuing this thread: ", "The RPI3 is 32bit.  Is there going to be a 32bit version of Kinetic for ARM?", "There are Xenial armhf builds. I\u2019m kind of disappointed to hear that it seems that the Raspberry Pi community has not started supporting aarch64 images. Why have the armv8 chip and not use it.", "yes, the Kinetic buildfarm does armhf and arm64 builds for both Jessie and Xenial. It\u2019s a lower priority to support Debian and ARM builds in the initial release, but we do encourage maintainers to fix platform-specific problems that the build farm finds.", "Unfortunately we have only demonstrated the ability to build armhf and arm64 for Xenial and Jessie both. However we\u2019ve tried to pick a covering set by turning on Jessie arm64 and Xenial armhf.", "We can consider changing the supported architectures, however we need to trade off the costs of running the buildfarm. Note that the armhf and arm64 builds take close to an order of magnitude more server time.", "As far as I understand it, Raspbian doesn\u2019t have any builds that target an architecture above armv6 with hard float. (The original Raspberry Pi). They didn\u2019t want to support multiple arch builds, and they don\u2019t want to confuse beginners.", "Other than not wanting to confuse people, the RPI has some reasons to not run 64bit software.", "The Pi 3 only has 1GB of ram, so going to 64bit will consume more of that memory storing pointers.", "The speedup of the clock is enough to justify going to the newer chip, even if the newer 64 bit stuff isn\u2019t used. Just like Pi1->Pi2.", "  does this mean that only 64bit Jessie for the time being?", " for the moment yes.", "Though if what ", " says is right about all the images being armv6 + hard float, the armhf builds are not necessarily raspbian compatible either. Though the Pi 2 can run standard debian armhf binaries according to this: ", "There is a Debian wiki page on this here: ", " with more info on the Raspberry Pi 2 here: ", "RPI3 is different so I hope that they will updated it.", "\nAnyway it works with Ubuntu 14.04 in chroot on indigo.", "We are planning on using an ARM A53 based solution for our products and RPI3 or DB410C are great platforms to use.", "\nhopefully Kinetic will be supported on the RPI3 somehow", "(old thread but some additional background)", "The RPi 3 CPU core was picked because it\u2019s a great 32 bit processor (and  happens to also work in 64bit mode). That said, the 1GB of RAM and the rate at which the hardware is being iterated, there has not been much movement in the Linux community to put effort into an arm64 build.", "I understand the interesting in ROS on Raspbian. That said, we do have a workaround now that Ubuntu Xenial (with and without GUI) is available and supports the RPi3 hardware well.", "It does leave limited options for RPi-like hardware which has focused on Raspbian Jessie as their prefered OS.", "Just adding on, as you mentioned Xenial images, Here is a link to a bunch of Ubuntu 16.04 Pi2/3 images, offering a variety of desktop and non desktop variants.", "We have done what we can to optimise the builds for the Raspberry Pi 2\nand Raspberry Pi 3 but microSDHC I/O throughput is a bottleneck so we\nhighly recommend that you use a Class 6 or Class 10 microSD", "Yes. I\u2019ve responded to a couple ROS Answers posts, suggesting those images. I suggest we continue to them as the starting pion to most. It would reduce the frustration level for many.", "While those images are not perfect for everyone, they are likely the best option for most users on RPi hardware.", "Aside: the RPi3 w/ onboard wifi support is the basis for the LoCoRo (low cost robot) project.", "We got raspberry pi3  running on ubuntu16.04(MATE) with ROS kinetic, but it\u2019s 32bit.", "\nHere\u2019s is the image if anyone need.", "\n", "duckietown-bunny === ![](https://i.imgur.com/Lwgsi3c.png)   ---   a fork from MIT autonomy open-sour", "\n", "\nIt\u2019s original for  project \u201cduckietown\u201d and \"duckietown-bunny\"", "\n", "  Enjoy ", "Do you know if the RPI Camera works?  I am about to try mate 16.04 using your image.", "The Pi cam works on Ubuntu Mate 16.04.", "There is a ros node for it here:", "\n", "raspicam_node - ROS node for camera module of Raspberry Pi", "\n", "The documentation is a touch out of date, but we are working on it.", "Yes with normal raspberry pi CSI interface camera or fisheye camera", "\nHere is how you start it", "\n", "Camera test === Hey Bunny! Wake up! ![](https://i.imgur.com/FuMKr2G.jpg)  #### TODO *\tstart camera n", "\n", "\nThe original project is base on RPI2 see here ", "\nIf have further questions, leave me messages ", "Thank.  I will try it out.", "The RPi 3 CPU core was picked because it\u2019s a great 32 bit processor (and  happens to also work in 64bit mode). That said, the 1GB of RAM and the rate at which the hardware is being iterated, there has not been much movement in the Linux community to put effort into an arm64 build", "So I\u2019ve got my pi 3 running in 64 bit mode with Gentoo. I\u2019m slowly updating ROS  ebuilds to work on arm64, but have not done them all yet or tested ROS compilation on the architecture (because I don\u2019t want to mess up my portage tree, as getting 64 bit mode was quite a hassle).", "That said, I might attempt a 64 bit ROS Gentoo image, and could upload it for everyone if there is demand for it.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/re-indigo-on-raspberry-pi-3/141"}
,{"title": "Best ARM board for ROS", "thread_contents": ["I would like to get an opinion on what people think is the best ARM board for ROS.", "I have tried the dragonboard, raspberry PI3 and beaglebone black.  beaglebone does not have the horsepower to run ROS (moveit,etc).  I think that a quad core A53 might be the minimum required.", "Anyway comments welcome.", "Shawn", "HI ", ",", "I\u2019ve tried all those and my advice for new projects would be to have a look at the new NVIDIA Jetson TX1 module. IMO it\u2019s by far the best ARM embedded board where to run ROS and friends.", " how is the TX1 compared to the TK1?", "I have used the TK1 myself, and it has the capabilities to be extremely powerful, however all of nvidia images are given out with low power settings so you have to configure it all yourself (i also noticed that it had a issue with IRQ balancing, which till this day I dont believe got fixed)", "Hi,", "Yes,  the TX1 would be a great board (or the TK1).  I should have clarified and said best board under 120.", "\nI am considering Pine64 but I will have to wait and see about it.  I am a bit disappointed by the RPI3\u2019s OS and lack of 64bit support.", "Hi ", ",", "\nI jumped directly to the TX1 so I\u2019m afraid i can\u2019t comment on that.", "Has anyone used an old flagship phone as a \u201cARM target\u201d? I\u2019d like to use some old android phones that still have a good 2GB of RAM + 64-bit ARMv8 that include a swath of radios, sensors, self contained power supply. Buying one retail might be above your $120 mark, but if you have one siting around in a relatives junk droor or with a cracked screen off ebay\u2026", "I know there is ROS for android with ROSJava, but flashing phones with a more common flavor of Linux and treating as a traditional embedded target has always been appealing to me. I think mobile device hardware support is a bit fractured thanks to device manufactures, so I\u2019ve only seen posts with Nexus and Ubuntu Touch, nothing like an old Samsung I have.", "\nRelevant ROS Answers post: ", "I would like to get an opinion on what people think is the best ARM board for ROS.", "I have tried the dragonboard, raspberry PI3 and beaglebone black. beaglebone does not have the horsepower to run ROS (moveit,etc).", "I was hoping to use the BBB for a mobile robot doing things like SLAM, localization, and navigation (move_base). So no need for image/video processing, only spinning LiDAR. From my testing, it doesn\u2019t look adequate even for that, unfortunately.", "How do people feel about the Odroids? ", "ODROID XU-4 is pretty awesome. It has a USB3 host, and if you have a USB3 peripheral that you need to talk to, I don\u2019t think there are many (any?) similarly-sized and similarly-priced options at the moment.", "+1 for the XU-4. It can run a surprisingly serious ROS setup (motion planning, depth image processing). I\u2019ve just bought a C2 as well; haven\u2019t had a chance to try it out yet though.", "spmaniato", "You can use the BBB but for me using MoveIt it the CPU ran around 80-90 percent during planning.  Which is not good.  Using the DB410C or RPI3 it runs around 60-70 percent.", "I have not used an Odroid, but have used SolidRun cubox and Radxa rock and TK1.", "Since you mentioned depth image processing: did you try connecting an Asus Xtion to the XU-4? Does it work?", "Nice discussion with interesting answers.", "Just to mention my experience:", "I also used ROS in rpi2 rpi3, bbb and Odroid XU4 successfuly in several", "\nprojects.", "\nI did not used MoveIt but I can tell that it is enough to support some SLAM", "\nsystems if the algorithms parameters are well tuned for efficiency.", "\nSpecifically odroid is quite powerful and it is able to execute this kind", "\nof heavy applications fluently.", "Kind Regards.", "we have used xu4 with turtlebot with asus camera", "I also used ROS in rpi2 rpi3, bbb and Odroid XU4 successfuly in severalprojects.I did not used MoveIt but I can tell that it is enough to support some SLAMsystems if the algorithms parameters are well tuned for efficiency.", ", have you ever tried running the navigation stack (i.e., ", " + ", ") on a BBB by any chance? I\u2019ve found that, even with relaxed parameters, it cannot handle it. But I may be doing something wrong. (It can definitely handle ", " plus the laser scan publisher and other drivers. It\u2019s ", " that takes it over the edge in my experience.)", "I also like the ODROID XU-4 for using USB 3.0. On the one hand \u2018Intel\u00ae RealSense\u2122 Robotic Development Kit\u2019 is now available. It is fantastic for the user using RealSense or other USB 3.0 device (but a little bit expensive than other SBC). How about it?", "\n", "check our\n3d model\nHigh performance and low power consumption features of the latest tablet technology", "\n", "Yeah, we also used it with an asus xtion. We set the depth resolution very low (QVGA or QQVGA) mainly because we didn\u2019t need the extra pixels. Overall It worked quite well though.", "i tried Pine64+ and installed Ros from source on it ros indigo with ubuntu 16.04", "\nit is very good", "\nyou can download image from", "\n", "PINE64-_ROSINDIGO_Ubuntu16.04lts_image - image for pine64+ installed on it ROS package From source and Distro Ubuntu 16..04 lts", "\n", " It\u2019s great to know that it works for you with Indigo on the Pine64.", "Thanks for taking the effort to share however I have to not recommend people grab it. Binary images of unknown provenance are potential security risks.", "If you wouldn\u2019t mind it would be great if you could share your experience bringing up Indigo on the Pine64 with as much info as you can remember in a new thread in this category. Also did you try using the Debian Jessie builds with Kinetic from debian packages?", "i tried Ubuntu 16.04 LTS with indigo and i will make it for  Kinetic also", "\ni tried  Kinetic with it but i stopped to make meta-ros yocto core-image-minimal for raspberry pi 2 and it works now i will upload it soon also and i will build Kinetic with Ubuntu16.04 LTS", "I have Kinetic running on Pine64 under debian.  works great.  Kinetic is still missing some packages that should be synced in two weeks.", "All of the ARM A53 boards perform similarly to me.  I do like the 96Boards form factor the best", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Intel\u00ae Atom\u2122 x5-Z8350 Processor (2M Cache, 1.44 GHz up to 1.92 GHz) CPU with 64 bit architecture; Quad Core", "Intel\u00ae HD 400 Graphics", "1~4G DDR3L", "USB 2.0 and USB 3.0"], "url": "https://discourse.ros.org/t/best-arm-board-for-ros/152"}
,{"title": "wolfSSL and wolfCrypt for ROS", "thread_contents": ["Hi!", "We\u2019re thinking about porting the wolfSSL suite of security tools over to ROS, and would like some feedback.", "We think that the order of priority is:  1.  wolfCrypt Cryptography library, 2.  wolfBoot secure boot, 3.  wolfSSL TLS.", "We think the wolfSSL products are a pretty good fit for ROS because we support some useful things for the industry verticals that ROS supports.  For example, we support FIPS 140 crypto for government use, MISRA-C for automotive, DO-178 DAL A for things that fly.", "We also support a lot of the hardware cryptography and secure elements that we think should and will  be used in deployments.  Hardware cryptography is especially important for real time performance requirements.", "Those are our thoughts, but we\u2019re not experts on ROS!  As such, we could use your help as we prioritize what parts of our product line we bring over to your environment.  Please let us know what you think!", "LS", "Where do you see this work fitting in with the existing SROS/SROS2 stuff?", "Do you have any usage with DDS Security?", "Hi!  I don\u2019t have a good enough handle on SROS/SROS2 to answer that with specificity.  That said, we expect wolfCrypt could be consumed by SROS/SROS2 as needed, especially where FIPS, MISRA, or DO-178 are required.  The same is true with DDS Security.  I understand that RTI\u2019s implementation of DDS Security is currently consuming OpenSSL cryptographic primitives, but it can consume ours as well.", "The alternative to DDS Security for data in motion that we\u2019ve implemented and released in the market is DTLS or TLS over DDS, as we are transport agnostic.  We\u2019ve also implemented a key server for DTLS over DDS that has a fair bit of fault tolerance built in.", "LS", "Hey there ", ", welcome to the community! It\u2019ll take a little while to get a handle on the interplay between DDS and ROS 2\u2019s security features, we\u2019ve all been there ", "  . ", " should help to bootstrap you somewhat.", "The challenge I see in this with regards to ROS 2\u2019s security features is that the DDS features we\u2019re relying on right now are the built-in plugins from DDS-Security. As such they\u2019re a bit prescriptive in the cryptography department (see section 9.5 of the ", "), for example:", "DDS:Crypto:AES-GCM-GMAC provides authenticated encryption using Advanced Encryption Standard (AES) in Galois Counter Mode ( AES-GCM). It supports two AES key sizes:  128 bitsand 256 bits. It may also provide additional reader-specific message authentication codes (MACs) using Galois MAC ( AES-GMAC).", "In order to do anything else, I think we\u2019ll need new plugins that don\u2019t follow the built-in spec, but still abide by the overall interface (section 8.5 of the DDS-Security spec). That\u2019s probably not an insignificant amount of work, particularly considering that this is on a per-DDS-implementation basis.", "Regarding just swapping wolfSSL for whatever library may or may not be used, that will again be down at the DDS implementation level. The ROS security features build on top of that.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/wolfssl-and-wolfcrypt-for-ros/12266"}
,{"title": "ROSCon 2019 Macau: Videos are Live!", "thread_contents": ["ROSCon 2019 Macau videos are now live on ", " and the ", " along with slides! We apologize for the delay in getting the videos out. Please let us know what talks you found the most helpful as your feedback helps the ROSCon review committee make better selections next year.", "We would like to thank ", " for making it possible to share these videos with our global community.", "We also want to thank or ROSCon speakers, volunteers, and sponsors that make the event possible.", "\n", "Thank you, ", "! Been looking forward to these.", "Awesome! Thanks so much!", "Thanks a lot for posting them! I did not have the chance to attend any of the speeches and the agenda was impressive. Eager to start consuming them!", "Thank you~~! I 've been looking forward to these videos.", "Its great making these videos available soon. Very eager to experience this! Lets enjoy this weekend with these\u2026", "Hey, turns out if I didn\u2019t talk ", " fast wouldn\u2019t have fit in my time slot ", " Almost 20:00 on the dot.", "Dear ", ",", "there is a funny mistake. I did not do the lightning talk but I created the Awesome Robotic Tooling List:", "\n", "\n", "The talk is done by Florian Friesdorf, Co-Funder of ", "Thanks for the fame ", "\nTobias Augspurger", "Thank you for waiting for the publication of ROSCon2019 videos until after ROS-Industrial Conference 2019 at Fraunhofer IPA. ", "Enjoy Christmas holidays watching what happened in Macau.", "\nIn January we will then publish videos form our event in Germany. ", "The talk is done by Florian Friesdorf, Co-Funder of ", "Thanks for the fame ", "\nTobias Augspurger", "Thanks for the notice ", " we\u2019ve updated the listing.", "If you\u2019re like me, and never actually sit down to watch videos, but have lots of commute time to burn, I\u2019ve been ripping the audio and putting it in a ", ".", "The MPAA\u2019s gonna get you.", " you are brilliant and I might crib your idea. As of yesterday I have a master CSV of all 320 ROSCon talks and their associated metadata (we\u2019re in the process of giving all of them DOIs). Now that we have the data in one place it should be super straightforward to make a podcast for every ROSCon talk. Perhaps what I can do is generate a podcast for every talk then make a master list of ROSCon presentations that\u2019s easy to search.", "Do you happen to have an ffpeg configuration you like for podcasts? Getting ffmpeg params right is always half the battle.", "A csv of all the talks would have made this a bit easier. I didn\u2019t use any ffmpeg params, I just ripped the audio track from vimeo and then used pydub to add the intro/outro.", "Super, super weird question - a little off topic but wanted to bring it up.", "I\u2019m curious about the ownership of the talks. They\u2019re posted on the OSRF Vimeo - no issue - but then under a creative commons license. This basically says they have to credit \u201cyou\u201d but who is the \u201cyou\u201d \u2013 OSRF or the presenter?", "Redistribution of the talks as podcasts, with titles and metadata including the author, clearly meets the CC intended licenses either way. But this seems a little ambiguous to me having been on both sides of this and I don\u2019t recall signing or agreeing to anything in submission of presentations releasing IP rights to the presentations for OSRF to redistribute.", "(so, so not trying to make a problem, just looking for clarification since this topic kind of sparked my internal \u201copen source license review\u201d monologue)", "My understanding of the relevant copyright rules is that:", "Open Robotics, as the organization doing the recording, owns the videos. So the attribution required by the CC license should be to Open Robotics.", "The videos are themselves derivative works of the presenters\u2019 on-stage performances, which the presenters own. Before each ROSCon we notify presenters that their presentations will be video recorded and that we will distribute the resulting videos. Over the years we\u2019ve had one or two presenters not want to be recorded (e.g., because their employer doesn\u2019t allow it), and we\u2019ve been responsive to those opt-out requests.", "Thanks for raising the issue. We\u2019ll make the situation more explicit for presenters in future editions of the conference.", "That all makes sense to me. I think that closes the loop on most things. I think then the only topic that is open ended is the request to post slides on the ROSCon website.", "Memory serves its an email that comes along with a link to upload the slides. There may be some statement of rights to agree to when uploading the slides, off hand I do not recall. I think if the video itself is derivative and the slides provided as appropriately signed off, that makes things as iron clad as can be expected.", "I think then the only topic that is open ended is the request to post slides on the ROSCon website.", "As with the videos, we make it clear when requesting the slides that we\u2019re going to distribute them, and we abide by the wishes of those who opt out.", "Unlike the videos, we (Open Robotics) don\u2019t want any ownership of the presenters\u2019 slides. We just want the presenters\u2019 permission to distribute the slides. We don\u2019t declare any license regarding the rights of others to use or redistribute the slides that we post.", "As a result I believe that we\u2019re in the default situation for copyright of \u201call rights reserved\u201d, which is to say that individuals can download and view the slides but cannot, for example, redistribute them, perform them (i.e., give a presentation using those slides), or create derivative works from them (e.g., copy slides, images, or text from slides verbatim into other media) without permission of the presenter (and/or possibly the presenter\u2019s employer).", "Like with the videos, for future editions of the conference, we\u2019ll consider additional language in presenter interactions and on the website to make the situation more explicit.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/roscon-2019-macau-videos-are-live/11925"}
,{"title": "Migrating ROS/ROS2 off of catkin/colcon (towards pure CMake)", "thread_contents": ["One of the biggest difficulties I had when I started working with ROS and ROS2 this year was learning how to deal with catkin and colcon. The documentation for these tools is absolutely terrible*, they make bizarre choices for options (colcon\u2019s idea of \u201c\u2013verbose\u201d is \u201c\u2013event-handlers=console_cohesion+\u201d. The prosecution rests), and I\u2019ve found that they really get in the way of managing a large codebase. For example, if I want all my code to build with warnings as errors, I need to set -Werror in each package\u2019s CMakeLists.txt, and if a new package is added I need to make sure to set it there as well. Compare this with a project defined with CMake at the top level, I just need to set CMAKE_C_FLAGS in the root CMakeLists.txt and that\u2019s it.", "When I finally understood how catkin/colcon look through the folder hierarchy for package.xml files and use those to build a dependency tree and then parallelize builds I started to appreciate that. I guess that this package based architecture is also supposed to be useful to multi-language builds? I haven\u2019t tried that myself, but I\u2019m guessing it still needs to go through CMake?", "What I\u2019d really like to see is support from OSRF for building ROS code in a pure CMake way, in addition to the catkin/colcon package.xml way. I tried to do this and got pretty close, but ran into some roadblocks. For a ROS1 project, I managed to get everything to build the standard CMake way (mkdir build, cd build, cmake \u2026, make) but it took 1-2 minutes for CMake itself to run (this was for a couple dozen packages). It was mostly really sluggish on various catkin macros. For ROS2, I tried to clone the demos repo (", ") and build the pendulum_msgs package with CMake. It didn\u2019t produce the setup.bash file. Running make install produced a local_setup.bash file, but not the setup.bash file. Apparently that\u2019s done through a colcon extension.", "So to reiterate the question, would OSRF be interested in supporting a pure CMake build of ROS or ROS2? I think it could really benefit the ROS community to at least have this option alongside the catkin/colcon build system. It gives a better onramp experience for those familiar with those tools, and gives more debugging options for those who already use catkin/colcon.", "* If you look up catkin documentation, you\u2019ll probably land on this page ", " which describes catkin as \u201cLow-level build system macros and infrastructure for ROS.\u201d. This tells me virtually nothing about what it is or what it does. The page proceeds to jump into how to install catkin and points the user to another documentation page for a user guide, which just jumps into the package format without explaining what it is. The conceptual overview page is not much better, it is similarly too vague and high level to be useful (\u201ccatkin was designed to be more conventional than rosbuild\u201d <- what\u2019s rosbuild? why is it unconventional?)", "The documentation for these tools is absolutely terrible", "These?", "They look pretty good to me.", "When I finally understood how catkin/colcon look through the folder hierarchy for package.xml files and use those to build a dependency tree and then parallelize builds I started to appreciate that.", "I\u2019m not a build system expert, but how would you achieve this with pure CMake? ROS systems are very modular/package-oriented. Seems like this is a pretty good argument in favour of catkin & colcon.", "For example, if I want all my code to build with warnings as errors, I need to set -Werror in each package\u2019s CMakeLists.txt, and if a new package is added I need to make sure to set it there as well. Compare this with a project defined with CMake at the top level, I just need to set CMAKE_C_FLAGS in the root CMakeLists.txt and that\u2019s it.", "You ", " pass CMake arguments through catkin & colcon (", ", see the documentation above). With colcon you can even move that to a ", " file so that you don\u2019t need to include it in your command every time (see documentation).", "Again, I\u2019m not an expert, so I\u2019ll let other people chime in, but I kind of fail to see the point.", "Please, also note that there is \u201ccatkin\u201d (which provides \u201ccatkin_make\u201d and \u201ccatkin_make_isolated\u201d binaries), and then there are \u201ccatkin_tools\u201d (which provide the \u201ccatkin\u201d binary). These are two distinct tools which both utilize the python/CMake Catkin library inside, but expose a different CLI. The docs referenced by ", " refer to \u201ccatkin\u201d package and the CMake library (and these really are a bit messy), whereas ", " refers to \u201ccatkin_tools\u201d (whose docs are great).", "One more question to make this thread more focused - are you interested in building the whole ROS system from scratch with pure CMake, or is it enough for you to build a few packages that depend on system-installed ROS packages? I think it makes a big difference whether you want the core packages to be buildable by plain CMake or not\u2026", "Also note that there might (should) be other tools involved in the build process. E.g. rosdep for installing dependencies. Would you like to get rid of that, too? It seems like a step back to me\u2026", " I hadn\u2019t seen those links. They echo my existing complaints about the documentation, which is essentially that it\u2019s written for audience that already knows how the tool works. For example, the docs don\u2019t mention that you need a directory called ", " in the same directory in which you invoke catkin, or that it\u2019s making a separate CMake project for each package.xml it finds. But the documentation is more of an annoyance rather than my main point.", " Thanks for your points. I think your discussion about catkin helps to paint just how confusing it is, but again I digress ", "I think it would be really cool to be able to build the whole ROS system from scratch with CMake, but to start out with it probably makes more sense to start with individual packages, like I mentioned in the example I gave with ros2 demos pendulum_msgs, and then move on to larger groups of packages, like the whole ros2 demos repo.", "Imagine a model like python, where there\u2019s lot of packages, but there\u2019s freedom as to whether you want to package your code or not. With ROS, if you just want to use the C/C++ libraries and headers, you can, but you have to sort of bend over backwards to do it (I tried).", "The whole reason ROS is package oriented is because the build tools won\u2019t allow you to create ROS code any other way. Again going back to the python example, there\u2019s some tooling to help you make a package, but it\u2019s entirely optional.", "As with the python example, they have pip and so I think tools like rosdep would remain necessary, although I haven\u2019t used rosdep very heavily myself so I\u2019m not sure if pip is a good analogy?", "And could you describe your idea of the toplevel CMakeLists.txt? Would you order the packages to satisfy dependencies in your head and then \u201cadd_subdirectory\u201d each of them?", "Single packages can be built using plain CMake pretty well. I do it all the time within CLion. Dependencies and inclusion of 3rd-party libs not installed in the system is where it gets tough.", "Otherwise, the analogy between rosdep and pip is quite good ", "Sorry you didn\u2019t find what you were looking for right off the bat, but maybe my answers can help clear some things up for you.", "I think it would be really cool to be able to build the whole ROS system from scratch with CMake", "There are a lot of reasons this will be difficult. I\u2019ll comment on that below.", "but to start out with it probably makes more sense to start with individual packages, like I mentioned in the example I gave with ros2 demos pendulum_msgs", "For ROS 1, if you want to build individual packages with cmake, you can. Just do the normal ", ". The catch is that you need to make sure that all of the dependencies of that package are installed and on the ", " and that any other necessary environment variables are set, e.g. ", " or ", ". This is part of what the ", " does. This should work for any ROS 1 package, as they are all catkin and based on CMake or even are pure CMake.", "If you want to build a library or executable with C++ and do not want to use ", " or ", ", you can do that too, with some exceptions. ", " made examples of this a long time ago:", "Contribute to gerkey/ros1_external_use development by creating an account on GitHub.", "Notably, you cannot generate messages with just CMake, as we need a bunch of functionality provided by CMake macros in ", " to make that work easily. It would be possible to make that easier to do from \u201cpure\u201d CMake, but no one has had the need.", "For ROS 2, you can still build any individual package without ", " or any special tools. Some of the packages are CMake but use ", " (equivalent to ", " from ROS 1) CMake macros, some are pure CMake, but others are something else, e.g. pure Python packages which use ", " or pure Java packages that don\u2019t use CMake at all. For the first two cases you can again use ", ", so long as the dependencies are installed and all the environment variables are setup.", "Again, if you want to make executables or libraries using C++ that depend on ROS, you can do that with pure CMake if you want using the ", " branch on ", "\u2019s repository:", "Contribute to gerkey/ros1_external_use development by creating an account on GitHub.", "As for why we don\u2019t build multiple packages at once in a single CMake invocation\u2026 there are a lot of reasons, but first I\u2019ll say you can do this in ROS 1.", "We started out with ", " which does exactly this, it adds a group of packages you have locally to a single cmake project using ", ". In ROS 2, with ", ", we explicitly decided to no longer do that. See:", "One common technical hurdle, just to give one example, is that if two packages define the same target name, e.g. ", ", then adding them to the same CMake context will cause a failure during configuration.", "For the core ROS 1 packages we made sure this wasn\u2019t an issue (we regularly built everything with ", " all at once and fixed any issues), but we found that for users it didn\u2019t scale. Because they would take packages which were developed separately, and try to add them together where before that never happened, then they\u2019d run into issues, an error if they\u2019re lucky or cross-talk from global state in CMake if they\u2019re not lucky\u2026", "Also, since in ROS 2 we don\u2019t have all CMake based packages (we have pure Python and others like Java or Rust), it\u2019s not always convenient to include them into a CMake project. We could use ", " or ", ", but we feel that this is much more difficult to implement and maintain than doing the same thing essentially except in Python as an extension to ", " or ", ".", "Now for some random comments.", "colcon\u2019s idea of \u201c\u2013verbose\u201d is \u201c\u2013event-handlers=console_cohesion+\u201d. The prosecution rests", "I don\u2019t think that\u2019s ergonomic either\u2026", "I\u2019ve found that they really get in the way of managing a large codebase", "These tools are for handling highly federated packages, it doesn\u2019t impact large code bases in my opinion. You may have a single package for yourself which contains as much as you want, in a single CMake project, modularized how ever you like\u2026", "How does it get in the way?", "For example, if I want all my code to build with warnings as errors, I need to set -Werror in each package\u2019s CMakeLists.txt, and if a new package is added I need to make sure to set it there as well. Compare this with a project defined with CMake at the top level, I just need to set CMAKE_C_FLAGS in the root CMakeLists.txt and that\u2019s it.", "If you just do ", " it will have the same effect\u2026 I use it all the time. You shouldn\u2019t need to edit other peoples code in order to turn on warnings or configure them as errors. I personally use ", " so I don\u2019t have to specify it each time.", "In the case that you set the ", " in the \u201croot\u201d ", ", what happens if one of the sub projects (that you didn\u2019t write) also sets it and wasn\u2019t expecting it to be set already?", "When I finally understood how catkin/colcon look through the folder hierarchy for package.xml files and use those to build a dependency tree and then parallelize builds I started to appreciate that. I guess that this package based architecture is also supposed to be useful to multi-language builds?", "Yes, sorry if I reiterated that above without acknowledging it until now.", "I haven\u2019t tried that myself, but I\u2019m guessing it still needs to go through CMake?", "For ROS 1, everything is CMake based, so yes. But for ROS 2 and with ", ", no, something things do not use CMake at all.", "So to reiterate the question, would OSRF be interested in supporting a pure CMake build of ROS or ROS2? I think it could really benefit the ROS community to at least have this option alongside the catkin/colcon build system. It gives a better onramp experience for those familiar with those tools, and gives more debugging options for those who already use catkin/colcon.", "If you mean, \u201cbuilding more than one package at a time in a single CMake invocation\u201d, then I\u2019d say not really. In ROS 1 you can do this for many packages, this is exactly what ", " does. In ROS 2, it may or may not work, but since there\u2019s non-CMake stuff in the dependency tree then I\u2019d say it\u2019s unlikely to be easy.", "The whole reason ROS is package oriented is because the build tools won\u2019t allow you to create ROS code any other way.", "I don\u2019t think that\u2019s right. The reason it\u2019s package based (federated) is that otherwise anyone wanting to contribute something to the ecosystem would need to get us to accept the change.", "Take OpenCV as a counter example, they have a single CMake project, so if you want to add a new feature, it either has to be a separate CMake project (in a different repository) or you have to get a merge request through their process. Now, the former is fine at first, your README will be like \u201cinstall or build OpenCV, then cmake/make my project\u201d. But at scale this falls apart, it becomes, \u201cinstall or build OpenCV, some other persons addition, and then some other addition that uses that, and \u2026, then my project\u201d. For robotics it becomes even more complicated as instead of chains you end up with graphs of dependencies due to there being multiple sources of information and separate systems working in parallel.", "As with the python example, they have pip and so I think tools like rosdep would remain necessary, although I haven\u2019t used rosdep very heavily myself so I\u2019m not sure if pip is a good analogy?", " performs many similar functions, but importantly it only works for Python, if you mix languages, as we do, then you need tools that are broader. Our pure Python packages in ROS 2 use ", " (which is what ", " uses) and ", " will call ", " to install dependencies for you. So we\u2019re not reinventing the wheel in that case.", "These are two distinct tools which both utilize the python/CMake Catkin library inside, but expose a different CLI.", "True! Honestly I thought everything (e.g. tutorials and all) had moved to ", ".", "I hadn\u2019t seen those links. They echo my existing complaints about the documentation, which is essentially that it\u2019s written for audience that already knows how the tool works.", "Fair point. Perhaps that could be revisited.", "I like the pip vs. single package analogy as well. However, like ", " mentions there\u2019s a few other things/layers to consider!", " Thank you for the detailed reply!", "These tools are for handling highly federated packages, it doesn\u2019t impact large code bases in my opinion. You may have a single package for yourself which contains as much as you want, in a single CMake project, modularized how ever you like\u2026", "How does it get in the way?", "I think I gave a bad example. Here\u2019s another one: Let\u2019s say I want to run something before my build, maybe a script to install dependencies or create a timestamp file or something. With CMake I could add an ", " in the toplevel CMakeLists.txt, but with colcon I can\u2019t do anything of the sort. I could wrap the command I want to run alongside the colcon call in a build script and make that my build command, but I don\u2019t like the path this goes down. Custom build scripts have a tendency towards becoming dumping grounds, and it\u2019s hard to keep the documentation up to date with the functionality. I really value having an easy onramp experience for a project, so the idea of being able to use the standard ", " is really appealing.", "Here\u2019s another one: I wanted to add static analysis to my project, but the tool I was using, cppcheck, expects one project. I ended up exporting compile_commands.json for each package and calling cppcheck for each one individually.", "Here\u2019s another one: CLion is designed to work with a single CMake project, so I can\u2019t use it for my overall project, or rather I can, but I have to give up some features of its CMake integration.", "The point is, if I could structure my project with a toplevel CMake build, it would make certain tools and integrations much easier, not to mention onboarding.", "I don\u2019t think that\u2019s right. The reason it\u2019s package based (federated) is that otherwise anyone wanting to contribute something to the ecosystem would need to get us to accept the change.", "Take OpenCV as a counter example, they have a single CMake project, so if you want to add a new feature, it either has to be a separate CMake project (in a different repository) or you have to get a merge request through their process. Now, the former is fine at first, your README will be like \u201cinstall or build OpenCV, then cmake/make my project\u201d. But at scale this falls apart, it becomes, \u201cinstall or build OpenCV, some other persons addition, and then some other addition that uses that, and \u2026, then my project\u201d. For robotics it becomes even more complicated as instead of chains you end up with graphs of dependencies due to there being multiple sources of information and separate systems working in parallel.", "No, definitely not. There\u2019s definitely not a need to require all contributions to the ecosystem to be approved by OSRF. I think we\u2019re conflating package management with building here, and again pip provides a good model with requirements.txt. Of course, as you point out, a pip package only needs to specify compatibility with a limited set python versions, whereas system managing Python/C/C++/Rust/Java packages needs to keep track of Python version, gcc version, architecture, rustc version, javac version, etc.", "The current model is clearly working, as there is a large and thriving ecosystem around ROS, but I think there are ways to improve while keeping the aspects of it that work. Some concrete suggestions:", "To your point about how a single-context build leads to issues like name collision on targets, you\u2019re right. I would argue that the end-user should choose whether or not they want to deal with those issues and accept the tradeoff in order to have a single context build that can more easily integrate with various tools. Right now the user is effectively being blocked from doing so. I doubt this was intentional, but with the number of things that are needlessly coupled with the build system in an undocumented fashion (see the rosmsg example), this is effectively the result. While for ROS1, and even ROS2 to some degree, one can claim that it\u2019s possible to build without catkin/colcon, this is true only in a very technical and academic sense and I think it would be great if this were more of a first class feature of ROS.", "The OP of ", " brought up some interesting points, but the later comments seem to go more into the direction of \u201cwe need better documentation\u201d and \u201cmy use-case/preferred use of a build tool isn\u2019t sufficiently being covered by catkin_make/catkin_tools/colcon\u201d. Those are valid concerns, but the thread started with \u201chow about doing all of this with pure CMake?\u201d and I\u2019d like to hear more about that part specifically.", "Over the years there have been quite a few comments about \u201cwhy a custom build system/tool?\u201d. I expect ", " may have some comments on the rationale for Colcon fi, but as ", " mentioned, some of it has to do with managing sets of packages with (inter)dependencies (and their dependencies, and their dependencies, and so on).", "When I finally understood how catkin/colcon look through the folder hierarchy for package.xml files and use those to build a dependency tree and then parallelize builds I started to appreciate that.", "I\u2019m not a build system expert, but how would you achieve this with pure CMake? ROS systems are very modular/package-oriented. Seems like this is a pretty good argument in favour of catkin & colcon.", ": could you comment on how you\u2019d approach this with pure CMake projects? Especially also with multiple different types of projects and many of which may not use CMake or a language that needs to be compiled or needs special setup of the (build) environment for other projects to be able to find them?", "The whole reason ROS is package oriented is because the build tools won\u2019t allow you to create ROS code any other way.", "This is a strong statement which I doubt is true (Python nodes don\u2019t need to be Catkin / ROS packages at all fi). The most you could say (and based on my understanding) is that the build tools we have work in a certain way because ROS uses a package based approach to distribution of code. Not the other way around.", "I would argue that the end-user should choose whether or not they want to deal with those issues and accept the tradeoff in order to have a single context build that can more easily integrate with various tools. Right now the user is effectively being blocked from doing so. I doubt this was intentional, but with the number of things that are needlessly coupled with the build system in an undocumented fashion (see the rosmsg example), this is effectively the result.", "I\u2019m not sure it was completely intentional, but freedom-", "-choice is a very valuable thing for many users. Especially those not up-to-speed on how build tools work but \u201cjust want their packages built\u201d.", "This invariably means that more advanced (or involved) use-cases are going to be slightly less supported (or somewhat more involved to get working), but it doesn\u2019t mean that everything is horrible. Just that different choices were made.", "Being able to \u201cchoose whether or not they want to deal with those issues and accept the tradeoff in order to have a single context build that can more easily integrate with various tools\u201d is valuable, but it would also mean the (average?) user would have to know ", " to make that choice, which is not apparent and would make it even harder to get started.", "I would also add that you have to remember that quite a bit of what you might see as duplication or unnecessary has only become unnecessary because modern CMake started doing many of the same things over the past few years (exporting target dependencies, interface only libraries, transitive dependency resolution, federated builds, etc).", "Take OpenCV as a counter example, they have a single CMake project, so if you want to add a new feature, it either has to be a separate CMake project (in a different repository) or you have to get a merge request through their process. Now, the former is fine at first, your README will be like \u201cinstall or build OpenCV, then cmake/make my project\u201d. ", "Admittedly I\u2019m very familiar with the way Catkin & Colcon work (so I\u2019m biased), but really, this (ie: the emphasised part) is one of the things I like about the tools we use. It\u2019s very, ", " frustrating to deal with projects that don\u2019t use something similar and require me to follow 18+ steps in a readme with every step running ", ", ", ", ", " or other tools manually (and 18+ points at which I could make a mistake, something doesn\u2019t work, or I used an incorrect or unexpected way of specifying flags, build options or other configuration).", "The point is, if I could structure my project with a toplevel CMake build, it would make certain tools and integrations much easier", "True, but it would also complicate quite a few other use-cases (if not done carefully), many of which are part of the core development workflow of many of us, such as building packages from others without having to deal with their dependencies, not having to care about which package(s) to build first, etc. Granted, this is not all perfect, but I would not like to start doing that manually again.", "[\u2026] not to mention onboarding.", "This reads as if you have a particular target audience in mind for which this would be true. Could you elaborate?", "Personally, for a few of my target audiences (students fi), it\u2019s been a blessing to be able to summarise the entire build process as:", "First, run ", " and then run ", " in the root of the workspace and wait for it to finish", "instead of:", "For every project, please ", ", then ", ", then ", ". Do not run this in the ", " directory (or wherever the project stores its source files). Solve any problems it notifies you of, then run ", " and then ", ". Be sure to specify a suitable ", " and/or other way of specifying where the installable artefacts should go, and please make sure to not try and run ", ". Also please do not do any of this for non-CMake projects (which don\u2019t have ", "). Use the regular ", " for Python (add ", " for a non-system-wide install, you should know when you\u2019d want that). If you are trying to build a Java based package, then do whatever you\u2019d do with those. For other languages / build systems: please follow the readme, but make sure to maintain a sane build environment at all times.", "And above all, please make sure you figure out the proper build order across all packages (both your own ", " those you downloaded earlier from others), and make sure to have all dependencies installed first.", "This is of course exaggerated but helps get the point across.", "The current model is clearly working, as there is a large and thriving ecosystem around ROS, but I think there are ways to improve while keeping the aspects of it that work. Some concrete suggestions", "In my experience, one of the best ways of seeing these suggestions implemented would be to show that they work. Show an example CMake-only build of a workspace with a representative set of packages (ie: containing ", ", ", " (or ROS 2 ", "), multiple different languages, specific environment setup hooks, etc). Show how you\u2019d deal with dependency management and how you\u2019d support the federated workflow.", "I\u2019m not saying \u201cyou get something for free so you cannot complain\u201d, but with the community as it is right now, code is king and is how you get things changed.", "code is king and is how you get things changed.", "If I wanted to make a PR I\u2019d do it. In fact I\u2019ve made several PRs against ROS2 documentation. However the issue I\u2019m trying to bring to light is not one that\u2019s solvable in a single PR, or even a couple. And any change I make could easily be reversed by a future PR that perpetuates the existing model in which \u2018catkin/colcon is king\u2019.", "That\u2019s why I\u2019m raising this in a thread, because I want to see if there\u2019s any sympathy or currency to these ideas and if so, to try to gather some resources to move things in a direction in which catkin/colcon is not the only game in town.", "freedom- ", " -choice is a very valuable thing for many users", "I agree, but freedom-of-choice and freedom-from-choice do not have to be in opposition. The way to structure a system to have both is to build it on simpler underlying ideas that can be combined in multiple ways, and provide a canonical implementation(s) that combine those ideas in a particular way. The first part gives freedom-of-choice, the second provides a freedom-from-choice.", "To give an example, consider the organization of code in C/C++. Most of the time it is organized in header and source files, but this is not strictly necessary. ", " is simply copy paste, so if you want to ", " a source file, you can. I think I did that once in order to test a static function in a source file (I suppose maybe I could\u2019ve used a prototype but I digress).", "To give a bad example, consider CMake. Currently it\u2019s possible to use both old-style and Modern CMake in the same project, and I would say there isn\u2019t enough reference documentation using Modern CMake to make it clear which version one should use. Slowly, more and more Stack Overflow answers are getting comments like \u201cprefer target_include_directories over include_directories\u201d, but overall I\u2019d agree with you that the multitude of options provided by CMake is confusing and makes it a difficult tool to pick up for both students and professionals alike.", "To return to the main point, I think the ROS ecosystem would benefit a lot from this approach of having a canonical way of doing things, but providing enough support and documentation for those who want to try something on their own to be able to do so. Right now it\u2019s either catkin\u2019s way or the highway.", "To help motivate the discussion, I\u2019ve modified the geometry2 package to build with pure CMake ", ". This is meant for discussion purposes as opposed to an actual proposal against geometry2, so please don\u2019t bash it too much, but if you clone the repo at that commit you\u2019ll be able to ", ". You\u2019ll notice that the CMake step takes 10 seconds. Normally I\u2019m not one to complain about 10 seconds, but you\u2019ll notice this scales with the number of packages (try commenting out the last 5 ", " commands and time it again. Hint: ", " in front of a command will time it, i.e. ", "). For a project with 60 packages, this means 60 seconds for every change in a cmake file, even just a comment change. The extra time is coming from the catkin_package call. In order to make it practical for people to use plain CMake in their projects, I think this has to be addressed, does that seem reasonable? Obviously catkin projects would benefit as well, but with the distributed nature of the catkin build I think it would be less noticeable.", "Of course, the overall trajectory I\u2019m proposing would require many more steps beyond optimizing ", ", in particular colcon might have to take a few steps back as it relies on a non-CMake extension to generate setup.bash, but how do people feel about moving towards a model where BYO-Tool is supported, but there\u2019s a canonical path for those just want to use ROS and don\u2019t want to be bothered with getting up to speed on build tools?", "It\u2019s not a complete solution, but I do builds through a single top-level CMakeLists to play nice with a CMake-based IDE (CLion).", "Here\u2019s my top-level CMakeLists.txt file, which uses Colcon to gather packages and build them. It can\u2019t successfully build the ", " ROS2 C++ stack for various reasons:", "Still, it\u2019s good enough to do multi-package development on C++ stacks. It could probably also be extended by using ", " commands instead of ", ".", "\n", " +1 for CLion. Have you discovered setting up remote build/run/debugging using ssh yet? ", "  I just got it working so I can build and debug ROS2 modules on my RPI from my desktop computer.", "I am a big fan of cmake and the more we can lean on cmake vs colcon I am in favor of. There are some weaknesses in cmake where colcon fits in, so I am not sure it can completely go away.", "To add a note about the geometry2 example above, the overall goal would be to get it to compile without the catkin_package macro, instead using cmake\u2019s target_add_dependency. As it is in the commit I linked, it relies on catkin_package generating the cmake files used by find_package, and the order of the add_subdirectory commands reflects the dependency tree. Other steps that would be required to achieve this would be modifying generate_messages to produce a target, and either documenting rosmsg/roslaunch well enough that anyone could add what\u2019s necessary to their project in order to use those tools, or making some new CMake macros that can automatically add what\u2019s necessary to use those tools (or both). I think it would also be necessary to add documentation/macros for setup.bash. Hopefully this helps clear up the picture of what I\u2019m talking about for those who were asking what a pure CMake build would look like.", ": could you comment on how you\u2019d approach this with pure CMake projects? Especially also with multiple different types of projects and many of which may not use CMake or a language that needs to be compiled or needs special setup of the (build) environment for other projects to be able to find them?", "I can\u2019t speak to handling dependencies for languages other than C/C++ here, but in all of my ROS/ROS2/DDS projects, I have used Conan for both build tool dependencies and library dependencies with fairly great success. Conan has come a long way and I\u2019ve used it in production for a couple of projects, one of which had support for multi-arch builds as well. It has excellent integration support with CMake and overall helps to create highly reproducible builds. This matters a LOT in production environments. I really hate to rely on system dependencies in any of my production software, as you can very easily end up with non-reproducible builds if you don\u2019t take a ton of precautions to lock system dependencies down, which can often be complicated by things like automatic upgrades of packages, optional dependencies, etc. Conan does have support for system dependencies if you absolutely must rely on them (generally because some dependency has not been packaged for Conan yet). Additionally, many of the most popular C/C++ libraries are available as Conan packages already.", "I\u2019ve fantasized about the ability to simply include ROS2 and the various components as a Conan dependency, and target_link_libraries( ros2:: ) in my CMakeLists.txt, and just \u2018cmake \u2026 && make\u2019 without the need to install anything as a system dependency or go through the colcon workflow. Additionally, the IDL generation tools can be included as a build dependency, much the same way you might use <build_depends> in package.xml.", "I\u2019ve gone through most of this workflow for pure DDS systems (RTI and FastRTPS) but haven\u2019t dug nearly deep enough into all of the various components and tools necessary to achieve this for ROS2. If there is interest in this, I\u2019d be happy to help in the effort and provide support on the Conan packaging front.", ", thanks for raising this important topic! We\u2019re always open to suggestions on how to improve the user and developer experience.", " and others already covered much of what I would have said, so I\u2019ll just add a few thoughts:", "What I\u2019d really like to see is support from OSRF for building ROS code in a pure CMake way", "I don\u2019t see this goal as an end in itself, but rather an approach to addressing some underlying shortcomings in the current workflow.", "If I\u2019m a user of a project, then so long as I have a reliable way to install it and then a well-documented way to use it in a manner that\u2019s familiar to me, then I\u2019m satisfied and I don\u2019t worry about how the developers of the project make the sausage. If I want to start contributing to the project as a developer, then I expect that I\u2019ll have to adopt the conventions (code style, file organization) and workflows (CI, code review, test running, build procedures) that the project uses. Eventually, once I\u2019ve established myself among that developer community, I might propose changes to those conventions and workflows, but I\u2019ll assume that the status quo is likely to be preferred so long as it\u2019s working well for the project.", "So, rather than saying that ROS should be built using pure CMake, can we figure out what the underlying problems are? I\u2019ve seen two issues raised here so far:", "I\u2019ll address each in turn below:", "build it on simpler underlying ideas that can be combined in multiple ways, and provide a canonical implementation(s) that combine those ideas in a particular way.", "That\u2019s a clear, concise, and desirable design goal. To my knowledge, we are achieving it, albeit with areas for improvement.", "In creating the ", " of using ROS 1 packages without ", " or other ROS-specific tools, I repeatedly experienced two alternating feelings:", "This back-and-forth is perhaps best embodied by the ", " of doing ROS 1 message-generation directly from ", " and ", ". I\u2019m happy that it works, but it was hard to figure out and the resulting workflow is not especially pretty.", "Personally, I\u2019d be happy to see improved support for these \u201cexternal\u201d use cases. When they\u2019re difficult, it\u2019s not because we intend them to be or because we want to force ROS users to use the workflows we use when developing ROS. It\u2019s just that we don\u2019t regularly exercise those external use cases and as a result they suffer. I\u2019d welcome PRs (to code and docs) that make the underlying tools easier to invoke directly.", "We build ROS from source in a package-by-package process. As others have noted, there are important advantages to this approach, including inter-package isolation of CMake targets and variables, and support for non-CMake packages.", "But as many of us have experienced, a big cost to this approach is that it\u2019s slow. The constant overhead of invoking ", " (or ", ", or another tool) once per package adds up quickly. There\u2019s clearly a lot of redundant work happening over repeated builds and it costs time to developers and CI systems.", "I don\u2019t have a particular solution to suggest here. It would be great to see proposals for how to improve the speed of a ROS build without sacrificing the established advantages of the per-package build approach.", "Finally: others have pointed it out, but I want to emphasize that there\u2019s a distinction between two things:", "A library of CMake macros such as ", " or ", ". In my experience, every substantial project that uses CMake ends up having a library of CMake macros that are reused throughout the project. We\u2019ve just been more explicit than most projects by externalizing that library into a separately named entity. And while we find those macros to be useful and convenient, we don\u2019t force anyone else to use them. In any case, using macros like we do is still, in my opinion, ", ".", "An executable tool used to build a bunch of packages, such as ", " (packaged with the ", " CMake macros), ", " (distributed separately in ", "), or ", ". (You\u2019d be forgiven for confusion over the naming for the ", "-related software. That confusion is one reason that ", " and ", " are named differently.) This one is potentially more controversial, as we\u2019re recommending that you run ", " to build (potentially) ", ". It should always be possible (and ideally easy) to build your code using whatever tool you prefer. Wherever that\u2019s not the case, we should fix it!", ": I would be glad if you could share your insights and experiences with ROS2 and conan.", "Ease of use of ROS software from outside ROS", "This is exactly a use case I\u2019m dealing with: a rather simple (from build perspective) single executable, which interacts with ROS/Ignition simulation. I don\u2019t care about how ROS itself is built, but I do care how my simple application is built.", "Here are the problems I\u2019ve run into. Firstly, I\u2019m a beginner ROS user and on top of that, I was just thrown from Windows to Linux, Visual Studio to VS Code with CMake, naked executable to Docker image, etc. In general, I\u2019m dealing with completely new development environment and a lot of confusion. Understandably, confusion minimization is my goal, at least in initial stages. My project requires a single executable, but ", " forces me into a workspace configured for potentially multiple packages. Visual Studio Code has multiple CMake extensions and a ROS extension, each with somewhat different ideas about project configuration. It took me a while sifting through documentation to find out relationships between catkin workspace, ", ", ", ", VS Code autoconfigure mess, ROS packages, ", ", ", ", etc. Part of it is a necessary learning curve, but a lot of it was not needed for my simple ", " scenario. Sorry for ranting, but I thought a new user experience may add some value to this discussion.", " Would it have helped if you\u2019d encountered a tutorial that shows you how to use plain CMake to build your roscpp application, without any mention of ", "? That would be easy to do (or promote more heavily, if it already exists somewhere).", "The ", " workflow is important if you\u2019re developing multiple ROS packages that need to depend on each other, and/or you want others to be able to easily consume your ROS package(s). But if you\u2019re just building a leaf application that uses ROS, and you\u2019ve installed ROS from binaries (e.g., on Linux via ", "), you shouldn\u2019t need to know about ", ".", " Thanks for the reply. I\u2019m glad to see this topic generating some interesting discussion ", "If I\u2019m a user of a project, then so long as I have a reliable way to install it and then a well-documented way to use it in a manner that\u2019s familiar to me, then I\u2019m satisfied and I don\u2019t worry about how the developers of the project make the sausage. If I want to start contributing to the project as a developer, then I expect that I\u2019ll have to adopt the conventions (code style, file organization) and workflows (CI, code review, test running, build procedures) that the project uses.", "That\u2019s fair, but it\u2019s very possible for a project to have conventions that are, well, conventional, as opposed to internal only. Take Linux or git for example. They can be built with just ", ".", "That\u2019s a clear, concise, and desirable design goal. To my knowledge, we are achieving it, albeit with areas for improvement.", "From a robotics perspective, perhaps, but from a programming perspective I disagree. There is only one way to build software, and this is to go through catkin/cmake. Trying to cut through that complexity and just get to the headers and libraries is an exercise in frustration, as you pointed out shortly after that quote.", "Overall you\u2019re right to point out that building with pure CMake isn\u2019t the right goal, I think the goal is better support for non-catkin/colcon (or even non-cmake) workflows. And I think it\u2019s achieved by the development process of ROS being more conscious of this as a goal. Otherwise, what ends up happening is that some documentation is created, never advertised properly, and effectively lost (i.e your example repo, which I duplicated at one point because I was unaware of it).", "One other thing I\u2019d like to point out re:documentation, because I think better documentation is a big part of this, OSRF is much better positioned to consolidate and create effective documentation than a contributor. There are 4 separate pages that claim to be documentation for catkin. As a user, I have no access to user stats to see which of those sites is getting visited, and from where. And if I want to contribute to documentation, am I supposed to do it in 4 places? f that, seriously. There are good models to follow here, i.e. Eigen, CMake (they could have more examples, but at least it\u2019s all in one place). Also, there are certain undocumented decisions that inhibit the kind of development we\u2019re talking about. It\u2019s one thing to build an executable that can use ROS message bus, but it\u2019s another to build one that can take advantage of rosrun/roslaunch/rosmsg, mostly because catkin is taking care of the installation step for you and hiding all the details. I\u2019m totally fine with abstracting away details in a friendly interface, but I\u2019d love it much more if I could look up documentation about that interface and get some information as to what\u2019s going on.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["catkin: ", "\n", "colcon: ", "\n", "Performance: It takes about a second for the ", " macro to run. For a couple packages this is fine, but for our ROS1 system we had about 60+ packages, which meant every call to ", " takes 60+ seconds when we configured it to use a toplevel CMakeLists.txt. A single character changed in a single CMakeLists.txt meant 60 seconds of reconfiguring. When I did some profiling it looked like a lot of time was going to regenerating files from templates using ", "\n", "One of: Documentation/CMake macros/consolidation of requirements, in the context of being able to use ROS CLI tools like rosmsg/rosrun/roslaunch with non-catkin packages. For example, for ", " to work, the environment variable ROS_PACKAGE_PATH needs to contain a directory that has a ", " folder with the messages and a package.xml with a valid ", " tag. Having that documented somewhere, or having the ", " macro look for a valid package.xml and warn the user if it\u2019s not found (and what the consequences are) would help with decoupling ROS functionality from the build system. Arguably the package name could be taken from ROS_PACKAGE_PATH. I can provide additional notes on roslaunch/rosrun if you like. Edit: I see that gerkey\u2019s repo has the info for roslaunch/rosrun, about having a valid package.xml, having a .catkin file and having the path appear in CMAKE_PREFIX_PATHS.", "For the ROS2 side, similar documentation for the CLI tools would be great. Overall ROS documentation tends to be too high level and abstract (and spread out) to be useful. More details that are grounded in how things really work would be helpful, for example \u201ccatkin crawls your ", " directory for package.xml files, and reads them in order to create a directed acyclic graph of package dependencies, and then builds each package as a separate CMake project starting with the leaves of the tree and working upwards\u201d", "Unlike variables, functions have global scope, so two packages declaring the same function name can interfere in weird ways.", "Some packages declare the same target name (e.g. the way osrf testing tools pulls in GTest causes build problems for me).", "As a ROS user I want to build my ROS-dependent code without using ROS-specific tools.", "As a {ROS developer ", " ROS user who frequently builds ROS from source}, I want my builds of ROS to go faster.", "Hey, we really can do everything build-related outside of our usual workflow. That\u2019s great!", "Holy cow, making that work was not intuitive. We should improve our documentation and maybe also modify how some of the underlying tools work!", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/migrating-ros-ros2-off-of-catkin-colcon-towards-pure-cmake/12104"}
,{"title": "Compressed PointCloud2?", "thread_contents": ["Hi,", "I had a thought that I had on the back of my mind for a while and I would like to share it with the community.", "We know that PointClouds messages can be quite large.", "\nThe image transport allow us to send compressed images. This has great benefits when we are connecting remotely to a robot over Wifi or when recording with a rosbag.", "But we miss a similar functionality iwith even large messages, i.e. poit clouds.", "I wonder if anyone has addressed this issue already or if we can propose to have an \u201cofficial\u201d solution to this problem.", "In my mind, a potential simplistic solution would be to:", "I would personally use a very fast compression algorithm such as LZ4 (we don\u2019t want to add too much latency).", "What do you think?", "Davide", "Do you have any data on the compression efficiency for point clouds?", "\nConsidering that they are usually highly unstructured data, I\u2019m not sure if off the shelf compression algorithms perform well.", "\nI haven\u2019t tried it, though.", "In general, I think compressing point clouds is a good idea.", "\nMaybe even with a lossy option for tasks that do not need accurate data such as visualization.", "This might be worth checking out:", "\n", "Draco is a library for compressing and decompressing 3D geometric meshes and point clouds. It is intended to improve the storage and transmission of 3D graphics. - google/draco", "\n", "Pointcloud compression is an active field of research, and there are quite a few algorithms available (at least in papers). PCL already supports some aspects: ", ".", "In ROS contexts the topic has come up earlier: ", " on ROS Answers for instance, and ", "  on the ", " tracker itself.", "Up till Groovy the ", " stack even contained a ", " package: ", ".", "The ROS Answers Q&A has a comment that links to ", " which seems to be a package that implements a nr of the things you suggest ", ". Might be worth a look.", "I am thinking about lossless compression ONLY.", "I hacked a solution one year ago and my experience is that LZ4 is so fast that the time wasted to do compression and decompression is by FAR lower than the time wasted to transmit a large message over Wifi.", "Rephrasing, not only we reduced the bandwidth used, but the latency was actually better.", "We also made a solution for this last year.", "\nAlthough it\u2019s not entirely lossless, one can change voxel size to individual needs. In our lab, we run the solution on six jetson TX2 nodes that map a space of 10x10x4m in 4cm voxels ", ". We get 40.5 in compression ratio and use the point-count per voxel to calculate an intensity value.", "\nYou can read about it here: ", " (free) and try the ROS-package here: ", "Atle", "Just to mention a ", " solution to this issue we came up with a few years ago.", "While developing communication solutions for the NASA Space Robotics Challenge and its limited bandwidth (team Olympus), we investigated the possibility to convert point clouds to range images and back-- PCL <----> OpenCV. The point cloud transport would then take the shape of a usual image. If I recall correctly, this image could be further compressed as a sensor_msgs/CompressedImage.", "Given that we were converting sparse point clouds from sensor reading -as opposed to dense object mesh- this solution worked really great in terms of compression ratio and reconstruction accuracy (not so great in term of CPU consumption ^^)", "The package is not in good shape but one might be able to scavenge some code from it: ", ".", "Maybe ", " could provide some more info, possibly numbers?", "I\u2019d also suggest working on the base of ", ". This package has been written by a student of Czech Technical University a few months ago and the goal was exactly to mimick the behavior of image_transport. It implements the very same plugin behavior.", "What is already implemented is the lossy compression with draco library.", "If you want some lossless solution, it should be pretty easy to implement it as a plugin into this framework.", "As for support in RViz - there is none yet, but you can launch a node similar to image republisher (also implemented in the package) which converts the compressed pointclouds to raw PointCloud2.", "I don\u2019t have the exact numbers of the original pointcloud size. But by transporting a 26KB depth image and a 85KB rgb image we were able to reconstruct the original pointcloud.", "As a matter of fact, I wrote something similar that can turn a PCL into a depth image. Its original purpose was to merge depth images from different perspectives into one as it would be seen from an arbitrary perspective around the point cloud. Worked pretty well, albeit rather slow and costly wrt. calculation time.", "If you\u2019re interested in the code, it can be found there: ", "This seems like the most elegant solution. Nice!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Define a new Message type, let\u2019s call it ", "   ", ".", "Add this message to drivers, such as the Velodyne or RealSense-ROS ones. Compression and publication can be skipped in no one subscribe to the compressed topic.", "Create an official C++ and Python library to convert the compressed type to a PointCloud2 message or, better, a pcl::PointCloud.", "Add this new compressed cloud to RViz"], "url": "https://discourse.ros.org/t/compressed-pointcloud2/10616"}
,{"title": "Some things to know as Python 2 approaches EOL", "thread_contents": ["Hey folks. We got an interesting question from a customer today, and I think ", " might be helpful to a wider audience.", "Python 2 will ", ". This shouldn\u2019t be news to anyone who hasn\u2019t been living under a rock, and ", " to use Python 3 in Noetic (whereas ROS 2 has always used Python 3). However, the question from our customer was this: ", " They are still using (and will continue to use) Python 2.", "The answer really depends on where you\u2019re getting Python 2. ", "Ubuntu\u2019s ", " are split into a number of components: main, restricted, universe, and multiverse. You can read that link to learn all about them, but let me briefly quote it so you can understand the difference between main and universe:", ":", "The main component contains applications that are free software, can be freely redistributed and are fully supported by the Ubuntu team. [\u2026] When you install software from the main component, you are assured that the software will come with security updates and that commercial technical support is available from Canonical.", ":", "The universe component is a snapshot of the free, open-source, and Linux world. [\u2026] Canonical does not provide a guarantee of regular security updates for software in the universe component, but will provide these where they are made available by the community.", "All Ubuntu releases prior to Eoan (19.10) included Python 2 in main. In Eoan it was demoted to universe. This means that for Xenial (16.04) and Bionic (18.04), Python 2 falls under the main guarantees quoted above. It will continue to be fully-supported and receive free security updates.", "In other words, as long as you\u2019re running Kinetic or Melodic against its recommended Ubuntu release targets, you should be golden.", "(hopefully this is the correct venue for this followup question)", "\nOne of our issues comes from the fact that we use rosdep to resolve dependencies and some times you need to use a pip dependency for a myriad of reasons. Since rosdep (to my knowledge) doesn\u2019t allow us freeze pip package versions we end up in a situation where it can end up installing a python 3 version of a package (or if the package maintainer is smart we end up with an install time / CI time crash).", "Our current approach is to fix package versions using ansible (and remove those packages from out package.xml which is really not ideal). Is there a recommended fix for that for Kinetic users?", "On Arch we already build all melodic packages against python3, works totally fine, you only always have to  pass ", " to as catkin cmake option. Most simply put this into an alias.", "hopefully this is the correct venue for this followup question", "Totally the right place; that\u2019s an excellent question. To be absolutely clear, anything not contained within the \u201cmain\u201d component of an Ubuntu repository does not receive that guarantee of support from Canonical, and this includes pip dependencies. I agree that you\u2019re in a bit of a sticky spot if you\u2019re using a pip dependency that ends up dropping support for Python 2 in its codebase, because you\u2019re right: as far as I know rosdep doesn\u2019t support a freeze or requirements.txt files like pip does.", "I know that ", " allows for version qualifiers, perhaps rosdep can grow support for that. However, I\u2019d also be concerned with ", ". Make sure you\u2019re installing pip from the Ubuntu repos as well, in that case.", "Our current approach is to fix package versions using ansible (and remove those packages from out package.xml which is really not ideal). Is there a recommended fix for that for Kinetic users?", "I agree, removing those from your ", " isn\u2019t ideal. Perhaps ", " can grow support for ", ".", "Worst case, I don\u2019t think it would be overly difficult to write your own dependency discovery tool using the catkin_pkg API (so you can extract the version qualifiers), use rosdep to resolve the keys into actual dependencies, and then do the installing/freezing logic yourself. You would of course need to deal with potential version clashes (e.g. package A depends on foo v1.1, package B requires at least foo v1.2).", " might have some insight to share on this as well.", "Our current approach is to fix package versions using ansible (and remove those packages from out package.xml which is really not ideal). Is there a recommended fix for that for Kinetic users?", "I can only sit in my armchair and daydream/brainstorm as I do not ship ROS on robots myself. Iterating on ", "\u2019s suggestion, if I had an ansible based deployment one thing I\u2019d be keen to try is using rosdep as part of an optional pre-run step that feeds dependency information into ansible rather than using it during deployment. rosdep has a lot of moving parts and I wouldn\u2019t want it running on every deploy. Invoking rosdep as a part of the pre-ansible pipeline also means you don\u2019t have to filter anything out of the package.xml. This would allow you to re-deploy independent of updating dependencies. If I were depending on packages from pip in a production environment I\u2019d also be very keen to keep a local mirror containing the pip packages I need. ", "Care to explain this part:", "Python 2 falls under the main guarantees quoted above. It will continue to be fully-supported and receive free security updates.", "The security updates for python 2 come from the python developers. They are not going to bother with securing python 2.", "And to quote Guido:", "Let\u2019s not play games with semantics. The way I see the situation for 2.7 is that EOL is January 1st, 2020, and there will be no updates, not even source-only security patches, after that date. Support (from the core devs, the PSF, and ", ") stops completely on that date. If you want support for 2.7 beyond that day you will have to pay a commercial vendor. Of course it\u2019s open source so people are also welcome to fork it. But the core devs have toiled long enough, and the 2020 EOL date (an extension from the originally annouced 2015 EOL!) was announced with sufficient lead time and fanfare that I don\u2019t feel bad about stopping to support it at all.", "The security updates for python 2 come from the python developers. They are not going to bother with securing python 2.", "You are of course, correct, assuming you\u2019re consuming python 2 straight from the python developers. However, if you\u2019re using the python from the Ubuntu archive, you\u2019re not: you\u2019re consuming it from Ubuntu. Going back to Guido\u2019s quote, I suppose you could call that a fork, but this is just how distros work. Ubuntu releases are supported long enough that it\u2019s not unusual for its archives to include software that upstream considers end-of-life, but we have teams dedicated to supporting them in Ubuntu. That\u2019s what makes an Ubuntu release supported.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/some-things-to-know-as-python-2-approaches-eol/11175"}
,{"title": "Security issue on ROS build farm", "thread_contents": ["The machine hosting ", ", which is the build farm for ROS 1, was recently compromised. We took that machine offline and are in the process of deploying a new farm, with the known exploit patched.", "The compromise included local privilege escalation sufficient to access the GPG private key used for signing Debian packages and to push Debian packages into the public-facing repository. ", ", but at this point we are unable to rule it out.", "So, in an abundance of caution, we are using a newly generated GPG key pair for the new ", ". Because the same GPG key is used to build ROS 2 packages we also swapped the key on ", ".", "As a result of the farm redeployment and key change, users who are installing or updating ROS packages may encounter service disruptions.", "We are working diligently to get back to normal operation as quickly as possible and will provide more updates as we have them.", "Thanks for the heads up ", ". Given the inability to rule out malicious uploads, can we safely assume the new repo will only consist of newly-rebuilt packages? In which case I expect it\u2019ll be quite a while before everything is back up and running.", ", your supposition is correct. We\u2019re in the process of rebuilding the core of ROS 1 Kinetic and will fan out from there once we\u2019re sure that everything is working well. The newly built binaries are going into a new apt repo that will be made available when it\u2019s ready for public consumption.", "We took that machine offline and are in the process of deploying a new farm, with the known exploit patched.", "For those of us running private instances (but perhaps publicly accessible): could you provide a link to a CVE ID or other information about the specific exploit?", "The newly built binaries are going into a new apt repo that will be made available when it\u2019s ready for public consumption.", "Excellent. The new key is C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654, correct? Let us know if we can help update docs once the repo is ready.", "For those of us running private instances (but perhaps publicly accessible): could you provide a link to a CVE ID or other information about the specific exploit?", "We believe that the infiltration point was via this vulnerability in the Jenkins Groovy Plugin:", "All Jenkins users should update their Groovy Plugin to patch that exploit.", "Excellent. The new key is C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654, correct? Let us know if we can help update docs once the repo is ready.", "That\u2019s correct, and we\u2019ll definitely take you up on the kind offer for help! But I\u2019d advise against updating anything just yet while things are in flight.", "We are continuing to investigate the compromise of ", ", including working with independent third-party security experts. We\u2019ll report back here as we learn more from them. So far we have no indication that the intrusion was anything more than a commodity attack by a group looking to hijack CPU cycles. But we are unlikely to ever be able to completely rule out malicious interference in the ROS binary packaging pipeline. So in an abundance of caution we are (i) continuing to rebuild everything that we reasonably can and (ii) relocating the rest.", "These changes are coming Thursday or Friday (2019-06-06 or 2019-06-07) this week.", "We are currently working on full rebuilds of Indigo, Kinetic, Lunar, and Melodic across all architectures on their supported Ubuntu Long Term Support distributions. We expect to have these builds completed by Thursday and available on ", ".", "As we fill them in, ", ". The packages will comprise currently supported ROS distributions (plus Indigo) and only on their respective Ubuntu LTS platform. Debian Stretch packages for Lunar and Melodic will become available some time later.", "Packages for the unsupported ROS distributions will be moved to ", ". Those package indexes will be resigned with a GPG key used specifically for the ", " host (this was always a different key from the one used for ", "). But ", " Users should make their own risk assessments regarding whether to use those packages (including the risk of using unsupported software in the first place).", "We are currently hoping to make these packages available on ", " tomorrow or Wednesday (2019-06-04 or 2019-06-05) so that there is no time when they are completely unavailable but if we have to make a choice between making unsupported packages temporarily unavailable or delaying the deployment of the newly built ROS packages, we\u2019ll choose to deploy the newly built packages.", "While we\u2019re making these changes to our repository structure we\u2019re also taking the opportunity to rename the testing repository which has been carrying the name \u201cros-shadow-fixed\u201d to avoid breaking anyone using it. The new name will be \u201cros-testing\u201d to match the \u201cros2-testing\u201d repository already available.", "  Everyone who installs ROS packages from ", " You can do it right now!", " Exact time to be determined, but not later than Friday this week (2019-06-07)", "\nSet up the new repository key", " Everyone.", "  After the ROS repositories have been redeployed with the new signing key. Exact time to be determined, but not later than Friday this week (2019-06-07)", " You should remove it as soon as the ROS repositories are redeployed. Leaving the key trusted won\u2019t cause anything to break but we won\u2019t be using the key to sign anything new in the future so leaving it just leaves you open to possible misuse in the future.", "\nRemove the key from your apt keyring", " Anyone who needs to access ROS distributions other than Indigo, Kinetic, Lunar, and Melodic or the Debian Jessie packages for Kinetic", " I\u2019ll update everyone here when I\u2019ve created snapshots for all the old ROS releases. After that announcement you can switch to the snapshot repositories.", " After the ROS repositories have been redeployed with the new signing key. Exact time to be determined, but not later than Friday this week (2019-06-07)", "We\u2019re making some changes to the snapshots repository layouts to support this. I am waiting to add docs until I know they\u2019ll be accurate. In brief the steps will be", " Anyone who is currently using the ", " repository to test ROS packages before a sync.", " After the ROS repositories have been redeployed with the new signing key. Exact time to be determined, but not later than Friday this week (2019-06-07)", " Once the deployment is complete, the shadow-fixed repository will be purged of release contents and you\u2019ll see an error message when updating apt repositories until you update the repository url.", "Check your ", " and ", " for ", " and replace it with ", "Let us know if we can help update docs once the repo is ready.", "This is definitely help we\u2019ll need going into the weekend. I\u2019ve opened ", " to track documentation that will need updating after the changes above.", "Just adding this as I don\u2019t see it mentioned anywhere: anyone who is building custom Docker images (or using any other sort of containerisation technology) and does ", " start from any of the official ", " or ", " images will also have to update their ", "s (or whatever it is called for the tool you are using) to use / import the new keys.", "Also don\u2019t forget to update your ", " / ", " etc. files if they contain the string ", " or ", ".", "The Snapshots repository now has ", " snapshots of all unsupported ROS distributions and distributions with unsupported platforms (i.e. Kinetic on Jessie) with the exception of Indigo on Trusty which is being rebuilt and will therefore be on ", " for a while longer as well as having a final snapshot made when it\u2019s done.", "If you\u2019re using one of these unsupported rosdistros. I\u2019d recommend updating your repositories now so you can let us know if you run into any difficulties. We are still on track to sync the updated repositories this Friday (2019-06-07).", "Information on using the snapshot repository can be found on the ROS wiki ", "Wouldn\u2019t this be a great candidate to feature on the ", " also? There may be a great set of users from old times that may not be regularly checking the discourse threads\u2026", "Thank you for your hard work guys!", "We have rebuilt all debian packages for indigo, kinetic, lunar, and melodic. And the packages have been redeployed with the updated GPG key to ", " see the announcement: ", "We have rebuilt all debian packages for indigo, kinetic, lunar, and melodic.", "That is great news.", "I\u2019d like to take this chance to extend a ", " to my colleagues who moved heaven and earth over the past week and a half to respond to this security compromise and get our systems back into a healthy state for the entire community. In particular, ", ", ", ", and ", " put in many many hours over nights and through the weekend, so cheers to them!", "Also a big thanks to ", " and his colleagues at AWS who helped along the way with diagnosis and mitigation of the initial problem and planning for the redeployment, and also served as our early testers of the outputs from the new farm.", "And thank you to everyone in the community for your patience and support as we\u2019ve worked through an unfortunate incident.", "Here a jenkins security advice: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Signing key used for the primary ROS repositories will change.", "Packages for unsupported rosdistros will move to ", ".", "The testing repository is being relocated to ", " from ", ".", "Add the ROS Snapshots repository key (different from the key above)", "Add the snapshot repository for your target ROS distribution"], "url": "https://discourse.ros.org/t/security-issue-on-ros-build-farm/9342"}
,{"title": "New Software Release - Py Trees", "thread_contents": [" and ", " were developed and used on robots at field tests around the world over the course of the last couple of years for Yujin Robot. I\u2019m happy to announce that they\u2019re now document complete with a great list of tutorials and ready to be consumed! A brief list of features:", "As you can probably infer, there was a focus on making sure we could easily debug problems, especially when they occurred at a remote test site without an engineer (the replay tool especially was very useful). On the other hand, a graphical designer frontend was not prioritised and never eventuated - scripting a python tree remained simple and flexible enough to suit our needs. We also had the trees pass a litmus test of being able to pass the work of building robot-specific behaviours and trees off to interns. It took three iterations of the core libraries to pass that point, but since then there has been negligible change in the core libraries.", "Additionally, the trees started being used beyond the scenarios they were originally designed for. The control engineers started shifting all non-reactive control logic to the trees - it was just easier to follow patterns already implemented and to make use of the bagging/visualisation/replay tools, e.g. implementing an entire subtree for initialisation from a composition of context switches (small dyn. reconfigure changes), enabling/disabling of sensors and simple motions instead of having a separate c++ or python node that would handle the entire process as a custom state machine inside an action. Also, as the logic accumulated in the trees, the data to support it also centralised there, and it eventually became the natural portal point between robot and server/web applications.", "All the py_trees packages (py_trees, py_trees_ros, py_trees_msgs, rqt_py_trees) are on the ros build farm and repository details on the ros wiki. You can also use the py_trees package itself directly from pip as a pure python module.", "Get started at the ", ". If you just want a quick glimpse of how they can be use in ROS, jump directly to the ", ".", "This is an awesome piece of software and I\u2019m looking forward to trying it.", "There are a couple of other implementations of behaviour trees floating around in ", ", one from ", " and one from ", ". Is there enough interest in trying to unify some of the messages and interfaces so these libraries (especially the tools) can be used together?", "Aye, there\u2019s a few floating around. Perhaps I can throw in a bit of a summary. Authors please feel free to embellish/correct me if I get something wrong!", "Pat Goebel had the closest thing to what we needed with his pi trees. He was using it for his textbook and while it was a nice start, it was not as comprehensive as we would have liked. I also wanted to build it around python generators, so had a chat with him and decided to take inspiration from what he had and move it forward with py_trees.", " was already starting work on his when we started, but there wasn\u2019t much in his repo yet, and was in c++ targeting robot control. A bit orthogonal to our needs.", " looks great, and honestly I never did try it. At the time I was caught in the whiplash of trying to use SMACH to control the entire robot (which it really isn\u2019t designed for) and was not keen to try another ", " framework. Behaviour trees have worked for games and it seemed prudent to give that a good run before using or reinventing yet another framework. We were more than pleased with the result.", "Alison from TRI also recently started a python behaviour trees framework (I am not sure if it is open yet). It doesn\u2019t yet have as many bells and whistles, but it was reassuring to find that she ended up making a few similar decisions along the way. That one might be worth integrating with if it is (or will be) open.", "I am glade to see that behavior trees are becoming popular.", "With my implementation (that I haven\u2019t officially released to the ROS community) I tried to put together the best of C++ and runtime configuration, rather than compile time hard-coding.", "\nAdditionally, I am trying to create a core implementation that has no ROS dependencies and a ROS wrapper around it.", "I was planning to propose a presentation for Roscon 2017, but I didn\u2019t make it; my fault for being late ", "This thread reminded me: is ", " related to anything here?", "I\u2019m not trying to hijack this thread (very nice release ", "), I\u2019m actually interested to see whether fi we could do something with the editor and multi-agent parts of behavior3.", "Apparently the editor serialises trees to a JSON format. I don\u2019t know if that is something that could be used to programmatically construct trees ", "?", "I have my own editor that is very similar to theirs, even if it is written in C++/Qt.", "\nIn terms of common format, their format has some data and metadata missing, IMHO.", "multi-agent parts of behavior3", "Gaming is really focused on this aspect of behaviour trees because they are trying to control hundreds, if not thousands of npc characters that all interact with each other. This leads to all kinds of wonderful multi-tree co-ordination techniques and parallelising optimisations to make sure that it remains manageable and latency stays low.", "If you\u2019re just looking to co-ordinate a single robot\u2019s behaviour though, this is a non-issue and greatly simplifies the behaviour tree implementation. There is just one agent and the complexity is typically only in the order of hundreds of behaviours. No parallelisation nor multi-agent co-ordination is required. If you\u2019re starting to consider thousands of behaviours, or looking for a higher level robot fleet co-ordinator with an extreme number of robots, then you might start thinking about behaviour trees ala professional gaming considerations. I mention this briefly ", ".", "If your use case is instead to handle a few or few tens of robots, I do not think anything special is required. Any behaviour trees implementation (incl. py_trees and ", ") could easily do so with a few simple mechanisms that fit whatever use case they have and the variety here is quite considerable. e.g. if behaviour3 has the implementation described in their ", " then it has a rather special means of decorating a synchronisation that fits their use case.", "My own two cents, I think people try to over-coordinate at this level - I much prefer keeping it very simple and adding as much autonomy to the robots themselves so they can engage and interact robustly without a hive mind instructing them every step of the way. Given that multiple robots are at the mercy of wireless communications this is an imperative consideration.", "Apparently the editor serialises trees to a JSON format. I don\u2019t know if that is something that could be used to programmatically construct trees ", "?", "The ", " format has string based key-value dictionary types that could be used to store esoteric information that the py_trees serialisations have. That means a converter or loader should be trivial. Having said that, it probably wouldn\u2019t be that useful since you\u2019d have to manually plug all of this information in (i.e. you lose the benefit of the gui) and can\u2019t properly visualise it. py_trees dot graph visualisations of a python ", " function are just as good (latex style workflow). The editor though might have ways of extending its functionality, or could probably be forked and hacked on directly as a good starting point. For us though, this starts to sound like a great deal of work with little benefit over the py_trees dot graph generator approach (which also happens to fit neatly into a CI workflow to generate graphs of your tree/subtree/behaviour library).", "I do like how behaviour3 has focused on a serialisable format and used that as a focus for multi-language implementation. I would like at some point to write a c++ version of py_trees, but I doubt it would be the right thing to do - a c++ version should have requirements that would enable it to meet \u2018gaming\u2019 considerations, especially that of low latency. Similar scripting implementations of py_trees would be a nice thing though.", "You are most probably right about the fact that a single robot doesn\u2019t need a low latency, low overhead implementation.", "\nI am happy to admit that I implemented my own version because I prefer C++ over Python.", "\nTo be fair, a lot of design tradeoff and complexity come from the fact that I want to create the tree at run-time, instead of compilation time, using a definition stored in a XML (could be JSON or whatever).", "\nThis would be a non-issue on Python.", "NEVERTHELESS:", "I am not trying to make a point here, simply to say that it feels easier for me to do everything in a single language.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["All the bells and whistles\u2019 you\u2019d expect from a comprehensive behaviour tree library", "Assemble trees in relatively simple and short python scripts", "Additional ROS behaviours for common ROS operations (subscriber, move_base, \u2026)", "A ROS behaviour tree manager with status publisher for connection to an rqt monitoring plugin", "Automatic bagging of the tree status when it changes", "Replay bags in the rqt monitoring program to identify problems", "Simple to mock with a robot layer for debugging, testing on CI and as a rapid simulation for use with web application development.", "I have strong feelings about having a BehaviorTree core library with NO ROS dependencies. NONE at all. ROS can be used seamlessly, though.", "Since all my Actions are written in C++, the only way to interact with Python would be through IPC, ROS services/topics, or Python/C++ wrappers. It feels more natural for ME to do everything in C++.", "The infamous BT blackboard would be harder to use in a Python/C++ mixed environment."], "url": "https://discourse.ros.org/t/new-software-release-py-trees/2046"}
,{"title": "Journals for ROS contribution", "thread_contents": ["Hello everyone!", "\nduring this year I was working on optimizing the bandwidth consumption during a web-based remote manipulation in ROS with tf2_web_republisher. The internet connection is limited here in Algeria and we couldn\u2019t perform a smooth remote manipulation on a robot, so the idea was to reduce the amount of TF sent from the manipulation server (remote site) to web client. I was able o reduce over 70% of bandwidth consumption with the same effective result as by tf2_web_republisher. I experimented my algorithm on pioneer mobile robot P3-DX and on the 07Dof robot arm Cyton_gamma_300. Now I intend to publish my results on an international review, the problem is I can not find a journal that accepts these kind of work for reviews, it\u2019s either: \u201cthere\u2019s no reviewer for ROS development\u201d or \u201cmy work is not a general contribution in robotics\u201d or \u201ca contribution on an open source tool is not useful\u201d! I would like to find an internation jounral indexed on scopus (or ISI thomson) which is not Open access and has more than five years, I know this is nonsense, but these are the criteria of my faculty. Can anyone help me please. I\u2019ll be grateful to all of you, thank you in advance!", "I\u2019d first suggest you make them clarify the reason for avoiding open access.", "I suspect it is to keep costs low but probably they are not aware of the many free open access journals out there.", "According to the director of Post-graduation service, some OA journals may accept any work and any paper just  to gain money from OA fee\u2026 which is ridiculous, because there are thousands of prestigious reviews which offer open access service!!! ", "If an IEEE conference would be enough, you can try IROS.", "Thank you for your reply but I\u2019m afraid that it has to be a publication", "Would you advice me some free open access revues on ROS? Thank you", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/journals-for-ros-contribution/3559"}
,{"title": "Announcing imagezero_transport, a fast, lossless image transport plugin", "thread_contents": [" is a lossless, very fast compression algorithm for 24-bit color photographic images.  Its compression ratio is comparable to PNG, but it compresses over twenty times faster and decompresses almost twice as fast.", "I\u2019ve made a new ROS stack that makes it easy to use ImageZero as a transport mechanism for any ROS topics that are already using the ", " mechanism.  It\u2019s available on GitHub at ", ", and it has been successfully built in the ROS Shadow repository, so it should be available in the next releases of ROS Indigo, Jade, and Kinetic. The ", " stack provides three packages:", "For a little bit of backstory and a use case, I\u2019m working on a system that is doing image processing on 10 Hz video feeds coming from stereo cameras connected to one computer than is publishing the video across a network as ROS Image topics.  The uncompressed image data was consuming over 400 Mbps of bandwidth by itself, which was enough to choke other services on the 1 Gbps network.  We looked at using the built-in JPG and PNG image transports, but JPG was unsuitable for our image processing algorithms because it\u2019s lossy and PNG was unusable because it is so CPU-intensive that the host computer\u2019s processor could not compress the image stream in real time.", "After searching around for a while for image compression algorithms, I found ImageZero, which suits our needs perfectly.  It\u2019s lossless, designed explicitly for 24-bit color natural photography, and fast enough that our processors can easily do it on a real-time video feed.  The bandwidth from our cameras is now down to about 150 Mbps.", "P. J. Reed, Senior Research Analyst", "\n(210) 522-6948", "\n", "\n", "Cool ", ".", "It might be worth adding a note on the ", " wiki page which points out there are other options for compression like this one:", "Feel free to add a pointer to your documentation.", "It might also make sense to add a tutorial on how to use your transport:", "It would probably fit in nicely with the tutorial about adding a new transport type.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": The original ImageZero algorithm built as a shared library using catkin", "\n", ": A shared library that provides convenience methods for converting between ", " and ", " messages using ImageZero", "\n", ": A plugin for the ROS ", " package which adds an ", " sub-topic to ImageTransport topics that can be used to transparently use ImageZero to compress your images"], "url": "https://discourse.ros.org/t/announcing-imagezero-transport-a-fast-lossless-image-transport-plugin/628"}
,{"title": "Pcl_ros with PCL 1.8 (ROS Lunar and newer) means linking to 139 VTK dynamic libraries", "thread_contents": ["Compiling ROS from source I\u2019ve noticed that PCL 1.8 (and thus pcl_ros) links to a newer version of the VTK library, which yields unbelievable 139 dynamic libraries to be linked to my programs. Crazy! According to ", " , there is some performance penalty for every dynamically linked library.", "Together with dependencies of the VTK libraries, we get to this number:", "And all of that just to be able to transform a pointcloud (no visualizations etc).", "I think pcl_ros should be refactored to two packages, one for \u201ccomputing\u201d and the other for visualizations. That could probably push the number of linked libraries much lower for computing-only uses of pcl_ros. Or it could be modularized even more\u2026 Not everybody needs surface reconstruction capabilities, for example.", "What\u2019s your opinion on this? Or is there some good reason to keep it all in one package?", "Just for comparison:", "Splitting the visualisation parts out of PCL is certainly worth doing, in my opinion. PCL itself could also be modularised even more, but whether the return on that investment makes it worth doing is an open question.", "PCL can be compiled without visualization, this removes the VTK dependency:", "\n", "\n", "I would interested in what it takes to split this out. Currently I\u2019ve been blocked compiling PCL from source, presumably from the large amount of dependencies. I\u2019ve tried doubling memory and swap space (and of course on a fixed machine (laptop) with only 8GB of ram, it seems like 15GB isn\u2019t even enough. Every time it runs out of memory, unfortunately that now means I have to change my workflow, and compile this on a much larger system, with substantially more RAM, when I have compiled quite large packages without much of an issue (i.e. opencv, kernels, etc.). I\u2019ll try out disabling VTK to see if that helps move the process along, before investing in recompiling everything and setting up a new build machine.", "I\u2019ve tried doubling memory and swap space (and of course on a fixed machine (laptop) with only 8GB of ram, it seems like 15GB isn\u2019t even enough. Every time it runs out of memory, [\u2026]", "Have you tried limiting the nr of concurrent jobs? The default is probably something like the nr of cpus/cores, which could easily lead to significant memory usage with C++ (templates, et al).", "Absolutely, it was trying to compile gstreamer, and a kernel originally, but just compiling pcl by itself, consumes all memory even with running a single thread.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["    include(\"${QT_USE_FILE}\")", "  endif (QT4_FOUND)", "elseif(\"${PCL_QT_VERSION}\" STREQUAL \"5\")", "  include(cmake/pcl_find_qt5.cmake)", "else()", "  message(SEND_ERROR \"PCL_QT_VERSION must be 4 or 5\")", "endif()", "endif(WITH_QT)", "\n", "# Find VTK", "option(WITH_VTK \"Build VTK-Visualizations\" TRUE)", "if(WITH_VTK AND NOT ANDROID)", "find_package(VTK)", "if(VTK_FOUND)", "  if(NOT DEFINED VTK_RENDERING_BACKEND)", "    # On old VTK versions this variable does not exist. In this case it is", "    # safe to assume OpenGL backend", "    set(VTK_RENDERING_BACKEND \"OpenGL\")", "  endif()", "  message(STATUS \"VTK_MAJOR_VERSION ${VTK_MAJOR_VERSION}, rendering backend: ${VTK_RENDERING_BACKEND}\")", "  if (PCL_SHARED_LIBS OR"], "url": "https://discourse.ros.org/t/pcl-ros-with-pcl-1-8-ros-lunar-and-newer-means-linking-to-139-vtk-dynamic-libraries/4433"}
,{"title": "Local roscon benefit", "thread_contents": ["I found in ", " there will be an inaugural localized version of ROSCon this year, co-hosted by Open Robotics. While I\u2019m definitely happy to see such an expansion, I\u2019m curious what\u2019s the expectation out of it from a global community member\u2019s perspective.", "Looks like the presentation can be done in the local language for this particular event I referred to, which makes me a bit worried that technical and any information won\u2019t be easily shared in the global community, unlike all previous ROSCons done in English. I believe a part of the reason why ROS has been so successful, same as some other great OSS projects, is because of the global community, the amount of information stacked there and how amazingly well it\u2019s been organized. So I personally think ideally every member should gear even more toward the global community. But I know I\u2019m way too narrow-sighted, thus simply curious.", "Thank you in advance.", "\nIsaac", "P.S. I found there are some studies (", ", but I don\u2019t have an access) about local OSS community role.", "I can understand the desire to have a united global community. Interaction at the international level is one of the greatest things about open source software for me.", "Having said that, the concept of local events held in the local language is as old as open source itself. I have seen events held in Japan in Japanese for projects as large and diverse as HTML5, Firefox and Google\u2019s various projects. These sorts of events are important because not everyone can speak English with the level of confidence necessary to participant actively in the global community. Local events allow those who can to provide information of new projects and trends, and knowledge of using or contributing to a project, to those who can\u2019t get this information directly in English.", "The concept of local/local-language events is not new to ROS, either. Although ROSCon JP is the first to be officially licensed by the OSRF, there have been numerous events in the past. The biggest example I can think of is the ROS summer camp that has been held annually in China for several years now. They get several hundred participants and I don\u2019t think they are doing it in English. In Japan, we have had informal Japanese-language events for several years now.", "The reason we are working with the OSRF for ROSCon JP is to create a central event for the local community in the same way that the international ROSCon has become a central event for the global ROS community. It allows us to make the scale much bigger than an informal event can be. Being officially licensed by the OSRF helps us to attract sponsors, which in turn lets us do things that cost money but are important, like provide exhibition space, bring in valuable overseas speakers (with translators), get a high-quality logo by the same artist as the main ROSCon logo, provide food, record the talks and archive them online, etc. All these contribute to making the event memorable, which in turn makes it valuable as a means to motivate the local community to participate in a project that does not have that much information available in Japanese, as well as motivate the community to grow itself organically. As with ROSCon, ROSCon JP is a by-the-community-for-the-community event.", "Having the OSRF on board has meant that we are getting valuable organising support from the organisers of ROSCon, we get a set of rules and guidelines to guide us in planning an event that meets their standards (these have been much more useful than you might think), we get to use the ROSCon trademark, and additional things like use of the OSRF video archive site for the talk recordings (so we don\u2019t have to do that ourselves).", "I don\u2019t want to speak for the OSRF, but my understanding is that for them having a local event that is licensed means they can control how the ROS and ROSCon branding is used, ensuring that the event meets their standards. We are effectively their test case for them to figure out how to do this, and if we can make our event a success then I think you will see more OSRF-backed local events in non-English speaking countries in the future.", "I too want to ensure that ROS remains strong on international cooperation. I see this sort of local-language event as an important part of that, providing a local feeder to the wider community in order to attract people to it who ordinarily would not have a way to begin participating.", "Speaking as a ROSCon co-organizer/co-author of the local-language strategy\u2026What Geoff said ", "I think that is best if anyone can speak fluently English , after all English is the world language , instead of Japanese , or Chinese , French , even Deustch, so if you want to communicate with your guy in Japan , Japanese is enough ,", "\nbut ROSCon is world event , I insist we should or have to speak English fluently ,  so , learning English , guy", "I think having local ROSCon (affiliated with the Open Source Robotics Foundation to keep track of what is being shown/discussed for the benefit of all the community) is a very good thing. When I was in Barcelona, Spain we did for a short while a Robotics meeting (", ") where we talked a lot about ROS in Spanish (and Catalan!).", "This allowed people that are not that fluent (or not at all) in English to ask lots of questions that otherwise would get hardly answered. It builds up an even bigger community. Some translation efforts came from that too. Think of the younger generations that are still learning English as a second language, this is great for them.", "In the worst case scenario a local group grows so much in size and activity that they become very important/influential. At that point I\u2019m pretty sure we could find a way for everyone to benefit from that group\u2019s knowledge sharing. If the talks are recorded it\u2019s a matter of writing subtitles for them. If there are tutorial packages, well, some translation would need to be done if they are that interesting.", "Its Awesome , I want You done this project Successfully so that other can Organised also in there Home Country . I Know local language give more freedom to peoples so that they can put there enormous questions in-front of ROS community .", "I think the ROS community is being grown rapidly and we need to discuss the next step.", "\nThe official ", " described:", "ROSCon is a developers conference, in the model of PyCon and BoostCon.", "As you may know that PyCon has lots of ", " such as PyCon APAC (Asia Pacific) and PyCon JP (Japan).", "\nIt\u2019s time to consider holding regional / domestic ROSCons in cooperation with OSRF!", "Speaking for Open Robotics (and specifically for our non-profit parent organization OSRF, which is the owner of the ROSCon trademark and organizer of the main ROSCon event): I\u2019m really excited about this development. Over the years we\u2019ve had numerous inquiries about doing ROSCon events that are localized (by region and/or language) or customized (by region, institution, and/or application domain). When ", " came to us with a concrete proposal for doing ROSCon JP, we decided to give it a try.", "Working with ", " and the ROSCon Organizing Committee, we drafted a Rules of ROSCon document that explains the essential characteristics that make an event a ROSCon event and also the terms under which we may choose to license the ROSCon name for use with such an event.", "We\u2019re using ROSCon JP in September this year as the first trial of this new idea. If it goes well (I\u2019m confident that it will, but I\u2019ll be there myself to check ", "), then we\u2019ll refine and publish the Rule of ROSCon and set up a way for other would-be organizers to propose their own ROSCon events. So stay tuned for that. And if you want to start work on an event in the meantime, contact us directly and we\u2019ll discuss it with you.", "To be clear, we don\u2019t want to get in the middle of the various ROS-related events that happen around the world already, from local user group meetings to tutorials to summer schools. Those are great events that happen on their own without our involvement. We\u2019re looking after the ", " name, ensuring that people know what it means when they\u2019re attending an event that uses that name, and also providing guidance and advice to organizers of such events.", "I know we should be open\u2010minded in any opinion.", "I\u2019m Yuki Nakagawa, president and CEO of RT Corp. and trustee of ROSConJP.", "\nI watched ROS since 2009 and our company sell many TurtleBot series in Japan.", "I\u2019d like to talk about local activities in local language for ROS as OSS.", "Global community of OSS is being grown step by step such as local, global, local and global.", "\nI can say that from my experience that I watched \u201cthe rise and fall\u201d and joined so many global activities like RoboCup since 1997, google android developer community since 2008 and so on.", "Localization becomes a big problem when Global OSS activity spreads in each country, as you know.", "\nLocal community and activities help to solve these issues.", "\nWe, ROSConJP, hope to facilitate such community in Japan.", "Especially, almost Japanese engineers are not required to speak in English in their work.", "\n(Of course, depending on company or school.)", "\nMother tongue supports many engineers to understand deeply if they can touch new idea from global activities.", "If you favor to community \u201cmust use English only\u201d, you can imagine what happens.", "\nI mean that you will eliminate engineers and young engineers\u2019 eggs who can not understand English.", "I think it\u2019s time to step into new stage to bridge beyond language.", "I hope all of you to be \u201cOpen\u201d mind. ", "Thank you.", "Yuki Nakagawa", "\nRT Corp.", "Hello,", "I hope you don\u2019t mind my intruding\u2026", "My name is Hajime Saito. I used to be at General Robotix. I was at the first meeting at Willow Garage, which was hosted by Eric Berger. So I do have an interest in ROS, and have been following it\u2019s development over these years. Sadly, I\u2019ve not really been involved in the development side, much.", "I think what Isaac is trying to ask is, are the local RosCon events mainly for information dissemination, or will they be vehicles for active technical dialog? Do you expect local groups to pray at the altar or ROS, or does the communication go both ways?", "If you want the communication to go both ways, you\u2019ll need people to take up the dialog with RosCon(main) or with the OSRF. Having been on the receiving end during an international cooperation effort, the amount of work, at the beginning, is not trivial.", "Please tell me your thoughts on this, and whether you think I\u2019ve taken a few steps too far.", "Best regards,", "Hajime", "are the local RosCon events mainly for information dissemination, or will they be vehicles for active technical dialog?", "Both. It would be denigrating to Japanese users of ROS to assume that they are solely there to consume information, and have nothing to contribute themselves. Many of the submitted proposals are aiming to use ROSCon JP as a first-run for a talk at ROSCon.", "Both. It would be denigrating to Japanese users of ROS to assume that they are solely there to consume information, and have nothing to contribute themselves.", "+1", "I\u2019m excited to see the contributions from our community members in Japan.", "Perhaps we\u2019ll eventually have ubiquitous high-quality translation of all audio and text from and to any language. Until that happens, I\u2019ll happily muddle through with free access to the presentation content (ROSCon JP videos and slides will be archived at ", "), and more importantly, the underlying code, which we can all read and understand.", "On that note, I\u2019ll just namedrop ", " as a crowdsourcing tool for subtitle generation and translation that I\u2019ve used with satisfaction in the past.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/local-roscon-benefit/5064"}
,{"title": "New BNO055 I2C driver for ROS", "thread_contents": ["The Bosch BNO055 is an excellent IMU at its price point and extremely common among hobbyists and industry alike. Many are using it in Jetson TX1/TX2/Xavier boards over I2C, as well as other boards that have exposed I2C ports.", "I found a RS232 driver for ROS, as well as an I2C driver that depended on RTIMULib, but that project looks seemingly defunct and cumbersome as an install requirement.", "So for all of you BNO055 fans out there I wrote a ROS driver that doesn\u2019t depend on anything except libic2-dev. You might need to give permissions to the ROS user by adding it to the i2c group as well.", "BNO055 I2C driver for ROS. Contribute to dheera/ros-imu-bno055-i2c development by creating an account on GitHub.", "I\u2019ve tested this on a TX2. It should work out of the box on other Jetson boards. I haven\u2019t yet tested it on a Raspberry Pi but you\u2019ll need to slow down the I2C clock to use it with a BNO055 as the Pi doesn\u2019t support clock stretching.", "The calibration service isn\u2019t implemented yet. I welcome pull requests or contributions.", "NIce! do you think this will work natively with the Jetson Nano as well? Or will it has the same clock speed issues as a Raspberry Pi?", "Thanks!", "If you refer to the ", ", that is probably unique to the rpi.", "I can confirm that the Jetson Nano does ", " have the clock stretching bug.", "(I can also confirm that this driver works on a Raspberry Pi with ", " i2c, although it will consume the majority of one core doing so. If you don\u2019t need data at 100Hz, you can reduce that to cut down on CPU usage. This is not an issue with Jetsons or other platforms that support the full I2C spec including clock stretching.)", "Does the Raspberry Pi 4 suffer the same clock stretching bug? Or did they fix it in the new silicon?", "The rpi4 has another core so it ", ".", "But I guess ", " can better answer that question.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-bno055-i2c-driver-for-ros/7940"}
,{"title": "\"Deterministic\" navigation in ROS", "thread_contents": ["We are working on a research project where we are investigating the ROS Navigation Stack in an industrial setting (", "). The partners in the project want autonomy to a certain level. For most areas in a plant, the behaviour of the robot must be predictable.", "We have been looking into commercial solutions such as Navitec and Bluebotics and they have graphical tools to guide the navigation behaviour. In certain areas, the robot is free to navigate. In other areas, the robot is only allowed to follow a virtual line. The tools are some sort of vector-drawing tools, where you can draw routes and areas on top of a map. The planners use this additional information to come up with appropriate paths.", "We are wondering if there are open source tools that can do these kind of things. We did not find them yet. If they are not there, it might be interesting to develop such a tool in the ROS ecosystem.", "It\u2019s very early days still, but we just opened up our in-progress \u201ctraffic editor\u201d for this type of thing. Documentation is currently non-existent, publicly-viewable examples are not there yet, etc., but it\u2019s coming along.", "You can specify a floorplan image, draw lanes on it, trace the walls if you want, and then export the path data to YAML and/or a simulation model for Gazebo. The GUI is built using QtWidgets in C++ and saves all the data to YAML. The exporters are Python scripts.", "Again, it\u2019s just a work-in-progress in a public repo, not a polished product. But I think this style of robot operations is becoming a common use case in many domains, so hopefully the editor can become useful.", "Contribute to osrf/traffic-editor development by creating an account on GitHub.", "This is just an editor for these paths; it doesn\u2019t touch the problem of actually following them with real robots.", "Cheers", "Thank you. Your description is close to what I was looking for, so I will definitely have a closer look at your repository. We want to plan our routes on top of the maps generated by Gmapping or Carthographer. But essentially, those are also just bitmaps. We want the following flow: the robot drives around and builds a map, then the operator draws routes/paths, restricted areas, and free-to-navigate areas, speed zones, etc. on top of that map, and finally, the robot uses that data as input to the planner.", "I think it is logical to separate the editor from the functionality of the planner. The interesting part is to have a common data format for the routes in YAML/XML/etc. Do you use a standard for storing the routes/paths? Currently, we have a group of students working on a planner that follows predefined paths.", "Ps. I tried to build your code, but I got some errors. I will have a look at it tomorrow.", "Currently the annotations are just stored in a YAML format of our own dreaming. There are Python \u201cgenerators\u201d in the repo which process that YAML into other \u201coutput formats\u201d such as Gazebo worlds (XML) or a \u201csimpler\u201d YAML format that is just the navigation data (lanes, etc.), intended to be consumed by nav stacks. We can create generators for any other formats or navstacks; I wasn\u2019t aware of standards for this, but if there are, we can certainly convert the data to whatever format is desired. It\u2019s just Python ", "I added a GitHub Action to the repo now which does an automatic build on Ubuntu 18.04 every code commit, so if the badge is green on the repo, it should build (at least on Ubuntu 18.04). Please create an issue ticket if you\u2019re seeing any build errors with details about your platform. Cheers!", "The build errors were my fault\u2026 I was first trying it out on an Ubuntu 16.04 machine. On 18.04, the project builds without problems.", "This application is really similar to what we have in mind. We are now playing around with it. The application really helps to discuss the requirements with colleagues and we are sharpening our user requirements. I will get in touch for next steps. I think it would be nice to join efforts to further develop the tool.", "Web-based GUI plz.", "Preferably as composable widgets.", "(It would make it easier to integrate the tool into application-specific UIs.)", "Yeah, maybe rev 2 ", "Currently we\u2019re running this tool \u201coffline\u201d to create map files and export various products (simulation models, navstack configs, etc.) in the local filesystem, so it would seem that a web-based approach would make things a fair bit more complex. But maybe that\u2019s just because I\u2019m a dinosaur.", "There is probably room for both \u201coffline\u201d and \u201conline\u201d editors in the ecosystem eventually.", "But maybe that\u2019s just because I\u2019m a dinosaur.", "You, like me, are a dinosaur.", "All the young mammals (including at my company) are running around building beautiful-looking UIs that run locally but are accessed through a web browser.", "I don\u2019t pretend to know how it works (OK, I do a little bit but just because I\u2019m curious), but I do know that they seem to whip up new UIs in a matter of hours using nothing but libraries with weird names they found on GitHub.", "I certainly agree that there is room for both. I\u2019m more interested in the (system) interfaces side\u2026 the data formats taken in and spat out, any online communication channels, etc.", "I am also ok with an off-line tool and personally I have most experience in that direction (Qt and Java). However, I agree that it would be nice to have web-based tools to edit the maps. I saw a demo of the MiR 100 robot and it looked like they already had a web-based UI for editing the map and for fleet management. However, when I look at their website I can\u2019t find anything about it.", "We also came up with two other features that might be interesting for this tool. One is support for .pgm files, because that is the format that ROS uses to store maps. The other is to support curves, because our robots need to have smooth trajectories and no sharp turns. This could also be solved at the planner level, but I think it is better to do that in the editor. I did some experiments in Java with curves and used 4 vertices to define bezier curves.", "Would it be interesting if I integrate the curves in your traffic-editor?", "I have written this very web-based application twice now for different companies, but both in proprietary formats.", "\nWhat you\u2019re looking for, if you don\u2019t want to write your own custom UI/system, is QGIS (", "). At its core, what you\u2019re looking to do is pretty standard GIS stuff. QGIS is designed for these kinds of spatial data workflows. It contains everything you need to digitize, manage, analyze, and ultimately serialize (eg. into GeoJSON) your robot \u201ctraffic plan\u201d.", "Also be aware of QGIS-ROS, which I wrote to help bridge QGIS into the ROS world (", ") ROSCon presentation linked in the readme.", "I strongly encourage trying to utilize available open source GIS tooling (that\u2019s been in development for decades) before deciding to make a ROS-specific flavouring of a subset of these tools.  These aren\u2019t novel spatial data authoring problems we\u2019re trying to solve.", "Thanks ", " ! Yes certainly, adding curves/splines would be great, since they are actually physically realizable by robots. We started off with straight-line segments just because they are super easy and because (in my limited experience) it seems that is what many/most companies are currently doing in their proprietary editors anyway.", "Thanks ", " for the feedback. Indeed, it seems that every robot company has their own internal proprietary editor. Thank you for the pointer to QGIS; it\u2019s an interesting idea to use a GIS system for this. I guess I had been stuck in a mental rut that \u201cGIS is for outdoor large-scale maps expressed in lat/lon,\u201d but I see the overlap with indoor mapping now. GIS seems particularly relevant for indoor+outdoor robot operations (deliveries to loading docks, etc.). I guess the purely-indoor, large-building domain still feels \u201ca bit different\u201d to me, in that multi-level buildings are so much more constrained than entire city maps, so an \u201cintentionally limited\u201d UI subset of something like QGIS might make the tool easier to use and the workflows more straightforward. The input (in my limited experience) is often a pile of PDF floorplans provided by the building operator, rather than satellite imagery or other traditional GIS data. But I\u2019ll definitely dive deeper into the GeoJSON RFC and QGIS to challenge those assumptions. Thanks for the pointers!", "Perhaps there is enough interest in this area to create a new ROS Discourse category called \u201cMulti-Robot Operations\u201d or something like that. It wouldn\u2019t necessarily be locked to a particular robot software platform (ROS1 / ROS2 / various other options) but instead about creating higher-level tools to deal with multiple robots sharing the same space, no matter what software is running on the robots themselves. I\u2019ll create a category proposal now and anyone interested can help evolve the definition and direction. Cheers!", "If you\u2019re going to consider GeoJSON as an option for interchange, just know that you can completely ignore long,lat in the spec and just pretend it\u2019s x,y. Been doing this for many projects well over a decade and can say it works fine, assuming you\u2019re utilizing tools that don\u2019t just assume a CRS like WGS84.  There\u2019s hundreds of geodata formats so pick what makes sense, but resist making your own. GeoJSON is great because of the countless tools and libraries already available for it.", "For QGIS all you need to do is set a custom CRS to a planar Cartesian system (just set everything to zeroes). Everything else just works including the extensive suite of raster and vector tools (see my ROSCon talk for examples of QGIS showing an indoor facility with robots in real-time).", "Orthorectified imagery or PDF floorplans are basically the same thing when you think about it. Set control points from known fiducials and begin drawing. Thinking all the way back to school, we\u2019d just rasterize the PDF and run it through QGIS akin to how one would an unrectified satellite/aerial image.Though often a SLAM map is used as the \u201cground reference\u201d and you draw on your vectors relative to it.", "Yes, there is a lot to be said about a limited UI, which is probably why I keep end up making them. Depends what your goals, timelines, etc. are.  QGIS may fill a gap shorter or longer term, and it\u2019s always a phenomenal analysis and data processing tool.", "Feel free to email me with any questions you\u2019ve got with getting started.", "Thanks! Ticket added: ", "The QGIS approach is definitely interesting. I think it is better to use existing tools if they are available. I will have a look at the software to see if it is possible to use in our projects.", "Currently, we are aiming at ", " as the fleet/traffic manager, because it is open source and one of our partners already has experience with it. However, OpenTCS manages on an very abstract level. It uses a graph of the environment and there is no direct connection between the information in OpenTCS and, for example, a planner in the ROS Navigation Stack.", "Our current approach is to have one data set with maps based on sensor data (from GMapping or Carthographer) with an overlay of the paths and areas where the robot can navigate. From that data set  we can export graph data to OpenTCS and use the whole structure in ROS Navigation.", "It\u2019s been over 8 years since I was involved in anything that used GIS, but I do recall that back then the GIS community was making a ", " big push into indoor GIS. A quick Google search turned up plenty of results so I\u2019m guessing the community didn\u2019t give up. I can\u2019t remember details, unfortunately, but there were open format specifications for buildings and built spaces and things like that.", "I dived into the QGIS software and it really has a steep learning curve. However, based on my experiences it definitely has the functionality to draw vectors on top of raster images. That was what I was looking for. I still have to look into the interoperability between QGIS projects and ROS. Thank you ", " for the pointer to QGIS-ROS.", "I have also been looking into standard map data formats. I encountered the IEEE Standard for Robot Map Data Representation for Navigation. Based on the name, it seems like the thing I was looking for. However, I am not sure whether to use such standards, because they are closed source and we work in an open source projects.", "I am still interested in your view on the last part of my previous post. We are looking into data exchange between different navigation systems. Specifically, we want to use OpenTCS with our our robots that are running ROS. The standard is really targetted at this topic. However, the standard is not open source and I can\u2019t find fleetmanagers implementing the standard.", "I found a question on this topic on answers: ", " They have a similar conclusion: not open source and no implementations.", "In our project we need functionality to exchange map data. So, we can implement it according to a standard or we can build our own custom solution. At the moment, I don\u2019t know the best solution yet.", "Word of warning, below is a textwall that may be interesting to some, but may not answer the posters question. Read at your own peril.", "I did some research as part off my Master thesis where I created a map using JOSM (java editor for OpenStreetMap) using indoor mapping plugins and then hosted the database (map nodes and relations) locally (could also be hosted online). A simple python node can then be used to query this database using overpass API. I put in information like different floors, hallways, door colours, which sensors could be used in which area. It was all kind of experimental, but technically possible. It gave me a multi-layered map containing the low level x-,y-, occupancy information, a low level topological map, high level topological map all the way up to a high-level semantic map. I never got to the point of actually using the map for navigation though ", " (the mapping effort was luckily enough to graduate on).", "The advantage of creating such a map is that the robot does not have to think a lot. All of the information about traffic rules and which methods of localization and navigation to use in which area are all embedded in the map. The obvious disadvantage is the insane mapping effort required to get such a map. Additionally there is no standard way to this that I am aware of. I came up with my own model, my own hierarchy and own key-value combinations. I just wanted to share my experience and point out that OSM can be used for indoor mapping as well and that OSM has a pretty big open source community so lots of tools and plugins are available.", "Another student from Germany was working on the same project at the time.", "\nThis could be an interesting read for you (summary of a couple of slides):", "\n", "I have also been looking into standard map data formats. I encountered the IEEE Standard for Robot Map Data Representation for Navigation. Based on the name, it seems like the thing I was looking for. However, I am not sure whether to use such standards, because they are closed source and we work in an open source projects.", "There\u2019s nothing to stop you using a closed standard to design open-source software, except in rare situations (such as the AUTOSAR specifications) where the license explicitly forbids it.", "The catch is that only people who have access to the standard will understand your design decisions.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/deterministic-navigation-in-ros/11442"}
,{"title": "Technical Steering Committee (TSC) Meeting #11 Minutes", "thread_contents": [" Geoffrey Biggs", "Question for all TSC members: If the safety case example documents contain the following notice, are they still usable?", "By using this safety report (\u201cthe Report\u201d) produced by the Connected Places Catapult (\u201cCPC\u201d) you accept this disclaimer in full. The Report has been prepared in good faith on the basis of information, findings and analysis of our specific research activity entitled \u201cAutonomous Valet Parking\u201d. All information contained in the Report is provided \u201cas is\u201d and CPC does not guarantee or warrant the accuracy, reliability or completeness of the information in the Report or its usefulness in achieving any particular outcome or purpose.  CPC does not owe a duty of care to any third-party readers.", "\nYou are responsible for assessing the relevance and accuracy of the content of this publication. You must not rely on the Report as an alternative to seeking appropriate advice.  and nothing in the Report shall to any extent substitute for consultation with an appropriately qualified advisor.  You must obtain professional or specialist advice before taking, or refraining from, any action on the basis of the content of the Report.", "\nTo the fullest extent permitted by law, CPC excludes all conditions, warranties, representations or other terms which may apply to the Report or any content in it, whether express or implied. CPC will not be liable to any user for any loss or damage, whether in contract, tort (including negligence), breach of statutory duty, or otherwise, including without limitation loss of or damage to profits, sale business, revenue, use, production, anticipated savings, business opportunity, goodwill, reputation or any indirect or consequential loss or damage.  Nothing in the Report excludes or limits CPC\u2019s for any liability that cannot be excluded or limited by English law.", "\nAny entity seeking to conduct autonomous vehicle trials will need to develop and publish a safety case specific to their own trials (as specified by the government\u2019s Centre for Connected & Autonomous Vehicles (CCAV) Code of Practice for Automated Vehicle Trialling) and gain permission to do so.", "Held 2 Autoware Maps WG meetings", "\n26 September 2019: ", "\n10 October 2019: ", "As a WG, we\u2019ve made quite a lot of progress developing use cases for maps and creating requirements documents associated with those use cases.  There are 2 logical next steps:", "The architecture work seems to be more of the Autoware WG remit, so we\u2019ve proposed to join the Autoware WG and generate agreement there before implementing the solution.", " ", " ", " regarding the \" Notice in safety case example\" - I have no problems with it and would love to read it.", "I fully understand that CPC does not want to be liable. Other safety reports (e.g. the one from ", " also contain disclaimers.", "Just to be clear, we are talking about a safety case such as you might hand to TuV for certification, not a safety report, which is made as much for public consumption as anything else.", "The primary purpose of this documentation is to construct a safety plan to supply to a regulator (e.g. DfT in the UK) and which would be considered in court if there was serious incident during testing the automated vehicle. It\u2019s purpose is to show that the risks have been considered and mitigated where possible.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Alfredo Bencomo (Open Robotics)", "Geoffrey Biggs (AWF)", "Leo Fang (Hesai)", "Esteve Fernandez (Apex.AI)", "Kenji Funaoka (TierIV)", "Brian Holt (Parkopedia)", "Dan Isaacs (Xilinx)", "Shinpei Kato (AWF board)", "Seonman Kim (LGE)", "Taylor Lochrane (DOT)", "Angelo Mastroberardino (Parkopedia)", "Sathya Prasad Nanjundaiah (TCS)", "Tsutomu Otake (Macnica)", "Dejan Pangercic (Apex.AI)", "Brian Shin (LGE)", "Stephane Strahm (Kalray)", "Daisuke Tanaka (Tier IV)", "Akihiko Tsukuda (eSOL)", "Josh Whitley (AutonomouStuff)", "Opening remarks and new member introductions", "\n", " Confirmation of previous minutes", "\n", " Action items from previous meeting", "\n", " Autoware", "\n", " Map formats", "\n", " ECU/Platform", "\n", " Simulation", "Release the safety case example\n", "\n", "Review the notice proposed to be added to the safety case example documents\n", "All", "\n", "Create working group wiki pages\n", "\n", ", ", "\n", "\n", "Follow up on the use of the reference platform hardware in the AVP demo\n", "\n", "Provide a backup plan for the AVP demo\u2019s ECU\n", "\n", "Get a summary of the LGSVL license review conclusions and make it publically available\n", "\n", "List requirements for simulation and provide them to the TSC\n", "\n", " / Maps WG", "\n", "Taylor Lochrane, Federal Highway Administration of the DOT\n", "Government Member", "Developing their Carma programme and hoping to work with Autoware as they grow this across the US.", "\n", "Minutes approved.", "Release the safety case example\n", "\n", " Still can\u2019t release, but the documents are in much better state. Now licensed as CC-BY 4.0; currently under review by the legal department which still needs a little convincing.", "\n", "Create working group wiki pages\n", "\n", ", ", ", ", ", ", "\n", "\n", " Pages created for Reference Platform and Maps WGs. Still none for the simulators working group.", "\n", "Post the Autoware software architect recruitment notice to any useful forums\n", "AWF board", "\n", " None", "\n", "Provide HD Map for AutonomouStuff carpark\n", "\n", ", ", "\n", "\n", " Map has been made available and a new version is going up today that irons out many bugs. The map format is Lanelet2. Plan to release several more maps soon along with Carla simulation environments. The end-goal of the Maps WG is Carla simulated environments using OpenDrive maps. A common maps repository is needed. (", " to create one.)", "\n", "Merge Request registration deadline passed at the start of October.", "Over 100 MRs were merged prior to the deadline.", "60 MRs remained to be reviewed and merged after the cut-off.", "Making good progress towards getting all of them reviewed in time for the MR merge deadline at the start of November.", "Tier IV will be doing in-car testing over the next two days. Further testing will be done in November.", "AutonomouStuff will be doing in-car testing in November.", "Target release date (December 3rd) is still on track.", "IMU/GPS driver ready for merge after addressing Ito-san\u2019s feedback: ", "\n", "Request community involvement in reviewing MRs in Autoware Auto\n", "Pending MRs on ", " and ", " work:\n", "\n", "\n", "Pure pursuit:\n", "\n", " : ", "\n", "We are working on porting to Autoware.Auto ", "\n", "\n", "\n", " \u2013 planner and controller are functional, but features are being added in the coming months", "Planning the integration hackathon in Bay Area early 2020 should begin \u2013 shooting for end of March 2020", "London Autoware.Auto hackathon with Parkopedia", "Autoware Meetup mid-September 2019: meetup had very good attendance of about 45-50 people and everything went very smoothly.", "Autoware presented at the ROS Meetup in Munich: ", "\n", "Who is going to work on NDT mapping for Autoware.Auto?\n", "\n", "Rationalise the Use Cases and Requirements into a single coherent set", "Begin work on the architecture that will fulfil these requirements", "Apex.AI: Is the Maps WG looking for use-case specific requirements, or requirements from the whole stack in general?\n", "\n", " : The WG has plenty of use cases and requirements, the question now is, how do we fulfil these requirements?", "\n", "Apex.AI: We need a software architect who can do that stuff and who has lots of experience in autonomous driving. We don\u2019t have one, so we should focus on achieving the autonomous valet parking use case early next year, even if it is not the greatest architecture.", "\n", ": It feels like the Maps WG has done a lot of work and achieved much, but it is not clear what we can do next due to other aspects of Autoware.Auto not progressing.", "\n", ":  There are a few people who can at least provide feedback on an architecture, so get them involved.", "\n", ": The next Maps WG meeting will work on a candidate architecture that we can take to the Autoware WG meeting.", "Meeting every two weeks, with a two-meeting schedule (NA-friendly Asia-friendly)", "First meeting was held on the 15th/16th of October, and was an introductory call.\n", "12 participants, which is a good number for a first meeting.", "\n", "Have listed the initial list of requirements for the platform and for the working group to start getting feedback.", "Have described the initial reference platform which is intended as a starting point to kick-start discussion and work.", "Need to focus now on creating requirements, and on attracting people to work on the reference platform.", "What tool and process should we use for requirements management?", "The initial reference platform is being worked on at Autocore.", "The Reference Platform WG would like to have oversight of the ", " project.\n", "In particular the interfaces for sensors such as cameras and LiDARs, etc. need to be determined; these may be out of scope of the reference platform WG but they are part of ", ".", "\n", "\n", " Hardware/Software[Autoware] interfaces at the ROS level are the scope of the Autoware WG (because they are part of the Autoware architecture), but the reference platform WG needs to be heavily involved.", "\n", " Is it feasible to have the hardware reference platform available by March next year for the AVP demo?\n", "\n", " Yes, probably, but need to confirm with Autocore", "\n", " What is the backup plan if it is not ready? What we need is the Lexus vehicle equipped with sensors performing the AVP demo, and that vehicle needs a computer on which Autoware will run and the correct sensors.", "\n", " Will follow up offline after confirming with ", ".", "\n", "\n", " We need a backup plan using a standard computer and Linux, or something like that. But ideally we want to use the reference platform with an RTOS.", "\n", " eSOL is not yet working on Autocore\u2019s hardware for use with eMCOS, so we would like documentation on the hardware so we can look into beginning that work.", "\n", " Is Autocore\u2019s hardware currently the only available hardware for the reference platform? If it is a reference platform there should be more than one.\n", "\n", " Currently Autocore\u2019s is the only reference platform that has been proposed, and it is still being brought up.", "\n", "\n", " will follow up on if the Autocore hardware will be ready and the backup plan if not.", "\n", " will be looked after by the Reference Platform WG. Analogous to the Autoware working group looking after the Autoware.Auto project.", "Second WG meeting was held. Discussed the simulation facilities required for the AVP use case.", "Discussed features that a simulator should be judged on, including factors such as supporting a scenario format, being deterministic, and so on.\n", "Next WG meeting will begin reviewing simulators according to each of these requirements in order to identify suitable simulators for the AVP use case.", "\n", "Apex.AI has decided to use LGSVL to demonstrate its technology at ROSCon and at CES. The goal is to show Apex.AI\u2019s LiDAR data processing using a simulated environment. Setting up the demo was relatively easy and LGE provided good support. Apex.AI sees no reason not to use this simulator right away for the AVP use case. Can LGE volunteer to integrate the same demo based on Autoware.Auto (not the Apex.AI proprietry software)?\n", "\n", " We can look at it, but not yet clear what the integration work is that is required. There are several undecided points still, such as map format. Perhaps this is better work for the WG to do.", "\n", " Reviewing simulators provides valuable information, but doing this sort of practical contribution is probably more immediately useful to the Foundation.", "\n", "\n", " Is there interest in Uber\u2019s web-based visualisation tool?\n", "\n", " This particular tool may not meet our needs, but the ROS community in general is heading in this direction so in the medium term expect to have options available we can just use.", "\n", "\n", " Has the work to review the LGSVL license been completed and what was the outcome?\n", "\n", " We are using the simulator for demos where we can\u2019t bring a vehicle which fits within the license.", "\n", " The review did happen and the conclusion was that the license is legally not a problem but usability-wise not using a standard license makes it a bit annoying for users to be sure if the license is OK.", "\n", " The Unity license does not come into play unless you start acting as a Unity developer, i.e. you open a project and start making content to use with the simulator. Normal users targetted by LGSVL (using just the binary distributions) don\u2019t have to make a payment to Unity.", "\n", " to get the board to summarise their license review and make it available widely so that potential users of LGSVL can have peace of mind.", "\n", "The Maps WG seems to be targetting Carla, but the AWF in general is preferring LGSVL. We need to get a demo/product out and simulation is a key enabler, but we are still talking about two simulators and not even one is being properly used. We must agree on one well-integrated and used simulator rather than having two simulators, neither of which is used properly. Whether this is Carla or LGSVL is not the problem, what is needed is choosing one and all using it.", "\n", " The Maps WG has done all its work in the open, so there shouldn\u2019t be any surprise in the route chosen (using Carla and OpenDrive). Also Autoware is supposed to be open to any simulator. Carla was chosen because it has the best support for maps, especially OpenDrive, as well as the overall ASAM toolchain for autonomous vehicles (OpenScenario, etc.).", "The Simulation WG is aware of the need to support OpenDrive, OpenScenario, etc.", "LGSVL is currently finishing up support for importing OpenDrive maps. LGSVL plans to support ", " scenario format but is not currently working on OpenScenario.", "The Maps WG should inform the TSC and the next meeting what their requirements for simulation are."], "url": "https://discourse.ros.org/t/technical-steering-committee-tsc-meeting-11-minutes/11165"}
,{"title": "ndt_mapping Method Types and parameters", "thread_contents": ["Is there documentation regarding the method types and other parameters for ndt_mapping? Most of the settings are straightforward, but some are opaque. There is no documentation regarding pcl_anh or pcl_anh_gpu or pcl_openmp as far as I can tell. Does \u201canh\u201d refer to the adaptive octree region-growing method devised by ", "? Extracting assumptions with which to justify node settings for map generation is challenging using the source code alone. A simple explanation (or README) would prove very helpful.", "Followup question - What are the trade-offs when comparing the use of pcl_generic vs pcl_anh?", "I guess \u201canh\u201d stands for the author\u2019s name of the implementation. pcl_generic uses PCL, while pcl_anh* uses its original implementation. pcl_generic uses radiusSearch, while pcl_anh* uses direct mapping from x,y,z for searching neighbor cells. pcl_anh* consumes a lot of memory compared to pcl_generic. I\u2019m not familiar with pcl_openmp.", "Like ", " stated ", " is the last name of the author. \u201cAnh Viet Nguyen\u201d a PhD student in Nagoya University, who specialized in GPU acceleration. I\u2019ll ask him to link here his research paper.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["<?xml version=\"1.0\"?>", "<package>", "    <name>ndt_gpu</name>", "    <version>1.11.0</version>", "    <description>The ndt_gpu package</description>", "    <maintainer email=\"yuki@ertl.jp\">Yuki Kitsukawa</maintainer>", "    <maintainer email=\"anh@ertl.jp\">Anh Viet Nguyen</maintainer>", "    <license>Apache 2</license>", "    <buildtool_depend>catkin</buildtool_depend>", "\n", "    <build_depend>libpcl-all-dev</build_depend>", "\n", "    <run_depend>libpcl-all</run_depend>", "\n", "</package>"], "url": "https://discourse.ros.org/t/ndt-mapping-method-types-and-parameters/8482"}
,{"title": "[WG RP] Autoware Reference Platform Definition Documentation", "thread_contents": ["This document is presenting the intents of the Autoware Foundation (AWF) Reference Platform Working Group (RP-WG). This is a live document, providing to reader a view of the purpose, plan and orientation of the Working Group. This document has to be reviewed by all participants to the RP-WG and will be submitted to the Technical Steering Committee on regular basis.", "Autoware Reference Platform Working Group  ", "Document Information  ", "Use Cases and Configurations  ", "Features  ", "Roadmap  ", "Reference Software Platform  ", "Reference Hardware Platform  ", "In July 2019, AWF TSC launched the creation of Working Groups in order to focus technical topics and oversight them (", "). The Reference Platform WG (RP-WG) has been announced at the same time (", ").", "Co-leaded by Autocore and Kalray representatives, it is calling for participants to contribute to the different activities to be covered:", "Define the initial Reference Platform: hardware and software-wise", "Work on roadmap for alignment of hardware and software with Autoware.AI and Autoware.Auto", "Establish and maintain a roadmap longer term", "Contribute for hardware and software integration", "The principles of the RP-WG can be summarized as being:", "Blockquote The ultimate goal for AWF is to be able to present to its users a system that he can acquire and use for creating prototype vehicles and reduce his time to market for production. It will contain best in class compute platform, including accelerator capabilities to run in an optimized way the latest Autoware.Auto release.", "Several intermediate steps prior achieving and maintaining this goal will be necessary. This is one of the purpose of this Working Group to define these steps and perform them.", "This is working document, regularly updates", "Some parts are are under investigation, under discussion by the Working Group.", "Draft:", "P1 provide CAN/UART capability device drivers (ROS/ROS2) and sensor data synchronize.", "P2 provide LVDS capability and CV/Fusion features for camera data.", "P3 running autoware.ai stack.", "P4 running autoware.auto stack.", "About deployment of P1-P4:", "Support different deployment strategy with different computer platform configuration.", "You can put P1-P4 together in one powerful computer.", "Can also deploy different features in different computer unit.", "Generally, P1 should compatible vehicle network including AutoSAR Stack", "If using multi boards solution, use ETH or TSN as network solution.", "CNN accelerator can be a part of P2/P3/P4 as algorithm needs.", "Opens:", "To be studied", "Will be aligned with Autoware.AI (first) and Autoware.Auto features as suppoted by Hardware.", "To be studied", "The AWF Reference Platform will use Autoware Software.", "Initial platform will rely on Autoware.AI  and support Autoware.auto from early stage.", "To be align roadmap SW/HW", "Demo/dev car configuration of our current autoware.auto development team(Apex)", " : Lexus 450 LH with the ", "DBW interface", " :", "4 ", "(or comparable sensors, e.g. VLP-32C)", "16 Sonar sensors", "4 ", "(180 degree FOV)", " :", "aarch64 computer", "rugged x86-64 desktop computer", "The Kalray MPPA (Massively Parallel Processors Array) is a Manycore architecture allowing to executing compute and acceleration algorithms in an optimized and low power consumption.", "The integration with Autoware.AI consists as of today in offloading the following algorithms onto the MPPA", "Apollo 5.0 sensor configuration:", "\n", "Hardware deployment:", "For clarification, the reference software is the responsibility of the Autoware working group. This working group\u2019s responsibility extends only as far as software drivers for ECU-specific hardware. For anything else, please coordinate with the Autoware working group.", "Fully agreed ", ", no confusion on this point.", "we proposal several different kinds of node (P1-P4) for below reasons:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Moving market to benefit from Autoware Eco-System\n", "From Experimentation to Reference Implementation to Production", "Autoware members have all bricks for Reference Implementation Step", "Experimentation Step is on-going", "Let\u2019s move to Reference Implementation Step", "Define it, plan it, make it", "All Autoware members have a brick", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Sensors supported: need to discuss with community, need to know sensor configurations of different useage.", "Support boards: need to discuss with community.\n", "Current Autoware.auto support IPC and Xavier. It\u2019s nvidia GPU solution.", "Heterogeneous solution: 96boards automotive and standalone accelerator (Autocore and Kalray)*", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Reference Accelerator Boards\n", "PCIe boards with MPPA\u00ae2 Generation, MPPA\u00ae3 upcoming", "Accelerator daughter board modules", "\n", "Reference Programming Software\n", "CNN Inference Optimizer", "CNN Inference Runtime Engine (additional algo support and growing)", "Computer Vision OpenCV", "OpenCL 1.2", "\n", "Hardware Integration\n", "Intel and ARM hosts", "\n", "Software Integration\n", "Additional Use Cases support (Companion, StandAlone)-to simplify System integration", "Linux and Autoware Member RTOS supported", "\n", "Image Detector\n", "CNN based object detection (car/person/\u2026)", "Autoware uses Darknet(YOLO networks)/SSD/RCNN project", "SSD and Yolo V3 CNN have been tested within Autoware, using Kalray inference engine solution on MPPA\u00ae", "Other CNN can also be used", "\n", "LiDAR Localization\n", "Based on \u201cNDT matching\u201d algorithm from PCL library", "Compute car coordinates by matching its LIDAR data to a precomputed map", "Fully implemented on MPPA\u00ae (both initialization and runtime functions)", "\n", "considering next gen E/E arch of vehicle.\n", "P1 could be a gateway device which support different connections and can support gerneral sensor message interface(driver).", "P2 could be a smart camera which are used in current ADAS system. most of them are based on vision purpose SoC.", "P3/P4 are general purpose computing unit.", "different unit target different ASIL level in plan.", "P1(Gateway) and P2(Smart Cam) target ASIL B.", "P3/P4 should support hardware redundancy (as request) and has ASIL D decision maker unit(MCU) inside.", "\n", "enhanced P1/P2 which can deploy some peception features will be benifical to HCP system\u2019s algorithm optimzation. can left the optimize work to device provider.\n", "for example, P1 can filter and fusion lidar data by dedicate hardware package accelerator.", "\n", "if user are using x86 IPC, everything should be working fine and no difference.", "Software requirements for Autoware WG:\n", "code organization (repo split should be better, I think)", "build standard message spec/definiation. it should be very useful for realization of special hardware/accelerator.", "\n"], "url": "https://discourse.ros.org/t/wg-rp-autoware-reference-platform-definition-documentation/9949"}
,{"title": "Lanelet2 ROS Message Design", "thread_contents": ["Currently, there is no ROS message defined for Lanelet2. The official Lanelet2 seems to take some time to define them. ", "However, it is urgent to replace current vector map format with Lanelet2 since the closed format is slowing down the development of other modules (e.g. perception and planning). Therefore, I suggest we define ROS message with Autoware community and use it for the meantime. Perhaps we can create PR to official repository if it works well within Autoware.", "For Lanelet2 ROS message definition, I have considered three approach so far.", "I prefer the third approach since I never used ", " to track down the issue related to vector map in my experience. I have used to check if the topic is published or not, but I never used it to actually check the content of topic data.", "Any suggestions or feedback would be appreciated.", "Thanks for opening the discussion Mitsudome-san.", "I agree on your comments, working with the binary topic has been the most easy so far.", "For 2) there is also the need to project back into long/lat to write the xml file, and then on the subscriber to project into the lanelet map structure again. Not such a big problem, but for MGRS projection there was some issue with the MGRS base code sharing for reverse projection? Anyway, it introduces a layer of processing not needed by binary format.", "For 3) is it an option to publish the full map, and nodes that desire subsets of the map (either geometric or primitive subsets) could use a ROS service to ask for a defined subset - the map loader could then construct a purpose built lanelet map of the subset, and send in single topic format?", "Similarly for 1) rather than define individual message types and filling out all information, subset maps (containing for instance just point primitive layer) could be generated and published as subset lanelet maps and recombined if necessary at the subscriber. This would simplify the message structure and take advantage of the existing API to construct messages.", "Thank you for the detailed explanation,", "\nI agree with you, option 3 is good.", "\nalso lets consider that in the future map information will be updated regularly, such as dynamic maps , so we semantic based ROS topic.", "\ncurrently in OpenPlanner I use option one, although we can solve the multiple topics problem by making one message holding the whole map data (or part of the map) .", "\nthe processing cost here is only rebuilding the network to achieved faster search.", "\nexample of the functions needed for planning:", "it will be good if we can build the road network and send the map as ros topic, one problem with that is data size. but if we consider a map server who send small part of the map frequently then this is the best solution in my opinion.", "Please consider in your options:", "Please also state your assumptions, requirements, and detailed use cases that will drive the decision on what sort of messages to create.", "Thank you for your comment.", "\nHere are some of the information that I can think of", "Assumptions:", "Requirements:", "Detailed Use Case:", "I overall agree, but need to add a couple of things", "Assumptions:", "Detailed Use Case:", "Some thoughts on Geoff\u2019s suggestions.", "Regarding Structured vs Unstructured messages: I initially started implementing structured messages with each lanelet2 primitive having its own message type. The only problem at the time was that it was convoluted accessing the underlying data from the lanelet2 API to build each message, but now we have more experience with the API it should be relatively easy. As far as usage goes however, I don\u2019t know if it makes sense to structure the data in messages.", "Regarding an ideal structure for map messages oriented towards use, I conceptually agree, but in practice, how we use data is largely defined by the internal format. The messages being passed are being consumed through the internal format. If it makes sense to build a ideal message format, then why not build an ideal map format?", "I disagree that much code duplication would occur even if every node that utilized map data in an unstructured Lanelet2 format has to do some sort of decoding - this is the purpose of shared libraries. A single encode and a single decode function should exist (or be created) in a shared library which can be linked against by any nodes needing to handle Lanelet2 format data. The decode function would return a class structure holding the in-memory representation of the map (or subset of the map) which can be manipulated as-needed by the consuming node and the encode function would simply return the encoded, unstructured data which can be published. Minimal code duplication.", "On a related note, I agree with ", " - a structured representation of the data being passed around doesn\u2019t serve much practical purpose with the design of Lanelet2 having such a strong decoupling of the primitives from what a grouping of primitives represent. The overhead inherent in encoding/decoding them as structured messages and the additional bandwidth required for the metadata are also non-trivial. Given that network I/O is second only to disk I/O in order of the most expensive operations you can perform on a modern computer, I lean heavily toward unstructured transport. I also think the actual implementation of how we access the map or a subset of the map (making requests to a server, segmenting the map, etc.) are somewhat irrelevant to representation of the data at the transport layer. Whether we split this up into multiple nodes or have a single server handling the map, they are still passing around the same basic datum and, in the case of Lanelet2, those datum are not easily reconstructed into the objects they represent without the structure of the Lanelet2 class objects.", " Hi, I guess the idea of code duplication concern arose because of the suggestion that some nodes might not want to link to the lanelets shared library, and instead work directly on the message data. If a shared library is to be linked anyway, we may as well use lanelets, or might there be a need for a lighter encoder/decoder lib with less dependencies?", "Assumptions:", "If you do not know how the data will be used, you should not make this assumption.", "Message data must contain enough information to reconstruct LaneletMap data structure", "This places no requirement on the message data itself being in the Lanelets structure.", "Message should be flexible enough to be used in different use cases. e.g. message should be able to express custom tags without changing message definition", "Message should be scalable. e.g. It should be able to represent subset of a map.", "Detailed Use Case:", "These are not detailed. I recommend you follow a use case template. It will help you fill out the necessary detail. A good example is the ", ". Martin Fowler has also produced one, and I\u2019m sure there are thousands of other templates.", "Keep in mind that this is just the start. From your use cases, you will need to derive requirements.", "Thank you for your reply and sorry for my late response.", "You are right that we should not design message without defining use cases first. I have started writin use cases of map information with Cockburn\u2019s template(with some modifications).", "\n", "TEMPLATE Use case name: <the name should be the goal as a short active verb phrase> Context of use: <a longer statement of the goal, if needed, its normal occurence conditions> Level: <one of: summary, user-goal, subfunction> Primary Actor: <a role...", "\n", "\nI wrote them based on functions of current Autoware and functions that are likely implemented in the future, although it is not finished yet. We are planning to discuss about use cases and requirements in detail in the ", " so hopefully we can provide more sophisticated use cases in next few weeks. Then, we can continue our discussion on requirements and designs.", "Having said that, I would like to start Lanelet2 integration with unstructured(binary) message approach for v1.13, as it seems to be most supported message type so far. I believe fundamental purpose of replacing Aisan vector map implementation is to make Autoware\u2019s HD Map format public (both physical storage format and internal data), and having best message design is not crucial to the purpose as long as it transfers sufficient information. Current closed format is likely to discourage developers to create new features or even enter into discussions like this, and dropping Lanelet2 features from v1.13 could be a huge loss to the community. Therefore, I suggest to start implementation with binary message in order to hedge the risk. I am aware that the ", " is longer than last release, but I would like to submit merge requests in advance because there are many nodes that uses vector_map_info/* topics that needs to be replaced.", "I understand the importance of following engineering process, and we should continue designing best map message for Autoware in this thread as well as in Map WG. We will do our best to keep implementation of Lanelet2 nodes for v.1.13 to be independent from message structure so that we can replace it easily once we have the new message design.", "\nHow does this sound to you?", "Hi ", ",", "so, if we go for option (2) or option (3) we have something like that right ?", "So, because we are using the same library we don\u2019t need a big structure of ROS msg (it would duplicate the work), and just use lanelet to manage errors in read/write a msg.", ". should we have a way to send a ", " to the server in case the map delivered (XML or BIN) is good or bad ?", "Also \u2026 should the msg be a ", " or ", " ? in our case, for parking garage maps, request response is good enough (you ask for the map of a given area when you are getting closer to it). Is it similar to what you are doing ?", "So, because we are using the same library we don\u2019t need a big structure of ROS msg (it would duplicate the work), and just use lanelet to manage errors in read/write a msg.", "That was my idea.", " . should we have a way to send a ", " to the server in case the map delivered (XML or BIN) is good or bad ?", "I am not sure if we should have this feedback. If we are going to have one, then the server should do something when they get the error message, but I don\u2019t know if there is anything server can do about it. Maybe sending the topic again if it is pub/sub? Also, what do you mean by good or bad? Are you talking about corruption of data?", "Also \u2026 should the msg be a ", " or ", " ? in our case, for parking garage maps, request response is good enough (you ask for the map of a given area when you are getting closer to it). Is it similar to what you are doing?", "Our work on replacing current Aisan vector map implementation is done by pub/sub. This is just because current implementation is done that way. We should probably think about it again once we have all requirements listed up.", "I understand the importance of following engineering process, and we should continue designing best map message for Autoware in this thread as well as in Map WG. We will do our best to keep implementation of Lanelet2 nodes for v.1.13 to be independent from message structure so that we can replace it easily once we have the new message design.", "\nHow does this sound to you?", "If you want to do it that way for Autoware.AI, then I don\u2019t really mind. But remember two important things:", "ok, thanks for the explanation. It\u2019s clear.", "For the ", ", yes I mean the client could notify the server if it gets corrupted data (basically a map it cannot read).", "In that case, the server could", "\n(1) loop and resent the map until the client has a good map, or until it made N-failed-attempts,", "\n(2) notify another component, say ErrorHandler, or a human to log the error so that developers can investigate why in that area / to that client the map breaks.", "Maybe it\u2019s not necessary in this phase, but when the product is fully developed and used, it\u2019s a good tool for crisis management.", "Hello, excuse me for cutting in. Please let me write my idea.", "As for data corruption and feedback, what is the reason of data corruption?", "In my understanding, generally, if the sender sent correct data, the receiver will:  1. receive correct data,  or 2. not receive any data.", "\nIt won\u2019t receive corrupted data because it is detected in the transport layer.", "Or, if the map file itself is corrupted, it should be detected and fixed in the verification step, not in the runtime, and the sender should not send the data.", "So my opinion is,", "As for pub/sub or server, it depends on the target, Autoware.ai or Autoware.Auto.", "For Autoware.ai, since the map node sends whole map data, I think using Service is more natural than pub/sub.", "\nHowever, since the current implementation uses pub/sub, to keep using pub/sub might be better, considering the re-implementation cost.", "For Autoware.Auto, it should be drastically redesigned for more safety, including Lanelet2 library.", "\nI think sending structured data using pub/sub is best for now, but as we discussed at the last meeting, let\u2019s consider this using the AVP use case.", "By the way, I feel we need to have the same understanding of this WG\u2019s goals.", "\nSince map-related things have many aspects and the best solution depends on the target, we have to clarify what target we refer to.", "Therefore, please let me summarize the goal(for .AI/.Auto), current status, and milestones of this WG.", "\nI will write them maybe tomorrow in this thread: ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\nThis is similar approach with current vector_map_msgs. Map data is divided into multiple topics based on object types(point, line, lane, signal, traffic sign, etc.), and other nodes only subscribe to interested object. This keeps away from different nodes having all copy of map data which increases memory efficiency.", "\nIf we are going to use similar design for Lanelet2, we probably would divide topics into premitives: point, linestring, lanelet, and regulatory elements. (See here for lanelet2 primitives ", ") One of the concerns is that if we are going to use Lanelet2 library, then a node would need to subscribe all the topics anyway and have no merits of having separate topics.", "\n", "Merits:\n", "Memory efficient when there are many nodes subscribing to map data (but not likely with Lanelet2)", "Easy to look for specific object from ", "\n", "\n", "Demerits:\n", "Needs to synchronize the topics if there are any updates at run time", "Needs to subscribe many topics to obtain map data as a whole", "\n", "\n", "\nMap will be stored as xml data as string member of the message. Lanelet2 has functions to write maps into OSM(XML) format file, and we can write into ROS string message instead of writing into filestream. This way, we can convert the message back to LaneletMap(C++ class used in Lanelet2 library) just like loading map from a file. Developers can also check data with  ", "  and read the message data in XML format. I do have concern that output  ", "  do not process newline, and it just outputs the code itself(\u2019\\n\u2019). Therefore, although output of the command is in XML format, it is very hard to comprehend.  ", "\n", "merit:\n", "no need for synchronization (single topic)", "can reuse code from Lanelet2 writer/parser", "message data will be exactly same as loaded osm file", "\n", "demerit:\n", "data size would be relatively large since they are in text format", "\n", "\n", "\nThis approach would be same as the second approach, except that the data will be stored as binary instead of XML. Lanelet2 also has function to write maps in binary format(BinWriter class), so we can also reuse the code for this approach as well. Since the data will be binary, data size should be smaller, but output from ", " would be totally incomprehensible. However, I think this wouldn\u2019t be a problem if we have good visualization in RVIZ. (In fact, pointcloud data is using binary format for ROS Message and have been working so far).", "merit:\n", "no need for synchronization (single topic)", "can reuse code from Lanelet2 writer/parser", "data size will be smaller than XML data", "\n", "demerit:\n", "\n", " output would be incomprehensible", "\n", "Find road boundary around the vehicle.", "Find next traffic light position.", "Find next sign , stop line , \u2026", "Extract current lane information (center line waypoints,  direction, overtaking, left and right lanes, \u2026)", "Use of a map server with services to provide a subset of the map based on certain criteria (map in a specific area, lanes along a route, etc.), as well as a service that can provide the whole map.", "Whether you can send structured map data rather than wrapping an XML or binary chunk in a ROS message, because this would give more flexibility for other nodes to use the data even without the lanelets2 library, and would ease debugging for those who wish to debug at the topic level. This may require modifying the lanelets2 library but you should give this consideration.", "Whether using structured map data in ROS messages with that structure oriented towards use rather than source format would give more flexibility to support different map formats in the future. In other words, make your ideal map message(s) rather than lanelet2 messages.", "All nodes that access to map data should use Lanelet2 library. Although you mentioned about flexibility of using data without lanelet2 library, I think we should access to the map through the library if you don\u2019t have any critical reasons. This would keep us from having similar codes everywhere in Autoware.", "Message data must contain enough information to reconstruct LaneletMap data structure", "Message should be flexible enough to be used in different use cases. e.g. message should be able to express custom tags without changing message definition", "Message should be scalable. e.g. It should be able to represent subset of a map.", "retrieving traffic light related to current lane", "calculating route to goal from current position", "retrieving crosswalk area", "retrieving intersection information (yield, right of way)", "finding stop lines", "retrieve geometrical subset of map", "if we need to access the map via lanelet2 library, we need to keep the file into ", " instead of plain vanilla OSM", "retrive the nearest parking space available from the map, or a section of it", "The structure does not delineate any useful subsets of the data. The hierarchical nature of the format means that it hard to imagine requesting subsets of the map information that does not contain most information (i.e most requests will be at lanelet level, which requires points, linestrings and related reg.elems anyway). A lanelet message by itself does not have any spatial information, just attribute data. Conversely just sending points has little meaning either. I would argue using structured messages to send subsets of map (based on structure of format as opposed to geometry deffined subsets) would not be particularly useful. In a similar manner, nodes using the map data without lanelet2 library would have to reconstruct the lanelet data for it to be useful, leading to replication of code as Mitsudome-san mentioned. Perhaps excluding the lanelet2 library would be desirable for very lightweight nodes?", "The other consideration is that structured messages could be useful for topic level debugging. My initial reaction is that map data is only coherent viewed at higher levels, and that viewing the contents of individual primitives may not be so helpful for debugging. I it might be useful to verify that well formed data is being sent, or that a particular type of lane is being sent. Perhaps visualization tools would be more helpful in this regard? However, I certainly can appreciate that topic based debugging could be useful, and am open to including structured messages if others think in necessary", "We could support both? For certain cases it might make sense to debug using the structured messages, or that structured messages might encapsulation useful subsets of map data. In other cases binary data message is fast and easy.", "All nodes that access to map data should use Lanelet2 library. Although you mentioned about flexibility of using data without lanelet2 library, I think we should access to the map through the library if you don\u2019t have any critical reasons. This would keep us from having similar codes everywhere in Autoware.", "Define those use cases.", "Be very careful about allowing custom data in messages. You need to have a strategy in place for ensuring correct behaviour in the face of incorrect data.", "Although I agree with this, you need to define why it is necessary so you know exactly what a \u201csubset of a map\u201d means.", "retrieving traffic light related to current lane", "calculating route to goal from current position", "retrieving crosswalk area", "retrieving intersection information (yield, right of way)", "finding stop lines", "retrieve geometrical subset of map", "The Maps Working Group is for the Autoware Specification and its reference implementation, Autoware.Auto. You cannot ask the Maps WG to make decisions based on a need from Autoware.AI.", "If you make a hasty design decision now to get something out soon, you are encruing technical debt. This is an accepted software engineering practice, but in this case you will be paying back that debt by re-implementing all your Lanelet2-related stuff again later on.", "A simple resending feature is required because sometimes sending data might fail, but probably it can be done by ROS/ROS2 features.", "No excessive feature is not necessary."], "url": "https://discourse.ros.org/t/lanelet2-ros-message-design/9932"}
,{"title": "Autoware Map Data and Formats working group: Call for participation", "thread_contents": ["Following on from ", ", the announcement of ", " and the ", ", I\u2019d like to announce the ", ".", "This working group will be led by myself, ", " and ", ". The purpose of the working group is to consider the various options for map formats and define the architecture within Autoware to consume the maps. The working group\u2019s remit will cover the following topics at least:", "We will begin by meeting every two weeks. The first meeting time is  Wednesday, July 17, 2019 2:00 PM Europe: London (see below for the meeting information). The meeting will last for 30 minutes.", "The regular time is to be decided based on who wants to participate in the working group, so if you are interested then speak up here even if you can\u2019t join the first meeting!", "Most of our coordination will be ", " and on discourse. The teleconferences will mainly be used to catch up on progress overviews and discuss tricky issues that are hard to do via text.", "We are open to all, no matter your affiliation or skill set and we welcome your views and contributions.", "Brian Holt is inviting you to a scheduled Zoom meeting.", "Topic: Autoware Map Data and Formats Working Group", "\nTime: Jul 17, 2019 02:00 PM London", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "One tap mobile", "\n+493030806188,489169590# Germany", "\n+493056795800,489169590# Germany", "Dial by your location", "\n+49 30 3080 6188 Germany", "\n+49 30 5679 5800 Germany", "\n+49 69 7104 9922 Germany", "\n+1 408 638 0968 US (San Jose)", "\n+1 646 558 8656 US (New York)", "\n+81 3 4578 1488 Japan", "\n+81 524 564 439 Japan", "\n+61 2 8015 6011 Australia", "\n+61 8 7150 1149 Australia", "\n+43 670 309 0165 Austria", "\n+43 72 011 5988 Austria", "\n+32 2 290 9360 Belgium", "\n+32 2 588 4188 Belgium", "\n+33 1 7037 9729 France", "\n+33 7 5678 4048 France", "\n+972 3 978 6688 Israel", "\n+972 55 330 1762 Israel", "\n+64 4 886 0026 New Zealand", "\n+64 9 801 1188 New Zealand", "\n+44 203 481 5237 United Kingdom", "\n+44 203 966 3809 United Kingdom", "\n+44 131 460 1196 United Kingdom", "\n+44 203 051 2874 United Kingdom", "\nMeeting ID: 489 169 590", "\nFind your local number: ", "To begin with, I propose that interested parties read the following threads and resources prior to the meeting if possible:", " do you know where we can find links to", "I am sharing some analysis between OpenDRIVE and Lanelet2 ", "We at Ridecell-Auro use Lanelet2 in our stack as the vendor independent format and to support importing and exporting to different common map formats,  and are building support for OpenDrive and would love to share the outcome or work together with other contributors.", "Hello, I would like to join this working group, please include me in any future conversations.", "I would like to share the idea about custom tags that we have to define to use Lanelet2 in Autoware.", "\n", "Lanelet2 Format Extension for Autoware", "\n", "\nI created this after investigating how current vector map information is used in Autoware and looked for missing information in Lanelet2 format.", "I also want to discuss about the ROS message for lanelet2 data. This must be done in order to integrate Lanelet2 into Autoware.", "Hi everyone,", "Thank you to those who joined the meeting today.", "Here follows the meeting minutes:", "40km/h (need to consider surface characteristics, road camber etc.)", "Ideally we would have a single map format that would work for all the various use cases.", "Our first discussion was to list the various possible Physical Storage Formats (PSF) and consider the pros and cons of each according to the following criteria:", "This is the list of possible PSFs that we will consider for the next meeting:", "Our next meeting will be on Wednesday, July 17, 2019 10:00 AM Europe: London (see below for the meeting information). The meeting will last for 90 minutes.", "Brian Holt is inviting you to a scheduled Zoom meeting.", "Topic: Autoware Map Data and Formats Working Group", "\nTime: Jul 24, 2019 10:00 AM London", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "One tap mobile", "\n+496971049922,130749793# Germany", "\n+493030806188,130749793# Germany", "Dial by your location", "\n+49 69 7104 9922 Germany", "\n+49 30 3080 6188 Germany", "\n+49 30 5679 5800 Germany", "\n+1 929 436 2866 US (New York)", "\n+1 669 900 6833 US (San Jose)", "\nMeeting ID: 130 749 793", "\nFind your local number: ", "Thank you for arranging the next meeting.", "\nFor the meeting time, is it possible to push it back an hour?", "\nI am occupied until 11AM(London time) on 24th.", "Hi Mitsudome-san,", "I\u2019m happy to move it back by an hour. Does anyone else have any views or comments on time?", "Regards", "\nBrian", "We would be happy if it is more convenient for PDT ", "Hi Jit,", "Thanks for your comments. As you will no doubt know, it\u2019s always challenging finding a suitable meeting time when working across USA/Europe/Asia.", "The current time slot was chosen because so far we\u2019ve only had interest from Europe and Japan, therefore it made sense to choose times that worked best for them.  With the request to include USA I suggest that start a rotation, much like what the Autoware TSC does.", " ", " do you have any thoughts or comments?", "Hi Brian,", "yes a rotation through the time zones sounds like a good compromise. Perhaps keep the scheduled time on this occasion and then give the USA participants preference for next meeting?", "For tomorrow, I am open to scheduling it later, but it seems pushing it back long enough to make a difference in PDT (8AM PDT \u30fc24:0\uff10Tokyo) would be difficult.", "Hello,", "I would like to participate in this working group. Is there anything I need to do beyond attending the next meeting?", "Thanks,", "\nMichael McConnell", "\nStarting a rotation sounds good to me as well.", "For tomorrow, I am available after 11AM London(7pm Japan). Perhaps we could change it to 2PM or 3PM London just like the first meeting. It\u2019s still early in the morning in PDT, but I think it is better than 11AM. If we are changing the meeting time, then I would like to know it before tomorrow noon in Tokyo.", "Hi Michael,", "Great to have you on board! There is nothing more required than turning up for the meeting.  The minutes will be posted afterwards for those who can\u2019t attend.", "Regards", "\nBrian", "Hi ", ",", "Let\u2019s keep it now at 11am UK time (7pm Japan time) because I fear that if we change it too many times people will get confused and we have have low turnout to the meeting.", "Regards", "\nBrian", "Thanks for the clarification. See you there!", "As our commitments are limited let\u2019s not change time to support us, I would still expect some more meeting notes so that we can collaborate", "Hello.", "I\u2019d like to summarize this WG\u2019s current status so that make discussions more beneficial.", "\nI didn\u2019t refer to mass production level HDMap like NDS because I think it\u2019s not the stage yet.", "Would you confirm the content and let me know if there are any points to be corrected?", "\nThank you.", "Lanelet2", "OpenDrive", "As for Lanelet2, since it is too extendable, we might need to restrict the format and define XSD-like document.", "As for OpenDrive <-> Lanelet2 conversions, Ridecell-Auro created a slide: ", "OpenDrive -> Lanelet2 is easy, but Lanelet2 -> OpenDrive is a bit difficult.", "As for other formats, since Lanelet2 is a simple format, any format can be supported if the users prepare a converter by themselves.", "We have to consider these aspects due to safety requirements in the future.", "This is strongly related to Internal Map Model.", "e.g. Full search is tolerable or indexing is necessary.", "I think this is not clear and should be determined.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["identify the physical storage format (map format) that Autoware should support considering factors such as\n", "ease of creation", "adoption of the format", "relationship to production systems", "expressiveness (features that can be encoded)", "\n", "consider the library(ies) to read and write to that format to supply relevant data (map-server) to\n", "the perception modules (e.g. traffic light detection, road markings etc)", "path planning modules (routing)", "localisation modules (for those localising using techniques other than NDT).", "\n", "propose an architecture to support these functions including message definitions", "\u2026", "Github thread on new maps in Autoware ", "\n", "OpenDRIVE reference documentation ", "\n", "Lanelet2 paper ", "\n", "CommonRoad ", " including the ", "\n", "Lanelet2 presentation by Parkopedia at Autoware@IV2019 workshop", "Lanelet2 presentation by Tier4 at Autoware@IV2019 workshop", "Carla + ROS + Autoware presentation at Autoware@IV2019 workshop", "We need to consider the various use cases for maps.  At minimum these are the use cases\n", "Autonomous Valet Parking (Autoware.Auto reference use case)", "On-street driving\n", "< 40km/h", "\n", "\n", "\n", "\n", "ease of map creation", "tooling for reading/writing/visualising/simulating", "adoption of the format", "relationship to production systems", "expressiveness (features that can be encoded)", "Interchangeability with other formats", "OpenDRIVE 1.5 (to be evaluated by ", " )", "OpenStreetMap XML (Lanelets variety) (to be evaluated by ", " )", "Navigation Data Standard 2.5.4 (to be evaluated by Punnu Phairatt )", "Aisan Vector Map (to be evaluated by ", " )", "Physical Storage Format", "Internal Map Model", "HDMap API", "Map Creation / Editor", "HDMap Conversion / Supporting Various HDMap Formats", "Integration with Simulators", "Computational Cost / Resource Usage", "Replacing AISAN with Lanelet2 in Autoware.AI v1.13", "HDMap API Compatibility between Autoware.AI and Autoware.Auto", "OpenDrive is the best format.\n", "But Map Creation is not considered.", "\n", "Lanelet2 is temporarily used and will be replaced with OpenDrive later.\n", "But since the original Lanelet2 doesn\u2019t meet Autoware\u2019s requirement, some extensions are required.", "\n", "Whether using pub/sub or service.", "Whether sending structured API or the whole map.", "These should be considered using AVP use case.", "Currently, Lanelet2 is the only solution.", "But we might need to improve/re-implement it so that it would be safer.", "\n", "\n", "JOSM", "Tier IV Vector Map Tool", "Other GIS Tools", "\n", "\n", "\n", "Trian3d", "RoadRunner", "VIRES Road Network Editor", "\n", "Since most simulators support OpenDrive, if we prepare OpenDrive Map then we can integrate.", "\n", "\n", "\n", "\n", "\n", "\n", "Sending the whole map using pub/sub is a least-cost solution, but it\u2019s probably not the best solution.", "Since the solution in v1.13 would be quite different from the ideal architecture of Autoware.Auto, we need to consider how to make them compatible."], "url": "https://discourse.ros.org/t/autoware-map-data-and-formats-working-group-call-for-participation/9820"}
,{"title": "New perception architecture: message types", "thread_contents": ["We have been discussing the architecture of the perception pipeline in ", ". However, the discussion stopped", "\nbefore it converged.", "The current perception architecture does not contain the information necessary for planning to", "\navoid obstacles or stop in front of an obstacle. These features were not considered when the architecture", "\nwas developed. Based on these needs and other requirements from the planning layer, we would like", "\nto propose the following message types for use in the perception pipeline.", "Perception architecture (Not including localization)", " have you seen ", " msgs by Autonomous Stuff? It looks like they cover what you are proposing", "We tried to be as broad as we could with the ", " and ", " types in ", " to cover as many possible sensor-specific inputs as we could with generic structures (like ", " and ", ") but we are always open to feedback if these don\u2019t cover a specific need.", " I second ", " 's opinion to try to analyse work that AS has done, that is messages from ", ", and rather improve those than to come up with a completely new definition of messages.", "To comment on ", ", in this issue we tried to come up with the better decomposition of nodes that would a) allow code reuse and b) fusion of sensor data on multiple levels (raw data, features, objects). This is the result of the last discussion that I, ", " and ", " arrived to  ", ".", "What would be our next step is what you did, which is the data modeling step.", "From a quick glance there is nothing in your proposal that is not already included in ", " so I suggest to include messages from AS.", "I third using the AutonomousStuff messages. They have what we need, and while I can see a couple of things I\u2019d consider making more exact I don\u2019t see any problems, not even small ones, in using them.", "I update the slides.", "\n", " is made to be able to cover various information, and a lot of information can be defined. This includes not only the requirements for planning but also the data specific to algorithms and sensors.", "\nThe current definition has some issues.", "This time, we defined the message type from the information required for dynamic object (Something that can move such as a pedestrian, car, truck, etc.).", "\nThe details are written ", ".", "\nAs for paths, it is still undefined and will be added as we discuss in the future.", "\n", "\n", "derived_object_msgs is made with the same idea as the current message type as ", "   said.", "\n", " Can you give a detailed comparison of what each of the three message types can store?", " ", "Please feel free to add some comments.", "*1:", "*2:", "*3:", "*4:", " - Please see ", " for a version which contains the PoseWithCovariance, TwistWithCovariance, and AccelWithCovariance. The uuid is definitely a useful addition that we had not considered and would be happy to add. This was the intent of the \u201cid\u201d field but it is somewhat limited as an int.", "for a version which contains the PoseWithCovariance, TwistWithCovariance, and AccelWithCovariance. The uuid is definitely a useful addition that we had not considered and would be happy to add. This was the intent of the \u201cid\u201d field but it is somewhat limited as an int.", "thank you ", ".", "\nAs a side note, I think that the label specific shape of AS msg is No. It can not switch between polygon and SolidPrimitive. It can switch only in SolidPrimitive (Cone, sphere, bounding box, cylinder). This is also important from the point of view of multi object tracking and path planning.", "\nWho uses detection label and classification age for what?", "\nIf we add items widely, when we modularize detection, tracking, and prediction, these modules will have to fill in these items. Some algorithms may not be filled. I think that it is better to define only what is really necessary in msg.", " - I think the field you\u2019re referencing is  \u201cdetection_level\u201d which is meant to indicate whether this object has been \u201cdetected\u201d or \u201ctracked.\u201d From the notes in ", ", here are the definitions:", " indicates the number of \u201cscans\u201d or \u201cdetections\u201d made of the object where the classification type is the same. When a sensor classifies an object, it usually tells you how many \u201cscans\u201d of that object have been sent since the object was classified as that type. This helps determine the certainty of the classification.", "According to your experience in robotics and autonomous vehicles. I would like to hear your opinions ", " ", " and ", " about this messsage definition not including sensor data (i.e. ImageROI, PointCloud)?", "\nDo you think it is necessary? Or do you think it\u2019s better to keep it like this to add an abstraction layer?", "\nThanks", " I think this is based on your intent for the message. If you intend to follow a \u201cdomain-specific-controller\u201d approach, then you are trusting that the individual sensor processing nodes know how to correctly filter the raw data and produce abstracted objects for the most part. The uncertainty is then encoded into the covariance matrix and the classification quality data.", "However, if you intend to do either data fusion before object segmentation or fusion and segmentation in the same node, you would need the raw data in the message as well. My understanding is that Autoware is shooting for the first approach so I would say we don\u2019t need the raw data.", "Agreed with ", " comment, I would expect the raw data to be processed on a separate step so that the filtered sensor data is in a usable stage. I think it also makes sense in a setup where you might have an edge device on the sensor that is pre-processing the raw data for consumption by higher level nodes.", "I think this work is being blocked by a lack of a shared understanding of what it is we want to achieve. What objects do we want to recognise, where do we want to recognise them, what sorts of data do we want to use, what data rates, should data be synchronised or can information be added on to a detection after the fact, do we or do we not use consecutive detections to strengthen an object\u2019s presence, how interchangeable/optional do we want different algorithms and detection types to be, and so on. There are a huge number of unanswered questions that need to be defined and then answered before we can even begin to think about the messages used.", "In other words, we need to define our requirements before we try to solve them. Otherwise we are solving an unknown or undefined problem.", "We also need to keep in mind that we are designing Autoware for all Autoware users, not just for Tier IV\u2019s favourite sensor set, or AutonomousStuff\u2019s specific demonstration. I\u2019m not saying that that is what is happening, but it is easy to forget.", "Additionally, I think it would be useful to draw up a list of:", "According to your experience in robotics and autonomous vehicles. I would like to hear your opinions ", " ", " and ", " about this messsage definition not including sensor data (i.e. ImageROI, PointCloud)?", " in my experience you need to decide between performance and synchronization. That is if you do not have many nodes and you have fast middleware, then you can use message types that also include raw data. If you have the opposite case then you should go with small messages.", "If you split your message types too much you will have to deal with the time synchronization once the messages received by the end node. That is both hard to do and computationally expensive.", "In any case I believe that we should finish the computational graph architecture first (how many nodes and composition) and then define the messages and not the other way around. I assume that AS has a solid computational graph architecture that let them define such messages.", "I listed the differences between ", " and ", ", and heard why ", " is constructed in such a way. I\u2019d like to clarify the differences and reasons, and like to make more common view in the whole community.", " Would you comment about the different information?", "PoseWithCovariance[] past_paths is removed from DynamicObject since planning is not interested in past information. Though prediction will require past information, it can be resolved inside prediction.", "Thank you for the summary. Do you have any opinions?", "I think the reasons of DynamicObject are reasonable since I created the table based on the hearing ", " My opinion is already reflected into the table, e.g. object_classified and past_paths are not necessary.", " Sorry to bother you again. Would you comment about the different information? I\u2019m not familiar with the background of ObjectWithCovariance.", " I\u2019m sorry it has taken so long to get back to you. Here are responses addressing your issues:", "The object does have a header field. However, it does not need to be populated and ", " also has a header.", "The types listed for \u201cclassification\u201d are not exhaustive nor definitive. As far as I know, the message type has not been extensively used so it is open to modification. We have only done tests with it internally and have not released any packages that use it. I agree with your assessments for the \u201cUNKNOWN_\u201d types. They were types provided by a sensor vendor so we included them.", "Not a problem to change the classification_certainty to a float (0-1).", "Many algorithms use a convex hull bounding area to define an object. This is why \u201cshape\u201d was included. There is also ", " in the message for defining a non-normal polygons.", "Regarding the rest of the comments: I think the overall concept is that our message structure for these is flexible. We can add or modify just about anything in the message, though I would prefer not to remove much (if any) of the fields.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Issue 1: A large number of algorithm-specific data makes it difficult to define interface and modularize perception. The interface I\u2019m talking about here is what information is filled in with detection and tracking and which information should be filled out", "Issue 2: Unstructured and redundant message type", "Issue 3: Missing information required for planning", "The different types of sensors we expect to be used. Not just ones we use now, but also ones a potential Autoware user might use.", "The different types of data we might process. Obviously this closely relates to the sensors used, but don\u2019t forget using post-processed data as an input to an algorithm, e.g. merged dense point clouds versus individual sparse point clouds, or point clouds with or without RGB data added from a camera.", "The object locating, object identifying, object tracking, object predicting, etc. algorithm types that we might use.", "Possible orderings of algorithms.", "geometry_msgs/PoseWithCovariance pose", "geometry_msgs/TwistWithCovariance twist", "geometry_msgs/AccelWithCovariance accel", "geometry_msgs/Polygon polygon and geometry_msgs/Polygon Shape::footprint", "uuid_msgs/UniqueID id is ", " by ", "\n"], "url": "https://discourse.ros.org/t/new-perception-architecture-message-types/8564"}
,{"title": "Definition of data recording feature for Autoware.Auto", "thread_contents": ["Hi,", "This post is to discuss the definition of data recording feature for Autoware.Auto, which comes from the ", " on the Gitlab.", "First of all, I want to figure out the purpose(what, when, what for) of the data recording feature.", "\nAfter that, we\u2019ll discuss \u201chow to\u201d and \u201cwhere to\u201d record the data.", "I have shortly studied the definition of the existing EDRs(Event Data Recorders), and EDR-AD(Event Data Recorder for Autonomous Driving) which has been proposed recently.", "\nFor example, the definition of EDRs by the US government is as below.", "The definition of EDR-AD by CLEPA is as below.", "\n", "I think the concept of EDR-AD is a good example for us to discuss the data recording feature in Autoware.Auto. So I am thinking to break down the required data elements and map them to data in Autoware.Auto. However, I just hit a basic question.", "Does anyone have an answer to it? Any other suggestions or feedback would be appreciated.", "The following links would be helpful for reference.", "This is a critical feature for any AV that will be certified for road use.  The UK perspective on this question is put forward in the ", ".", "I\u2019ve had some interactions with the Law Commission trying to work out the relationship between privacy and the requirement to store data, and also about the length of time that data needs to be stored. These are difficult questions.", "Thanks. I\u2019ll check the document you shared.", "As far as I studied in several documents including the one which ", " shared for me, we can say:", "However, I also find some difficulty to define the requirement for this data recording feature because:", "My current idea to overcome the above is:", "Will it work for our users? Any other opinions or feedback would be appreciated.", "Autonomous driving vehicles should record additional data related to AD systems so that any third party can determine the responsibility of the driver after infringements or incidents (including non-accident events).", "My understanding of the intent behind EDRs is to exactly this. Ideally the information stored should be sufficient to reconstruct the scenario in simulation and I would recommend that goal as a guide for this work.", "While basic telemetry (pose, velocity, acceleration, status of indicators, horn, lights, etc) do not consume much space, sensor data such as  LiDAR and camera is very space-heavy.  Obviously the system will not know when the incident is going to happen, and the requirement to collect -30s to +10s means that the system will need to be constantly recording.  This is probably best implemented using a circular buffer, and storing the contents of the buffer when an incident occurs.", "Thanks, ", ". I agree with your opinion.", "And at the last WG meeting, ", " gave me a suggestion that Autoware should assume an EDR is not present and record everything.", "After that, I refined the definition of this feature. I would be happy to hear any feedback.", "\nAutoware should provide an appropriate security function considering the following requirements.", " do you have any more thoughts on the topic?", "You\u2019ve raised many points here so I comment in several posts:", "\nI reply to \u201c-30sec to +10sec relative to the trigged event\u201d", "Sections 5.12 to 5.16, pages 19 and 20 of this UK document:", "\n", "669.01 KB", "\n", "Normal recording frequency minimum is 10Hz, but \u201cIn the event of an incident, an event data recorder should be able to capture a suggested minimum period of 30 seconds before the incident, and 15 seconds after. It is recommended that the minimum recording frequency is 50Hz\u201d", "\u201cThe stored audio and visual data should be treated under relevant privacy laws.\u201d", "\n", "On 6 August 2017, in advance of proposed legislation, the UK government published 8 \u2018Key Principles\u2019 regarding the cyber security of connected and autonomous vehicles. This is the last of a series of 4 blogs regarding those principles. Vehicles today...", "\n", "\nThis article implies extra precautions if the data is to be used for prosecutions.", "Some vehicle may not have the hardware installed to enable the recording of all the data suggested.   For example, an ordinary StreetDrone does not provide for the autonomous switching of lights or indicators, nor the recording of their state.   It is now provided as an option or an upgrade.", "Thanks, ", ". I appreciate your comment!", "Normal recording frequency minimum is 10Hz, but \u201cIn the event of an incident, an event data recorder should be able to capture a suggested minimum period of 30 seconds before the incident, and 15 seconds after. It is recommended that the minimum recording frequency is 50Hz\u201d", "I think this requirement means we should set the default recording rate at 50Hz in a buffer because we expect that this feature should be used in the post-incident analysis.", "This article implies extra precautions if the data is to be used for prosecutions.", "This article is interesting. It also says that the definition of \u201csensitive data\u201d that \u201cusers\" can delete is still ambiguous and should be clarified by the authorities in the future.", "\nSo I think we should let the distributors of Autoware.Auto to configure the specific set of data elements to record, based on the regulations they have to follow in their region.", "Some vehicle may not have the hardware installed to enable the recording of all the data suggested. For example, an ordinary StreetDrone does not provide for the autonomous switching of lights or indicators, nor the recording of their state. It is now provided as an option or an upgrade.", "Yes, I agree. So the distributors of Autoware.Auto should be able to configure the specific set of data elements to record, based on their hardware specifications, too.", "This point hasn\u2019t been mentioned yet but I think it should be to be clear.", "The data recording feature will be a part of Autoware, but that doesn\u2019t mean it should be based on ROS 2 nodes for its architecture. This feature is going to be quite important for safety and so it should be as simple as possible. I think that therefore we should have a data recorder architecture that has minimal dependencies, and that we can add a sub-component to responsible for managing the interaction with ROS 2 to get the ROS 2 data. This sub-component could well not even use ROS 2, instead directly using a DDS client library to get the data.", "\nThanks, I agree with your opinion. I\u2019ll keep it in my mind to design this feature as simple as we can so that this feature works correctly even if some critical errors happen in ROS 2 layers. The same design rule should be also applied in recording vehicle data and system logs.", "I was told that the ", " has been established under the WP29 this year. I will watch the activities in this working party because I expect the upcoming regulations in the participating countries will follow the outcome from it.", "According to ", " for their sessions, I can observe the following.", "As far as reading their documents, it seems like to me that people in the working party gave up to record space-heavy data in DSSAD due to storage capacity and privacy protection, and decided to focus on the very limited data.", "So, I want to ask you again whether we should expect to record space-heavy data such as LiDAR and camera in Autoware.auto. Recording such data should be obviously important for debugging but it may not be required by regal regulations in the near future.", "\n", ", ", ", ", "\nAny thoughts?", " . It is used to determine who took control of the car when the incident happened.", "Does this mean \u201call data for several months\u201d or \u201ccontinuously record, and remain operational for several months\u201d?", "Why is several months of data necessary to determine the immediate time around an accident? A vehicle is likely to start and stop dozens of times in several months. This seems like an odd requirement to me.", "I think that even if the data is not required by any legal regulations, we still are obligated to record all sensor, control and state data for the immediate time around an incident. Otherwise it would become impossible to investigate the cause of the incident and determine if Autoware was the cause, and if so what in Autoware needs to be fixed. This is a basic requirement of any safety-critical system.", "Does this mean \u201call data for several months\u201d or \u201ccontinuously record, and remain operational for several months\u201d?", "It means the former \u201call data for several months\u201d. However, \u201call data\u201d will be a small set of AD specific events.", "They expect that some kinds of accidents cannot be detected by the AD system itself but should be claimed by others afterward. For example, they assume accidents such as traffic offenses, collisions with very small impacts and unsafe maneuvers without collisions.", "So the document says DSSAD should store data with timestamps for a long period(in the last X-months) and be able to deliver them when requested by an authorized entity. I have to check how reliable the recorded data without exact time synchronization between other computing nodes would be.", "I think that even if the data is not required by any legal regulations, we still are obligated to record all sensor, control and state data for the immediate time around an incident. Otherwise it would become impossible to investigate the cause of the incident and determine if Autoware was the cause, and if so what in Autoware needs to be fixed. This is a basic requirement of any safety-critical system.", "OK. It makes sense. We can also add any data elements that Autoware processed for the immediate time around an incident as an EDR-like feature.", "They expect that some kinds of accidents cannot be detected by the AD system itself but should be claimed by others afterward. For example, they assume accidents such as traffic offenses, collisions with very small impacts and unsafe maneuvers without collisions.", "OK, that makes more sense now. That implies that our data logging feature needs to have levels of retention that can be set on a per-data-stream basis.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Some data related to vehicle dynamics(Delta\u2013V, Speed, Engine throttle, Service brake, Steering input, ABS activity, etc) and driver\u2019s safety status(Safety belt status, Frontal airbag deployment, etc)", "Roughly -5sec to +300ms relative to the crash impact", "Post-crash investigations and for analysis of safety equipment performance", "Generic events(Event type, Timestamp, Location) in AD system for driving log or testing", "The same data as EDRs for crash event", "Additional data related to rule violations(driving manoeuvers, Camera images, GPS time/ position, AD active state) for malfeasance event", "While AD system activated for driving log or testing", "Roughly -30sec to +10sec relative to the crash event", "When AD system detect any rule violations for malfeasance event", "Disculpation/Exoneration of the driver in case of infringements (including non-accident events)", "Accident situation: if AD system was activated", "Product liability, product monitoring obligation, quality assurance and product development for OEMs and suppliers.", "Supply of factual data for legal proof.", "Should we assume that a generic EDR is already equipped on the vehicle?", "\nIf yes, synchronization between the data recorded in the EDR the one in Autoware.Auto would be the key point.", "\nIf no, this feature should contain all features of EDR-AD(including generic EDR).", "\n", ".", "Black Block Recorder: Immutable Black Box Logging via rosbag2 and DLTs, Ruffin White and Gianluca Caiazza (will be presented on ROSCon2019)", "Autonomous driving vehicles should obviously implement EDR-like features the same as manned vehicles in case of a crash event.", "Autonomous driving vehicles should record additional data related to AD systems so that any third party can determine the responsibility of the driver after infringements or incidents (including non-accident events).", "The recorded data related to AD features can contain visual and audio data that should be handled with relevant data protection or privacy laws in each region.", "Requirements for data recording features for AD systems are not as clear as ones for EDRs.", "It is not clear for me if we should expect an existing EDR equipped on the vehicle or not.", "Appropriate handling visual and audio data will change depends on the laws in each region", "Define a typical use case and fundamental functions to record and store data in Autoware specification.", "The fundamental functions will contain:", "\n\u25cb receiving vehicle data in the legacy in-vehicle bus like CAN via vehicle interface", "\n\u25cb external interfaces to start and stop recording", "\n\u25cb time synchronization between vehicle data and AD system data (if needed)", "\n\u25cb configuring a set of data elements to record", "\n\u25cb configuring the persistency and durability for each data element", "\n\u25cb encryption of the recorded data", "Categorize data elements to record(e.g. we can roughly categorize them into 3 types of data as \u201cvehicle data\u201d, \u201cAD system data\u201d and \u201cprivacy data\u201d)", "Define a set of default persistency and durability for each data category", "Implement a minimum set of recording functions on Autoware.Auto as a reference implementation.", "Any CAN data available via the vehicle interface", "\n(the set of data elements to record should be configurable)", "Any ROS2 topic and parameters available via the standard ROS2 APIs", "\n(the set of data elements to record should be configurable)", "Typical system logs generated by the operating system outside of Autoware", "\n(e.g. date, ps, env, syslog, dmesg, netstat in Linux)", "Identification data of the running system", "\n(e.g. software release version, system mode, system status)", "-30sec to +10sec relative to the trigged event.", "\n(Autoware should record all of the data constantly in a buffer while activated and store the content in persistent storage when triggered.)", "Provide the appropriate evidence so that any third party can determine the responsibility of the AD system after infringements or incidents (including non-accident events). Ideally, it can reconstruct the scenario in the simulation with the stored data sufficiently.", "The stored data should not be changed by anyone after the incident.", "The stored audio and visual data should be treated under relevant privacy laws.", "The working party proposes the two type of equipment\n", "EDR(Event Data Recorder)", "DSSAD(Data Storage System for Automated Driving)", "\n", "EDR is the existing one, which records pre-crash and post-crash data when triggered by airbag deployment. It is only used for post-crash analysis.", "DSSAD is the new one, which is supposed to record the specific AD data set(e.g. significative interaction between the AD system and the human driver) ", ". It is used to determine who took control of the car when the incident happened."], "url": "https://discourse.ros.org/t/definition-of-data-recording-feature-for-autoware-auto/10495"}
,{"title": "SingleThreadedExecutor creates a high CPU overhead in ROS 2", "thread_contents": ["Hello,", "We are looking into the performance of ROS 2 on Embedded boards and we find out that ROS 2 consumes high CPU because of the overhead introduced by SingleThreadedExecutor. We did some tests to profile the CPU usage and we observed that if we run 20 publishers and 200 subscribers in one ROS node, 70% of the CPU is consumed by SingleThreadedExecutor and 20% of the CPU is consumed by DDS implementation.", "By running the same example in Fast RTPS directly, it consumes 3.5 times less CPU as compared to ROS 2. The tests that we have performed along with their results can be found in this link: ", ".", "Is anyone else is also looking into measuring the CPU usage of ROS 2? Please share your findings here and let us know if we are doing something wrong.", "Our current analysis suggests that the SingleThreadedExecutor needs to be optimized otherwise normal ROS 2 cannot work properly on \u2018ARM A-class\u2019 embedded boards. We are willing to look more into this problem and can help by performing more tests and providing feedback to improvements. Please let us know if there is any other way to contribute to this.", "Thank you,", "\nIshu Goel", " and myself are also currently looking at this, but I\u2019m going to give him some time to work more on it by replying in his stead ", " He has looked into this based on his tracing work, see ", " It uses LTTng to directly instrument rclcpp and rcl.", "First of all, many thanks for describing your results so openly and so early, particularly for providing the initial benchmark programs. This makes it much easier to compare and combine results.", "In general, what I\u2019ve heard from the OSRF and others is that people are somewhat aware of the inefficiencies in the executor, but nobody had exact numbers so far, and therefore this problem was so far not prioritized. I think this has now changed ", "Regarding your analysis, one thing I would caution is that \u201cperf record\u201d is a sampling approach. This means it can miss executions which are too short. I don\u2019t think this compromises your results, but since you were asking, I wanted to mention it.", "Therefore, in our work, we use LTTng, which integrates both perf event and userspace tracepoints. We have tried both instrumenting every function automatically (which has noticeable overhead), and manual instrumentation of just the most relevant functions. The latter is a bit more work, but also gives more precise results.", "About the single-threaded executor, one thing that I noticed is that it operates in the following way:", "Could the overhead of the executor be due to the fact that after every message it checks again all the entities? this can potentially be very expensive in cases like the one that you tested (200 subscriptions in the same executor)", "Moreover, the ", " function in ", " is marked as ", ".", "\n", " do you already have any idea on how it should be improved?", " Yes, that is a big part of the overhead.", "Since the wait_set only really needs to be update whenever there is a change to the entity list, it is likely that some of this effort could be avoided, or made less expensive. However, without having had a more serious look at the design, I cannot currently say what the best option would be. Maybe ", " or ", " have some ideas.", "I noticed it operates in the follow way (although I could have missed something):", "There is a list of nodes, a node has multiple callbackgroups, a group has multiple executables (eg timer, subscription, client, service, any). So basically a tree: node -> group -> executable", "The tree can be quite large, and is walked often. There seems to be room for improvement by walking/copying/searching the list of executables less (mainly step 5 & 6).", "It\u2019s all weak_ptr by design, but it must keep the memory valid (shared_ptr) as long as it\u2019s in rcl_wait, which is a bit conflicting. It\u2019s also the reason for lots of lookups in the original tree: the only way to see if something disappeared is to rebuild it.", "Is there some information on the design somewhere? Typically a executor works by just submitting executables/callables/callback to a thread(pool), where the executor just maintains a queue of work to do. This is a more complicated design that is implemented in three different layers (rclcpp/rcl/rmw).", "I also saw something on the roadmap about changing the relation between nodes/groups en refactoring the executor. Are there already more concrete ideas about this?", "Could the overhead of the executor be due to the fact that after every message it checks again all the entities?", "i had the same concern with quick code scan and tried the following patch if it affects cpu consumption,", "so far it DOES NOT reduce cpu consumption.", "my environment is", "\ndocker: 18.09.8 ros:dashing", "\nHost: Ubuntu 16.04.6 LTS / Intel\u00ae Core\u2122 i7-4790 CPU @ 3.60GHz", "Hi all,", "what do you think about this?", "already checked if i can reduce cpu consumption, it does some but not a big deal\u2026", "will dig deeper.", "Hi ", ",", "Thanks a lot for your efforts. We are also working on creating a static scheduler to see how much performance gain can be achieved. We will share our result as soon as we complete our work. Please keep sharing the results of your work.", "got it, thanks!", "\nwe will do the same!", "tomoya", "Sorry for the delay! Here are the results of the investigation done by ", " and me.", "We could replicate the earlier results, showing that the Executor consumes a lot of CPU. In that, we could distinguish two cases:", "Compared to earlier work with similar results, we took care to minimize overhead and only count time spent actually on the CPU. Therefore, we consider not just the qualitative result, but also the absolute numbers to be trustworthy.", "This has been non-trivial, because the executor calls very many, very short functions (mainly to do with ", "s). This causes problems both for traditional profiling (which adds lots of overhead) and for sampling-based profiling (which may not notice these). Just to give an idea, initialising the nodes took at least a good 10 seconds when using ", "! Without profiling, it takes ~100 ms.", "To achieve this, we use 1) explicit instrumentation of only the relevant high-level functions and 2) we capture scheduling events. This allows us to sum CPU time only when the thread is actually executing on the CPU.", "Specifically, we only looked at the main ", " functions:", "See our executor instrumentation ", ".", "As mentioned before, based on scheduling information, we only count CPU time when the thread is running on the CPU, not when it is blocked.", "We chose the ", " test case, since it has the highest CPU usage. We traced it for a few seconds. The thread itself has a CPU usage of 55.87% (this is less than the 70% overall CPU usage reported earlier, because it does not include time spent in the dedicated middleware threads).", "In our first analysis, we looked at ", " in some detail, because of the high overhead numbers reported earlier.", "As you can see, from the \u201cfunction\u201d bar, the core ", " function indeed only takes ~32% CPU, the rest is Executor overhead. However, as you can also see from the \u201cthread\u201d bar, the whole method only makes up ~18% of the CPU usage of overall thread. This means that other parts of the Executor are more important.", "Therefore, we took at step back and looked at the two high-level functions in ", ": ", " and ", ".", "The ON CPU time for each function is compared to the whole thread and to the parent function. In this case, 79.21% of the CPU time for the whole thread is spent in ", " vs. 8.22% for ", ". These numbers are similar to ", ".", "Since ", " is likely dominated by running user code, we took a closer look at the functions inside ", ": ", " and ", ".", "Here, ", " represents 67.02% of ", "'s CPU time, and 53.09% of ", " the actual CPU time for the thread!", "Looking at the code, ", " checks its lists of timers/subscriptions/services/clients/waitables and returns once it has found one that is ready to execute. As a side note, having to loop over all the lists would explain the large CPU usage difference between the ", " test case and the ", " test case, since the latter has only one node.", "If we look at the CPU usage for each function individually, we can see that ", " is indeed the most CPU-intensive function.", "The full data is below.", "In conclusion, the executor should be optimized. Figuring out if \u2013 and which \u2013 executable is ready seems to take ", " of CPU time.", "We used LTTng and the ", " & ", " packages. The Jupyter notebook which was used to get the results above can be found ", ". This post can also be found ", ", which also shows how profiling overhead can really mess with the results.", "Thank you Christophe, very nice results. Good to see that we came to the same conclusions, this makes our case even stronger. I\u2019m currently working on posting an issue on the rclcpp github where I will reference this discussion. I think your findings will be very helpful!", "Edit: The issue is now available here: ", "btw, for reference with respect to the changes ", " did: No single call is to blame. The main issue is that, for every single timer-invocation or message, the whole internal representation is traversed. In contrast, if you use the middleware directly, you can attach a listener directly to each communication objects. This avoids traversal ", ".", "However, the listener approach has the problem that we have very little control over when which message is being executed. That\u2019s precisely why ROS 2 adds executors, and can even have different ones.", "IMHO, it would help to look at the interface between rmw and the executor, to pass more information across and thus avoid traversal.", "The main issue is that, for every single timer-invocation or message, the whole internal representation is traversed. In contrast, if you use the middleware directly, you can attach a listener directly to each communication objects. This avoids traversal ", " .", "i need more time to dig deeper but i do agree on this.", "besides, since this is optimization, we might as well define reasonable goal to achieve.", "tomoya", "Just FYI,", "\ncreate \u201cexecute_any_executable_list\u201d and \u201cget_next_ready_executable_list\u201d to reap the executable event as much as possible in single iteration. (that is said if the multiple executables are ready to fire, number of iteration to reap the executables will be much less.)", "so far, we do not see much improvement.", ", and all", "could you take a look at the following PR?", "\n", "\n", "thanks,", "\nTomoya", "Hello everyone,", "Our first POC for a Static version of the Executor can be found here ", " . This version works with the latest stable release of dashing giving the following results:", "\n", "Our StaticExecutor has been added to rclcpp in such a way that the old functionality remains intact. To use our executor please follow the README. The package also contains dockerfiles to quickly inspect the CPU usage on your PC for different executors (the LET executor created by Bosch for micro-ROS is also included in this comparison).", "If you try out the docker example please share your results. It would be even better if you could use our executor with your own source code. This way it can be tested for more use-cases. If you run into bugs, please let us know! We did make some assumptions with respect to the source code, given that this is a POC (assumptions are mentioned in the README).", "We think this POC is a good first step to highlight possible performance gains. The final goal is to get an optimized Executor with proper scheduling mechanics in the core ros2 stack. We are currently working on a fork from ros2 master to create a proper PR for this version. We will keep you updated on the PR progress here.", "Rather than a fork, you could probably provide your new executor as a separate library.", "Great!!! we will look into that.", "Rather than a fork, you could probably provide your new executor as a separate library.", "+1 on this.", "thanks", "Hello everyone,", "For now we created this PR for rclcpp ", " . We are considering making the code a separate library. Having the static executor as an optional package would prevent the bloating of ROS2. However, the package would also require a maintainer. Since we are a relatively small team that plans on doing more work (creating more packages in the future), we have to consider if and what packages we want to maintain. The static executor is a relatively small package, so we could consider picking it up (this is an internal discussion we are yet to have).", "Please leave your comments and thoughts on the code under the PR. Even if the PR does not get approved, we hope to at least draw attention to the CPU overhead of the current implementation.", "Small update: We updated the dashing version of our static executor to be semi-dynamic. The node guard_conditions are used as event trigger to rebuild the wait-set and executable list. This means that when a subscriber, timer etc. is added during spin(), the executor will notice (by checking the guard_condition) and rebuild, making the use of the static executor less restrictive.", "This updated version can (still) be found here ", ".", "We will create a master (eloquent) version of this, but we first want to fix some Jenkins linter errors and do some clean up on our PR.", "If you try out our code please share your results here. Please report any bugs you find. Possible optimizations are best posted on the PR when we apply the changes there.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a node starts spinning", "receives a message (awakening from spin)", "the executor calls ", " to retrieve the entity that has to handle the message", "the message is handled by the subscription", "the executor checks again all the registered entities", "if no entities have work to do, the executor goes to sleep again", "It populates a list of all executables by walking the mentioned tree into a memorystrategy. (promote weak to shared_ptr)", "The memory strategy is then converted into a wait-set (to call the rcl)", "The wait set is waited upon. Implementation is all the way down into the RMW layer. It differs per RMW implementation.", "After the wait, only ready executables are left in the waitset (not null).", "The memory strategy is updated with this list (remove all that are not ready, to allow weak_ptr to cease)", "For a ready executable, the group is retrieved from the original tree by searching the entire tree.", "Execute", "Go back to step 6, if more executables where ready, otherwise go to step 1", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "when there are few or no messages (e.g., for a timer-driven node), then the ", " method causes the majority of the overhead, with 70% of its time spent in ", " and only 30% (excluding waiting) spent in the RMW layer and below. We determined this using the \u201cnopub\u201d, pure timer benchmark.", "when there are many messages, the majority of the CPU usage \u2013 up to about ", " in our tests \u2013 is caused by the ", " function. This is pure ", ". We determined this using scg\u2019s \u201cros\u201d benchmark, which sends 10000 small messages per second.", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/singlethreadedexecutor-creates-a-high-cpu-overhead-in-ros-2/10077"}
,{"title": "ROS2 bagging w.r.t. services and actions", "thread_contents": ["Hi, I was hoping to get a bit of clarity on something that\u2019s been in the back of my mind regarding a migration to ROS2. At one point long ago, I remember reading that there was some consideration for building services over DDS pub/sub, and I thought \u2018great, that means we\u2019ll probably get first-class support for service bagging\u2019. Lack of services is a huge pain point with using bags as a system logging mechanism in ROS1.", "At some point, the landscape shifted and ROS2 services were implemented via DDS-RPC instead of pub/sub, which I imagine precludes using a side-channel recording mechanism like bagging. Sadly, with ROS2 actions being (rightfully) implemented via services, this means neither services nor actions are baggable in ROS2. That\u2019s really a shame - I\u2019m sure I\u2019m not the only one who occasionally used ROS1 ", " in some capacity just because they were baggable.", "Understanding that ROS2 development is a sea of shifting priorities - is this something that would even be possible to resolve without fundamentally upending the design? Can it be \u2018solved\u2019 at the rmw implementation layer, or would it require more fundamental changes?", "I had never even thought about bagging services and actions. Now I also want this!", "At some point, the landscape shifted and ROS2 services were implemented via DDS-RPC instead of pub/sub,", "DDS-RPC uses topics to implement services.", "And other implementations, like OpenSplice, use our own version of this on based on topics.", "However, Services in ROS 2 do not have to implemented with Topics, as you pointed out. They are their own concept in the rmw API, this was done to allow for optimizations for Services if desired. Making them on top of Topics always would perhaps not be the most efficient thing to do.", "What\u2019s prevented us from recording them is having some rmw API for observing the exchanges between a client and server by a third party (like rosbag). We could add this API, though it may make it hard for future rmw\u2019s which don\u2019t use a one/many to many mechanism to implement services, which presumably would be where the efficiency gains would come from (by using a one to one comm pattern like gRPC/HTTP2 or something).", "One thought, that wouldn\u2019t require changes to ", ", is that we could simply republish requests and responses on a well known topic name. For example, if the service were called ", ", there might be a ", " and ", ", publish to by the client and service respectively. They could be activated selectively (to keep overhead low, activating this by default for all services would be expensive I think), and that would enable recording of the services.", "I think recording is the only thing that makes sense though. I don\u2019t see how replaying requests or responses directly makes sense.", "One thought, that wouldn\u2019t require changes to ", " , is that we could simply republish requests and responses on a well known topic name", "That\u2019s an interesting suggestion. I would potentially take that one step further and introduce a new generic topic similar to ", " or ", " that has all the service and action request, response, and feedback logged to it. I think that could actually have a variety of useful consequences.", "For example the topic may have the information like action/service name, type, timestamp, a string/serialized-version of the request/response/feedback and the caller (assuming that\u2019s available, which I think it is). The downside clearly is in trying to make a general message to go over that topic, the request/response/feedback wont be in their native types but as strings or serialized blobs. Though it could be conceivable that with the type and string/serialized a ", " tool could convert them into real types for playback/reading.", "there might be a  ", "  and  ", " , publish to by the client and service respectively. They could be activated selectively (to keep overhead low, activating this by default for all services would be expensive I think)", "That\u2019s a great approach! What would be the expensive part? I imagine if a bag recorder exists that whitelists ", " for recording, it could just subscribe to those topics. If no bag recorder exists, isn\u2019t it a performance no-op to create a topic with no subscribers?", "I think recording is the only thing that makes sense though. I don\u2019t see how replaying requests or responses directly makes sense.", "I imagine there\u2019s some esoteric cases where someone may want a bag playback to emit service calls to an external node, but I can\u2019t think of anytime I\u2019ve seen a practical application.", "new generic topic similar to  ", "  or  ", "This would probably not be as useful as the namespaced version proposed by ", ". A topic like that would be an all-or-nothing firehose, and potentially prohibitive to subscribe to from a large node\u2026 short of a DDS mechanism like keyed topics, which may not even be exposed in ROS2 (yet).", "I would personally just find it annoying to have to enumerate all the servers to bag up, ", ", ", ", ", " for ", " servers. I perfer the one-stop-shop firehouse that I can pare down on debug.", "If no bag recorder exists, isn\u2019t it a performance no-op to create a topic with no subscribers?", "There\u2019s always overhead to having more things, even if there\u2019s no actual match. There\u2019s discovery traffic and memory usage at least. And since every node will likely have a few services the impact is large even if the individual fixed cost is small.", "I would potentially take that one step further and introduce a new generic topic similar to ", " or ", " that has all the service and action request, response, and feedback logged to it. I think that could actually have a variety of useful consequences.", "How would you log them? As a string? You cannot have different types on the same topic (or you shouldn\u2019t). If you\u2019re logging them as a string, you could just use ", ".", "I would personally just find it annoying to have to enumerate all the servers to bag up, ", " , ", " , ", " for ", " servers. I perfer the one-stop-shop firehouse that I can pare down on debug.", "A parameter to rosbag could be used to toggle recording all topics matching ", " and ", ". It wouldn\u2019t require the user to manually enumerate all services to record.", "I guess action feedback can already be recorded since it is a topic.", "I had never even thought about bagging services and actions. Now I also want this!", "That\u2019s\u2026 surprising. ", "How about bagging-via-tracing?", "It\u2019s a bit of a wild idea, and make it feasible, we would need to support the Windows and MacOS tracing frameworks, in addition to Linux, and we would need to look at making this location transparent, but it ", " an existing, low overhead, tunable, configurable, data capture mechanism which can support any source.", "That\u2019s an intriguing idea, but I\u2019m curious about how low the overhead would really be? One of the advantages of capturing via subscription is that you don\u2019t directly impact the execution time of the sender (assuming you allow for enough computing resources, etc., of course). How much would capturing large data via tracing impact the execution time? Or is there zero impact?", "How about bagging-via-tracing?", "That honestly sounds like a great idea, although probably a completely separate discussion :). Between that or something like bagging-via-pcap, it would be great to have an option to record data without having to depend on the underlying connections to function reliably.", "It looks like the rosbag2 storage backend is pluggable, but unless I\u2019m missing something, transport is not.", "How much would capturing large data via tracing impact the execution time? Or is there zero impact?", "In general, I would expect it to have less impact than the current rosbag approach. Before going down that route, we should probably test this hypothesis, however.", "The way this works is as follows: The tracing frameworks I\u2019ve looked at make a copy of the data and store it in a lock-free ring-buffer inside the process for later retrieval by the capture process. That happens at the point where the tracepoint inserted, and it will block until the data has been copied.", "The later retrieval and disk storage by the capture process happens asynchronously, however, so \u2013 apart from consuming CPU and disk bandwidth \u2013 storage does not impact the traced process.", "Of course, the exact impact also depends on where you\u2019re putting the tracepoint. The easiest way would be trace messages when they are serialized. That way, you don\u2019t have to deal with the exact type, and just store a byte array. This would be the same pathway that also sends data to rosbag via subscription.", "Now, when you have an application that sends around images intra-process, they would not normally be serialized. Adding ", " kind of recording will change that, and thus impact the system. With tracing, you could, in principle, generate a message-type specific tracepoint, however, and avoid (DDS) serialization. That\u2019s advanced stuff, but it could be done.", "What I\u2019m not sure about is how this all compares, performance-wise, to the new shared-memory transports that are currently being introduced.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-bagging-w-r-t-services-and-actions/11618"}
,{"title": "SROS2 - Securing certs and keys", "thread_contents": ["Hey guys,", "\nI was wondering if there was a technical document for SROS2. I\u2019m very much interested in what it offers and want to get to understand its inner workings. I also understand that the project currently is able to create signed certificates. How are the certificates and private keys getting saved? I was planning on using secure keystores to do that. Would something like that be valuable?", "Hello ", ",", "The current way the certificates are being constructed is quite basic: via subprocess commands to openssl\u2019s CLI. Once you exicute the ", " command via the SROS CLI, an ", " is triggered to bootstrap the openssl configuration files, then calls upon openssl command to generate the private keys, and singe the necessary public certificates.", "Later, should you decide to invoke access control, the ", " command will again use the keystore to sing governance and permission files consumed by the vendor specific middle ware to enable access control enforcement as defined in the ", " spec.", "I\u2019m working on refactoring this to use something like a modern python library such as ", " to more pragmatically control the key generation and signing of certificate authorities, like I did for SROS1. Last year I did spend a brief amount of time exploring more rigorous keystore solutions, such as open source projects like Vault:", "Centrally secure, store, and tightly control access to secrets across distributed infrastructure and applications.", "However, in the end I figured most end users would not enjoy installing and learning a host of other dependencies and frameworks, and would more likely impeded the ease of use and adoption of SROS. So I went with the simple method of optionally ciphering the private keys to disk. Most PKI frameworks support loading ciphered keys via secrets that can be supplied at runtime, as used in SROS1 ", ". End users can easily take additional steps from there to guard private keys via additional custom solutions if need be.", "What secure keystores methods where you looking at? It would still be nice if we could design SROS2 to interoperate easily with other 3rd party keystores methods.", ",", "In all honesty, I\u2019m still in the process of figuring it out. My idea was creating a package that would help users make use of a TPM to secure the certificates and keys. This work is meant to go towards my Master\u2019s research.", "\nOnce again, this is just an idea and hasn\u2019t fully taken shape yet.", " Good point, can you share more details regarding your enabling to TPM ? thank you . ", "I have another question for the SROS2, you know, the SROS supports AppArmor, why is it dropped from the SROS2 ? is it not necessary to protect from the system level now from your perspective ? thank you !", " Thanks for your interest in SROS and SROS2.", "SROS supports AppArmor, why is it dropped from the SROS2 ?", "The development of SROS2 has been focused on the communication security rather than the system security so far. I wouldn\u2019t say that system-security is \u201cdropped\u201d but rather \u201cnot implemented\u201d yet.", "\nAs you may know ROS2 targets a wider variety of platforms (non-Linux or non-posix) and applications than ROS1. Currently SROS2 has been geared towards providing secure communication at the lowest level of the ros client library (rcl) to allow users to leverage it regardless of the programming language, platform or implementation of the communication protocol. Currently we test on Ubuntu, MacOS and Windows, in C++ and Python, and with eProsima\u2019s FastRTPS and RTI\u2019s Connext.", "We haven\u2019t yet looked into the best way to interface system security tools to the current SROS2 implementation but we definitely want to do it in the future. Given that it is pretty orthogonal to the encryption of the communication it can be addressed separately. While Apparmor is awesome, it is Linux only and thus will target only one of our supported platforms. Ideally SROS2 will provide a way to define the permissions of your application in a platforms-agnostic manner and be extensible to implement generators that will provide \u201cconfiguration files\u201d or \u201cprofiles\u201d according to the tool or platform you want to use. And the tool on Linux can very well be AppArmor.", "As ", " well put it, AppArmor support has not necessarily been dropped from SROS2, but rather not yet implemented. My present focus has been integrating ROS2 with DDS Security, and have not yet had time to expand upon the apparmor profile to support ROS2. Another reason is that I haven\u2019t yet surveyed ROS2\u2019s installation structure yet, as the majority of the apparmor profile library for ROS1 helps users to author profiles of there own by abstracting the ROS1 file directory layout.", "However, as mentioned before, apparmor support is somewhat orthogonal to SROS2 interrogation with DDS Security, and so could be developed in parallel. I would certainly invite contributions and pull requests from the rest of the community to add support for ROS2, or help review what ", ".", "Contribute to apparmor_profiles development by creating an account on GitHub.", "Hmm\u2026 What literature have you found on the subject of Trusted Platform Module and robotics? I do like the idea of having the private credentials used for SROS2 isolated from the host OS, but that may require coding 3\u2019rd party plugin, as the default DDS crypto plugin must load the private key from a path on disk, or serialized PEM string.", "Funny you should mention TPM though, as last week or so when I was at the RoboCub 2017 Symposium, I met a PhD student, Sarah Haas from Graz University of Technology, who was presenting work on a similar topic:", "Secure Authentication for Industrial Mobile Robots using Biometric Data", "You may want to investigate the lab Sarah is from. I recall some of Sara\u2019s peers working on using TPM for the Diffie-Hellman key exchange and establish a symmetric session key without revealing the private key to the network host.", " ", " thanks for your infomation, let me have an basic gap/work evaluation to implement the AppArmor to SROS2 and I may try to enable that if no conflicts after evaluation ", "It seems that this solution focus more on external authentication to an IMR(Industrial Mobile Robot), you know, now the ROS nodes can employ the key/certificate produced using SROS2 to authenticate/encrypt/access control for the nodes/topics etc, but do you think it\u2019s necessary to provide the security to the key/certificate itself ? now they\u2019re placed on the disk without any protection, that means it\u2019s easy to be accessed or tampered.", "\nBTW, you know more details regarding the details of SROS2 implementation, so could u please help double confirm/clarify the following questions:", "Many thanks", "\nBRs", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Keystore: does it support key exchange between publisher / subscriber, etc? Can keystore be changed?", "Encryption of data AES-GCM-GMAC: used to have license issue on GCM, has its license model been changed?", "Tools to create keystore, certificates: are they out-of-band tool that user needs to use this tool to create keystore manually first then pass it to DDS? How does the whole solution work?", "Is there any access control for SROS2: tools, encryption, etc?"], "url": "https://discourse.ros.org/t/sros2-securing-certs-and-keys/2400"}
,{"title": "Generate .idl files for Fast-RTPS from ros2 messages", "thread_contents": ["I read here(", ") that currently the default implementation of dds( Fast_RTPS) doesn.t consume .idl files, that\u2019s why it is not visible when we build ros2 packages.Will future ros2 releases include any tool which will generate .idl files for FastRTPS, from ros2 messages ?", "The package ", " already generates ", " files from ", " files which is being used for ", " and ", ".", "But ", " is different since it uses introspection rather than generated code specific to each message so for FastRTPS you don\u2019t have to generate ", " files.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/generate-idl-files-for-fast-rtps-from-ros2-messages/2964"}
,{"title": "IPC in ros2", "thread_contents": ["Does ros2 use inter-process communication when we use it in  linux OS ? What about in case of bare metal micro-controllers where there is no OS?", "In the future, please set the category to \u201cNext Generation ROS\u201d when discussing ROS 2.", "Does ros2 use inter-process communication when we use it in  linux OS ?", "First, I\u2019ll assume you mean \u201cdoes ROS 2 use any operating system IPC mechanisms between nodes on the same machine, rather than sending data over UDP?\u201d, because the term \u201cinter-process communication\u201d only means communication between two processes, but does not imply what the mechanism is (i.e. UDP or TCP can be IPC).", "It depends on the middleware implementation you are using. Currently the default of Fast-RTPS does not use anything other than UDP single cast or multicast. It could, RTPS allows for this in the protocol. I believe some of the proprietary implementations provide a shared-memory based IPC mechanism when on the same machine. Fast-RTPS will hopefully support this in the future.", "What about in case of bare metal micro-controllers where there is no OS?", "There have been some experiments which use RTPS on a OS-less microcontroller to communicate with ROS 2 on a desktop and some other experiments to get our ROS 2 API compiling for microcontrollers, but there is no out of the box support for OS-less ROS 2 at this time. We did enough work to convince ourselves that is possible, but haven\u2019t followed through with sustained support because we didn\u2019t have time, though we would love to do that in the future.", "In that case though, with no OS, you probably would not be using processes so the idea of \u201cInter-", " communication\u201d doesn\u2019t really make sense. You would be using something more like our \u201c", "-process communication\u201d, which can avoid sending the message data over the network (UDP in most cases).", "RTI\u2019s DDS does use shared memory internally when possible, I think.", "Hi sagniknitr,", "Fast RTPS will support shared memory in future releases, is in our roadmap. Regarding microcontrollers we are developing a lightweight version of Fast RTPS based on an OMG future standard for constrained resources devices. Of course it is not a full DDS/RTPS implementation, but a reduced API and different protocol.", "You can see the first prototypes in the ", " use case, and we are working in an release now. This will let you to use ROS2 in micro-controlllers.", "If you want more details, please contact me.", "Jamie,", "Thanks for the update about how Fast RTPS will be evolving.  A few questions, is the prototype with Dronecode you\u2019re referring to the work you did on the ", "?  Additionally, can you point to any public links regarding this new OMG standard for constrained resources? I know that the ", " is publicly available, is a draft of the document you\u2019re referring to available?", "Thanks", "\n- Eddy", "The DDS XRCE (\u201ceXtremely Resource Constrained Environments\u201d because all OMG specifications need a cool acronym) specification isn\u2019t even close to finished yet. The ", " but you have to be an OMG member to get the working documents, unless someone involved is willing to provide them. When the specification is adopted, that becomes publicly available.", "As seems to happen a lot with the DDS specs these days, the RTI/eProsima/Twin Oaks camp and the PrismTech camp have produced their own versions of the XRCE specification and are struggling to reconcile them into a single specification that the OMG can adopt. I won\u2019t wade into the debate over which is better, but I\u2019ll let you draw your own conclusions from the fact that one of the two camps only has one company it it. ", " I hope it\u2019s not going to be the RPC for DDS spec all over again; the arguments over that could be heard in other meeting rooms. But it seems likely that it will be a year and a half or more before the final specification is released.", "Thanks for the informative reply ", ", I had no idea the work was under way.", "I attended the OMG meeting last week, so I took the opportunity to hear what PrismTech and RTI/TwinOaks/eProsima had to say about their competing submissions. The following are the notes I took during their presentations. Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "Prismtech presentation:", "RTI/eProsima/TwinOaks presentation:", "My summary:", "Thanks for the summary ", "!  I\u2019m very interested in how the work progresses", "Thanks Geoff, great stuff!", "Hello ", ",", "First of all I\u2019d like to point out that I am one of the author of PrismTech\u2019s/ADLINK XRCE proposal, thus you may think I am giving you my perspective, in that case I urge you to read original documents to see how what I am providing here are facts. That said, let me make a few rectifications to some of the points above.", "You say:", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure ", " You can find the result of our analysts here ", " but you are welcome to derive them by reading both specs.", "This is also partially correct. We had asked to present what would have been our final proposal in Bruxelles to give a chance to the task-force to digest and one more opportunity to the competing team to join. This has nothing to do with the Vote-to-Vote and the fact that the task-force did not decide to allow the Vote-to-Vote procedure. The OMG has very complex rules\u2026 I know that can seam strange, and they still surprise me after having spent more than 10 years dealing with it.", "The RFP (which I wrote) asks for a protocol not for an API.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "\nAdditionally, you may argue that this may add some complexity in the message parsing \u2013 but again \u2013 I think that checking a flag and deciding wether some field is going to be present or not does not belong to the realm of hard. It is also worth  pointing out that some flags are just informative and don\u2019t require any kind of branching in the decoding. Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes ", " And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple ", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples. Our proposal currently has 4 compared to that of RTI/TwinOaks/eProsima which has 16 bytes. We have had our implementation run on a Makeblock robot using BLE with an MTU of 20 bytes.  You can check the comparison on this deck ", " and should wonder why the competing proposal did not provide a proper analysis of the wire overhead. Additionally when leveraging batching we have a wire overhead of (3+n)/n where n is the number of samples being batched.", "Some other thing worth point out is that the protocol allows for data to be pushed, pulled or periodically pushed or pulled. The write protocol also allow to pace the streaming of data that results from a remote query. This is extremely important when dealing with resource constrained nodes that need to consume data little by little.", "I did not hear RTI saying:", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those. The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP. The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "I\u2019d like to understand why you say this:", "What do you think is our submission cutting out? We are wire efficient yes, extremely wire efficient but at the same tine we support:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions. I\u2019ll also be more than happy to answer any question you may have about our protocol.", "A+", "Dear ", ",", "You are being to nice to PrismTech as everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises. I think it is best for people to look with their eyes at what the two submission can do and make their own decisions.", "On my side I like debates, I like inquisitive minds and I like hard questions. Thus I\u2019ll be more than happy to answer any question on the XRCE protocol proposed by the PrismTech/ADLINK team explain why it is better than RTI proposal\u2026 And actually it is better than RTPS.", "Thus, please let\u2019s start the open debate to dissect the reasons why our proposal is the one which should be voted and by far the better one.", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being ", "A+", "Thanks ", " for jumping into the community in such an energetic manner. I think we all appreciate having one of the authors of the proposals providing feedback. That said:", "Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "which answers:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I think I\u2019ll stop here\u2026 for the time being ", " .", "Hi Angelo (", "),", "Coordinate different companies to get a common view on a complex matter is always hard, and in this case, there are multiple design options leading to different tradeoffs.", "Our submission (RTI, Twin Oaks & eProsima) already aligns the views of three different DDS vendors, and sure we will try to incorporate ideas of your submission.", "Our submission tries to accomplish the following:", "We coded a PoC of our submission, and during the presentation, you asked about numbers. At that point, with no optimizations at all, and in debug mode we answered 43Kb. I asked my team to optimize a little bit the code, and here are the numbers we have now for the client:", "Total Memory Use: 8 Kb", "But we could squeeze that even a little more. We are releasing this as Open Source (Apache 2) so anyone can review the results.", "But again, we are not aiming to be as small as possible. We are covering all the requisites of the DDS-XRCE RFP, and testing our solution in what we consider typical microcontrollers today.", "Regarding the process at the OMG meeting, we (RTI, Twin Oaks & eProsima) didn\u2019t want to confront both specifications and choose one of them, but have the time to incorporate ideas from your submission, and that is why now we have an extended deadline.", "Let me join in the fray. I\u2019m the other author of the PrismTech submission and the one who built our tiny prototype. I wasn\u2019t present at the OMG meetings, and I won\u2019t waste any words on what may or may not have happened there.", "Firstly, I don\u2019t think a contest of bytes of RAM adds real value to the discussion, although of course it is an honourable contest in itself ", " I am surprised that you, ", ", had never even had a proper look at the memory use of your implementation given the purpose of the exercise in the first place, but if it is 8kB now then it is much better already \u2014 if still 7kB overweight ", " In any case, memory use is determined more by the implementation than by the protocol messages.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already. Yet even that is not of such great interest to me in this discussion.", "What really matters in my opinion is a difference in philosophy. The two proposals suggest very different views of what one would ideally want to accomplish.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that. In a sense, it is just a hand-crafted alternative to CORBA with a lower overhead. (Simply using CORBA actually \u201cjust works\u201d if the DDS implementation is faithful to the IDL interface mappings, even if it is ugly.) To me, this route is a pragmatic way of going about satisfying the RFP, but at the same time, an uninteresting one. (Sorry ", " and others \u2026)", "We chose to design a compact protocol that can support what amounts to performing DDS operations remotely, but doesn\u2019t limit itself to it. Instead, it also supports a DDS-like peer-to-peer network with vastly lower overhead, and, in many ways a level of flexibility in specifying what data is of interest (through URIs and selections) that DDS doesn\u2019t natively support.", "All of that would be of little value if it doesn\u2019t perform well or doesn\u2019t scale well. Just like the code size and memory use are mostly determined by the implementation, so is maximum sustainable performance more determined by implementation than by the details of the protocol headers. Size-wise, my prototype can run as a client on an Arduino Uno (8-bit CPU, 2kB RAM). A small test application using the same implementation configured as a peer easily sends ~700k 8-byte msgs/s from one RPi3 to 3 others (CPU is << 100%, network load ~75% of Fast Ethernet, so I really should investigate why it isn\u2019t faster), and goes another order of magnitude faster when run over local loopback on my MBP. That\u2019s better than your typically DDSI implementation. Is this relevant? That depends on whether you have high rate, tiny messages \u2026", "Now my test application doesn\u2019t implement all of DDS \u2014 not even close \u2014 and this is another significant reason why it can do this with only a few kB of code and RAM. At the same time, this is, I believe, where it gets interesting for ROS2.", "As ROS2 has its own middleware abstraction layer that uses only a fraction of the DDS feature set, putting ROS2 directly on our protocol would get you a smaller and a faster system. Smaller and faster usually allows doing more interesting things, even if I can\u2019t say what exactly those interesting things will be.", "Disclaimer: I can\u2019t do run ROS2 over it today, there\u2019s more work to be done on my prototype before it supports all that is required. And I wish none of you would have to take my word for the data I mentioned, but that is not something that is in my power to solve today.", "It looks like I started something of a minor storm moments before starting a holiday\u2026", "I\u2019m thankful that the DDS vendors, PrismTech, RTI, Twin Oaks and eProsima, are all engaged enough in the ROS community to be present on the Discourse board. It is encouraging to future adopters of ROS 2.", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure", "I wasn\u2019t implying religious believe. It\u2019s an simply expression to describe someone making an assertion. ", " I fully agree that PrismTech\u2019s submission has much smaller messages than the RTI/Twin Oaks/eProsima submission based on the two presentations alone.", "The RFP (which I wrote) asks for a protocol not for an API.", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "It is quite common to use flags. TCP is full of them. My concern is that the protocol is, in my opinion, undoubtedly complex and, based solely on the presentations, more complex than the other submission. Complexity and size are often a balance and in this situation we appear to have one submission at each end of the balance.", "Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes", "With the sorts of overhead you are achieving, I\u2019m not surprised performance is amazing. I\u2019d still like to see numbers, though. ", "And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple", "As was stated elsewhere in this thread, eProsima\u2019s implementation wasn\u2019t optimised. Since you said you are trying to bring yours to market already and eProsima claimed theirs was a tech demo, I\u2019m not surprised yours is more optimised and thus smaller. Of course, the numbers are definitely in your favour for RAM usage. But I\u2019m curious how much program memory each implementation requires, too. This is often the limiting factor on embedded microprocessors rather than the RAM usage.", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP.", "In that case I am very interested in seeing what these AB members wrote.", "The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "What do you think is our submission cutting out?", "\u201cCutting down\u201d, not \u201ccutting out\u201d. I meant that you are trying to reduce the size of the messages on the wire as much as possible, at the expense of possibly needing more complex parsing code. I didn\u2019t mean to say that you are cutting out features. It was clear from the presentation that PrismTech supports more features and has more flexibility than the other submission. But there are trade-offs involved.", "Dynamic Discovery (RTI does not, please read their spec!)", "\nPeer to Peer Communication (RTI does not)", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Non IP Transports (RTI does not)", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "Generalised Queries (RTI only supports DDS-like queries)", "Well, it is ", "-XRCE, is it not?", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions.", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises", "While this is true, we are operating at a standardisation organisation, not a rubber stamp provider. There are interested parties beyond just the implementers. We would prefer not to just hold a vote on which submission to go forward with. We would prefer the submitters to actually work together and produce a single submission that combines the best of both without any technological compromises (yes, I\u2019m aware how hard that is).", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being", "Or, you could provide the answers to those questions, along with the equivalent answers for your submission, so we can see and compare the data to back up your claims.", "Our submission tries to accomplish the following:", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "\nNeither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use.", "This is the strongest impression I got from the presentation, as I said in my own notes. The data model is similar to DDS, which makes adoption by existing DDS users easier, and the ability to implement the protocol in a relatively simple piece of code (which makes it easier to verify and certify) was considered as important as saving bytes on the wire. I\u2019m not sure where the correct balance is between these two requirements, but the PrismTech implementation really gave the impression of being at one extreme.", "testing our solution in what we consider typical microcontrollers today", "This is something that I think is really relevant but that PrismTech have not addressed at all, and RTI/Twin Oaks/eProsima have not addressed enough. What are the typical microcontrollers in use today? What are the target environments for this protocol to be used in?", "The RFP explicitly says this:", "From a high level perspective, the key requirements that a DDS-XRCE implementation has to satisfy are (1) extremely low footprint \u2013 addressing targets with less than 100 Kbytes of RAM, (2) extremely efficient wire protocol \u2013 inducing a protocol overhead of just a few tens of byte over the user data, and (3) support devices that undergo aggressive sleep cycles.", "Both submissions fit within both the RAM usage (with much room to spare) and the protocol overhead.", "More specifically, the actual mandatory requirement is:", "6.5.3. DDS-XRCE protocol overhead, when sending or receiving user data, shall not exceed 24 bytes per packet.", "Let M be the size of the DDS-XRCE message sent out, and U the size of the user data, then M-U <= 24 bytes.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already.", "Again, this should have been in the RFP if it is so important. That would have saved a lot of trouble. All we got was an evaluation criteria:", "Protocol overhead shall be considered when evaluating submissions. Submissions with smaller overhead are preferable.", "This is a miserably small set of criteria for a complex design space. Even you, ", ", say that protocol overhead is not as important as the design philosophy.", "Which I agree is fundamentally different between the two proposals, and that this is where the root cause lies in the failure to reconcile them.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that.", "The RFP not-so-subtly pushes submitters in this direction. You cannot fault them for taking it at face value.", "I will read both submissions and when I do, I will report back with more technically-informed comments.", "Hello ", ",", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "And you see this as a positive aspect? Our model is simpler and more user friendly. For instance, how many people can digest DDS partitions? That said, we have a well defined mapping between XRCE resources and DDS topics.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "Are you an OMG member? If so I\u2019ll forward you the reviews. Both submissions went to the AB and the reviews were posted both on ", " and ", ". If you have access to those mailing list you\u2019ll be able to see them. I also recommend you take a look at those.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "It was impossible as the other vendors did not want to agree on such a low bound. The 24 bytes was the least we could agree on. This is why there is an evaluation on wire-efficiency. This matters were discussed at length, but again I don\u2019t think you attended those meetings, thus you are missing part of the history and the context. In any case, all of those documents are on the OMG archives, thus if of interest you to reconstruct it. Just search for presentation I did on XRCE for almost a year. starting from 2015!", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "Yes, that is correct as it is since the very beginning that we are trying to do a joint submission. They\u2019ve refused with futile arguments \u2013 if you ask me. We have put lots of effort to trying to join but that has not been corresponded. A pity that you were not in the Bruxelles meeting, otherwise you would have had a taste of it.", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Again, you did not attend the end-less arguments we had during the RFP drafting. RTI does not want peer-to-peer in XRCE because they fear it could become as substitute for DDSI-RTPS. Again, this is not something I am inferring, but something that was openly debated during the RFP drafting. We don\u2019t have any issue with that as we think that having a more efficient protocol than DDSI-RTPS for some use cases would be extremely useful.", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "For me that disqualifies completely the submission as in LowPAN environments nobody can afford to use TCP/IP\u2026", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "I am glad that this helped clarifying the situation, please don\u2019t hesitate to ask any other question. Concerning the code availability we are working on that. I\u2019ll keep you posted.", "A+", "Dear All,", "I wanted to let you know that we have just released under Apache 2 a peer-to-peer implementation of our zenoh protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating zenoh as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", "N.B. For those of you that are familiar with XRCE, zenoh is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as zenoh.", "A+", "Kydos,", "Thank you for publishing the Zeno-He library so we can all begin to interact with it.  It is especially useful for the ROS 2 user community to be aware of the effort since it implements the ATLab XRCE proposal.", "I know I would be extremely interested in someone benchmarking Zeno and the proposed epromisa XRCE implementation, and perhaps can find time to do that.", "This is really appreciated ", ", thanks for making this available.", "At present it\u2019s unlicensed code though (", ").", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They believe that their proposal is more efficient in the wire protocol (trying to save every single byte possible), and supports brokered as well as peer-to-peer communication.", "They have invited the competing submitters to join the PrismTech submission.", "PrismTech believes that they have presented their final submission in Brussels (June) and so were expecting to vote for acceptance in New Orleans (September), but because there are still two competing submissions, and no final submission document was received (only a presentation), this is not possible.", "PrismTech\u2019s reasons for going forward with their own submission:\n", "XRCE tries to target the most wire/power/memory efficient protocol, targeting not just IP infrastructures but a whole range of infrastructures.", "Their submission provides reliability and fragmentation.", "Their current prototype runs on an 8-bit microprocessor with 1 KB of RAM and has a wire overhead of 4 bytes for data samples.", "They got a review from the AB which was favourable (only editorial comments), and they believe that the AB review shows that they satisfy all the mandatory requirements.", "\n", "XRCE applications can be brokered into a DDS data space, or they can discover one another and communicate P2P.", "Their submission is only about the protocol, it does not say anything about the API.", "A DDS-XRCE Agent running on permanently connected hardware provides the access to the rest of the (non-XRCE) DDS domain.", "XRCE provides a data space abstraction in which applictions can read and write data autonomously and asynchronously.", "Data read and written by XRCE applications is associated with one or more resources identified by a URI.\n", "An XRCE resource is a closed description for a set of named values.", "Resource URIs allow for wildcards, which means that more than one resource can be targeted in one definition, which is useful for capturing collections of sub-namespaces.", "A resource with a cardinality of one is called a Trivial Resource.", "Resources also have properties, which are used to attach QoS settings to them, such as \u201cthis resource is transient\u201d or \u201cthis resource is reliable\u201d.", "An XRCE selection is the conjunction of a resource and a predicate over the resource content and properties, used to filter a selection. For example, finding all lights with a luminosity greater than zero. (It seems to provide functionality similar to a very basic subset of SQL.)", "\n", "There is a mapping from XRCE resources to DDS topics.", "Resource serialisation uses the same format as DDS.", "All XRCE messages are a single-byte header and a body.\n", "There are flags in the header for reliability and synchronicity.", "Messages can be \u201cdecorated\u201d by prefixing them with a one-byte pre-header, which determines some of the properties for the following header byte, such as fragmentation.", "The protocol is little-endian.", "Variable-length encoding is used to save space.", "\n", "For addressing, the source address for each message is assumed to be a unique address of the sender.", "The protocol is modular, with a core profile (required for any communication), an optional query profile, and an optional discovery profile.\n", "e.g. If using a communications system like Bluetooth, which already has discovery, the XRCE discovery profile can be removed, saving some space.", "Discovery happens by scouting (sending a scout message to ask for types of nodes to reply, i.e. broker nodes or durability services or peers or clients).", "Other nodes reply to a scout with a HELLO message.", "\n", "After discovery, two nodes need to open a session, which enables publishing and subscribing between those two nodes.\n", "Authentication data is included in the session open message.", "Locator information, telling the other node how it can be reached, is included in the open message.", "The receiving node replies with an accept message or a reject message.", "There is an FSM describing the states a session goes through.", "\n", "Resources and selections are uniquely identified by numerical IDs to save space on the wire.", "A declare message is sent to declare what resources, selections, etc. a node has or will publish or subscribe to.", "Subscriptions can be push, pull, periodic pull or periodic push.", "All data messages and declarations are transported over a conduit (for the session), which is a pair of a reliable and a best-effort channel. Multiple conduits may be used in parallel to avoid head-of-line blocking and allow multicasting.", "One decorator is used to select the conduit for the next message.\n", "It is possible to make this decorator one byte instead of two if the number of conduits is less than five.", "\n", "There is a sync message available to set the next sequence number to expect (e.g. for when not starting at zero).", "The ACKNACK message can acknowledge multiple messages at once, usually up to the given sequence number. It has the option to optionally request retransmission of one or more messages after that point.", "There is a one-shot write function in the protocol, which can do a write of a resource without needing to do any prior registration or discovery.", "This proposal appears to be a very complex protocol with a lot of options in message header structures. An implementation would have a lot of choice points during the decoding of a stream of messages. The \u201cdecorator\u201d idea especially may save bytes on the wire (and in message construction buffers) in some cases, but it complicates the protocol implementation.", "PrismTech are already trying to bring their submission to market as a product.", "This proposal also uses an XRCE agent to provide access from DDS-XRCE nodes and the DDS domain.", "An important use case for them is that devices will tend to mostly sleep, and only wake up occassionally to do some processing, and send and receive data. This means that two devices may never be awake at the same time. This is why they have the agent, which is permanently present and provides a way for XRCE nodes to communicate with each other.", "They say that their proposal focuses not just on the XRCE protocol, but also on the interaction between the XRCE protocol and the DDS domain.", "It is possible for an XRCE agent to behave as an XRCE client to another agent, allowing for a hierarchical structure of agents and clients.", "Their proposal is based on the web-enabled DDS specification, with a DDS-XRCE object model in the agent that has a one-to-one mapping to the DDS data model.", "eProsima have put up a demo on Youtube: ", "\n", "Their demo uses 43 KB of RAM (much bigger than the PrismTech proposal, which can fit in 1 KB).", "The XRCE Agent object model is very similar to the DDS object model, making the mapping very simple.", "XRCE objects are modeled as resources that are addressable by their name and have CRUD operations.\n", "Each resource has a name to address it within the agent and a context; a representation describing the resource; and an ID.", "Resources can be represented as a name, an XML description, or a binary representation.", "\n", "Authentication capability is built into the proposal.", "Types are typically pre-defined profiles in the XRCE Agent.\n", "It is possible to transmit types as binary or XML representations.", "\n", "Message structure:\n", "A message header is either 4 or 8 bytes, depending on if a clientKey is used.", "There is a sub-message header, which is another 8 bytes.", "It is possible to send data in a sequence, meaning the header only needs to be sent once for a bunch of samples.", "\n", "The transport can be message-oriented or packet-oriented.", "CDR and DDS-XML representations are reused.", "Message overhead is typically 12 bytes.\n", "They consider that further reduction in overhead would increase complexity and reduce robustness. e.g. They do not need different code paths for multiple sessions, re-connections, handling variable-length encoding.", "They say it compares favourably to TCP/IP overhead (40 bytes).", "They think they could save 6 bytes from their message header, but the reduction is only 6% in the context of total overhead (when considering the use case of using TCP/IP as the underlying transport protocol) and so is not worth it in the face of increased complexity.", "\n", "They also received 3 AB reviews, which they claim were supportive and did not find any non-editorial problems.", "The submissions are very different. PrismTech is aiming for cutting down the bytes used by the protocol to the absolute minimum at the expense of all else. RTI/eProsima/TwinOaks are favouring some overhead in order to achieve a simpler and more robust protocol and implementation.", "PrismTech\u2019s protocol is overly complex with too many choices during the decoding of a message.", "The extra overhead of the RTI/eProsima/TwinOaks submission is not likely to be a problem in the majority of embedded micro-processors used these days (although I\u2019m not sure how many 8-byte, 1KB-of-RAM micros there are in use in new products). However, their consideration that TCP/IP is going to be the most common transport may not be accurate.", "Dynamic Discovery (RTI does not, please read their spec!)", "Peer to Peer Communication (RTI does not)", "Client to Broker Communication", "Non IP Transports (RTI does not)", "Generalised Queries (RTI only supports DDS-like queries)", "Push/Pull/Periodic-Pull and Periodic-Pull Readers", "Unicast and Multicast communication \u2013 for both client to broker and peer-to-peer", "Durability", "The view ", " provides is a) unbiased, b) the view of a roboticist (that\u2019s what this community is about ", " ! )  and c) based on the information someone got from hearing \u201cyour presentation\u201d. Note the following:", "I believe we all appreciate technical argumentation and slides. I love slides. But what I love even more is code and things that I can reproduce. How can I verify your arguments through experimental results? Is there any open code that supports your arguments? More than bashing around, i think it will do a lot of good to facilitate implementations that others can reproduce in common platforms. Even early stages will do. You might find that you could even get some support (and feedback!) before launching it officially and furthermore, that\u2019ll definitely convince a lot of people on why your approach is better.", "Last but not least, I really hope that the passion you\u2019ve shown answering this thread is shown by supporting and making OpenSplice better.", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "Neither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use."], "url": "https://discourse.ros.org/t/ipc-in-ros2/2619"}
,{"title": "Latency and throughput in ROS2", "thread_contents": ["The paper ", " evaluates the latency of ROS2 (different node configurations, different DDS implementations, different QoS policies, etc.).", "Does someone know if these results have been updated by some more recent work(s)?", "We will release a plugin-based tool that will let you evaluate latency and memory resource while altering number of pubs/subs, QoS, RT settings, large and small message types, publishing frequency, security settings, inter and intra process communication, etc. We need 2-3 more weeks, stay tuned.", "That\u2019s fantastic ", ", looking forward to have a look.", "On our side, we recently released a first tech report (", ") that\u2019s part of a series that hopefully will characterize better the latencies and throughput in ROS 2 while taking in consideration Real-Time aspects. This first tech report treats OSI layers 1 and 2.", "Hi all, we released the performance_test tool: ", " to e.g. benchmark latency, jitter, lost samples, etc. in different DDS implementations.", "\nCurrently supported benchmarking is for communication mean over:", "We plan to extend it to use ROS1 comms as well.", "Hope you find it useful, any feedaback is more than welcome.", "D.", "Hello !", "Nice tool ", " !", "What are the requirements on the RMW layer to be tested ?", "\nI have a ", ", but it currently only support simple pub/sub.", "\nIt would be interesting to measure performance with your tool and compare it to DDS implementations.", "Also (for latency), is it only measuring the end-to-end latency, or can it be more comprehensive ? For example measuring the time spent in each layer (RCL/RMW/DDS/Network).", "Hi ", ",", "I am the maintainer of the performance test tool.", "If you already have RMW implementation which supports pub/sub you should be able to directly test your communication mean without any additional work. You just need to set the proper environment variable ", " before starting the tool.", "It can not messure latency based on application layers unfortunately, for this it would have to be invasive in all these layers.", "But you can create a new communication plugin for the NDN transport as I did for FastRTPS here:  ", ".", "This will allow you to compare only the communication frameworks performance and will also give you some insight over the overhead the various RMW layers introduce.", "If you run into issues implementing the plugin I will be glad to support you.", "Thank you for your answer ", ",", "I was lacking some features (typesupport, proper management of multithreading, etc\u2026), but in the end I was able to test my stack with your package. Also, I already implemented the invasive solution for measuring the latency of each layers. I will ask you if I need help, thank you again.", "Hi ", ",", "Also, I already implemented the invasive solution for measuring the latency of each layers. I will ask you if I need help, thank you again.", "Would you mind sharing some more details about your solution for measuring latency in each layer? Maybe we could even integrate it into the performance_test itself.", "I didn\u2019t do anything complex, but since it is an invasive way of measuring, I don\u2019t know if it is easy to integrate into the performance_test. ", ", and then post processed with a python script. It is not very precise, but since I don\u2019t want the real latency (only being able to compare the two implementations), it is ok for me. To avoid the print extra cost during the experiment, you can register the events+timestamp in a (pre-allocated) table, and print everything at the end.", "Very nice. I was thinking if someone could do a  ros message extension to add-in the latencies at each layers in a private area of the message at each layer (encode as a BLOB) and could be retrieved at the subscriber to measure-in.", "For those interested, a new technical report studying this topic is available: ", ".", "Here\u2019s another update: ", "In this work we present an experimental setup to show the suitability of ROS 2.0 for real-time robotic applications. We disclose an evaluation of ROS 2.0 communications in a robotic inter-component (hardware) communication case on top of Linux. We benchmark and study the worst case latencies and missed deadlines to characterize ROS 2.0 communications for real-time applications. We demonstrate experimentally how computation and network congestion impacts the communication latencies and ultimately, propose a setup that, under certain conditions, mitigates these delays and obtains bounded traffic.", "Compared to other results:", "Hello Victor, thanks a lot for your reports, I\u2019m reading them right now and I came to a sentence that I don\u2019t understand. On ", " on the 4th page you say:", "Additionally, as Ethernet is asynchronous, the high priority frames sharing the same link can content between them.", "What does ", " mean in that context?", "Sorry if it\u2019s a bit of a picky question.", "Oops, later on I found the usage of ", " which now makes sense to me. I guess that one was a typo ", "Hey there ", "! It certainly sounds like a typo. Many thanks for reporting. Let us review it internally and report back if our mistakes go beyond that.", "Cheers!", "An another one:", "A new generation of robot systems which are modular, flexible and safe for human-robot interaction are needed. Existing cobots seem to meet only the later and require a modular approach to improve their reconfigurability and interoperability. We propose a new sub-class of cobots named M-cobots which tackle these problems. In particular, we discuss the relevance of synchronization for these systems, analyze it and demonstrate how with a properly configured M-cobot, we are able to obtain a) distributed sub-microsecond clock synchronization accuracy among modules, b) timestamping accuracy of ROS 2.0 messages under 100 microseconds and c) millisecond-level end-to-end communication latencies, even when disturbed with networking overloads of up to 90 % of the network capacity.", "Read the tech report at ", "Hi Victor,", "Thank you for the reports related to latency using RT_PREMPT linux and ROS 2 with various network settings. It was very interesting to read.", "Have couple of questions.", "In the base RT_PREEMPT linux kernel report (", "), I understand Table-III and Table-IV is what matters. But, while looking at, Table-II (Roundtrip latency results with RT normal), was curious if you know what might be the reason for TX traffic at 100Mbps, the MAX latency is considerably high at 25ms? I would expect latency to be high when RX traffic at 100Mbps", "In the ROS 2 evaluation report (", "), in Fig 5-a, when the system is idle, DDS2 has high MAX latency(4ms) compared to others. Was just curious which DDS implementation is this and what might be the reason?", "In the Fog 6-f of the ROS 2 evaluation report, at 80Mbps, where it cannot meet the deadlines and dropped packets, was curious, is ksoftirqd processing the packets the primary cause for the latency or can it be the DDS layer causing the latency. Also, about packets being dropped, would setting the size of kernel socket buffers ( net.core.rmem*, net.core.wmem*) would help too.", "Thanks", "\nAnup", " see 3 questions above by folks from Apex.", "Hi ", ", ", "Thanks for your feedback, I will try to clarify some of your doubts:", "Both TX and RX paths are suffering from the context change to the ksoftirqd threads but in a different way. In the transmission path both streams are going trough the same Qdisc queue. When there are packets pending to be transmitted in the Qdisc queue they are sent from the ksoftirqd context. At some point the fair scheduler decides that the ksoftirqd thread has consumed enough CPU and it is preempted. During this time, packets are accumulated and we observe high latencies in the order of milliseconds. For 100 Mbps it looks like the RX path, packet are processed more efficiently. This is probably because the Ksoftirqd context is not triggered all the time and part of these packets are processed in the Ethernet IRQ thread which has real-time priority. However, when we increased the network load of the concurrent traffic (>200Mbps) we observed also high latencies even in the RT normal case.", "For fig 5a and 5b we were using the default configuration of each DDS. In the case of that DDS the default configuration might not be optimized for low bounded latencies but for other purposes. However, when for the real-time settings (fig 5c and 5d) we customized the configuration of that DDS and the problem was solved.", "In this case we had 80 Mbps non-ROS 2.0 concurrent traffic with the ROS 2.0 round-trip traffic. As there is no contention in the DDS layers the problem was very likely caused in the kernel level. Posterior analysis tracing the kernel confirmed our suspects. Changing the socket queues may prevent packet drop but would not solve the root of the problem which is going to cause latency. The real problem is caused by how the net processing is deferred to ksoftirqd context. For the moment we can only mitigate these problems and expect this is solved in the new kernel releases.", "My colleague ", " just answered ", ". Ping me if you guys are around IROS and would like to discuss this face to face.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["FastRTPS directly", "ROS 2 rmw layer (and thus any supported rmw_* implementation)", "Connext DDS Micro directly", "All the measurements have been made in embedded devices.", "We measure latencies in a inter-component scenario. Given the lack of synchronization mechanisms (in this particular work we did not set them up), we use round-trip (ping-pong).", "Previous work focuses on the measurement of local latencies while we measure distributed ones.", "We measure how communications are affected in stressed conditions. This is the best way to show if the communication stack is well configured for real-time (which connects to previous work ", " and ", ").", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/latency-and-throughput-in-ros2/4367"}
,{"title": "ROS 2 Collaboration Bulletin Board", "thread_contents": ["Let everyone know by posting here!", "Ask here!", "Thanks!", "This thread is meant to be a place where people can announce what they\u2019re working on so that people working on ROS 2 related things can find one another.", "We\u2019ve seen that different groups who are experimenting with ROS 2 are duplicating work and don\u2019t know about one another, so hopefully this thread will be useful for discovery within the community.", "Hopefully it will also be useful for people to find out what others have already done and where development is focused at the moment.", "I\u2019d like to try and keep some ground rules for this thread to keep it succinct and browsable. We\u2019ll see how it works out and adjust if necessary.", "Hopefully these rules will keep this more of an announcement bulletin board like thing, rather than pages on pages of memoranda.", "I\u2019ll probably be moderating (removing comments, sticky-ing things if I can, and/or updating this top post) to help keep it easy to consume.", "Thanks!", "I\u2019m working on rviz for ROS 2 by refactoring rviz from ROS 1 into parts and replacing ROS 1 specific parts only with ROS 2 ones. Here\u2019s my plan:", "I know some other people have already been working on this, so if you are let me know and hopefully we can work together on it!", "I have been working on porting many packages which I will organize into various groups. All of the following packages are linked with more detail ", ". Some of the packages were ported merely to a compiling state and have not been properly tested.", "As recommended by ", " and OSRF, I plan to submit upstream PRs for each of these packages in the coming days.", "I am in process of combining my own changes with those of ", " so these link to the (soon to be) combined repos.", "I was looking to see the level of difficulty of getting rqt tools working and ported all the packages necessary to get rqt_dep to an initial working state.", "I\u2019ve also been porting ", ". My approach is not as thoughtful as ", "\u2019s approach: I removed all plugins and am adding them back in one at a time. So far I have: tf, map, and laser scan to some level of functionality.", "One of my main motivation in doing this was to avoid needing to publish OccupancyGrid\u2019s through the ros1_bridge.", "How well has actionlib been ported? I\u2019m interested on working on that, but in a more improving-not-just-porting way. Specifically, I want to make actions a first-class citizen (like messages and services) and write up a REP that specifies how they should be defined and used, so that tools can be more certain about what they are dealing with. It would be great to have an \u201cofficial\u201d rosaction tool, for example, and to not see 5 topics when I list the currently in use topics.", "It\u2019s on the near term roadmap, but we haven\u2019t started it yet.", "We will likely release  an overview (maybe as detailed as a design document?) in the near future describing how we expect actions to look like in ROS2. Regarding a rosaction tool (for ROS1), there is ", " discussing what the feature set of such a tool would be. We could use the ideas in that ticket as a base for a ", " tool in the future. Feel free to pitch ideas there!", "Do you have a draft of that overview available yet? If not, would you like me to start working on one?", "I think if all the guys work on ROS2 github, everyone can contribute to the community and easy to know the progress and synchronize the patch.  If everyone fork one branch to develop , someone don\u2019t know this web page. It will result in many duplicated work.", "\nContributing to ROS 2 github, it would also make developer feel the ROS2 community very active and open.", "\nAll the above , just my suggesiton:slight_smile:", "We try to keep things on the ros2 github repository (at least for now), but not everyone has the permissions to create things there, and not everything belongs there. Since all administration of the organization has to go through the owners of the org, sometimes it is easier and more appropriate to have them on a different organization.", "I consider this an alternative way to notify people of what your working on, not mutually exclusive with pushing things to the ros2 org.", "This is great. How close is this to us being able to bag and playback data without needing the bridge anymore?", "I haven\u2019t done any work on rosbag or playback, so that is entirely unimplemented and not working.", "As for not using the bridge, you can run rviz using ROS 2 and make use of several of the plugins to avoid having to pass data through the bridge. Some of the interesting working plugins are: tf, laser scan, map, and robot model. Others (odom, path, point) may work but I have not tested them.", "Note that to use the robot model plugin it requires a change to the robot state publisher to publish the robot model (see this ", "). I just created a ", " for these changes.", "You said you got tf working to some level of functionality.  We have been running into issues when querying for transforms with lookupTransform.  We query for a transform at a given time, and we always get a transform back that does not match the time stamp that was asked for.  It is anywhere from a few milliseconds behind to 4 seconds ahead of the timestamp that was asked for.  The error actually cycles through from being a bit behind to far in the future.  Did you run into this when you were getting tf to work?  I would give more detail (code, etc.) but did not think this thread was the right place.  We can continue this discussion elsewhere if you have any thoughts that could help.", " If you can reproduce it and believe that it\u2019s a bug please open a ticket and we can dig into it there. ", "Edit: Actually you\u2019re ahead of me, I see the ticket at ", "What is the status of this work? Did you submit this PR? I am mostly interested in diagnostics.", "Haven\u2019t had time to work on any of this recently. ", " created a PR a couple weeks back to update diagnostic updater to work with ros2 ardent (and updated the urg packages as well). The other diagnostic packages have not yet been updated.", "I just created ", " to make the ros2 changes more visible and eventually get them merged into a single repo.", "Hope this answers your question.", "Hi,", "I have been working on ROS2 nodes for the Raspberry Pi camera (a.k.a. raspicam) and the AprilTag detector. Both are implemented as components and are therefore composable for intraprocess communication.", "I am open for all contributions and also don\u2019t mind if the packages get moved to official ros2 groups like ", " or ", ".", "ROS2 node for camera module of Raspberry Pi. Contribute to christianrauch/raspicam2_node development by creating an account on GitHub.", "This is a port of the original ", " from ROS1.", "ROS2 node for AprilTag detection. Contribute to christianrauch/apriltag2_node development by creating an account on GitHub.", "The implementation uses the original apriltag2 library from ", ". For integration with ament workspaces, I am using a CMake version of this library: ", ".", "Given the camera parameters, ", " computes the pose of the marker in the camera frame. It also publishes the raw detections (including the quality of the detection for filtering) and therefore depends on ", " definitions from ", ".", "Hi, I have already ported the ", " which is necessary for ROS2 image processing in the perception applications with OpenCV, and I am trying to upstream it to hopefully benefit more ROS2 users, please let me know if you find any problems, thanks !", "Here is some update to the ros2 cv_bridge, now ", " has been officially upstream to the original ros-perception vision_opencv with a dedicated ros2 branch, please submit/fix issues there when you find to make it benefit more community users from now on, thanks a lot !", "Hi,", "\nCan anyone update me regarding the porting of message_filters to ros 2 environment ? I will be happy to take up this task.", "Actually, we have been porting the message_filters to ROS2, however, you know, it has some dependency to C++ Boost in ROS. now it works except for keeping its Boost dependency in ROS2, next to clean its dependency of Boost with the way in C++11/14, I think it\u2019s good if you would like to help, maybe ", " can update you for more details.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Try to only post once per project/effort, and link to a another discourse post or GitHub issue to continue in depth discussions\n", "Update the first post (rather than post new comments with updates) if you want to keep current details there", "\n", "Feel free to express interest in a topic or in porting a ROS 1 feature/package or to ask if anyone else is already working on it, but try to keep the back and forth short, preferring to split off into a separate Discourse thread or GitHub issue once you\u2019ve connected with one another.", "Try to avoid long discussions about unifying overlapping efforts, preferring to create a new discourse thread instead, linking to it from this thread.", "Before asking if a feature is in ROS 2, check the announcements, Roadmap, and Documentation first. You should not use this thread for a casual \u201cdoes ROS 2 do X yet?\u201d. Only post here if you\u2019re pretty sure it\u2019s not in ROS 2 yet, but you\u2019re interested in helping make it happen and looking for peers to work with.\n", "\n", "\n", " (also see ", ")"], "url": "https://discourse.ros.org/t/ros-2-collaboration-bulletin-board/2239"}
,{"title": "ROS2 Logging", "thread_contents": ["Hello all,", "My team at Amazon is looking at extending the existing logging functionality in ROS2. I\u2019ve described below what our initial plans are for logging and the justifications. I welcome any feedback on the plan.", "Currently in ROS2 most of the logging implementation exists in the rcutils library. The interface into it is a set of macros that get generated during the build and a few functions in rcutils that back them. A string representing the logger name is used as input to these functions to identify which logger is being used/modified. Each implementation of the RCL then provides its own logging interface. In the RCLCPP library, for example, it is again a set of generated macros that are backed by the rcutils macros. When a logger is created in one of the language RCL libraries, it does not call down into the rcutils library to initialize any state for that logger.", "Loggers can be created in association with a node or on their own. Loggers associated with a node are identical to loggers not associated with the node except that the name of the logger is automatically set based on the associated node\u2019s name/namespace. Logs can be created in a hierarchy based on their name. The hierarchy can be used to adjust the severity level at which a logger operates.", "The logging functionality in the rcutils library currently allows for only a single output function  to be set for all logs. It defines a typedef for the output function header and allows the output function to be changed by calling a setter function with a pointer to a new output function. The rcutils library also includes one output function implementation which sends logs to stdout. The output function cannot be changed for different loggers in the hierarchy. Every logger uses the same output function.", "These are the changes my team is looking to implement.", "Increase the number of output functions that can be set in the rcutils layer", "I would suggest to keep a single output function in the core API (simpler, less memory management required). To enable multiple handlers you could create a new function which is capable to dispatch the call to N other functions.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "Yes, the proposed approach should be able to support fluentd. From what I can see, the easiest way to hook fluentd into this proposal would be to use a ", " input to get the log events sent to to the log files. If more advanced features are needed you could always recompile the nodes and swap out the shared logger library with something that hooks directly into fluentd.", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Yes, that should be possible. Since the shared library is linked at runtime it should be possible to just link in a different implementation. I\u2019ll need to research a little more to see what the most convenient way to do this is. It may still require you to recompile a shared package and source it in your workspace. If other people have thoughts on an easy way to do this I\u2019d be interested in hearing.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "(This is Forrest from the same team in Amazon) Yes our goal is to have an interface so that different types of sinks can be implemented and hooked.", "That said, I would suggest the sink should be fluentd agnostic even if you want to use fluentd to consume the data \u2013 or perhaps I misunderstood you in this case\u2026", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Technically this is doable, but we have to be very careful because IMHO logging mechanism should be straightforward to achieve the best robustness possible. Is there a specific use case in your mind that needs such capability?", "I\u2019m thinking about how nodes are typically installed from binaries. If I install nodes from binaries but want to use a different logging daemon to the original node developer, then I lose the ability to install from binaries.", "My understanding is that even when you install Nodes from binaries that ROS dynamically links them from your sourced workspace when they run. That should allow a developer to use some environment variables or some other mechanism to swap out the logger for any predefined ones at runtime or replace the shared library logger by providing their own with the same interface to be linked in instead. I don\u2019t know off hand how difficult or easy that is to do, but it is all possible.", "As noted in a ", ", ros1 logging doesn\u2019t support unicode strings, so it would be nice to support that in ros2.", "We\u2019ve created the first set of pull requests for the new logging features (links below). Our next steps are to work on adding rosout topic capability. As part of that we are planning on porting the Log message from ROS 1.", "For the new ROS 2 message I would like to remove the list of topics the node is publishing on that is in each log message as I don\u2019t think it makes sense to include that with every logged line. I\u2019ll also be adjusting the verbosity level constants to match the values defined in the rcutils package. The final change I was looking at for this message was to move it into the rcl_interfaces package instead of keeping it in a rosgraph_msgs package.", "Links to pull requests below.", "rclcpp: ", "\nrcl: ", "\nrcutils: ", "\n", "A logging implementation for ros2 that using log4cxx.  - ros2/rcl_logging", "\n", "Edit (Nov 14th, 2018 9:45am PST) - Changed the package we\u2019re planning on moving Log message into from common_interfaces/diagnostic_msgs to rcl_interfaces to avoid adding another dependency in rcl.", "Another pull request for the Log message definition.", "The initial implementation of the rosout features is code complete as well and available in the branches listed below.  I\u2019m going to wait to start the pull request for the rosout changes until the existing pull request is complete.", "Common C functions and data structures used in ROS 2 - nburek/rcutils", "\n", "\n", "Library to support implementation of language specific ROS Client Libraries. - nburek/rcl", "\n", "Please go ahead any missing PRs asap (and add a note in it what it is blocked on). We are very close to the API freeze and can\u2019t consider what is not visible anywhere.", "I\u2019ve gone ahead and created pull requests for those features, though I could not find a way to decouple them from the earlier commits being reviewed in the existing pull requests. Once those PRs are done I can rebase these so only the new commits show in the rosout PRs.", "\n", "\n", "\n", "There was some interesting development in rosconsole from ", " to use pluginlib to support different logging sinks, particuilarly to support journald (", "). I wonder if you could consider a similar approach for logging in ROS2, which would provide a familiar mechanism for swapping logging sinks without recompilation.", " is your desire to be able to actually develop custom logging sinks (a reason for using pluginlib), or are you simply suggesting that ROS2 should be able to log to a number of backends which should be configurable at runtime (not necessarily requiring pluginlib)?", "I have a pretty limited number of sinks in mind (stdout, /rosout, journald), but given the fluentd suggestion above (and that ROS2 is cross platform), being able to develop a sink plugin outside of the rcl source tree via pluginlib seems valuable.", "Of course, you can always just write /rosout to journald via an intermediate node, but this can be expensive when intra-process transport is not available.", "it would be very nice if we can choose the backend logging system, cz some vendors have their own specific logging system once it comes to the embedded system.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Increase the number of output functions that can be set in the rcutils layer. We will add new functions to add and remove functions from a list of output functions in the rcutils library, but will not add this capability to the RCL interface. For the initial implementation, we will not add the ability to change the output functions at different logger hierarchies.", "Write a new log output function in the rcutils library that will forward all logs from a logger associated with a Node to a ", " topic on that node.", "Define an interface for a shared library that wraps more power open source logging libraries such as log4cxx. The interface will include initialize/teardown functions, call throughs for when logging metadata changes (such as log levels and formats), and functions to output logs from the rcutils library. We will also define a way of passing through a configuration file location so that the shared library implementation can use the standard config format defined by the library that backs.", "Write a new log output function in rcutils that will call out to a shared library that implements the above interface.", "Write an implementation of that shared library that wraps the log4cxx library. This implementation will include a default configuration that will send all logs to a file.", "Modify the rcutils library so that when it initializes it will hook up the three output functions that now exist in the rcutils (stdout, rosout, and shared library). These will each be able to be individually enabled/disabled via an environment variable.", "Why not add the new interfaces for adding log output functions to the RCL layer so that people implementing their own nodes can hook up additional loggers?\n", "If someone really wants to do this it will be possible by the RCL handle for their node and directly using the rcutils library. The reason for not adding to the RCL interface to make it easy to do in every language is that we do not want to encourage node developers to add their own custom logging in this way. It is better to conform to relying on only the outputs in the rcl/rcutils libraries so that your node can be more easily integrated into other applications. If an application developer wishes to use your node in their application, but wants total control over how logging is handled, it is a better experience for them to only need to adjust the standard output mechanisms natively provided by ROS 2.", "\n", "Why not also add the ability to set different output functions for different logger hierarchies?\n", "We are not providing this as part of the native rcutils library as a simplifier for our initial work. This functionality would be available as part of the shared library output that we are proposing. So anyone who really wanted that level of control could get it for both stdout and file logging by adjusting the configuration file for the shared library logger.", "\n", "Why not send all logs to the rosout topic instead of only those sent to loggers associated with nodes?\n", "Because of the way ROS 2 associates DDS concepts with ROS Nodes, there didn\u2019t seem to be a clean way to setup a general topic on a process that wasn\u2019t associated with an existing Node. Since a process can also contain multiple ROS Nodes, we didn\u2019t think it would be good design to just pick one to have everything published to.", "\n", "Why the shared library?\n", "We went with the shared library approach in order to provide a way to hook into an existing logging library without tying ROS 2 to only using that library. Wrapping an existing logging library will give us the ability to have features such as file logging, log file rotation, and a hierarchy of output sinks without the need to implement all of that in the ROS codebase.", "\n", "Why keep the existing stdout logger implementation in rcutils instead of relying on the shared library to provide the stdout implementation, since most major logging libraries already have those created.\n", "We decided to keep the existing stdout output function because it provides a standard in the cases where someone does not want to rely on the shared library logger. Since you can enable/disable any of the three natively provided log output functions it is easy for a user who wants more control to disable the ROS provided stdout and rely on the", "\n", "Since the shared library will be used in the rcutils layer will it integrate with the custom allocators that the rcl/rcutiles use?\n", "No, we are not planning to provide a hook into the custom allocator for the initial implementation. This could be added later, however, most of the open source logging implementations that are well supported and provide rich features do not provide interfaces for providing your own allocator. We did not want to pass the allocator into the shared library when the shared library wouldn\u2019t actually be able to use it. In the case where someone needs the control over allocation they would need to recompile anyways and at that point could swap out the shared library implementation with a logger of their choosing that provides more control over memory allocation.", "\n"], "url": "https://discourse.ros.org/t/ros2-logging/6469"}
,{"title": "ROS2 Real-time Working Group Online Meeting - May 20th, 2019 between 7AM and 8AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the first ROS 2 Real-time Working group meeting on May 20th, 2019 between 7AM and 8AM PDT (UTC-7).", "See pre-discussion: ", ".", "\nApex.AI is inviting you to a scheduled Zoom meeting.", "Topic: ROS 2 Real-time", "\nTime: May 20, 2019 7:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "One tap mobile", "\n+16699006833,771375394# US (San Jose)", "\n+19294362866,771375394# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 771 375 394", "\nFind your local number: ", " ", " ", " ", " ", " ", " ", " ", " ", "  ", " see the invite above.", "Could I please join in as a listener?", "Thanks", "Vilas", "(", ")", " yes, very welcome to join.", "I would like to join the meeting.", "\nMaybe I can comment to some RTOS-related topics.", "i did not make it, could you share the recording or minutes, if available?", "Dear all,", "\nhere are the minutes of our meeting on May 20.", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "On the use case:", "Create an rmw implementation which works for a single process. Once that works connect with the underlying data link (e.g. TSN). Measure: malloc/realloc calls, page faults, context switches, memory consumption, blocking calls, \u2026 RTI Connext Micro is a suitable DDS implementation for something like this due it its staticity and intra-process communication feature.", ": find someone that will lead this work.", "Document current state of the art. Someone remarks that there were still mutexes used in e.g. rmw or rclcpp. Memory is still being allocated/de-allocated when receiving messages.", "\n", ": find examples in code.", "\n", ": get William to present his findings", "Invest in the tools for static and dynamic code analysis and tracing and carefully scan the code.", "\n", " create a shared repository for above tools. Bosch will e.g. open-source their integration of LTTng in ROS2. Silexica is integrating they dynamic analyser.", "We briefly scanned through the paper by Bosch: ", ".", "\n", ": present the paper in detail in one of the next meetings.", "Victor, Ingo, Dave and Geoff will co-submit.", "Create an rmw implementation which works for a single process. Once that works connect with the underlying data link (e.g. TSN). Measure: malloc/realloc calls, page faults, context switches, memory consumption, blocking calls, \u2026 RTI Connext Micro is a suitable DDS implementation for something like this due it its staticity and intra-process communication feature.", " : find someone that will lead this work.", " I can take this up. However, I would need some help to understand the problem statement and to find the solution. Let me know your thoughts.", "How about having another meeting in July?", "Bosch will e.g. open-source their integration of LTTng in ROS2.", "Follow-up: I made a ", " about the alpha release!", " I will send an invite for next week.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["select one of the use cases below and probably loose interest from people having other uses case.", "focus on somewhat generic parts of ROS 2 that will help independently of the selected use case.", "\nMy reading of your comments is that these parts are the following ones:", "create rmw layers for static and real-time middleware (RTI Connext Micro, Micro-XRCE-DDS)", "perform memory audit in rmw, rcl and rclcpp (remove unneeded memory allocations)", "split memory allocation in init and runtime phases, avoid memory fragmentation", "remove all blocking calls (or replace with timed calls, e.g.  ", "  vs  ", "  )", "implement real-time pub/sub (either using Waitset or modified Callback/Executor)", "integrate tools for static and dynamic code analysis (PCLint, LDRA, Silexica, LTT-ng)", "Create node architecture for deterministic execution (policy for message aggregation, nodes cohesion, parallelization, local error handling \u2026)", "Create a design for global error handling (history of failures, core dumps, fail-safe mechanism, \u2026)", "Create CI for RT testing (e.g. ", ")", "consumer robots (Ingo)", "warehouse logistics robots (David)", "mobile manipulation robots (Victor)", "autonomous driving robots/cars (Dejan, Geoff)", "Victor, Acutronic Robotics", "Tobias, Bosch Research", "Ingo, Bosch Research", "David, Ubiquity Robotics", "Akihiko, eSol", "Geoff, TierIV", "Lalit, Intel", "Lander and Carlos, Acutronic Robotics", "Matt, Intel", "Nick, AWS", "Sriram", "Dejan and Anup, Apex.AI", "Dejan to provide a use case description and requirements", "find someone that will lead the work on real-time able rmw", "get William to present his findings on real-timeness in rcutils, rcl and rclcpp", "create a shared repository for tools for static and dynamic code analysis and tracing", "present Bosch paper and real-time with Callback+Executor in detail in one of the next meetings", "Propose a use case => requirements => architecture => hardware", "Proposed canonical use case: a vehicle/robot  encounters an obstacle and has to stop safely for it", "Proposed other use case: autonomous driving\n", "\n", ": Dejan to provide a use case description and requirements", "\n", "create rmw layers for static and real-time middleware (RTI Connext Micro, Micro-XRCE-DDS)", "perform memory audit in rmw, rcl and rclcpp (remove unneeded memory allocations)", "split memory allocation in init and runtime phases, avoid memory fragmentation", "remove all blocking calls (or replace with timed calls, e.g. ", " vs ", " )", "integrate tools for static and dynamic code analysis (PCLint, LDRA, Silexica, LTT-ng)", "implement real-time pub/sub (either using Waitset or modified Callback/Executor)", "create rmw layers for static and real-time middleware (RTI Connext Micro, Micro-XRCE-DDS)"], "url": "https://discourse.ros.org/t/ros2-real-time-working-group-online-meeting-may-20th-2019-between-7am-and-8am-pdt-utc-7/9196"}
,{"title": "TF2 web republisher in ROS2", "thread_contents": ["Hello community,", "\nI hope everyone is good. In the last couple of years we were working on making a platform for teaching robotics remotely using ROS. We were using the RWT tools but the main problem was the internet speed of our country. Living in a country where Internet and network speed are still being issues can not allow us to use high bandwidth consuming tools for robot teleoperation. We worked on reducing tf2 web publisher, the initial version are shared in ", "  and ", " those tools reduce over 80% of bandwidth consumption than the tf2_web_republisher package, which is suitable for our condition.", "\nI was thinking if the idea of making an optimal tf web publisher for ROS2 can be considered useful, especially for countries where Internet speed is problematic.", "\nThank you very much!", "\nCordially,", "\nMeriem L. AARIZOU", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tf2-web-republisher-in-ros2/10255"}
,{"title": "Reconsidering 1-to-1 mapping of ROS nodes to DDS participants", "thread_contents": ["Hello,", "We are working on running ROS 2 in an Embedded board and we find out that ROS 2 consumes high CPU because every ROS node is mapped to a DDS participant. We have performed some tests to investigate the issue and the tests and the results can be found at this link: ", ".", "The roadmap of ROS 2 development mentions \u201cReconsider 1-to-1 mapping of ROS nodes to DDS participants\u201d ", ". We would like to see this happen rather sooner than later. We already observe that this leads to problems in CPU usage and can constrain people in their freedom to design an architecture for a robotic system. The ROS2 middleware should allow for a setting where everything can be grouped into a single DDS participant for the people that want to use nodes for modularity at the top level, but don\u2019t want the code fragmented at the bottom level. Many use cases exist where one would like to create multiple nodes that all run on the same hardware. This is especially important since intra-process communication does not work effectively at the time of writing this post.", "Does anyone face this same kind of problem?", "\nWe would like to discuss the idea of reconsidering the 1-to-1 mapping of ROS nodes to DDS participants here and would like the current 1-to-1 mapping implementation to change and would be willing to contribute to changing this if possible.", "Thank you,", "\nIshu Goel", "Instead of introducing an option the current idea is to associate the DDS participant with the context created during ", ". That would imply that common applications using a single init / context will only use a single DDS participant - even if they are composed of multiple ROS nodes.", "Thanks for your reply ", ". When can we expect this functionality to become available? Is there anything we can do to help?", "When can we expect this functionality to become available?", "If it gets implemented in time it will be available in the next ROS 2 r release which is Eloquent in Nov 2019.", "Is there anything we can do to help?", "Any help is appreciated. It will likely start with a design article to discuss the side affects of the intended change. E.g. the ROS node name is currently being used for the DDS participant name. When that mapping goes away there needs to be a replacement mechanism to communicate the node name.", "we find out that ROS 2 consumes high CPU because every ROS node is mapped to a DDS participant.", "After the ", ", we\u2019ve also looked into this. In your answer, you mentioned that part of the CPU usage is caused by the executor itself. However, we found that this is really the ", " cause, rather than the DDS participant mapping. Therefore, if we want to lower the overhead (with relation to DDS), we should be looking at the executor as a whole.", "Also, as a caution, we should not overlook the overhead that profiling adds, and how much it can really skew the results!", "I\u2019m working on a more in-depth analysis of the CPU usage of different parts of the executor. I\u2019ll provide some results sometime next week.", "Hi ", ",", "As mentioned in our research, both the SingleThreadedExecutor and the 1-to-1 mapping of nodes to DDS participants appear to contribute to the large CPU overhead. We were planning to open a separate discourse discussion for the SingleThreadedExecutor optimization. This way the discussions don\u2019t mix and both \u201cproblems\u201d can be addressed. The link to the SingleThreadedExecutor discussion will appear on our github page soon.", "We look forward to reading your findings.", "Yeah I wanted to make sure we didn\u2019t forget about the executor! Glad to hear that you\u2019re planning on opening a separate discussion for it ", "It depends a bit on the usecase. Perf shows about a 50/50 cause with this test (10 nodes, 20 topics/publishers/timers and 200 subscribers). Changing these numbers will also change the usage numbers.", "\nAlso have a look at ", " which skips the entire dds for intraprocess communication ", "Hi, I\u2019m the author of the intra-process communication PR mentioned by ", ".", "First of all, thank you for the showing your results.", "Using 1 participant per process (or context) is definitely an interesting idea.", "\nEspecially with Fast-RTPS since it would also reduce the memory usage a lot.", "Fast-RTPS does not implement shared memory transportation yet, but it recognizes \u201clocal publications\u201d i.e. messages where the publisher and the subscription are in the same participant. In this case the message is not sent over the network, but directly passed to the subscription.", "I run your performance tests together with the new intra-process communication.", "This still does not reach the same results of Fast-RTPS alone for this particular example.", "\nNote that the new Intra-process implementation adds an additional entity to the waitset of the nodes for each subscription (possibly slowing down the SingleThreadedExecutor).", "Note that other RMW implementation allows to reduce CPU usage. For example CycloneDDS ", " where a sort of intra-process communication is already implemented.", "\nThe performance of the CycloneDDS are slightly worst than the ones of the rclcpp PR since in this case it is possible to easily skip serialization and to save some copies by knowing in advance all the subscriptions.", "Not trying to debate the performance gains of this approach, but I think it\u2019s worth pointing out that SROS 2 (and indeed DDS-Security in general) currently only supports security at the domain participant level. Using the same participant for multiple nodes will make security very difficult, as all nodes will have effectively the same identity and thus the same access control.", "Using the same participant for multiple nodes will make security very difficult", "good point, thanks for this comment.", "applications using a single init / context will only use a single DDS participant - even if they are composed of multiple ROS nodes.", "is that supposed to mean 1 process space : 1 participant always?", "thanks for bring up this issue, could you let us know once issue is registered?", "Hi ", ",", "\nThank you for your comment. Should I raise an issue for this on rwm github page: ", "? Or what is the common way of registering an issue? And in the issue can I link to our github or Discourse page?", "that works for me at least,", "and we are going to prepare Pi3 Model B+ with Ubuntu18.04/Dasing to make sure this problem on our side.", "thanks,", "\nTomoya", " I opened the issue here ", "thanks, we will share update via ", " .", "tomoya", "Just FYI,", "[Environment]", "\nPi3 Model B+ with Ubuntu18.04/Dasing", "Hi ", ",", "Thanks a lot for sharing your results.", "There\u2019s a design document with a proposal about this change: ", ".", "Hi ", ",", "Thanks a lot for your efforts. It is a nice document.", "Regards,", "\nIshu", " what is your expected timeline for these changes?", "\nFor someone who is new to this process (getting changes implemented into the core of ROS2) it is hard to judge how long implementing something like this could/would take.", "I\u2019m  also very interested in getting a high level description of the entire process if this is possible.", "\nThe developer guide  ", " is written more with \u201cnormal\u201d packages in mind. I don\u2019t think a major overhaul like this (or for instance other changes to rclcpp, rcl and rmw that can have far reaching impact), are simply handled by creating multiple disjointed pull requests.", "\nFrom watching from the sidelines what I\u2019ve seen so far is that:", "This leads me to the following questions that maybe you or another member of the community could answer:", "\nAfter writing the design document.", "\nHow are the next steps decided?", "\nWho does the actual implementation, how is this decided?", "\nHow does one keep track of all the activities in multiple layers between multiple people?", "Do multiple interested parties just respond to the issue/design document and figure something out from there? Is there a certain structure to this? Who is \u201cresponsible\u201d for the final outcome?", "Thanks in advance to anyone that can give me some clarity on the process.", "\nAlso thanks for all the hard work everyone has been putting in so far!", "Greetings,", "Martin", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A discussion is started on discourse / an issue is raised on the github of the package", "A design document is written together with community members (and members of the TSC)", "The document is reviewed by (community members and) members of the TSC", "?"], "url": "https://discourse.ros.org/t/reconsidering-1-to-1-mapping-of-ros-nodes-to-dds-participants/10062"}
,{"title": "ROS2 Real-time Working Group Online Meeting 2 - Aug 21st, 2019 between 7AM and 8AM PDT (UTC-7)", "thread_contents": ["Hi all,", "\nthis is a call for our 2nd ROS2 Real-time Working Group meeting:  Aug 21st, 2019 between 7AM and 8AM PDT (UTC-7).", "I would like to propose the following agenda constructed from the action items of the previous meeting: ", "[10min] Create an rmw implementation which works for a single process. Once that works connect with the underlying data link (e.g. TSN). Measure: malloc/realloc calls, page faults, context switches, memory consumption, blocking calls, \u2026 RTI Connext Micro is a suitable DDS implementation for something like this due it its staticity and intra-process communication feature.", "[10min] Performance testing on target hardware", "\n", "[10min] Invest in the tools for static and dynamic code analysis and tracing and carefully scan the code.", "[20min]", "[10min]", "Document current state of the art. Someone remarks that there were still mutexes used in e.g. rmw or rclcpp. Memory is still being allocated/de-allocated when receiving messages.", "LMK if you agree with the agenda and if you can join.", "D.", " ", " ", " ", " ", " ", " ", " ", " ", " See above.", " ", " ", "  ", " ", "   see above.", "One remark - regarding the following action item:", "Looks good. Can you also please share the meeting link.", " ", " will join, possibly ", " too", " and I are also interested in joining this meeting. We have been looking at the current implementation of the SingleThreadedExecutor to investigate the high CPU overhead. We have an ongoing discussion here: ", " . As a first step we are starting work on creating a static version of the executor to investigate how much performance can be gained (POC). We think this is a separate issue from the scheduling \u201cproblem\u201d discussed in Tobias\u2019 work. Both issues are important to us and probably to others who want to run a \u201cnormal\u201d ROS2 stack on an embedded board.", " I\u2019d like to join the WG as a listener for the time being", "[20min]", "Yes, that should be enough.", "As a first step we are starting work on creating a static version of the executor to investigate how much performance can be gained (POC). We think this is a separate issue from the scheduling \u201cproblem\u201d discussed in Tobias\u2019 work.", "Very nice work on the CPU overheads!. I agree that the two problems are not related.", "I\u2019d like to join, thanks for taking care of this thread.", "tomoya", "Hi ", ",", "Is it the same Zoom meeting like last time?", "\nI\u2019d like to join it.", "Kind Regards,", "\nAndrei", "We might want to spend some more time thinking about this, before someone goes off and starts on it. I\u2019m not sure 10 minutes is enough here \u2013 maybe have it as the main topic of an upcoming meeting?", "I think there might be a connection to tracing here\u2026 Just saying ", "[10min] Invest in the tools for static and dynamic code analysis and tracing and carefully scan the code.", "\n[\u2026]", "\n4. ", "In principle yes, but I\u2019m not available on the 21st of August, as I\u2019m on vacation. Let\u2019s connect afterwards.", "Hi all,", "\nthe meeting details are here:", "Topic: My Meeting", "\nTime: Aug 21, 2019 07:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "One tap mobile", "\n+16699006833,264081213# US (San Jose)", "\n+19294362866,264081213# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 264 081 213", "\nFind your local number: ", "We might want to spend some more time thinking about this, before someone goes off and starts on it. I\u2019m not sure 10 minutes is enough here \u2013 maybe have it as the main topic of an upcoming meeting?", " Dejan_Pangercic:", "Yes - we absolutely should not do this without the proper design document.", "I think there might be a connection to tracing here\u2026 Just saying ", " Dejan_Pangercic:", "Correct. Added.", "In principle yes, but I\u2019m not available on the 21st of August, as I\u2019m on vacation. Let\u2019s connect afterwards.", "OK.", "This is the same time as an AWF board meeting so I may or may not be able to join. Don\u2019t wait for me if I\u2019m not there.", "I would like to join.", "Sincerely,", "\nKosuke", "Sorry guys I was stuck on a plane while this meeting was happening and I only just got clear. Are there any meeting minutes for this?", "David", " Can you please share MoM or meeting recording", " and all:", "\nhere the recordings of the 2nd meeting (I will follow-up with the short transcription tomorrow):", "Topic: 2nd ROS Real-time working group meeting", "\nStart Time : Aug 21, 2019 06:51 AM", "Meeting Recording:", "\n", "Hello,", "\n", "\nI would like to join this group and attend the next meeting please.", "Thank you,", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", " : find someone that will lead this work.", "\n", "\n", "\n", " (Jaime, Miguel) or", "\n", " (Eric, Joe)", "\n", " also volanteered", "\n", "\n", "\n", "\nThis is the topic that I am adding in addition and it was originally proposed by ", ". The proposal is to create a buildfarm with couple of select computers (ECUs) on which to run the ", " and measure performance regressions. ", " realized that several parties are trying to work on this (Amazon, iRobot, Apex, eProsima, Open Robotics, \u2026)", "\nWe would need to figure out the following:\n", "target HW => could Amazon host?", "target (RT)OS => Linux with RT-PREEMPT, QNX, VxWorks, \u2026", "how to automate flashing of (RT)OS and provisioning of DDS, ROS 2 and ", " => Apex works on this", "how to configure and automate running of ", " in a clean environment => Apex works on this\n", "how to run ", "\n", "\n", "how to store the results => Apex works on this", "how to visualize the results (e.g. ", ") such that we can compare historical runs, view results for different configurations of HW, RTOS, DDS, ROS 2 => ???", "which additional metrics do we want => ???", "\n", "\n", "\n", "\n", "  create a shared repository for above tools. Bosch will e.g. open-source their integration of LTTng in ROS2. Silexica is integrating they dynamic analyser.", "\n", " ", "\n", "\n", ": ", "\n", "\n", "\n", "\n", "implement real-time pub/sub (either using Waitset or modified Callback/Executor)", "\nWe briefly scanned through the paper by Bosch: ", ".", "\n", "\n", " : present the paper in detail in one of the next meetings.", "\n", "\n", "\n", "perform memory audit in rmw, rcl and rclcpp (remove unneeded memory allocations)", "split memory allocation in init and runtime phases, avoid memory fragmentation", "remove all blocking calls (or replace with timed calls, e.g.  ", "  vs  ", "  )", "\n", "\n", "\n", " : get William to present his findings", "\n", "Propose a use case => requirements => architecture => hardware", "Proposed canonical use case: a vehicle/robot encounters an obstacle and has to stop safely for it", "Proposed other use case: autonomous driving", "\n", " : Dejan to provide a use case description and requirements", "I did not have time to provide this. ", " and one of our engineers are working on this however.", "implement real-time pub/sub (either using Waitset or modified Callback/Executor)", "\nWe briefly scanned through the paper by Bosch: ", ".", "\n", " : present the paper in detail in one of the next meetings.", "[10min] Create an rmw implementation which works for a single process.", "[10min] Performance testing on target hardware"], "url": "https://discourse.ros.org/t/ros2-real-time-working-group-online-meeting-2-aug-21st-2019-between-7am-and-8am-pdt-utc-7/10279"}
,{"title": "All of parameter activity via \"parameter_events\" topic", "thread_contents": ["Hi All,", "we would like to hear some comments on this. any idea will do good, so let us hear what you guys are thinking about this. also any milestone for extension are planned or not.", "currently any parameter events such as changed or deleted parameter using ", " will be notified via \u201cparameter_events\u201d. this single topic will be used for all of parameter activities.", "it is really simple enough to use only single topic to notify parameter events, but from user perspective we like to receive the event that we are interested in only. we can filter the event from parameter_events topic but we must receive all of the events anyway. we do not actually want to receive the event if not necessary.", "there would be some options i guess,", "thanks,", "\ntomoya", "This would seem to be a similar situation as with TF: clients receive all broadcasts, but may only be interested in a subset. Clients are shielded from this by the listeners and buffers in TF, to which the filtering you mention is delegated (sort-of: the Buffer does store everything that comes in, but clients only query the buffer for information they are interested in when they have a need).", "there would be some options i guess,", "This reminds me of the bus vs channel communication patterns. Either you receive everything and filter yourself (bus), or you make connections with just those entities producing messages you are (certain you are) interested in. Both are limiting you, as you cannot easily switch.", "To avoid implementing something like the TF client library for parameter events, and looking just at DDS, perhaps content-based subscriptions could be used. They would allow you to express interest in only subsets of messages published on certain topics. Filtering would be delegated to DDS, and callbacks would only be called whenever the filter predicate matches.", "I think content-based filtering at the DDS level would be the most efficient way to handle this, but a nice API at the parameter/node level would make it more user-friendly and future-proof.", "\n", "thanks for the thoughts, i think that DDS contents filtered topic would work too.", "the question is any DDS implementation supports \u201ccontents filtered topic\u201d?", "\nas far as i know, contents filtered topic is one of the extension, so not sure what vendor supports that feature right now.", "and more information is welcome!", "thanks,", "\ntomoya", "the question is any DDS implementation supports \u201ccontents filtered topic\u201d?", "\nas far as i know, contents filtered topic is one of the extension, so not sure what vendor supports that feature right now.", "A quick check would seem to indicate the following vendors have at least some support for content based subscriptions (or filtering):", "a nice API at the parameter/node level would make it more user-friendly and future-proof.", "Certainly. This needs to be exposed at the ROS level in some way or other.", "Not just for parameter events, but subscriptions in general I would say.", "To keep things connected: here is an old ROS Answers Q&A about this: ", ".", "From user perspective I believe the best would be to have a node which I can roslaunch in only in case I need parameter updates (events). So this node can be configurable on start and send events only for those parameters I\u2019m interested in", "\ne.g. roslaunch prameter_events default.launch update_list:=param1,param2", "I had submitted a PR to the rclcpp repository to add a ParameterEventsSubscriber class which I\u2019d hoped would address some of the issues mentioned here. Namely, the class subscribes to one or more parameter events topics (on either your current namespace or remote namespace), can filter events by parameter name and node, and can set a custom callback per parameter.", "Here is the current PR: ", "This week I am addressing the current feedback, so feel free to offer any additional comments.", "thanks i will look into the PR and leave the feedback.", "the class subscribes to one or more parameter events topics (on either your current namespace or remote namespace), can filter events by parameter name and node, and can set a custom callback per parameter.", "That sounds like an analogue to the TF Listener/Buffer approach.", "i think that we need to consider DDS contents filter topic to see if we can actually do what we want to do. and then bring up those interfaces up to ROS generic common API, so that parameter_events can be modified with new nice API. that is just idea and keep posted if we got anything.", "We will do feasibility for DDS Contents Filtered Topic 1st and then come up with high level design.", "seems like that ", " supports ContentFilteredTopics, we are gonna use this for feasibility check.", "I\u2019m somewhat hesitant about content filtered topics, as they introduce quite a lot of complexity in the rmw layer. The mechanism for expressing filters is complex and would be a huge burden for non-DDS rmw implementations.", "I\u2019m not 100% convinced about the motivation, as I do not think the bus approach (receiving all and filtering for what you want) is going to be problematic in practice. It\u2019s actually worse for tf as data is usually streaming continuously (especially before tf_static and with things like moving robot arms). Though it is not disallowed nor technically wrong, I don\u2019t foresee parameters change so much that you cannot subscribe to all events and filter down. It will be most intense during startup of a large system occurs. And I don\u2019t think many things will need to subscribe to this topic (mostly logging tools and introspection tools).", "Additionally, there are other ways to address this problem which may not be as elegant, but are still sufficient in my opinion. For instance, we could have a composable node that you could attach to any process which would subscribe intra-process to ", " and then filter as you configured it to do, and republish it on a new topic for external consumption. This solution keeps the middleware simple (and therefore more portable), and it should achieve similar performance to the content filtered topics.", "Also, though many of the implementations support content filtered topics, I believe many will not do publish side filtering (this is not a requirement of the feature in DDS, AFAIK). And therefore it will simply be doing what you the user could do (or a library on your behalf) and receiving all data, but filtering it before delivering it to you.", "That all being said, some significant value could be gained by using content filtered topics with keys and instances. We do technically allow key\u2019ed types via IDL now, but we lack the API support to make use of this effectively right now.", "So it\u2019s worth investigating still, but I just wanted to temper the interest in content filtered topics slightly, and/or prepare you guys for the fact that adding them to the middleware API is a significant undertaking.", "thanks for sharing practical thoughts!", "And I don\u2019t think many things will need to subscribe to this topic", "but as long as parameters are there, user wants to do something via parameter events. i believe that this is reasonable. and mostly user is interested in specific parameter for each node, it will receive all of the parameter events anyway. of course that is always depending on use cases.", "would be a huge burden for non-DDS rmw implementations.", "possibly, yes", "I\u2019m not 100% convinced about the motivation", "me neither, we are just trying to figure out what is suitable for.", "I believe many will not do publish side filtering", "So do I, but this is the key for improvement we want.", "\nincluding this, i believe that it is worth to investigate the feasibility.", "thanks,", "\ntomoya", "ParameterEventsSubscriber is introduced in ", "\ni think that we could improve that using Contents Filtered Topics base on ParameterEventsSubscriber.", " ", " ", " , and all", "We tried to use ContentFilteredTopic with opensplice, i see this could be really useful to optimize /parameter_events to be filtered with user interests. opensplice supports ContentFilteredTopic but only on  DataReader side filtering which is not efficient enough for network transport.", "Does anyone have plan to support ContentFilterTopic on DataWriter side with multiple expressions and array types? i just trying to get more information around here.", "thanks in advance.", "Hi ", " ,", "Does anyone have plan to support ContentFilterTopic on DataWriter side with multiple expressions and array types? i just trying to get more information around here.", "Yes \u2014 but, there\u2019s no ETA yet.", "but partition is produced", "All DDS implementations support partitions, and there are quite a few tricks one can play with them (you can publish in multiple partitions, subscribe in multiple partitions and use wildcards, too). Currently ROS2 doesn\u2019t rely on partitions so one would think there is a way to use them for this. It\u2019d be worth a try at least.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["all parameter events via single topic \u201cparameter_events\u201d (current)", "parameter events via topics for each node (user can specify whose parameter is interesting)", "each parameter via each topic (user can specify which parameter is interesting)", "all parameter events via single topic \u201cparameter_events\u201d (current)", "parameter events via topics for each node (user can specify whose parameter is interesting)", "each parameter via each topic (user can specify which parameter is interesting)", "Twinoaks", "RTI", "Prismtech", "OpenDDS", "eProsima"], "url": "https://discourse.ros.org/t/all-of-parameter-activity-via-parameter-events-topic/10767"}
,{"title": "Generating 'dev' and runtime artefacts from ROS packages", "thread_contents": ["First: I\u2019m not entirely sure this belongs here, but I couldn\u2019t think of a better category. It\u2019s probably something that will touch the buildfarm if we can come up with something, hence why I posted it here.", "I\u2019ve been interested in reducing the size of deployments of ROS applications, both native and in Docker (and Singularity) images for some time now, and one thing that I have been curious about is whether it would be possible to generate separate runtime and ", " archives from ROS packages (to use the Debian/Ubuntu terminology).", "Some Googling led me to ", " on ROS Answers, which has an answer by ", " mentioning the support for automatic generation of ", " symbol packages (which has been active for Melodic and newer releases), but couldn\u2019t find additional discussions about the topic.", "Using multi-stage ", "s tremendous savings can be accomplished (going from 4GB+ to 200MB images), but this is non-trivial with ROS packages in the mix.", "Thinking about it, I can see two aspects which may make this either difficult or reduce the gains significantly:", "Pt 1 may not be too problematic. It\u2019s an assumption and would be something to figure out.", "Pt 2 could perhaps be approached very naively: assume everything exported by ", " in a ", " call (in ROS 1) ", " is local to the exporting package is what should go in a ", " package. Assume everything else would go into the runtime package. That information would somehow be transferred to Bloom when it populates the package structure and ", " templates.", "Another option could be to use CMake\u2019s support for ", " in ", " rules (as that could make it work for ROS 2 as well), but I have no idea whether that would help when generating Debian/RPM/something-else packages.", "I also realise that to do this right is probably going to be complex and will require quite some effort.", "This post here on Discourse is to find out whether others have thought about this as well or perhaps even have implemented something in this direction.", "This is definitely something that has been considered at various times. And it\u2019s been thought about recently by ", " who has been looking into generating rpms. As rpms have stricter linters.", "The automation of separating dbg symbols was eventually resolved with it becoming part of the standard debian toolchain:", "From that issue you can find links to various other discussions of adding ", " packages as far back as 2011. But on reflecting on it I think that the reason that we haven\u2019t found a solution for created them is that it doesn\u2019t make sense to \u201cautomatically\u201d split packages. There are many heuristics to say when to put something into the dev version or the runtime component. But in reality there\u2019s many corner cases that will never be met so the maintainer will basically need to list out resources to land in either regular or dev variants of the package. In the Debian world this is the role of the debian package maintainer. Each resource is placed into one or the other bucket. But more importantly the downstream package maintainer then has to add the right dependency on either the runtime or devel package. That downstream maintainer also splits their package into runtime and devel packages. But more importantly they also split the dependencies conditionally on the devel and runtime dependencies appropriately.", "Now you can see that we actually have 4 different cases for one dependency. There\u2019s both devel and runtime versions of both packages. And the dependencies declared on the downstream package have to be conditionally evaluated for the subset of the package that is being used to build or made available.", "We have a rich language of dependencies with at least 5 different versions of dependencies for a package including ", ", ", ", ", ", ", ", ", ". And that\u2019s excluding the extra test and doc dependencies as well as groups etc. We would then have to include a way to declare associations of each of those dependencies to the runtime or devel versions of a package.", "Looking at this from a little bit further away, the main reason for having ", " and ", " packages is because historically ", " was released as a tarball and then the debian maintainers would pick up the tarball, and select what went to each subpackage, and add the appropriate dependency declarations. Since then the process has not evolved much, though now often it\u2019s pulled directly from the source repository. But in general the upstream maintainers are not thinking about the impacts of releasing their software and packaging.", "By comparison in the ROS ecosystem core developers actively use packages as atomic units and leverage those packages even when developing, and they use and pay attention to the package dependencies. The efficiency gains from modular packaging that are usually gained by the debian maintainers work can be leveraged in the development environment too.", "As such I think that it might behoove us not to think of this as a problem to be fixed at the release stage, but as an area of efficiency that we can gain by structuring our upstream packages similiarly. The benefits of separating the build and runtime dependencies helps overall with compile times. Dependency trees do not grow as quickly if they are not the union of build and runtime dependencies. Parallel build tools can build faster when these potentials are paid attention to in the development environment.", "And I think that many of our common practices that the ROS community has developed such as recommending making message only packages gets a lot to this end. They are the higher level equivalent of headers and break apart the dependency trees by providing standard interfaces. And through encouraging small modular packages we get the ability to leverage the find grain dependency control.", "Specifically separating the build and runtime dependencies is something that has been shown to be quite effective that we\u2019re not actively doing. However, instead of trying to do that at release time. I\u2019d suggest that we consider making that a best practice for the ROS maintainers to create full packages that separate concerns into runtime and build time dependencies. This will give the developer the ability to fully specify the dependencies for each and where content goes is also clearly obvious. We could even pick up the naming convention of debian to name the content designed for build time to end with ", " however that would then conflict with the Fedora/RPM convention to name things with ", ".", "In ROS 2 we\u2019ve leveraged this separation of dependency types to enable swapping out the runtime libraries and hide the underlying implementations yet building them all against the same headers by defining a clear interface specification. There\u2019s always a matter of tradeoffs between making things more modular and keeping things as simple as possible.", "So in conclusion I think what I\u2019d like to suggest is that we already have support for all the bells and whistles needed to very clearly define the necessary dependencies. And that if we can identify places where separating the build and runtime dependencies would be valuable that we consider simply making separate packages and not try to find ways to automatically split a package and then find ways to refer to those partial packages in downstream packages. We\u2019re generating packages, we have packages. Keeping it one to one is basically the simplest solution.", "At work I am in a similar situation. We extensively work with docker images and found that ROS certainly doesn\u2019t make it easy to have small docker images. On the larger end they could easily reach 12GiB (although a lot of that was our own negligence). Even a base install of ROS resulted in close to a 800MiB compressed image.", "You\u2019re absolutely correct that there is an inherent limitation at the moment with ROS packages ", " including the dev artifacts and that has given us real trouble in a few areas. One of the things we did actually find however is that nine times out of ten it isn\u2019t actually the ROS packages themselves that are the problem - virtually all of the built debians contain just shared libraries and headers which is only going to be barely larger than a non-dev package. It\u2019s the upstream dependencies that they hold which are consistently problematic.", "As an example the ", " deb package, which is in any core installation of ROS, has a dependency on libboost-dev. This boost metapackage is ", " and has massive dependencies of its own, installing it on a fresh Ubuntu docker image installs 586MB worth of stuff even with ", ". Some of the dependencies that get pulled in are just plain useless for actually ", " ROS applications:", "To workaround this problem we came up with a novel solution. Before installing our ROS dependencies into our docker images we install custom, fake, metapackages. These metapackages are special in that they don\u2019t install anything themselves, they simply \u201cprovide\u201d (in the debian packaging sense) all of the upstream dev dependencies of which ever ROS packages we\u2019re intending to install (plus a subset of ROS packages that we definately don\u2019t need) without actually adding anything to the host system. That way when we later install the ROS dependencies it doesn\u2019t actually install the dev dependencies. It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "The positive news is that solving this specific problem in ROS packaging more generally is probably much easier than implementing ", " packages in their entirety. In theory all that is needed is to produce ", " packages that just have extra ", " dependencies of their own, no extra files.", "To a degree this should be straight forward as package.xml already has a mechanism for declaring these dev only transitive dependencies: ", ". The downside of this approach is that it involves fixing dependencies on basically every package in the ROS ecosystem since that\u2019s mainly used to export headers at the moment.", "A hackier approach would be on the rosdep side. At the moment the ", " rosdep key is declared as being the dev packages for every given OS: ", ". It should be possible to create a \u201cparallel\u201d rosdep file that declares the non-dev versions of packages which can then be automatically used by bloom to generate runtime packages with the non-dev dependencies.", "The final approach is as ", "  has suggested, explicitly create \u2018-dev\u2019 ROS packages. The downside of this approach is legacy, there is a substantial amount of legwork to implement this approach and tons of breaking changes to go with it. It also seems overkill since ROS already has most of the mechanisms in place to resolve this on the release side.", "Before installing our ROS dependencies into our docker images we install custom, fake, metapackages. These metapackages are special in that they don\u2019t install anything themselves, they simply \u201cprovide\u201d (in the debian packaging sense) all of the upstream dev dependencies of which ever ROS packages we\u2019re intending to install (plus a subset of ROS packages that we definately don\u2019t need) without actually adding anything to the host system", "Are these REP-127/140/149 metapackages, or the Debian variant? If the latter, are you using ", " for this?", "As an example the ", " deb package, which is in any core installation of ROS, has a dependency on libboost-dev. This boost metapackage is ", " and has massive dependencies of its own, installing it on a fresh Ubuntu docker image installs 586MB worth of stuff even with ", " .", "Yes, that was something I quickly ran into as well.", "That way when we later install the ROS dependencies it doesn\u2019t actually install the dev dependencies. It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "The positive news is that solving this specific problem in ROS packaging more generally is probably much easier than implementing ", " packages in their entirety. In theory all that is needed is to produce ", " packages that just have extra ", " dependencies of their own, no extra files.", "Could you go into a bit more detail about your approach (haven\u2019t had my coffee yet)? Are you referring to extra ROS packages here which package the dev dependencies?", "To a degree this should be straight forward as package.xml already has a mechanism for declaring these dev only transitive dependencies: ", " . The downside of this approach is that it involves fixing dependencies on basically every package in the ROS ecosystem since that\u2019s mainly used to export headers at the moment.", "So if I understand you correctly you\u2019d like to also use it to export \u201cother\u201d build dependencies?", "Thanks for the insight, this is quite a creative way to deal with this (although technically still a work-around ", " ).", "Are these REP-127/140/149 metapackages, or the Debian variant? If the latter, are you using ", " for this?", "Debian variant. When I prototyped it originally I did use ", " although we create them with custom tooling nowadays.", "Could you go into a bit more detail about your approach (haven\u2019t had my coffee yet)? Are you referring to extra ROS packages here which package the dev dependencies?", "No. Ultimately we only care about the debian packages. Certainly one approach to achieve that ", " to have separate dev ros packages but that involves overhauling basically every C++ package in the ROS ecosystem since they all invariably depend on boost in some way.", "So if I understand you correctly you\u2019d like to also use it to export \u201cother\u201d build dependencies?", "Yes, so for example I could say my ", " package has ", " on boost-dev, but ", " on ", ". This would then allow bloom to automatically generate two debian packages:", ": depends on boost-non-dev, contains all of the foo package itself", "\n", " depends on boost-dev and ", "This would only require a fairly procedural update to fix up everyone\u2019s package.xml", "It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "Nice! Do you have any Dockerfiles you could share publicly that demonstrate this approach?", "At the moment the ", " rosdep key is declared as being the dev packages for every given OS: ", ". It should be possible to create a \u201cparallel\u201d rosdep file that declares the non-dev versions of packages which can then be automatically used by bloom to generate runtime packages with the non-dev dependencies.", "This is something that came to mind multiple times when trying to reduce the amount of packages being pulled.", "\nWhile very convenient to have these \u201cblanket\u201d rosdep keys, it is very uncommon for any package to need \u201call of boost dev packages\u201d or \u201call opencv dev packages\u201d. A great deal of space would be saved if packages were to use more specific and targeted rosdep keys. While some stacks like OpenCV and PCL used to have a single/couple deb with everything, they now provide multiple debs and most likely we should leverage them. For some stacks there is already already a narrower set of keys available, the ", " and the ", " one: ", "In an ideal world, all packages would define separately their ", " with the ", " version of the libraries they use and the ", " with the runtime libs they really need to run the package.", "Given the number of packages using (or implicitly relying) on these blanket rules, it would be very challenging to actually apply this to an entire distro.", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Nice! Do you have any Dockerfiles you could share publicly that demonstrate this approach?", " The would be great and could be inspiration to reduce the size of the official ROS images that are ginormous.", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Hear! Hear! A fresh a proper slate, no time like the present to set the president. ", "So in conclusion I think what I\u2019d like to suggest is that we already have support for all the bells and whistles needed to very clearly define the necessary dependencies. And that if we can identify places where separating the build and runtime dependencies would be valuable that we consider simply making separate packages and not try to find ways to automatically split a package and then find ways to refer to those partial packages in downstream packages. We\u2019re generating packages, we have packages. Keeping it one to one is basically the simplest solution.", "This would be a nice way to go about it, but as ", " also wrote, that\u2019s going to be a lot of work (and will result in many more packages as well, which has an adverse affect of build times). For ROS 2 (as ", " suggested) this might be possible still.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "What ", " describes seems like a low-hanging fruit that could allow us to significantly reduce the footprint of ROS 1 (and potentially ROS 2) deployments without requiring too much work.", ": could you provide a little more detail on what you are doing with those metapackages such that we could replicate it? I\u2019m sure ", " would be able to figure out whether we could apply a similar approach to the official ", " and ", " Docker images.", "In an ideal world, all packages would define separately their ", " with the ", " version of the libraries they use and the ", " with the runtime libs they really need to run the package.", "Given the number of packages using (or implicitly relying) on these blanket rules, it would be very challenging to actually apply this to an entire distro.", "Maybe I or someone else can come up with a graphviz/dot script to identify the worst offenders in individual subsets of the packages? I\u2019ll try my luck with ogre/rviz", "There are quite a few tools available for visualising dependency trees (", " and ", " can do it fi).", " indeed brings in approx 580 MB of dependencies (when installed on a bare ", "). ", " is essentially almost singularly responsible for this (207 pkgs for ", " vs 210 pkgs for ", ").", " already lists very specific parts of Boost as dependencies, but as it depends on ", " (as ", " wrote), installing just ", " on a bare ", " image wants to install 655 MB of packages.", "Of this only 8.1 MB is actually placed inside ", ".", "Edit: ", ": I imagine you have some way of figuring out the runtime dependencies of particular ROS nodes / packages and then generate metapackages based on that information. Have you automated this, is this a manual process, or are you relying on package manifests?", " already lists very specific parts of Boost as dependencies", "I guess that\u2019s what I meant by \u201crelying on blanket rules\u201d. While roscpp ", "'s and uses specific parts of boost, it doesn\u2019t declare dependencies on any part of it in it\u2019s ", ", relying on lower level packages to provide all the parts of boost it needs.", "Not sure how the proper subset of boost packages appeared in the control file though\u2026", "Edit: ", ": I imagine you have some way of figuring out the runtime dependencies of particular ROS nodes / packages and then generate metapackages based on that information. Have you automated this, is this a manual process, or are you relying on package manifests?", "Definitely tools helping to extract this information will be very useful", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Hear! Hear! A fresh a proper slate, no time like the present to set the president.", "Yeah getting these split for ROS 2 and allow to install only the nondev version of ROS packages would be great.", "\nLooks like ROS 2 is facing other challenges as the core is completely boost-free and the ", " image ", ", my guess is from the way message packages are currently packaged.", "While very convenient to have these \u201cblanket\u201d rosdep keys, it is very uncommon for any package to need \u201call of boost dev packages\u201d or \u201call opencv dev packages\u201d. A great deal of space would be saved if packages were to use more specific and targeted rosdep keys.", "There is nothing holding us back to do exactly that already. All it takes is a minute to make a PR to update any packages which do use these \u201cblanket\u201d rosdep keys (see below for an example).", "Maybe it\u2019s something that could be encouraged for ROS 2 ?", "I don\u2019t see why ROS 1 packages couldn\u2019t be updated too. I doubt the number of packages (which use these \u201cblanket\u201d rosdep keys) and enough people care about is actually that high.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "ROS Noetic is not even released yet and will be supported for 5 years. I would think that is motivation enough to still update package you care about. Also it doesn\u2019t mean that the maintainer of a package has to do it. Anyone can create a pull request with this kind of change - the maintainer only has to accept it and roll a new release.", "And as mentioned above an example how easy it is to improve the dependency size: ", " uses specific rosdep keys (", ", ", ", ", ") instead of ", " (which maps to ", "). The effect on installing ", " (assuming you have ", " installed anyway):", "While dev/non-dev ROS packages would certainly get us further we could already do something today. So please consider to create PRs to replace the usage of \u201cblanket\u201d rosdep keys with something more specific.", "Our Dockerfiles by themselves aren\u2019t all that useful since most of what they do is install fake packages.", "The generation of the fake packages is the interesting part, I\u2019m not sure how much I can share so I\u2019ll describe it in high level terms. The entire process operates entirely within the realms of Debian packaging, nothing ROS specific.", "When we want to build an image we declare its high level debian dependencies, typically this will be a set of ROS packages (e.g. ros-kinetic-ros-core) and some other libraries that we want to use with it. We then have a Python script which takes this list and traverses the dependency graph. As it traverses the graph it will start to prune off dependencies based off of certain criteria, some of this will be hard coded rules based on package names but mostly it\u2019s automated heuristics. The main one is the one which eliminates packages based on their ", ". While all the ROS packages sit in the ", " section (since bloom doesn\u2019t know any better) the packages in the upstream Ubuntu repos are nicely split up which makes it easy to eliminate all the packages in the ", " section or the ", " section. As we prune off packages we add them to a list of packages to \u201cprovide\u201d in the metapackage we\u2019re going to generate.", "Along with this we also explicitly add certain non-dev dependencies to the images. For example we explicitly add ", " of the non-dev boost packages (which turn out to be not that big) to our images to make up for not having the boost-dev meta-package.", "Here\u2019s an example of the pruned graph for ros-kinetic-ros-core (nodes are only shown the first time they\u2019re found in the graph):", "Maybe it\u2019s something that could be encouraged for ROS 2 ?", "I don\u2019t see why ROS 1 packages couldn\u2019t be updated too. I doubt the number of packages (which use these \u201cblanket\u201d rosdep keys) and enough people care about is actually that high.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "ROS Noetic is not even released yet and will be supported for 5 years. I would think that is motivation enough to still update package you care about. Also it doesn\u2019t mean that the maintainer of a package has to do it. Anyone can create a pull request with this kind of change - the maintainer only has to accept it and roll a new release.", "And as mentioned above an example how easy it is to improve the dependency size: ", " uses specific rosdep keys (", ", ", ", ", ") instead of ", " (which maps to ", "). The effect on installing ", " (assuming you have ", " installed anyway):", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", ".", "This seems to have happened before (from: ", "):", "Before the ROS package ", " was bringing in the boost dependency transitively. Since ", " now uses the system package of ", " boost is not exposed as a transitive dependency anymore.", "Anyway the packages in ", " should state their direct dependencies explicitly anyway and not rely on transitive dependencies for this. Please see ", " for the proposed fix.", "Technically those downstream packages would need to be fixed of course. I just wasn\u2019t sure that was a feasible thing to do in ROS 1 still.", "That was all really. I\u2019m more than happy to start submitting PRs. But I don\u2019t like breaking things and potentially problematic changes like this have been met with hesitation from OR maintainers in the past, so it\u2019s not too strange for us to first look at approaches that would result in similar gains without causing too much trouble \u201cupstream\u201d.", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", " .", " Same reason here.", "\nIn the above example of ", " not declaring its\u2019 boost dependency. It\u2019s likely that this ", " PR broke ", " in the process.", "\nIn the case where the changes are targeting noetic only, it wouldn\u2019t be too much of an issue as maintainers would update their packages before releasing. But for people not having a dedicated ", " branch it may propagate build failures every time one layer fixes the dep declaration.", "If that\u2019s a road we\u2019re comfortable going down for Noetic, I\u2019m happy to help submitting PRs.", "It\u2019s likely that this ", " PR broke ", " in the process.", "Seeing as ", " hasn\u2019t been released yet for Noetic that isn\u2019t much of a problem right now.", "Noetic would seem like the time to start pruning these package manifests.", "re: replacing blanket ", "s: moving dependencies on keys like ", " to ", " et al. would keep builds working. The runtime dependencies would be much lighter though.", "Perhaps a two-step process could be used: first tighten up the ", "s, then change build dependencies.", "And a related PR for ", ":", "As it traverses the graph it will start to prune off dependencies based off of certain criteria, some of this will be hard coded rules based on package names but mostly it\u2019s automated heuristics. The main one is the one which eliminates packages based on their ", ". While all the ROS packages sit in the ", " section (since bloom doesn\u2019t know any better) the packages in the upstream Ubuntu repos are nicely split up which makes it easy to eliminate all the packages in the ", " section or the ", " section.", "Bloom\u2019s patch support could perhaps be used to change the default ", " to more appropriate values. That would require some work, but would seem doable for at least stuff in ", " and similar metapackages.", "But should we perhaps consider adding support for categories to ROS package manifests? Packages could be placed in ", " by default, but if the author/maintainer provides the metadata, Bloom could use that to populate the ", " field.", "RPM did have \u201cgroups\u201d, but those seem to have been deprecated some time ago.", "Here\u2019s an example of the pruned graph for ros-kinetic-ros-core (nodes are only shown the first time they\u2019re found in the graph):", "Entries without a ", " are pruned then, I gather? And the second column shows the cumulative sub total of the packages installed by (and for) that branch of the tree (but the trees only include a dependency if it wasn\u2019t added somewhere else earlier (so ", " shows up with 23MB, but that\u2019s probably because it depends on all sorts of Python dependencies which aren\u2019t shown a second time))?", "With some primitive scripting and a very manual process I\u2019d arrived at a similar set of packages, but I hadn\u2019t gotten ", " down to ", " MB yet.", "Perhaps a two-step process could be used: first tighten up the ", " s, then change build dependencies.", "I started part of the process and opened PRs to ", " and its\u2019 dependencies to get rid of the blanket ", " key (in both build and exec). This was pretty straight forward (though not automated) and did not include any audit/tightening of the runtime dependencies. This allowed to propagate the ~300MB space gain to the all of ", ".", "re: \u201cfirst tighten up the ", " s\u201d: Migrating all these packages to format 2 or 3 would also allow us to leverage the ", " versus ", " and tighten them accordingly. Bloom could then use that information to segregate ", " vs actual runtime dependencies.", "And a related PR for ", " :", "Great !", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", " .", "Assuming that almost all packages depend on e.g. ", " the only chance to breaking downstream packages is if all packages including and below ", " are switching from ", " to something more specific for an already released distro. If only one package limits the change to Noetic (since it already needs a separate branch for other reasons) downstream packages will still have ", " available in Kinetic / Melodic. Therefore I think the chance for regressions is fairly low.", "Perhaps a two-step process could be used: first tighten up the ", " s, then change build dependencies.", "What would be the advantage of that? The first step already affects downstream packages. The second step is local to the package and is easy to test and doesn\u2019t affect other packages - so why should it not be done at the same time?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS has (very) few leaf packages: many ROS packages export libraries, headers and other resources that get consumed by others as part of a build. This means that when a package is installed, it\u2019s most likely installed because it\u2019s being used by another. Main contributor to package size are the libraries (this is an assumption), which would always be present and thus having a separate runtime and ", " package doesn\u2019t offer much net gain.", "it\u2019s not necessarily something that can be automated: creating proper Debian packages can be ", ". Part of what makes it complex is knowing what should go where, and this is not always obvious either. A single source package could spawn multiple binary packages, and the rules that govern this are typically largely hand written.", "gcc", "perl", "python3", "Before it pulled in ~500MB", "Afterwards it only pulls in ~200MB", "Before it pulled in ~500MB", "Afterwards it only pulls in ~200MB"], "url": "https://discourse.ros.org/t/generating-dev-and-runtime-artefacts-from-ros-packages/12448"}
,{"title": "Project Azure for Kinect (4th gen Kinect)", "thread_contents": ["I\u2019m stealing ", "\u2019s (and/or ", "\u2019) thunder a bit, but just thought I\u2019d start a topic about the \u2018new Kinect\u2019 that MS has just announced: ", " (random link to VentureBeat article). MS site here: ", ".", "Some impressive specs (copied from the VB article):", "The depth image resolution will probably require GPU accelerated processing, as the Kinect2 was already enough to bring many machines to their knees.", "Edit: some other articles/links:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["1024\u00d71024 depth image resolution", "Highest Figure of Merit (highest modulation frequency and modulation contrast resulting in low power consumption with overall system power of 225-950mw)", "Automatic per pixel gain selection enabling large dynamic range allowing near and far objects to be captured cleanly", "Global shutter allowing for improved performance in sunlight", "Multiphase depth calculation method enables robust accuracy even in the presence of chip, laser and power supply variation", "Low peak current operation even at high frequency lowers the cost of modules"], "url": "https://discourse.ros.org/t/project-azure-for-kinect-4th-gen-kinect/4729"}
,{"title": "Input validation as a metric for quality", "thread_contents": ["Security and robustness is a major concern to a lot of folks in ROS community. Keeping this in mind, I propose that sanitation and validation of inputs (and possibly outputs) be considered as a strong metric for calculating the quality of a package. Here are some of my thoughts:", "In order to increase security and robustness, as a first step, we can start with checking inputs from", "Some kinds of data require sanitation because they are constrained however, those constraints aren\u2019t explicit such as range (input to switch case, pixel values, euler angles), mathematical relationships (quaternions), internal structure (URL, compressed octree maps) among others.", "Ensuring the constraints are met in the input is important because of the ", " nature of topics and parameters. The lack of good command line argument parsing tools for C++ (an important language in the community) and the allure of using command line for passing arguments can result in an unstable configuration. As a result, some packages are only a key-stroke away from either crashing or worse propagating the error onwards to other packages.", "As a community, we should create packages that can handle erroneous inputs gracefully by", "I expect URL parsing to mostly be an issue in the near future with the increase in packages providing nodes to communicate over the internet [2]. However, others are a concern right now, especially during debugging. We can send erroneous input from command line tools which offer no validation (eg: default values for quaternion are [0, 0, 0, 0]).", "The effect on performance, code complexity, development effort is expected to be minimum with an assured increase in overall robustness of the software stack. Moreover, a lot of packages already adhere to the best practices like this.", "I\u2019d like to confess in advance that I don\u2019t know how to enforce or detect violations automatically, or even what all would constitute a violation of ", " since it might be a non-exhaustive list. Having said that, I\u2019m eager to know your views on the topic and to answer, clarify and discuss.", "Thanks", "\nKunal Tyagi", "[2]: By then, hopefully, the issues are resolved in the upstream libraries", "There is some discussion related to input validation here:", "There is some discussion related to input validation here:", "In the meanwhile I summarized considerations about \u201cDesign by Contract\u201d (which relates to input and output validation, could also be used for consistency checking on the node/nodelet level) and how it could be applied to ROS in one of my fun projects ", ". It didn\u2019t reach \u201cimplementation phase\u201d. Don\u2019t assume any progress in implementation\u2026 I decided to retire that project and continue with a project in ROS2 whenever I find some time and motivation ", " However the projects ", " is probably a shorter read than the thread.", "I\u2019d like to confess in advance that I don\u2019t know how to enforce or detect violations automatically, or even what all would constitute a violation of input validation since it might be a non-exhaustive list. Having said that, I\u2019m eager to know your views on the topic and to answer, clarify and discuss.", "The ", " points to a project where Python ", " is used for property based testing in ROS1. That would be a possible way to verify violations.", "It could be even easier to integrate with ROS2 because the ROS2 infrastructure supports ", " out of the box with ", ". Python ", " integrates with ", " out-of-the-box. If you search in the ", " you will find out how it integrates with ", ". (BTW: ", " is legacy\u2026)", "Having an DBC sounds the best way forward, however, for a lot of languages, libraries/standard don\u2019t exist to implement packages in such a fashion. I think with ROS2, we\u2019ll be seeing packages in a number of different languages (at least Go, Rust, and C apart from Python, C++ and Lisp).", "As such, I have a few open ended questions. How can we", "Since DBC implementations are immature, and we should start somewhere so as to enable simpler on-boarding of packages to DBC paradigm. I like the idea of using Python hypothesis to create simple test nodes for other packages to run and test against.", " for all major ROS packages with a message, we should add Python nodes to read and send messages (pre-defined types) to test the assumptions for that message. This helps people test if their system is resilient to wrong inputs by writing output tests themselves (following examples is easier than doing it from scratch).", "Maybe make this a standard practices so people can call relevant functions from the message package to test the assumptions themselves for their messages. (This might result in the same code being written multiple times in multiple language or writing the code in C for inter-operability)", "This also enables a debug launch method where people can launch these nodes to check if their packages are sending the correct data during normal run-time operations or run these nodes against a rosbag.", "As such, I have a few open ended questions. How can we", "I don\u2019t know if scoring is reasonable.", "You want DbC checking enabled in development/integration versions of the software only (not in production versions). E.g. compiled programming languages with built-in support implement this with conditional \u201cinjection\u201d of the DbC logic during compile time. \u201cBy throwing a compiler switch, Contracts code can be enabled or can be withdrawn from the compiled code.\u201d (", "). C++17 supports DbC as well (", "). I don\u2019t know if it is implemented in C++ compilers already and how. (If the language does not have built-in support the \u201cmanual\u201d conditional \u201cinjection\u201d of DbC logic is a potential source for errors. This applies on the ROS level as well.)", "I feel that a package with the correct checks on input and output would increase the robustness of any system it\u2019s used in. I really like knowing that none of my nodes can seg fault or show undefined behavior, no matter what you throw at them. This is hard and time consuming. In order to incentivize such practices in the community, we need to reward the packages which aspire for a higher standard for stability as well as inform the wider community about them.", "The ", " tag in the README of a repository gives me a little more confidence in a foreign codebase. A ", " tag increases my confidence but a ", " doesn\u2019t. A well-documented README (or similar file) ensures me that the developer wants other to use the package and has documented it\u2019s correct usage and failings. These anecdotal feelings without any fact make me think that a score for robustness is reasonable.", "Robustness is an abstract concept, but is influenced by a lot of factors. Resilience to random inputs (fuzzing) is one of them. Lack of seg-faults is another. There are a variety of factors, each with a different weight to a different segment of user. First step in increasing robustness is always ensuring you only accept the correct input and deliver the correct output. As a result, the first step towards a badge of robustness would be to have a badge related to Input and Output validation. It can be a simple ", " sticker or a ", " sticker or something a more complicated than that (if that\u2019s possible).", "I would be using Contracts to mean something to validate input/output against.", "It depends on the overall application context", "I completely agree with this. It\u2019s upto the user to choose the correct contract. Contracts can\u2019t be one-size-fits-all. Moreover, there is little reason not to parametrise the contracts. User might want to choose a lower and upper bound on acceleration along x-axis and y-axis and a total acceleration magnitude constraint. We can\u2019t know it in advance. However, we can know that for sensor_msgs/NavSatFix.position_covariance_type, there are only a fixed range allowed, and this can be an example of standard contract. User might include a run time check (only option if the node is in say, python) or a compile time check. We shouldn\u2019t limit the options either way. Providing contracts only to a certain language would change the capabilities of ROS as well as the results people expect in different languages.", "I think contracts would be like messages. ROS would provide a few standard contracts, but users would be free to combine a standard contract provided by ROS as they wish or write their own to suit their needs. Just like how users might use the Imu message or want to create a new Imu message without orientation or a new message with pressure.", "built-in support implement this with conditional \u201cinjection\u201d of the DbC logic during compile time", "I\u2019ve seen people use modified asserts and logging methods which activate based on the compile type (Debug, Release, ReleaseWithDebugInfo). There\u2019s no reason why it shoudn\u2019t be similar for contracts. This is what makes me skeptical of the contract proposal for C++.", "The user should be able to use the contract how ever required. It could be compile time or run time.", "\nCompile time would require using extra libraries or creating new macros or wait for future standard support. But this approach isn\u2019t language agnostic. For run time support, leveraging DDS might be a good option.", "C++17 supports DbC as well", "On a related note, ", " doesn\u2019t have contracts.", "In many cases the interface data types are too complex to define what \u201cvalid\u201d data is", "This is the issue with not only many sensor_msgs (among other messages from visualization_msgs, etc.), but also configuration parameters and dynparams for nodes. This doesn\u2019t mean we can\u2019t do something for the simpler messages or simpler checks.", "We can add simple contracts, specially the conditions ones written as comments in the messages (number of points is width*height. It\u2019s a straight path to segmentation fault in Python as well as C++ without this simple check). In fact, the initial contracts would be simply whatever checks people already use.", "I personally feel contracts shouldn\u2019t rely solely on compile time DbC because it restricts people from using a language the rest of the logic would be best written in.", "Sidenote: Any form of implemented contracts would need to be integrated with ", " command line interface or with the message type itself. Or else the user might face trouble diagnosing the reason the messages (from command line) aren\u2019t doing what they are supposed to do, and hunting down contracts in every subscribing node might be a headache. If contracts are good, but tooling doesn\u2019t exist, people wouldn\u2019t use them. If they are bad (the interest in DbC would like to say otherwise ", " ), they might as well not exist in the library.", "I really like knowing that none of my nodes can seg fault or show undefined behavior, no matter what you throw at them.", "To detect seg faults one usually runs the production code with a dynamic analysis tool (like e.g. ", ", ", " consider ", " or ", " (", ").", "To check against seg faults on the ROS level you would have to check if there is any response from the node (the test runner should not crash) after throwing some input at it. To generate as much as possible different input (for fuzzy testing) you would need some ROS level property based testing framework.", " uses property based testing ", " and ", " to generate input data for tests. The same approach could be used to generate input data for ROS nodes. However the ROS1 test frameworks would need (significant) modification/extension to get the data into test cases. (In case I am wrong please correct me!)", "Thanks for pyros. It looks like it can be the base of \u201cfuzzy testing\u201d.", "We should decouple pyros-dev capabilities depending on which package the message belongs to (right now the core code is tightly coupled with std_msgs). It\u2019d make it easier for people to write tests (parametrised) for their custom messages", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Subscribers and service servers", "Command line", "Parameter server", "checking range of inputs either against constants (defined in the message)", "using robust libraries like OpenCV which guarantee trimming of input to within acceptable ranges", "using mathematical constraints, such as by normalizing quaternions on input", "range checking while creating data-structures from compressed messages", "using compatible libraries for dealing with URL (", " Each library has a different quirk in URL parsing)", "score a given implementation (say on a scale of 0 being no contract to 5 being a contract with no assumptions left unchecked).", "handle trade-offs: Lots of package owners might face the issue of increase in latency in order to increase their score from say 4 to 5. As a result, we might want to categorize types of assumptions so the highest priority ones affect score more than one with a low priority.", "create guidelines regarding what a contract must have or better (next point)", "create a general guideline for all languages which stays applicable with/without a DBC support. This will enable a C package to adhere to the guidelines as easily as a Python package", "find out the major issues in integration between packages as a starting point for the previous points (some issues faced by me have been mentioned in passing but I\u2019m by no means a good representation of ROS community)", "score a given implementation (say on a scale of 0 being no contract to 5 being a contract with no assumptions left unchecked).", "\n", " If e.g. data input/output to a ROS node/nodelet is considered valid or not depends in many cases on the application the node/nodelet is used in and cannot be defined in the node/nodelet scope in a generic way. Consider a node wrapping an IMU. The data types are simple (", "). If e.g. the accelerations are beyond the hardware specification it is pretty obvious that the output (e.g. published topic) is invalid. An application level node/nodelet which uses the IMU nodes data as input could validate it. For a generic node which uses the IMU nodes data as input, lets say to stabilize the pose of a drone, the \u201cvalidity\u201d of the input would depend on the overall application setup and capabilities (IMU node, \u201cstabilization\u201d node, motor control node, etc.). The acceleration input would be considered invalid above levels which would lead the overall setup to fail in stabilizing the drone pose.", "\n", " E.g. consider a node/nodelet providing or consuming the ", " data type\u2026", "handle trade-offs: Lots of package owners might face the issue of increase in latency in order to increase their score from say 4 to 5. As a result, we might want to categorize types of assumptions so the highest priority ones affect score more than one with a low priority.", "create guidelines regarding what a contract must have or better (next point)", "See comment about \u201cscoring\u201d ", ". I think it is just possible to recommend what a contract should have (if reasonable)."], "url": "https://discourse.ros.org/t/input-validation-as-a-metric-for-quality/3732"}
,{"title": "[URDF-NG] Next-generation robot descriptions", "thread_contents": ["There is some ", " on how to improve the current URDF format and parsers to fill some immediate needs.", "This topic is for questioning everything and discussing what the perfect way to share and reuse information about a robot might be.", "What needs are there? What assumptions can we make?", "What is this robot we are describing? What information do we need to share?", "What counts as a robot really? For the purposes of this discussion a robot is any physical device whose interaction with the world is sensed, processed, and acted upon in an intentional manner.", "For instance, some robots can be described as a set of rigid bodies and constraints on those bodies. Others include soft bodies or flexures. Nonlinear constraints (such as joint limits) and losses (transmission inefficiency) are often known or estimated well enough that we want to include them in the description. Sometimes the constraints are simplified to the joints between bodies and described in the manner of a graph.", "Myself, I use URDF only when forced to by the tools needed for a particular project. I prefer an embeddable scripting language (lua, python) for the description. Cleaner than XML for editing and one has callback functions for non-linear or heuristic information. If you are unfamiliar with this approach,", ".", "Sounds great right? However I don\u2019t get any of the benefits of a well designed and dessiminated format.", "I see enough frustration in the world with the limitations of URDF that it\u2019s clear that there is a need for something more. What does the next generation of robot description look like?", "I have a couple of observations.", "Does the use of a scripting language provide any other benefits, such as programmable features? That would be an interesting concept.", "The lack of tools for creating and editing URDF files in a more suitable interface is the problem, rather than XML, in my opinion.", "I agree that there are nicer, more suitable interfaces for creating and editing the robot information. However, ASCII text is a format that almost everyone on almost every platform can edit. No one has to wait for the editing tool and all it\u2019s dependencies to be brought to their platform.", "A YAML format for URDF would probably be as readableandcwritable as the RBDL format you mentioned.", "I prefer YAML to XML for static data files as well. The RBDL uses the Lua programming language. (The RBDL format is Lua).", "Does the use of a scripting language provide any other benefits, such as programmable features? That would be an interesting concept.", "Yes. If the definition file is used statically and only loaded from disk, then the language can be used to encapsulate concepts, deal with repitition nicely, and increase readability. For instance, if one wanted to specify their link parameters in terms of of Denavit-Hartenberg parameters instead of xyz-rpy, one would just insert a small function at the top of the file to do the conversion.", "If the definition file is used dynamically, then one can write functions in the programming language that are called by the code using the file. For instance, one could have a joint_friction(joint_number, joint_angle, joint_velocity) hook function defined in the config file that would get called during simulation steps.", "It would be nice to have URDF\u2019s in python, like how dynamic_configure files work.", "\nThat way it is not yet another language to learn.", "\nThe robot description would be much cleaner, you could support generating the description from parameters. This is useful for highly configurable robots such as the turtlebot, which currently hack this functionality into the existing xacro/urdf system.", "We should have the gazebo people and the ROS-I people as part of the discussion.", "\n", " and I been discussing this topic for a while.  It would be good to have one format for all of the ROS / Gazebo projects.  SDF was something that we thought could be the format.  There is a long thread on this on the old google groups.", " You can see how creating URDF\u2019s in python can work in practice with ", ". I wrote when I needed to deal with non-trivial URDF\u2019s. I find working with XML directly (editing the files) unpleasant. The idea that one would write a python script to parse and execute a language embedded into xml (ie xacro), rather than just using python directly, confuses me to no end. But I\u2019ve seen colleagues invest a lot of time and frustration using xacro to try to solve the problems they have with the URDF format so I guess it makes sense to some.", "Though perhaps I\u2019m misunderstanding you. Do you mean instead that you\u2019d like to see a dynamic_configure API? A robot_model node that you can dynamically update using dynamic_configure instead of a static xml text robot_model parameter?", "Any chance you can link that thread here? Any key outcomes of the thread?", "EDIT: I think ", ".", "EDIT: It seems the general consensus is to standardize on the SDF format. That seems like a sane iteration of the current situation with ROS and Gazebo.", "That looks awesome, thanks for the link.", "Something like that is what I was looking for. I would like to see something like that become the standard way to do robot description.", "EDIT: This is a response to your first message, I didn\u2019t see your second one.", "I started this thread hoping for some discussion on, well, next generation robot descriptions. Harmonizing current formats (URDF/SDF/etc) is a really important discussion for the community; but that\u2019s really about cleaning up the current generation.", "Anyone interested in the next generation?", "That\u2019s why I brought up the Python description format.", "ROS2 replaces XML launch files with Python ones, I think that it makes sense to do something similar with URDF the other source of XML pain in the ROS ecosystem.", "Looks like your stuff generates XML and sends it out to stdout and eventually into the robot_description parameter to be used by robot_state_publisher.", "I think for the next generation of URDF, I would like to get rid of the XML entirely (if possible) and have the Python code actually act as robot state publisher and send out the description data on a latching ", " topic, as well as tf data.", "As for the actual format, I like odio_urdf but I haven\u2019t looked at it very closely.", " ", "Can we get the Gazebo team on this thread?", "Hopefully the next gen of robot descriptions is the same between ROS2 and Gazebo.", "I believe they\u2019re already listening in.", "This category grew out of the ROS Live event with robot description formats", "\nas the topic.", " has put together a nice recording as well as lots of links can be", "\nfound at: ", "I\u2019ll call out specifically the ", "\nIf you weren\u2019t there or don\u2019t have time to listen at the moment.", "Tully", "ROS industrial needs to be part of it.  I looped Paul in on this thread.", "We started a document that some of the key players commented on.", "\nProbably should push it a bit harder now if there is interest.", "This is a big", "Looks like your stuff generates XML and sends it out to stdout and eventually into the robot_description parameter to be used by robot_state_publisher.", "Yes, that code is specifically for the URDF way of describing robots. The internal hierarchy of objects mirrors the URDF elements, and each object has a string representation that matches the URDF XML.", "I think for the next generation of URDF, I would like to get rid of the XML entirely (if possible) and have the Python code actually act as robot state publisher and send out the description data on a latching /robot_description topic, as well as tf data.", "I\u2019m having a hard time picturing why one would want to do this.", "There are many uses for a robot description that have no need for ROS IPC.", "Also one could argue that the state of the robot should not be part of it\u2019s description. The ", " and ", " documentation has some nice discussion of this, though with a focus on simulation and the simbody implementation.", "I\u2019m not sure python would be an ideal choice for describing a robot. Python was not designed to be a data exchange language. In addition, web applications, which can parse XML, would have a hard time with python.", "Hi all, it seems that there is interest in figuring out what to do ASAP for ROS2, which was not the point of this thread. I\u2019ve ", " and I\u2019ll link in the external discussions mentioned above so there is a good start for that.", "It will be cleaner and clearer to ", ".", "Describing flexible link robots? Reconfigurable robots? Soft robots? Able to wrap your head around configuration languages, or why a shared object file with an API might make a good reuseable, shareable robot description? Continue below.", "I\u2019ve always found a lot of value in a non-interpreted interchange format, like URDF or SDF or even HTML or JSON. So, for me I don\u2019t like the idea of a description format that exists only in an interpreted form (like as Python or Lua).", "It\u2019s convenient to have these tools when writing the descriptions, but very inconvenient when trying to consume the description in many different places. I think a good middle ground is like what we have with xacro + URDF. You might have issues with either xacro or URDF, but I think a good idea that came out of that is to pair a well established interchange format with \u201ctemplating-like\u201d tools to make it easier to create the description by hand or programmatically. This is reflected in things like web development, where the interchange format is HTML, but commonly the server will use a scripting language to generate the otherwise tedious to specify XML content.", "We were leaning towards SDF during our discussions.", "\nand at Modbot we are all about flexible / reconfigurable robots so that", "\npart interests us.", "I\u2019ve always found a lot of value in a non-interpreted interchange format, like URDF or SDF or even HTML or JSON. So, for me I don\u2019t like the idea of a description format that exists only in an interpreted form (like as Python or Lua).", "It\u2019s convenient to have these tools when writing the descriptions, but very inconvenient when trying to consume the description in many different places.", "I think a key part of an interchange format is that it\u2019s content is very clear and easy to grasp and process by all. A simple declarative text file certainly achieves that.", "The strength of that approach is that when in inevitable need for dealing with repetitive and formulaic declarations arises, you can use xacro and I can use assembly and everyone is happy.", "So I agree there is a strong case for a non-interpreted format.", "Can you elaborate on what about an interpreted/script interchange format worries you? The second you touch ROS (or xacro) you are loading the python library, so the memory/processing overhead can\u2019t be the issue. Configuration scripts usually have a minimum of \u2018code\u2019 in them (cmake being an ugly exception) and thus a minimum of patterns or idioms that are tricky for the new user, but perhaps that is the concern?", "It\u2019s convenient to have these tools when writing the descriptions, but very inconvenient when trying to consume the description in many different places.", "I think a good middle ground is like what we have with xacro + URDF. You might have issues with either xacro or URDF, but I think a good idea that came out of that is to pair a well established interchange format with \u201ctemplating-like\u201d tools to make it easier to create the description by hand or programmatically. This is reflected in things like web development, where the interchange format is HTML, but commonly the server will use a scripting language to generate the otherwise tedious to specify XML content.", "Is this what you would choose if you had a blank slate going forward? Is there anything you would want to see change or improve?", "Can you elaborate on what about an interpreted/script interchange format worries you?", "If the data exchange (previously I used interchange, which I think is not the correct terminology now) format is a script or interpreted then everyone who uses it needs to have an appropriate interpreter loaded and any context that might be used in the script. For example, RBDL requires everyone that needs the description to have a Lua interpreter. Lua is by no means a heavy requirement, but there are plenty of platforms where that would be inconvenient at best, for example loading the description in a webpage (couldn\u2019t find a Lua vm in javascript) or iterating through the links in Matlab (couldn\u2019t find one here either), neither of which have a native way to hook into a lua interpreter. You can always parse the description lua file in C++ and then serialize the in-memory \u201cobject\u201d to send it to a browser or to Matlab, but then you just have an unofficial data exchange format.", "Even if you manage to get an interpreter at each place where you want to use the description then you\u2019re likely to need some additional context to be available at each of those places too. Because the description script can have logic in it, it\u2019s a natural extension to want to use command line arguments or environment variables or something else to modify the resulting description. You could limit this functionality, perhaps RBDL does, but this is a core utility that people use in xacro and really need to make it easier to describe robots with lots of optional configurations. If you have no static intermediate format, then this context needs to go along with the description script so it can be given to the interpreter at each end point where the description is consumed.", "The second you touch ROS (or xacro) you are loading the python library, so the memory/processing overhead can\u2019t be the issue.", "Why do you need python to touch ROS? I can write a URDF without using xacro and load it with a pure C++ library. In fact I\u2019d argue that having an intermediate format that is data only would considerably reduce cpu and memory overhead compared to an data exchange format that is interpreted, especially when being consumed in several places.", "Is this what you would choose if you had a blank slate going forward? Is there anything you would want to see change or improve?", "I would prefer an explicitly defined data exchange format for the description that is data-only (probably XML or JSON) and then pair that with a prefered templating interface (xacro, erb, empy). That way consumers of the description can remain as simple as possible and the developers who are creating the descriptions have tools to help them with authoring. It would be great to have a \u201cprefered\u201d or \u201cprescripted\u201d template engine to use, that way tools could be built to understand them, but by leaving that unspecified developers get the freedom to use whatever template engine they prefer.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Save time by not having to think about and create a project specific description. (Accompanied by the risk that the description designers made provisions to handle all aspects of a description that your project needs.)", "Share a description for reuse by others. You and the peers you are sharing with must agree on the meaning of some subset of the information in the description.", "Share a description between different softwares. By using a single description, errors introduced by data entry mistakes are eliminated. ", ".", "what else?", "Easy to create and edit across all platforms.", "Minimize difficulty in integrating the description into new codes/languages/platforms.", "A starting set of description information that is useful and whos meaning is agreed upon.", "The ability to be extended as needed with project specific info.", "The ability to be formally extended by the community as progress maches forward.", "The descriptions are primarily created by people and the one tool in common across all platforms is the text editor.", "The descriptions are primarily consumed by software.", "Any given description is usually project specific (No standard object model)", "Only my internal software reads the description, so not a lot of reuse between software.", "No reuse between peers either. As a consultant an annoying amount of my work is confidential.", "XML was intended as a language primarily for computers to work with, with human readability as a bonus. The lack of tools for creating and editing URDF files in a more suitable interface is the problem, rather than XML, in my opinion.", "A YAML format for URDF would probably be as readableandcwritable as the RBDL format you mentioned."], "url": "https://discourse.ros.org/t/urdf-ng-next-generation-robot-descriptions/222"}
,{"title": "ROSCon 2019: Call for Workshop Proposals", "thread_contents": [" is expanding! In past years, members of the community could propose only talks. This year, we are extending the conference format so that you can submit:", "This year\u2019s ROSCon will also include invited panels and keynote speakers.", "Today we are opening submissions for workshops. Please submit your workshop proposals ", ", no later than May 22nd, 2019.", "We cannot offer content that is not proposed! If there is a topic on which you would like to present, please propose it. If you have an idea for an important topic that you do not want to present yourself, please post it for discussion here on ", ".", "All topics related to ROS are invited. Example topics include:", "To get an idea of the content and tone of ROSCon, check out the slides and videos from ", ".", "All submissions will be reviewed by the program committee to evaluate:", "Additional consideration will be given to balancing the subject matter and duration of presentation.", "We encourage proposals from presenters of all backgrounds and experience levels.", "If you have any questions about whether your subject matter is appropriate, feel free to post on ", " or contact the ROSCon 2019 Organizing Committee at ", ".", "Workshop proposals must include", "Accepted presenters will be required to provide their materials 5 weeks before ROSCon for content review to ensure the quality of the event. Content that does not pass review may be removed from the schedule at the discretion of the program committee.", "Submit your workshop proposal by May 22, 2019 ", ".", "The opening of submissions for talks and for videos will announced separately.", "David Lu!! (Locus Robotics)", "\nMelonee Wise (Fetch Robotics)", "\nProgram Co-Chairs", "I hope that you prioritise workshops that are going to be very interactive, or even tutorials, rather than mini-conferences.", "Things I can\u2019t present myself but want to see:", "Thanks for organizing this ", "! I\u2019ll go ahead and share my wish list below:", " Same for MoveIt 2", "Acutronic Robotics and PickNik are the main forces so far behind the ", ". From Acutronic Robotics\u2019 side, we\u2019d be happy to provide an interactive tutorial here demonstrating the capabilities of MoveIt 2 with ", " during ROSCon 2019. Beyond moveit2, there\u2019re probably others doing interesting work which should be covered as part of this workshop. I\u2019ve just announced the creation of the ", " so I\u2019d go ahead and suggest that the tutorial should be called something like \u201c", "\u201d which should group both the moveit2 effort but also, hopefully, other related projects.", "Let\u2019s see the reaction and we\u2019ll then coordinate to make a proposal ", "We will propose one. ", " we will propose one too.", "Can you shed a bit more light on how workshops will happen? I would surmise that the schedule cannot afford very many 3 hour blocks\u2026", "Some questions:", "(subject to change) We\u2019re planning for a small number of workshops to run in parallel in the same slot in the afternoon of October 30th, the day prior to the start of the traditional presentations.", "It would be great if that could be fixed soon. I need to plan travel and might need an extra night in the hotel to make that.", "Understood. We\u2019ll should be able to get a draft program published early next month (after we\u2019ve received and reviewed the workshop proposals).", "run in parallel in the same slot in the afternoon of October 30th", "I agree with ", " that I have to take this into account when making travel arrangements and so it would be great if the exact details were known soon.", "btw, this may be unusual, but I usually make travel arrangements before getting acceptance notifications. It often saves a lot of money to be early\u2026", "May I suggest having a prominent notice about this on the ROSCon site, once you\u2019ve made a decision? Likely, people will not expect the conference to essentially start a bit early!", "Seconding this ", "Great idea! I just started (a bit late) the discussion for a MoveIt 2 Workshop:", "\n", "\n", "We would like to propose HCP (heterogeneous computing platform) for Autonomous vehicle as a topic.", "We are happy to announce that the workshops for ROSCon 2019 will take place on Wednesday October 30th, the day before the main conference event at the same venue from 2pm to 5pm. We\u2019ve just updated the ", " with this information. Please stay tuned for more information. We will be announcing the selected workshops and how to register for them in an upcoming announcement.", "One quick reminder, if you haven\u2019t submitted your workshop proposal, today\u2019s the last day!", "any registration needed to participate in workshop?", "\nor we could just jump in the room on that day?", "thanks,", "Registration is indeed required for the ROSCon workshops. You can select a specific workshop as an add-on to regular conference registration: ", ".", "thanks for the information!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " - As in previous years, the standard presentation format will be a talk, with the presenter(s) talking live in front of slides/video, with a brief question period at the end, fitting into a 10, 20 or 30 minute time slot.", "\n", " - One new format is the video presentation, with up to 2 minutes of self-contained audio-visual content, featured alongside the talks in the program. Videos will be presented without live narration or Q/A afterward.", "\n", " - Also new this year are workshops, in which the presenter(s) will provide a hand-on in-depth look at a particular topic in a more interactive format, over the course of up to 3 hours.", "New packages / frameworks", "Insights / improvements for existing packages", "Case studies on unique ROS deployments / use cases", "Developments for specific robots, sensors, platforms", "Competitions / collaborations / initiatives", "ROS in commercial / research / teaching environments", "Standards / best practices / development tools", "\n", " - The proposed content should use ROS in a substantial way, but beyond that, the work must also be relevant and compelling to a general ROS audience. Writing a ROS driver for a specific piece of hardware is an excellent contribution to the community, but describing the intricacies of its firmware may not be relevant to this audience. Furthermore, content should be relevant to a global and diverse community.", "\n", " - We encourage proposals to contain big ideas with high impact. It is preferable that proposals have a demonstrable quality as opposed to being purely theoretical.", "\n", " - Articulating your ideas clearly and grammatically is a key prerequisite for giving a compelling live presentation.", "\n", " - Content should be original and not something that has already been heard before. Will this be the 40th talk on a particular topic at ROSCon? Or are you presenting something new?", "\n", " - Because we are an open-source community, proposals for which the underlying code and other content is available under an open source license have a greater chance of being accepted. It is not a hard requirement, but proposals focused on proprietary systems should contribute in some other way to the community. Promises of future release are difficult to evaluate, so having your content released at the time of proposal submission is preferred.", "Title (maximum 70 characters)", "Presenter(s) (name and affiliation)", "Summary - for public consumption, used in the program schedule (maximum 100 words)", "Description - outline and goals, for review by the program committee. Describe the intended audience and what resources (if any) would be required. Please be sure to include enough information in your proposal for the program committee to evaluate the above review criteria.", "\n", " Navigation 2 tutorial", "\n", " Same for MoveIt 2", "Real-time (", ", ", " and others)", "Safety and standardization (", ")", "Security (", ", ", " and others)", "Will the conference become multi-track?", "Will there be a dedicated workshop day, before 31st or after 2nd?", "If not, will all workshops take place in the same time-slot?"], "url": "https://discourse.ros.org/t/roscon-2019-call-for-workshop-proposals/8855"}
,{"title": "ROSCon 2019: Call for Talks/Videos", "thread_contents": ["As ", ", ", " is expanding! While we are ", " until this Wednesday (May 22nd), today we are opening submissions for standard talks and videos.", "You can submit proposals for talks and videos on the ", " until July 15th, 2019.", "We cannot offer content that is not proposed! If there is a topic on which you would like to present, please propose it. If you have an idea for an important topic that you do not want to present yourself, please post it for discussion at ", ".", "Talk proposals must include", "This information must be formatted as a pdf, using the ", ".", "Accepted presenters will be required to provide their materials 5 weeks before ROSCon for content review to ensure the quality of the event. Content that does not pass review may be removed from the schedule at the discretion of the program committee.", "Video submissions must include", "This information must also be formatted as a pdf using the ", ".", "Videos do not need to provide an extended description, but will need to provide a link to the video online. Note that the submitter is responsible for ensuring that the video is viewable by the reviewers. Note that while revisions may be made between the submission deadline and the conference, it will be reviewed based on the submitted content, which should be \u201cfinished quality.\u201d", "The general content guidelines and review criteria were ", " ", ".", "David Lu!! (Locus Robotics)", "\nMelonee Wise (Fetch Robotics)", "\nProgram Co-Chairs", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " - Presenter(s) talking live in front of slides/video, with a brief question period at the end, fitting into a 10, 20 or 30 minute time slot.", "\n", " - Up to 2 minutes of self-contained audio-visual content, featured alongside the talks in the program. Presented without live narration or Q/A afterward.", "\n", " - Presenter(s) providing a hands-on in-depth look at a particular topic in a more interactive format, over the course of up to 3 hours.", "Title (maximum 70 characters)", "Presenter(s) (name and affiliation)", "Summary - for public consumption, used in the program schedule (maximum 100 words)", "Description - outline and goals, for review by the program committee. Describe the intended audience and what they can expect to learn. Please be sure to include enough information in your proposal for the program committee to evaluate the above review criteria.", "Title (maximum 70 characters)", "Presenter(s) (name and affiliation)", "Summary - for public consumption, used in the program schedule (maximum 100 words)", "Link to video online (maximum of two minutes). Do not feel the need to pad your video to two minutes: a concise thirty second video will present better than a slow two minute video."], "url": "https://discourse.ros.org/t/roscon-2019-call-for-talks-videos/9234"}
,{"title": "Announcing ROS snapshot repositories", "thread_contents": ["There is an established workflow for new releases within a ROS distribution. As improvements are made, changes are periodically synced into the main ROS repositories and those changes are announced via Discourse. While we strive to avoid regressions and compatibility breaks in new syncs they can occur, causing disruption in your workflow if it happens in packages you rely on. Even without regressions syncs can affect users who install new packages without upgrading installed packages between syncs. Or your build system may rely on storing and retrieving packages with exact version numbers which become unavailable after being replaced by new versions during a sync.", "With the aim of improving overall reproducibility in the ROS community we\u2019ve set up a new apt repository host that will provide persistent hosting for snapshots of each sync for active ROS distros. The snapshots will be taken just after every sync of the ROS stable repository and will be available for at least six months from the sync date.", "For teams that want to more closely control when they update to the latest sync, these snapshot repositories will make it easier to access the packaged binaries from earlier syncs.", "Snapshot repositories for the following ROS distributions are available now and will remain available for at least 6 months from their sync date.", "ROS Indigo: Ubuntu Trusty ARMv7 (armhf), Ubuntu Trusty i386, Ubuntu Trusty AMD64 (x86_64)", "ROS Kinetic: Ubuntu Xenial ARMv7 (armhf), Ubuntu Xenial ARMv8 (arm64, aarch64), Ubuntu Xenial i386, Ubuntu Xenial AMD64 (x86_64)", "ROS Melodic: Ubuntu Bionic ARMv7 (armhf), Ubuntu Bionic ARMv8 (arm64, aarch64), Ubuntu Bionic AMD64 (x86_64)", "ROS 2 Ardent: Ubuntu Xenial ARMv8 (arm64, aarch64), Ubuntu Xenial AMD64 (x86_64)", "ROS 2 Bouncy: Ubuntu Bionic ARMv8 (arm64, aarch64), Ubuntu Bionic AMD64 (x86_64)", "ROS 2 Crystal: Ubuntu Bionic ARMv8 (arm64, aarch64), Ubuntu Bionic AMD64 (x86_64)", "See the instructions in the ROS wiki ", "Snapshots do not receive bug fixes or security updates so it is still a good idea to follow ROS releases and upgrade periodically.", "ABI compatibility between snapshots is not tested or guaranteed so when switching a system to a new snapshot updating all ROS packages is recommended.", "We think most ROS developers should continue to use ROS from the primary repositories.", "The snapshot repository is still in preview so waiting for us to stabilize it before using it on production robots is prudent.", "But if your build and deploy process involves bundling ROS packages before transferring a comprehensive artifact, like a Docker or OCI container image, then the snapshot repository is worth checking out today. And after you do, reply to this thread with any feedback or to let us know you found it useful.", "Awesome work! I\u2019ve seen few folks who have been ", " repo for similar snapshotting capabilities. Although the DockerHub\u2019s registry allows you to ", ", we\u2019ve still encountered a few cases that would\u2019ve been better served with these snapshot repositories. Perhaps folks could now use docker registries in courses with the snapshot repositories to archive installed binary images beyond the six months from the sync date.", "For those who don\u2019t know, the Offical Library images for ROS are also archived in DockerHub\u2019s registry, the digests of which can be found by navigating the commit history of the Library\u2019s repo-info:", "Extended information (especially license and layer details) about the published Official Images - docker-library/repo-info", "Not sure how far back the archive stretches, but I can still pull ", ". Note: the offical library images themself however are still configured to use the primary repositories.", "I\u2019m glad to learn about the snapshot repos.  A few months ago I was burned by a regression in the main repo and didn\u2019t know where to find the old debs.  This looks like a great resource.", "Is the reproducilbility of ROS being tracked somewhere?  I\u2019ve been working on it some myself and was wondering if there were ways to contribute or learn what other people might be doing.", "Because of the regression I ran into I started caching the ROS packages locally using aptly.  I noticed ", " published the aptly docker container so I wondered if clearpath is doing something similar.", "I also posted this, ", "  a short while ago because I was concerned about reproducibility when using rosdep  Being able to get the old debs if required is a large step, but if you\u2019re using the rosdep db in github, it could shift underneath you even if you have stable deb packages.", "I don\u2019t mean to get too off track with that, but the more reproducible the better.  Thanks for the announcement!", "Is the reproducilbility of ROS being tracked somewhere?", "Not that I\u2019m aware of, but it\u2019s a concern I\u2019ve been pondering and iterating on since I joined Open Robotics. As I\u2019ve had my hands in different elements of the release process it\u2019s given me a pretty deep view (although I\u2019m sure it\u2019s still not complete) of all the ingredients that make up a ROS release and this discussion adds to the motivation to carve out time to document what I think I know.", "Thanks, that would be great.  I\u2019d be interested in helping once that is up.", "Snapshots will be available for at least six months from the sync date.", "Just a clarification; What about after a distro went EOL, e.g. would Indigo snapshot(s) remain available after it goes EOL this year, or would the snapshot(s) of Indigo be removed 6+ months after EOL?", "Either way this is absolutely great. I\u2019ll test this out in a (pre-)production setting at our team (probably combined with container approach ", " ", " for more portability). Thank you!", "Just a clarification; What about after a distro went EOL, e.g. would Indigo snapshot(s) remain available after it goes EOL this year, or would the snapshot(s) of Indigo be removed 6+ months after EOL?", "Thanks for the question. It\u2019s a good one.", "I\u2019m not making any extended commitments for the snapshot repositories yet so for now let\u2019s assume that final snapshots will expire just like any other. Because we don\u2019t remove EOL distros from the main repositories, once a distro goes EOL it\u2019s final snapshot would be nearly equivalent to the state of the main repositories (except releases of the python utilties: rosdep, rosdistro, bloom, etc). There are two overarching reasons we\u2019re not making an extended commitment out the gate.", "Because of the regression I ran into I started caching the ROS packages locally using aptly.  I noticed ", " published the aptly docker container so I wondered if clearpath is doing something similar.", "We (mainly) cache the full source using JFrog and rebuild the binaries using our ", " approach.", "The Snapshots repository is now being used to archive unsupported ROS distributions which are about to be removed from ", " (see ", " for details) using a datestamp of ", ".", "In order to allow for future snapshots to include repositories beyond just Ubuntu or Debian repositories, we\u2019ve also moved the apt repositories into an ", " subdirectory symlinking to ", " when appropriate. The earlier snapshots are available both at the datestamp root and the ubuntu subidrectory but future snapshots will be published only to the ubuntu subdirectory.", "In order to allow for future snapshots to include repositories beyond just Ubuntu or Debian repositories, we\u2019ve also moved the apt repositories into an ", " subdirectory symlinking to ", " when appropriate. The earlier snapshots are available both at the datestamp root and the ubuntu subidrectory but future snapshots will be published only to the ubuntu subdirectory.", "That\u2019s great thanks!", "Looking into using the snapshots repo for EOL distros docker images, I noticed that there is no ", " subdirectory for ", ", but there is one for ", ".", "\nWill the symlink be created in the future? Or is it recommended to use the ", " subdirectory for Debian distros?", "Edit: Is there a recommended repo / place for reporting issues with the snapshots repositories?", "I noticed that there is no ", " subdirectory for ", ", but there is one for ", ".", "\nWill the symlink be created in the future?", "This was an oversight on my part and has been corrected.", "Edit: Is there a recommended repo / place for reporting issues with the snapshots repositories?", "Not at the moment. I think for now questions about the snapshots repository can go on ", " and reporting issues like this is probably best done replying to this thread, which I\u2019m watching.", "Someday I would like to get the Chef recipes and shell scripts used to create the snapshots repositories cleaned up, extracted, and published which would create a natural home for issues with the canonical deployment as well. But I don\u2019t have time to do that work on the visible horizon.", "This was an oversight on my part and has been corrected.", "Thanks! It looks like the stretch one is an empty directory ", "Not at the moment. I think for now questions about the snapshots repository can go on ", " and reporting issues like this is probably best done replying to this thread, which I\u2019m watching.", " I\u2019ll post follow-up questions on ROS Answers", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "I want to leave myself room to iterate, including breaking changes like urls, and the six month lifespan allows me to do that gracefully.", "The cost to maintain these snapshots indefinitely, both in storage and bandwidth is not something we can responsibly commit to without having a better idea of what that cost will be. Before releasing this I did some back-of-the-envelope estimates but none of it replaces real-world data. So as we add more snapshots I\u2019ll be looking at the growth rate of our storage consumption and at the bandwidth consumption and that will inform how this service evolves."], "url": "https://discourse.ros.org/t/announcing-ros-snapshot-repositories/7705"}
,{"title": "Autoware Reference Platform Working Group Meeting #3", "thread_contents": ["Wednesday 30th of October", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Kenji Funaoka (Tier IV)", "Steve Kan (Tier IV)", "Stephane Strahm (Kalray)", "Cheng Chen (Autocore)", "Status on AutoCore board", "Connection with ", "\n", "Discussion on requirements categories", "Status on demo 2020", "Demonstration on-going ROSCon Macau with TierIV\n", "2 PCU boards with PC Simulator\n", "LiDAR stack", "Controlling", "No accelerator", "\n", "Software= Autoware.AI", "\n", "Accessibility of board to be confirmed by Cheng\n", "Requests by Kalray and TierIV", "\n", "Estimated power consumption : 20-30 Watts (without accelerator)", "Accelerator for the demo: Movidius, Google TPU, plus accelerator from Chinese companies", "Goal: ", " activities part of Reference Platform Group", "Yang is leading this ", "\n", "Stephane to contact Yang to make it happen ( + Cheng)", "Reference Platform Group is defining the ", "\n", "Managed by Foundation", "Documentation\n", "Interfaces", "Functionalities", "Integration of software", "Basic guidelines for design", "Performance expectations", "Collection of requirements defining the Reference Design", "\n", "Available through AWF", "\n", "Reference Platform Group will refer to ", "\n", "Actual implementation of Reference Design", "Provided by companies as product Platform", "Autocore PCU is the first Reference Platform according to Autoware Reference Design (upcoming)", "\n", "Suggestion: rename Reference ", " Group as Reference ", " Group\n", "To be submitted to TSC", "\n", "Need to initiate requirements definition process", "Proposal\n", "Collective work\n", "Each member AND each participation shall contribute (Discourse)", "\n", "Organize by categories of requirements for allow better focus\n", "Sensors\n", "TierIV can list sensors currently tested", "\n", "Hardware for application\n", "List of currently used", "\n", "Hardware for acceleration\n", "List of currently used", "\n", "Drivers\n", "\n", "Reference Design Configurations\n", "Demo part of this", "Performance expectations / guidance", "\n", "Software integration\n", "Autoware.AI", "Autoware.Auto", "\n", "Safety\n", "Redundancy considerations", "\n", "Security\n", "to be defined", "\n", "Simulation\n", "Definition of environment for simulation: To be handled by the Simulation team", "\n", "Testing\n", "SIL and HIL approaches", "Define test cases and test scenarios (leveraging system test such as dSpace or others)", "\n", "\n", "Need policy for req management and tool: to be checked with TSC\n", "Steph in touch with Geoff to initiate this", "\n", "\n"], "url": "https://discourse.ros.org/t/autoware-reference-platform-working-group-meeting-3/11451"}
,{"title": "Autoware TSC meeting minutes for April 17th, 2019", "thread_contents": [" Geoffrey Biggs (Tier IV)", "Nothing in particular.", "Minutes approved.", "To reduce noise on the GitHub issue tracker, we have, over the past two months or so, been enacting a policy of directing anything not a confirmed bug or feature request to Discourse for help. This has been effective, with help being provided actively by many Autoware developers at Discourse. However, the general ROS policy is that support should be done through ", " because that site is better suited to providing a searchable archive of problems and solutions.", "It has been the plan from the start to, in the long term, either set up an \u201cAutoware Answers\u201d site or use ROS Answers. Use of Discourse was intended to be a placeholder until traffic grew high enough to justify an answers site. Traffic has been higher than expected right from the start, however.", "Because of the existing ROS policy of not doing support on Discourse, the OSRF is concerned that Autoware using Discourse for this purpose will confuse other ROS users (who are not likely to notice the category of a thread) and cause an increase in general ROS support requests on Discourse. Therefore we have been asked if we can shift support to ROS Answers.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Jan Becker (Apex.AI)", "Geoffrey Biggs (Tier IV)", "Joseph Buckner (AutonomousStuff)", "John Buszek (AutonomousStuff)", "Chen Cheng (Autocore.ai)", "Victor Duan (Linaro)", "Esteve Fernandez (Apex.AI)", "Kenji Funaoka (Tier IV)", "Brian Holt (Parkopedia)", "Shinpei Kato (Tier IV)", "Seonmaan Kim (LG)", "Angelo Mastroberardino (Parkopedia)", "Dejan Pangercic (Apex.AI)", "Paul Sastrasinh (TRI-AD)", "Antonis Skardasis (StreetDrone)", "Matt Spencer (Arm)", "Stephane Strahm (Kalray)", "Daisuke Tanaka (Tier IV)", "Dmitry Zelenkovsky (LG)", "Yang Zhang (Linaro)", "Opening remarks and new member introductions (Board)", "\n", " Confirmation of previous minutes** (All)", "\n", " Removal of project-specific sub-categories on Discourse (Geoff)", "\n", " Moving TSC home from Slack to Discourse (Geoff)", "\n", " Moving Autoware support from Discourse to ROS Answers (Geoff)", "\n", " Action items from previous meeting (All)", "\n", " Autoware.AI/Autoware.Auto (Apex.AI/Tier IV)", "\n", " Map format (Parkopedia/Tier IV/Apex.AI)", "\n", " Use of code from Apollo and revisit policy towards Apollo (All)", "\n", " Hiring a lawyer to look at simulator licenses (Board)", "\n", " Unreal game engine licensing for simulators (Apex.AI)", "\n", " Invite Epic to join the Autoware Foundation (Apex.AI)", "\n", " Joining ", " and ", " (Apex.AI/Tier IV)", "\n", " Parkopedia commitment resources (Board/Yang)", "\n", " StreetDrone commitment resources (Board/Yang)", "\n", " Planned contributions from foundation members. (All)", "\n", " Common hardware platform: Lexus and the parameters for control (Tier IV)", "\n", " Milestones for development of Autoware.Auto (All)", "\n", " Arm and the ELISA foundation (Arm)", "\n", " Perception process unit hardware and software design (Autocore.AI)", "\n", " ", " for ", " (Apex.AI)", "\n", " Automating map and vehicle model imports with ", " (Apex.AI)", "Move TSC discussion place and AWF operations discussion place from Slack to Discourse\n", "Tier IV (Geoff); will do in stages by the next meeting", "\n", "Remove the public sub-categories of the public Autoware category at Discourse\n", "Tier IV (Geoff)", "\n", "Update the support guidelines and bug/issue templates for Autoware.AI to use ROS Answers instead of Discourse for support discussion\n", "Tier IV (Geoff)", "\n", "Arrange review of licenses for Unreal, Unity, CARLA and LGSVL\n", "Apex.AI/Board (Yang)", "\n", "Provide differences between Apex.Autonomy and Autoware.Auto, and between Apex.OS and ROS 2, in next meeting\n", "Apex.AI", "\n", "Review, discuss and improve milestones for Autoware.Auto development\n", "All", "\n", "Coordinate with Tier IV and Kalray to bring their work on algorithm acceleration into the computing platform effort\n", "Linaro (Yang)", "\n", "List planned contributions that go towards Foundation work and can be used to meet the milestone -demos. ", ".\n", "Tier IV", "LG", "Kalray", "Arm", "Linaro", "AutonomousStuff", "TRI-AD", "Autocore.ai", "Huawei", "Velodyne", "\n", "Coordinate on prototyping work for Lanelets2 use in Autoware\n", "Parkopedia, Tier IV", "\n", "Talk to OSRF about using one of their GSoC slots for Autoware work\n", "Apex.AI", "\n", "Set up a teleconference to talk about the vehicle interface work for Autoware (especially Autoware.Auto)\n", "StreetDrone", "\n", "The project-specific sub-categories on Discourse (under the Autoware public category) have virtually no traffic.\n", "\n", " (2 threads)", "\n", " (2 threads)", "\n", " (1 thread)", "\n", " (0 threads)", "\n", "We have been told that if they have almost no traffic, then they are probably not worth having.", "We need to either actively use these sub-categories, or remove them and move what little existing discussion there is to the main Autoware category.\n", "Geoff recommends the latter option, due to easier maintenance and less clutter.", "\n", "\n", ": Remove the sub-categories and move the existing threads to the Autoware category.", "No opposition, ", ".", "Slack is nice for holding a real-time chat, but it has several disadvantages:\n", "We are using the free tier, which means we only have access to the most recent 10,000 posts. Thus we are losing the background for decisions that have been made.", "It is very hard to find where specific conversations occur, because everything is organised by channels rather than ", ".", "It can be distracting and hard to keep up with as notifications can come in at a high rate and if you do not respond rapidly the thing you wish to respond to can be lost in the conversation.", "It requires direct engagement as soon as possible (relates to the above).", "Message formatting is limited - only good for short messages.", "\n", "By contrast, Discourse (provided by the OSRF) has many advantages, including:\n", "All history is available.", "Conversations are archived separately so the background for decisions is clear.", "Discussion does not happen in real time so most conversations are easier to manage across time zones.", "Direct messages are possible, and apart from rapid-fire chats are as useful as Slack direct messages.", "Subscriptions can be done at a fine-grained level so you can get only the categories or threads you are interested in in your inbox, or as desktop notifications, or just via the Discourse site.", "Necessary response time to join a conversation is much longer due to ability to reply to individual messages, easily quote parts of messages, etc.", "Richer message formatting options.", "\n", "Most important point is easier tracking of decisions and their background. This is important for the Autoware Foundation as an organisation.", "Secondary point for some TSC members is that Slack is very disruptive to their work flow and hard to keep up with.", "\n", ": Shift all TSC-related and Foundation operation-related discussion to the ", " private category on Slack.\n", "Proposed extension: Shut down the TSC and operation channels on Slack; when real-time, rapid-fire conversation is necessary, start a direct message channel specifically for it.", "Possible extension: Create a separate private category for Autoware Foundation operation discussion, separate from the TSC (like the Slack channel).", "\n", "Tier IV in favour.", "Apex.AI strongly in favour.", "AS in favour", "Parkopedia no objections", "Kalray in favour", "Slack channel shutdown also agreed", "\n", ": Move from Slack to Discourse private category and shut down the Slack channels. All TSC and Foundation-related discussion should happen on Discourse in the private category or via direct message, ", " Slack, to avoid confusion or loss of material.", "\n", ": Shift support for Autoware to ROS Answers, with support guidelines asking for questions to be tagged with an ", " tag so we can find them.\n", "Possible problem: Most new users are not likely to use the ", " tag, making finding their questions harder for Autoware developers; other ROS users do not have much experience with Autoware (its existence is surprisingly ", ") and so won\u2019t be able to provide support. The result could be an impression of Autoware having poor user support.", "\n", "\n", ": Set up our own \u201cAutoware Answers\u201d site. The downside is maintenance is non-trivial.", "Apex.AI in favour of using ROS Answers.", "Tier IV in favour.", "Parkopedia in favour.", "We will need to put effort into looking for mis-tagged questions and tagging them correctly, but this is the only problem.", "Has the side effect of us being more involved in the ROS community.", "Will investigate if we can provide a link that automatically tags a question with Autoware.", "\n", ": Will use ROS Answers. Geoff to do work to move to ROS Answers.", "Review opinions on the tool chain and information provided by TRI-AD and make a decision on what to use\n", "Geoff, Esteve", "\n", ": Applied for and qualified for GitLab\u2019s open source programme, so will use GitLab as the tool chain. Will need to do some engineering for CI to meet the fork and pull model.", "\n", "Coordinate with the Linux Foundation/AGL on their efforts to improve safety certification practices of open-source software\n", "Geoff to take point", "ARM, Linaro, TRI-AD, Tier IV, Kalray to participate as interested", "\n", ": None yet; AGL stated they won\u2019t have anything to talk about until July.", "\n", "Lead improvement of mapping format used by Autoware\n", "Brian Holt (Parkopedia)", "\n", "Produce some proposals for milestone demos that we could set to drive the development of Autoware.Auto and direct resources (choice and ordering to be decided in TSC meeting ", ")\n", "Geoff", "\n", ": Milestone draft created and circulated for discussion. See dedicated agenda item.", "\n", "Coordinate with Tier IV and Kalray to bring their work on algorithm acceleration into the computing platform effort\n", "Yang", "\n", ": None yet; will try to meet in the next week.", "\n", "List planned contributions that go towards Foundation work and can be used to meet the milestone demos (once decided)\n", "All premium members", "\n", "Follow up with premium members who have never attended a TSC meeting\n", "Yang", "\n", "Hire a lawyer with experience in open source licenses to look at licenses for all potential simulators\n", "AWF board", "\n", ": Decided to use Apex.AI lawyer, paid for from AWF budget; waiting to receive revised license from LG. Also need to include CARLA, Unreal and Unity licenses.", "\n", "Autoware.AI:\n", "Improving development practices through reduced write access to repositories and stronger PR rules.", "Splitting repositories and moving packages out of ", " repository.", "\n", "Autoware.Auto\n", "Ported two more algorithms from Apex.Autonomy", "Was waiting for tool chain decision before setting up infrastructure, CI, etc. This decision has been made now (in favour of GitLab Gold via the OSS programme), so this work can begin.", "Discussion going on at Apex.AI about how to sync Apex.Autonomy and Autoware.Auto. Will be testing a process using a GPS driver.\n", "Apex.Autonomy runs on top of Apex.OS (the real-time, deterministic, certified version of ROS 2).", "\n", "\n", "Focus has been on producing XML representations from the real-time data that they collect.", "Currently have an internal XML representation that can represent all aspects of both open and covered carparks.", "Next piece of work is to determine how to consume that map for the purposes of navigation and localisation.\n", "This is where lanelets comes in.", "\n", "Lanelets2 is an open source, open format map representation. It has a library that implements it available already.", "Lanelets2 doesn\u2019t cover as much as Parkopedia\u2019s internal format, such as multiple levels, ramps, etc. But Lanelets2 is open source, and so modifiable and acceptable and no challenges of balancing releasing some parts and not others.", "Parkopedia has had success converting their XML representation into OpenStreetMap.", "Brian has spoken to Atlatec and they are using lanelets2 as well.", "Parkopedia wants to support the effort required to integrate lanelets2 into autoware for navigation and localisation.\n", "Lanelets2 supports localisation but it may not support everything needed but Autoware for localisation.", "\n", "Parkopedia is working with leading OEMs on the use of artificial landmarks. They want to integrate this information into the lanelets2 format.", "Parkopedia believes that all maps that Autoware can read and write should be in an XML format that ideally uses the lanelets2 library.", "LG is concerned about the streaming of map data.\n", "The wider OEM community is not considering using XML. Most automotive companies are using NDS, which is much heavier than an XML representation.", "\n", "Apex.AI has talked to the authors of lanelets2 and found them very open to adding new features that Autoware requires, although they may want some kind of payment if they have to do any work themselves. It is worth considering trying to get them into the AWF.\n", "Lanelets2 development is very active and it is very well maintained.", "\n", "\n", ": Parkopedia wants to make the contributions to lanelets2 to support 3D.\n", "Apex.AI is concerned if Parkopedia can afford to do this work.", "\n", "\n", ": Apex.AI wants to see a lightweight prototype, e.g. using the NDT localiser node.\n", "Tier IV is working on prototyping to see how lanelets2 can be used internally in Autoware.", "\n", "Some code from Apollo has ended up in Autoware again recently.\n", "\n", "Do we need to revisit our policy towards Apollo?", "If not, how are we going to handle cases of wanting features they have already implemented?", "For copying of code:\n", "Board: Fine with this sort of code copying as long as the copyright is preserved.", "Apex.AI: We should be using source from external sources as a dependency rather than copying it into Autoware.", "Tier IV: The policy of not copying has already been established and clarified but there is a lot work to do for cleaning up existing code that has been forked unnecessarily.", "\n", "For relationship to Apollo:\n", "Board: We should be OK with using code from Apollo because it is open source.", "Apex.AI: Then should we work with them to make their code easier to depend on rather than copy?", "LG: If we understand the algorithms and they are better we should copy them because Apollo breaks their API all the time.", "\n", ": Decide to copy or not on a case-by-case basis.", "\n", "Esteve (Apex.AI) has spoken directly to Epic Games about how their licensing works for using Unreal in simulators for autonomous driving.", "Epic stated that we can do whatever we want with Unreal, and only if we were to ", " the simulator would the royalty policy kick in.", "Still want a second opinion from the lawyer that the AWF will hire to look at the licenses.", "Epic\u2019s revenue model is through support contracts, not through royalties.", "Epic Games has expressed an interest in joining the Autoware Foundation as a paid member.", "Their interest is obviously going to be in the use of their engine to drive self-driving simulators.", "Epic have invited someone from the AWF to attend their event in Detroit next week for people using Unreal in industrial applications (not games).\n", "\n", "\n", ": If they are interested in joining they should go through the usual process.", "These outreach programmes allow organisations to work with students and with people from underrepresented communities (women, people from developing countries, etc.).\n", "Targeted to people who want to be involved in open source projects.", "\n", "They are very successful at giving these people experience they may otherwise not be able to get and at encouraging them to get into open source.", "They are a good way to introduce new people to our projects.", "They provide another way to find new employees (OSRF has had good experience with this)", "For an example of the impact such diversity programmes can have, watch this short talk from ROSCon 2016: ", "\n", "For GSoC:\n", "Organisation makes a proposal to be part of the programme.", "Google selects organisations and allocates a number of slots to each selected organisation.", "Google pays the student a stipend.", "Organisations are responsible for mentoring the student with the goal of their work being integrated into the organisation\u2019s projects.", "\n", "For Outreachy:\n", "Not specifically for students, for underrepresented communities.", "Funded by donations (it\u2019s a foundation) such as from Microsoft. May need to contribute financially to be involved.", "Happens twice a year (summer and winter).", "In general works the same way as GSoC.", "\n", "Apex.AI suggests starting with one to avoid getting overworked while we figure out how to support it.", "Apex.AI suggests that because we are late for GSoC, we ask the OSRF if we can use one of their slots instead.", "\n", ": Try to use an OSRF slot this year for GSoC, and use that to figure out how to work with it so we can participate properly next year.", "See ", "\n", "Listed:\n", "Apex", "Parkopedia", "Streetdrone", "\n", "Not listed\n", "Tier IV", "LG", "Kalray", "Arm", "Linaro", "AutonomousStuff", "TRI-AD", "Autocore.ai", "Huawei", "Velodyne", "\n", "All \u201cnot listed\u201d companies must provide their proposed contributions ASAP.", "We need to settle on a common hardware platform. Momentum and inertia seem to be pushing us towards the Lexus.", "We need to know the detailed parameters of this car for:\n", "Control algorithm tuning (Tier IV needs this)", "Physical modelling for simulation (LG needs this)", "\n", "It has been suggested that AutonomousStuff can provide this sort of detailed information.", "AS is willing to help with this work, but they cannot spend weeks getting the exact parameters. They also need to figure out what they are already doing in relation to LGSVL, and how the control work relates to their commercial product.", "AS to talk to LG offline about the LGSVL work.", "StreetDrone has already done this work for their vehicle to get all the necessary parameters, and they provide a URDF model.", "\n", ": TSC members all agree on the need for a common format for defining the car\u2019s parameters (URDF).\n", "May need an automatic exporter from URDF to the simulator, depending on the simulator being used.", "\n", "\n", ": StreetDrone is driving vehicle interface abstraction. They will collaborate with AS.\n", "StreetDrone to set up a meeting to talk about the vehicle interface.", "\n", "We must choose milestones to direct our development efforts for the next generation Autoware.", "See ", ".\n", "This is the planning topic. Agreed milestones will be made public in the open Autoware category and on the Autoware Foundation website.", "\n", "We must agree on at least the first two or three milestones today.", "Recently the ROS 2 TSC\u2019s security working group has done quite a bit of good work in developing a threat model for two robots:\n", "The Turtlebot 3", "Acutronic Robotics\u2019 ", "\n", "\n", "\n", ": A member company should undertake to develop a thread model for Autoware.Auto and its ", ".", "A company called ", " has capabilities for automating the creation of simulation worlds and vehicle models.", "Should we get in touch with them and see if they want to work with us?"], "url": "https://discourse.ros.org/t/autoware-tsc-meeting-minutes-for-april-17th-2019/8938"}
,{"title": "Generalized Traffic Light Classification Architecture", "thread_contents": ["Currently, Autoware.ai contains 3 types of traffic light classification systems:", "Each of these assumes that it will be handed two pieces of information:", "These traffic light ROIs (regions of interest) are provided by the ", " (feature projection) node which uses information from the vector map and camera intrinsic and extrinsic parameters to project the bulb locations from the map in 3D space into the 2D plane of the camera\u2019s field of view.", "In developing our own traffic light classification node, we have run into the following issues:", "We think it is possible to provide a general-purpose architecture for traffic light recognition (and other constrained-ROI-type image classification problems) but some pros/cons must be considered for each approach. I\u2019m not going to speak too much to the types or capabilities of neural networks which would fit the bill since that isn\u2019t my area of expertise - I\u2019ll invite my colleague Joe Driscoll to speak on this topic - but more on the overall architecture. I think our intended goals for the new architecture look something like this:", "For implementing an architecture which does the above, these possible options come to mind:", "Well, now that I\u2019ve written a novel on the subject, please provide feedback and suggestions on how we can make an awesome traffic light detector that works well enough to be used in real traffic!", "\nThank you for starting the discussion. I believe a lot of people are interested in the topic.", "About approach 1&3, don\u2019t you still need to calculate ROI from vector_map information even with DNN? Determining the presence of relevant traffic light does not ensure that irrelevant traffic lights are not in the image. If DNN detects multiple traffic lights in the image, you need information to choose which one to look at.", "You are correct. That is a fact that I missed. However, we could use the centroid of the initial ROI estimate from the feature projection node to find the closest detected light from those that the DNN node found.", "It\u2019s really a trade-off between attempting to make the feature-projection node more robust to differences between the idealized world (vector map) and the real world and providing an alternative to feature projection that is less prone to the same real-world pitfalls as feature projection but needs feature projection as an anchor to the idealized world.", " thank you for your comprehensive analysis of the current status and for prividing possible solutions related to the classification of traffic lights.", "Here are some of my comments related to your post:", "If you agree with me, (A) even with current ADAS map format,  the definition of which traffic light applies to which lane is not well defined. Current format defines \u201cclosest lane ID\u201d, but not exactly which one it is, it also only allows the definition of a ", " lane. (not sure about other ADAS formats)", "(B) just like you mentioned previously, current definition of a traffic light is ambiguous. It requires the search of individual \u201clamps\u201d to define a traffic light object. This is not as straightforward as it should be. In my opinion a new layer defining the traffic lights objects need to be added to the ADAS format.", "Now, regarding the 3 of the proposed solutions. I still think that approach (2) of your list is the most \u201cdynamic\u201d, since different nodes can be used to classify regardless the DL framework. However, I also agree that it needs to be improved.", "The recognizer must publish three pieces of information:", "In my opinion, the ", " nodes should only publish an array of results, for each image in a similar fashion to ", ", but containing the Traffic Light classification results.", "Visualization (super_impose, and markers) should be handled in an independent ", " node.", "The recognizer must classify a found traffic light from the image as having one of four states:", "I mentioned above an array of classification results, instead of a single result, so the classifier can handle more complex combinations.", "\nCase in point, only in central Japan these three types of traffic can be found:", "\n", "Example:", "would match to:", "\n", "I believe this should be able to handle different combination in different parts of the world.", "What do you think?", "Is it possible to only emit a single ROI from feature-projection node by a simple filtering rule such as FOV cropping or largest ROI.", " I agree with your assessment of the shortcomings of the mapping format with regard to traffic lights. However, we are unable to modify the existing ADASMap format (it is Aisan\u2019s proprietary format) which is why Autoware is moving toward a new \u201cAutoware Map\u201d format. This format supports many-to-one relationships between lights and lanes (see page 17 of ", "). I believe the new format also supports what you are suggesting in regard to multiple lights on a signal. ", " may be able to comment more on this issue.", "I like the idea for ", " but it will run into the following problem:", "In this scenario, because of the latency introduced by the classifier, the ", " cannot correctly identify which raw image is tied to the classification results. If it publishes the most recently-received version of both, it will be superimposing old classifications on new images. The time delay won\u2019t be much but I just wanted to mention this issue.", "Publishing probability with the state would just push the logic of decision to the consumer module. Could cause duplication in the case of multiple consumer modules. IMHO tl modules are the best place to have definitive decision and output a definitive result. Please give a possible scenario where this probabilistic information is useful for a consumer module.", "Approach 1 I really like for the following reasons:", "PS: Crazy idea, draw ROI using perception node, run small NN on cropped ROI. eliminating feat_proj.", " - I agree with your assessment about passing the probability on to another node for the decision. However, these would be very useful for training/tweaking so I don\u2019t think publishing it is a bad thing. We could publish it and just not use it in any downstream nodes.", "Regarding approach 1, I would tend to agree with most of your points. However, as far as using the", "\nperception node for the light ROI, as ", " mentioned, we need to have some measure of certainty about which light applies to the lane we are currently in and we can\u2019t determine this without data from the vector map, thus necessitating ", ".", "which is why Autoware is moving toward a new \u201cAutoware Map\u201d format. This format supports many-to-one relationships between lights and lanes (see page 17 of ", ").", "Actually, we have decided to use Lanelet2 format as IO format and also as internal format in TSC meeting. (AutowareMapFormat will disappear.)", "\nHowever, we would need to add some extension to Lanelet2 format so that it has enough information to minimize the degradation of current Autoware functionalities. I will post the idea about extended format on discourse soon.", "I believe the new format also supports what you are suggesting in regard to multiple lights on a signal.", "Lanelet2 supports a single lane linked to multiple traffic lights and also multiple lanes linked to a single traffic light.", " I also agree that it would be simpler to have tl modules to have definitive decision. However, one of the concerns that ", " is mentioning is that having only four states (Stop, Yield, Go,  Unknown) is not enough in some situations (at least in Japan).", "Even the lighting pattern doesn\u2019t change, whether you are allowed to \u201cGo\u201d or not depends on which way the vehicle is going. In the following example from ", "\u2019s post, the vehicle is allowed to \u201cGo\u201d only straight or to the left, but not to right.", "\n                    ", "\n\n", "Therefore, you would need more than the four states.", "Yes more states should be helpful. Attaching probability to each state, not. I wonder if the following in enough:", "So, I think there are multiple ways to handle the light layout that ", " describes above. Here are a couple:", "Thoughts?", "I am having a hard time imagining how would approach 2 work. What is the image that the classifier will classify? It will have to be an image of a whole light cluster ie: ", "  I don\u2019t think there is a way to subdivide this image any further.", "Also what do you mean by \u201cdirection-based\u201d signals? Who create this signals? Do you mean 3 separate ros topics (left, right, ahead) to publish separately a [r, g, y] state for each?", "Please clarify.", "Maybe I misunderstood the meaning of the lights in your example cluster. I\u2019m not very familiar with traffic lights in Japan. Are each of the arrow lights on the bottom row associated with a single light on the top row or does the entire top row indicate something totally separate from the bottom row?", " will have to correct me. But If I was driving in Japan, I would interpret this as:", "In the above picture:", "Hence the full state of the signal for all 3 directions can only be obtained after recognizing the whole cluster. Hence we wouldn\u2019t be able to subdivide the cluster.", "It looks like the TLD need to be modified for geographical regions anyways. e.g.", "Hence I think it would be more important to design the topics to be flexible enough to cover most traffic rules. Then the implementation can be swapped in and out for different regions.", "WRT the discussion of detection + classification 2 stage pipeline V.S. a single stage pipeline, here are some research on both approaches:", "2 also references a large number of research papers using 2 stage solution, which uses either image processing technique as a first stage, or YOLO-like object detection CNN as a first stage.", "To offer maximum flexibility, I propose the following:", "In a graph", "\n", "To improve ease of internationalisation it would be helpful to split TLD black box into 2 parts,", " - In your revised graph above, wouldn\u2019t the purpose of the \u201cTLD_Vision_Detector\u201d only be to verify that the light exists inside the ROI? Doesn\u2019t this then become a two-stage detection system as was described above? I feel like we might be better off offering alternative implementations of the \u201cTLD_BlackBox\u201d described in your first graph for different countries or just flags which set the country to enable/disable functionality within the node. Thoughts?", "I think Feature_Projection and ROI can be deprecated. Neural networks are capable of detecting and classify traffic lights from the raw image. I left it in the graph to be a \u201coptional\u201d input.", "For example, there could be multiple traffic light in view of the camera. the Vision Detector would  detect all of them. TLD_internationalisation would then take the one \u201cclosest\u201d to the vehicle and output its state.", "This way if special sauce is needed for a particular country, for example right on red in US (default green for turning right), then one need only to modify TLD_internationalisation, and not have to touch any neural network bits in TLD_Vision_Detector.", "That is my intention, but I would have no problem if all this is done as a blackbox inside a single node.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A heuristic approach (", ")", "A DNN approach which utilizes the MXNet framework (", ") and CUDA", "A DNN approach which utilizes the SSD framework (", ") and CUDA", "A list of regions of interest which contain the traffic lights", "A raw image", "The results from the heuristic approach were terrible and nearly unusable in real-world scenarios.", "Both the MXNet and SSD/Caffe detectors contain backbones which were trained using the COCO dataset, which is not available for commercial use. This severely limits the applicability of these nodes since they were designed specifically for these frameworks.", "Much of the functionality is duplicated between the DNN-based nodes.", "Attempting to separate the cropping/ROI extraction functionality of these nodes from the classification functionality leads to either timing issues or chicken-and-egg problems (described in more detail in a bit).", "The ", " node is somewhat naive in it\u2019s cropping mechanisms. Variance in pitch/roll/yaw of the vehicle, multiple bulbs in certain vector map versions, and the field-of-view of the detection camera lead to ROI images which are either too large, too small, only covering a portion of the light, or not pointing at the light whatsoever.", "The recognizer must take in raw images from one or more cameras and detect if a traffic light which exists in the vector map also exists within those images.", "If a traffic light exists in the vector map and is applicable to the currently-occupied lane and direction of travel but is outside the field of view of the camera(s), the recognizer should provide this feedback.", "The recognizer must classify a found traffic light from the image as having one of four states:\n", "Stop (usually red)", "Yield (usually yellow)", "Go (usually green or blue)", "Unknown", "\n", "The recognizer must publish three pieces of information:\n", "An overall recommendation to the vehicle control system about how to proceed given the detected and classified traffic light(s).", "An image which contains the contents of the raw image plus superimposed squares around the detected traffic lights and their associated classifications (this is currently provided by all classifiers but is only useful for human interaction and not necessary for system functionality).", "A ", " which contains markers for the detected lights and their classification results (again, this is not necessary for system functionality but is provided by all current classifiers).", "\n", "Combine the problems of detection and classification into a single, neural-network-based approach.", "Pros:\n", "Simplifies the architecture immensely", "Reduces compute resources (no image re-publishing, cropping, etc.)", "Gets rid of latency and chicken-and-egg problems", "\n", "Cons:\n", "Difficult to make general-purpose given the feature, language, and library support of the many neural network frameworks that are available.", "Difficult to troubleshoot since the task domain has increased and you can\u2019t obtain partial results.", "People who are using Autoware without a GPU are out of luck.", "\n", "Keep the detection task separate from the classification task but improve ", "'s ROI selection and, in the classification task,separate the actual classifier from the ROS node by either creating the classifier as a library or a separate node with a ROS service call in-between.", "Pros:\n", "Reuses existing code.", "The classification task can be heuristic or DNN-based and the classifier-interface node doesn\u2019t have to care - it just makes a service call or overloaded function call.", "\n", "Cons:\n", "The classifier-interface node can run into latency issues. Whether using a library or separate-node approach for the classifier, the latency between receiving the raw image and ROIs for that image and publishing the superimposed image with classification results is non-trivial. To keep consistency in the published superimposed image, the classifier-interface node has to receive the raw image and ROIs, make a call to the classifier, wait for the classifier to return the result, annotate the image and marker arrays, and then publish them. This can cause incoming images and ROIs to not be processed while the classifier-interface node is waiting for the current classification result. Making this call in any sort of asynchronous way means that you have to manage a list of received raw images and ROIs and coordinate them with the returned classification results.", "If using a library-based approach, only one language can be used and the choice of the language for the classifier-interface node would determine which language would be supported for the classification libraries (e.g., if using C++, can\u2019t use Python for the classifier).", "The detection task essentially remains the same but contains band-aids and hacks to make it better at feature projection in a real-world environment.", "\n", "Use feature projection to either a) determine if a relevant traffic light is in the image(s) AND find the ROI for that light or b) if a GPU is available, just determine if a relevant traffic light is in the image(s) while handing the detection task to a DNN-based node if the traffic light is in the image(s). In addition, separate the classifier as described in 2.", "Pros:\n", "Makes all involved nodes \u201cgeneral-purpose\u201d and usable with or without a GPU.", "Improves the ROI generation with a common approach that is known to produce high-reliability results (the DNN-based detector).", "The placement of traffic lights in the vector map and the yaw/pitch/roll of the vehicle are much less relevant to the detection task because the DNN-based detector only uses the raw image as input.", "The neural network for traffic light detection could be very shallow if only looking for one type of object in the image.", "Each of the stages of detection and classification can be more easily troubleshot and fine-tuned due to increased visibility into the pipeline.", "Same pros as 2.", "\n", "Cons:\n", "More computationally expensive than other approaches because of multiple neural networks running simultaneously and more nodes.", "Same cons as 2.", "\n", "Stop (usually red)", "Yield (usually yellow)", "Go (usually green or blue)", "Unknown", "Camera publishes raw image", "Feature Projection publishes ROIs (or light positions, as suggested)", "Classifier consumes raw image and ROIs (or light positions, as suggested) and produces classification (takes unknown amount of time)", "\n", " consumes raw image and classification", "Simplicity, simplify code and compute graph, reduce large message passing", "More robust, given all problem with current feature extraction node, a DNN approach with some filtering from vector map information should give a lot more robust result. Especially in areas that vector map is inaccurate or lacking.", "Achievability, seems to be a well-explored approach, a lot of networks and training data that can be used. ", ", ", ", ", "\n", "Speed, people running autoware without a GPU should already be in a lot of pain given that perception and tl detection module already reply on NNs. A CPU only simple heuristic node could still be provided by combining current feat_proj and region_tlr nodes, but I suspect the resultant quality would fit nobody\u2019s needs.", "tfd node publish a light color for going forward, turn left, turn right. Default to red. Hence the tfd would be in charge of condensing the different shapes of lights into these 3 channels.", "DecisionMakerNode currently have logic to do straight/left/right recognition by using angle. This node can then make a go/hold decision based on tfd output.", "Have the entire light cluster classified by the tlr in one go. This is difficult to implement because of the required number of statuses for different light combinations and the complete re-training required for the existing neural networks, not to mention the number of images required for each state for each combination of lights for training.", "Create individual, \u201cdirection-based\u201d signals. Using the example image from above, create 3 signals with seperate classification states for each. We would need to add metadata to the mapping format to indicate the direction of travel that each signal controls and then either have the feat_proj choose the correct one based on a \u201cdirection-of-travel\u201d input or have a decision node after the classification decide which to use. The upside of this approach is that the existing tlr NNs would only require a bit of transfer learning to train rather than compete re-training.", "Create \u201cdirection-based\u201d signals as described in 2 but have the classifier learn to distinguish between the different signal types and produce both a signal type and a state classification.", "The top row is the normal traffic light scenario, I.E. 3 lights with R/Y/G. Meaning Stop/Slow/Go for all directions. (I think each position only shows one colour, otherwise, it would be difficult for colour blind people.)", "The bottom row contains modifiers that overwrite the top row instruction. (they are only ever green, they have 2 states, off/green)", "Stop for all directions", "For a left turn, override to Go. For going ahead override to forward.", "In Japan, the green light is blue, the pre-processing of the raw image need to account for that.", "In the US there is right on red, and TLD or other node needs to be able to recognize the (No turn on red) sign.", "In Europe, there is ever only one directional modifier in addition to the main light.", "Join traffic sign and traffic light detection and classification using a single DNN. ", "\n", "2 stage pipeline - ", "\n", "specify a single stage TLD. If multiple stages are required, they can be implemented as internal stages of the TLD.", "feat_proj will continue to exist but improved to take into account the pose of the car", "TLD takes the output of feat_proj as a suggestion but doesn\u2019t wholly rely on the map to provide accurate information", "TLD outputs a go/slow/stop states for each of right/left/ahead directions.", "A decision node will take the output of TLD and path planner to issue STOP/GO signals to the vehicle.", "To deal with geographical differences, TLD node will have different implementation depending on the application.", "Autoware can provide a generic TLD node trained using a publically available database. As there are established research projects in this area, a reference implementation can be done following one of the papers.", "TLD_Vision_Detector to perform purely the task of detecting the lights from an image.", "TLD_internationalisation to integrate information from other sources to condense vision detection into light/right/ahead light signals.", "TLD_Vision_Detector would only deal with running a neural network and is hence very simple.", "TLD_internationalisation would then take multiple signal and discern a left/right/ahead signal."], "url": "https://discourse.ros.org/t/generalized-traffic-light-classification-architecture/9354"}
,{"title": "RQT in ROS2", "thread_contents": ["Hello ROS community!", "We are excited to announce that ", " is working with ", " to port the plugin-based graphical user interface ", " to ROS2 for the December Crystal release. The RQT packages provide an easier entry-point for a broader user base than command line usage. RQT is built using the open source cross-platform framework ", ".", "Our primary approach to this effort will be:", "We invite the community to provide feedback and suggestions on RQT by replying to this thread. While this is an ideal time to break API, given time and resource constraints, enhancements will be secondary to porting existing functionality. However, this thread will also act as a rough roadmap to guide future enhancements.", "We believe this is an excellent opportunity to contribute to the ROS2 development effort and a first step in preparing PickNik to migrate ", " to ROS2. We are grateful to the community and corporate partners like Amazon that help drive improvements for ROS.", "Thanks,", "\nThe PickNik Team", "This is my rqt wishlist, in an order that is a rough combination of increasing difficulty and decreasing priority:", "This is my rqt wishlist", "Please do not mistake porting effort with arbitrary feature development. The main goal is to port the existing framework and several plugins to ROS 2 (Python 3 as well as Windows).", "If you would like to specific features please feel free to contribute them - either to the current ROS 1 branches or to the -to-be-created ROS 2 branch. A couple of brief notes to your bullets:", "rqt_plot - \u2026 implement from scratch \u2026", "There is no reason to implement \u201cyet another\u201d plotting plugin. There is already (at least) two: ", " as well as ", "rqt dynamic reconfigure", "Dynamic reconfigure conceptionally / intentionally doesn\u2019t exist in ROS 2. The ROS 2 parameters are node specific and replace the ROS 1 parameters as well as dynamic reconfigure.", "Individual crashed or unresponsive plugins", "In the past Qt offered the ability to embed widgets from separate processes (at least on X11). That is not available in Qt 5 anymore and therefore this way of isolating plugins in ", " isn\u2019t possible anymore. If you see another way to achieve this please feel free to propose it.", "rviz-in-rqt", "There is already a ", " plugin for ", " (it has some severe issues though). That could be ported as is. Publishing and rendering the ", " views and relaying the mouse / keyboard interaction sounds like a big no-go to me.", "Convert a .perspective file to a webserver + html5/js, or an android app", "This sounds like an incredible complex task - even ignoring that there are no html5/js or Android components atm which could be the target of any mapping.", "Please do not mistake porting effort with arbitrary feature development. The main goal is to port the existing framework and several plugins to ROS 2 (Python 3 as well as Windows).", "I\u2019m responding to the \u2018While this is an ideal time to break API, given time and resource constraints, enhancements will be secondary to porting existing functionality. However, this thread will also act as a rough roadmap to guide future enhancements\u2019 above.  If there were decisions to be made in immediate porting effort that would benefit the sorts of features outlined above then that is the substance to take away.  For instance, if only a subset of the common plugins where to make the cut for the targeted release, the first few items are what I would like to see- and others will have an opinion, and maybe some shouldn\u2019t be ported at all?  (I\u2019m drawing inspiration from other ros2 threads about if a given package or part of a package isn\u2019t used much, or has been superseded, or just is way less useful than some other parts or packages, don\u2019t bother porting right now or ever)", "There is no reason to implement \u201cyet another\u201d plotting plugin. There is already (at least) two:  ", "  as well as  ", "If it were quicker to do it from scratch, or just narrow support to a single back end, and that would make the difference between having rqt_plot by the desired time or not, and/or it would end up with better performance, then that would be the reason to consider not just a direct port.  (Maybe it should publish the plot as an Image, or a generic vector image message type if pixels seem clunky?  Maybe every visual plugin should publish an Image instead of drawing directly to a window- that would simplify several of them, to the point where they are no longer even rqt plugins, just regular ros2 nodes with parameters set through a generic ros2 param setting rqt plugin?  If there are strong reasons not to that ties into the rviz-rqt idea)", "Dynamic reconfigure conceptionally / intentionally doesn\u2019t exist in ROS 2. The ROS 2 parameters are node specific and replace the ROS 1 parameters as well as dynamic reconfigure.", "Instead of direct porting there would be an equivalently functional rqt ros2 plugin for viewing and changing per node parameters.  (Being able to hide some parameters and save that into the .perspective would be useful if every node has a lot of boiler plate parameters.)", "In the past Qt offered the ability to embed widgets from separate processes (at least on X11). That is not available in Qt 5 anymore and therefore this way of isolating plugins in  ", "  isn\u2019t possible anymore. If you see another way to achieve this please feel free to propose it.", "That\u2019s unfortunate.  In the much longer term maybe the qt in rqt should be something else (which could also benefit the web/android/whatever cross platform capability, or some of the other items here).", "Publishing and rendering the  ", "  views and relaying the mouse / keyboard interaction sounds like a big no-go to me.", "I\u2019d be curious to know why- if it has anything to do with underlying limitations of ros or qt it will help me guide my future pull-request efforts and comments (if it is just timeframe this is something for the longer term, e.g. \u2018rviz2.5\u2019 or \u2018rviz3\u2019).  I\u2019ve done similar things on a limited scale in ros1 with satisfactory results, I assume ros2 will perform better.", "Convert a .perspective file to a webserver + html5/js, or an android app", "This sounds like an incredible complex task - even ignoring that there are no html5/js or Android components atm which could be the target of any mapping.", "Yes that is true, but again I throw it in here in case someone else knows something can contribute, or in some decision between x or y it turned out x could be more easily converted to those platforms.  The short term would probably be a few of those components- a generic node parameter configuration widget, an image viewer, plotter, echo, publish, etc. and just a document to guide how to manually assemble them into something similar to the rqt file, and automation later.", "If you would like to specific features please feel free to contribute them - either to the current ROS 1 branches or to the -to-be-created ROS 2 branch.", "Of course, I\u2019ve always done this in the past.  I\u2019m hopeful that if the right minimum coverage of standard ros interfaces have good general ui plugins then writing ui code now will pay off in my (or someone else in my place) not having it to write much or any application specific ui code in the future, it will just be a matter of selecting and positioning and configuring the widgets (we\u2019d be writing ros nodes with ros interfaces mainly meant to be part of a ui that is a thin layer of generic plugins, but that is an improvement over having to write qt).", "Do we actually need two separate ways to build graphical user interfaces in ROS2? There is ", ", which provides dockable widgets, and ", ", which provides dockable widgets and also 3D visualization.", "\nI don\u2019t know why ROS1 has two separate GUI widget plugin architectures, but porting GUI widgets from ROS1 to ROS2 is a good opportunity to reconsider this. Is there a reason ROS2 needs to have two GUI widget plugin systems?", "I consider RViz as the more useful GUI to visualize a robot\u2019s state and control it, and I think that ROS2 should establish RViz plugins as the default way to compose GUIs. I do not see the advantage of having two plugin systems for widgets. In some cases (e.g. image view), it even creates multiple versions of the same visualization functionality.", "The only remaining issue I see with RViz is that the 3D view in RViz is not dockable at the moment, i.e. it is always visible and cannot be removed. I think developer time is better spent by making the 3D view dockable and porting Rqt widgets directly to RViz(2).", "I actually don\u2019t agree. The docking system in rviz is minimalistic and used primarily with things related to rviz\u2019s activities. rqt\u2019s docking system is far more sophisticated and necessary for some of the features it supports.", "One important difference is that rviz panels share state with each other though C++ interfaces whereas rqt plugins can either be c++ or Python and exchange (almost) all information via ROS topics, services, and actions.", "I actually see them having different goals, if anything were to be elided, I\u2019d say it should be that rviz\u2019s application code (docking and extra widgets) could be replaced with rqt, such that the rviz panels (i.e. the render window, panels, and tool bars) would become rqt plugins and the \u201crviz\u201d application would simply be a specially crafted rqt layout.", "I\u2019m actually not sure it\u2019s worth doing this, because of the aforementioned shared state between rviz parts which would require either rebuilding those interfaces as ROS interfaces (unnecessary and would have more overhead) or changing rqt so that different plugins could communicate with each other directly (a difficult issue to be sure, given you cannot assume the existence of other plugins nor that they\u2019re c++ or Python).", "Any update on rqt for ROS2?", "Hello Lalit,", "We ported most of the rqt common plugins to ROS2 for the Crystal release. Please see the tutorial page for more information.", "Stephen", "Thanks Stephen, I will check it out.", "If anyone wants to help, I plan to port ", " to ROS2. Any help from people with experience in DDS/ROS2 is welcome.", "Is there any plan to port ", "?", "There is no reason to implement \u201cyet another\u201d plotting plugin. There is already (at least) two: ", " as well as ", "If it were quicker to do it from scratch, or just narrow support to a single back end, and that would make the difference between having rqt_plot by the desired time or not, and/or it would end up with better performance, then that would be the reason to consider not just a direct port.", "This is the only thing I have to say about the latter comment ", "Is anyone porting* the rest of the rqt plugins? In particular, is anyone porting rqt_graph?", "(*Edit, fixed)", "rqt_graph has been ported and released!", " - Where? Can you point me to it?", "is any one porting rviz plugin for rqt?", "To install ", ". ", ", ", "I tried on ", ".", "Contribute to ros-visualization/rqt_graph development by creating an account on GitHub.", "@ harderthan- I am working on Ubuntu 18.04, ROS2 Dashing", "\nI have already installed rqt-graph what you have mentioned.", "rqt is listing only \u201cImage View\u201d and \u201cPlot\u201d options  under \u201cplugins/Visualization\u201d", "Do I need to install any other plugins to get rviz plugin for rqt?", "I\u2019m not sure anyone has taken on the port of the rqt_rviz plugin. When we ported the RQT plugins to ROS2, this one was specifically avoided due to the large amount of work involved and the timeline we were working under. Alberto from iRobot also looked into it, but I\u2019m not sure if they were able to make any progress on it. It\u2019s likely a large undertaking and would require some dedicated effort from knowledgeable individuals.", "Related thread:", "Stephen", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Target only the RQT-common plugins and dependencies", "Add support of Python3 and ament build system", "Support OSX and Windows", "Rework ROS middleware plugins to work with DDS", "Support the other core ROS libraries that are currently being developed by being first adopters of their new APIs, such as rosbag, roslaunch, and actionlib.", "rqt_image_view - I\u2019d like to see this before much anything else.", "rqt_plot - maybe just choose or implement from scratch a simple single plot type rather than integrating multiple ones, make it work well enough, don\u2019t consume tons of cpu with default settings.", "rqt dynamic reconfigure with an interface more like rqt_image_view- I\u2019ve made my own ros1 version of this: ", " (it has some issues with reloading and servers not responding I need to clear up) - instead of a tree of all possible servers there is just one that is saved as a setting into the perspective file, but it can be switched with a drop down menu.  I\u2019ll make a ros2 version myself given ros2 dynamic reconfigure (?).", "message publishing and echoing, service calling and action calling could also use similar treatment to be more like rqt_image_view, and with the configuration hidden be presentable to an end user who knows nothing about ros.  Save configuration to the perspective file so it would be possible to set up a widget in a ui that is meant to do something specific on a single pre-configured topic or service as opposed to a tool to explore all possible topics and services etc. on a system in a big tree or list.", "Individual crashed or unresponsive plugins ought to not bring down the entire rqt instance.", "rviz-in-rqt - I know rviz2 work is already underway but it would be fantastic if the whole of it could be a small number of rqt plugins.  A render core would publish the view/s as Images and be configured through standard ros2 interfaces.  rqt_image_view would need to send all mouse events rather than just left mouse clicks.  The side panels could be special rqt plugins, or some functions could be generic service caller or dynamic reconfigure plugins and corresponding servers within the render module.  Eliminate redundant plugins for rviz like the image viewer (though the one that under/overlays rendered objects is still useful).", "Convert a .perspective file to a webserver + html5/js, or an android app."], "url": "https://discourse.ros.org/t/rqt-in-ros2/6428"}
,{"title": "Astra driver for ROS2", "thread_contents": ["I noticed there is a repo to use the astra camera in a ros2 deployment.", "\nI forked the code and am trying to run it, and it compiles just fine, but it doesn\u2019t actually deliver any data.", "\nI can run OpenCV and it connects and delivers data.", "\nI can run the ROS1 driver, and it connects and delivers data.", "\nI can run the OpenNI sample viewer, and it connects and delivers data.", "So my question is:", "\nAm I doing something wrong in ROS2 for it to work?", "\nOr is this driver still under development?", "I\u2019m just putting the code into a workspace, like usual, and building it with \u2018ament build\u2019.", "\nWhen I run it, I see \u2018Starting IR stream\u2019 and \u2018Starting depth stream\u2019, and the topics are all there. But I don\u2019t see any data flowing.", "\nI used CLion to breakpoint into the code, and it didn\u2019t appear that any callbacks were being activated.", "Anyways, any help would be great.", "\nThanks!", "I\u2019ve used it in the past for sure, but it has been under active development lately for a demo we\u2019re trying to put together.", " may have more info for you.", "I do have an Astra Pro. Maybe there\u2019s an issue there?", "I know that the astra Pro had issue in ROS1 because it advertise itself as a UVC for the RGB and another USB device for the depth. The ROS2 version of the driver did work for me in ROS2 by just building it, installing the udev rules and running it. We never tried it on an Astra Pro (AFAIK) though. ", " and ", " have been playing with it lately they may be able to give more input.", "I did do some work on it, though I haven\u2019t really changed much in a few weeks.  In my testing, things seem to be working and I\u2019m getting good depth images out of it.", "The fact that you don\u2019t seem to be getting callbacks is interesting, though.  Note that there are two levels of callbacks: when the driver actually has a frame it calls AstraFrameListener::onNewFrame(), and if the frame is \u201cvalid\u201d onNewFrame() then calls the higher level AstraDriver::newDepthFrameCounter().  Where did you put your breakpoint?  Can you see if you are getting the onNewFrame() callback?", "One other thing I\u2019ll mention is that I have seen cases where the camera will stop streaming data after starting and stopping the driver a few times.  This seems to be some kind of firmware level thing, and if you unplug the camera, wait a couple of seconds, then plug it back in, it tends to start working again.  That would also be worth trying.", "So, I switched out my Astra Pro for an Astra Mini, and the driver just worked.", "\nThere are some issues, so I\u2019m going to open up an Issue on GitHub.", "Thanks for the help.", "Anyone looked into the performance comparison of the ROS1 driver vs ROS2 driver?", "\nI\u2019m noticing a HUGE degradation in performance using the ROS2 driver. Wondering if it\u2019s my setup or if the driver just isn\u2019t fleshed out enough to be performant.", "My test system: MacBook Pro i7 2.5GHz, Xenial connecting to Astra Mini.", "Yeah, I\u2019ve had performance issues as well.  I was able to mostly work around it by disabling the color and IR cameras, and set the depth camera to 320x240.  That gets me about 15 frames/sec with something like 100% CPU time, which is good enough for what I need at the moment.  But we should really look into improving it.", "Ouch.", "I did the same thing as you, and I was getting 30 fps from the driver. (although this was all going through the ROS2 dynamic_bridge, so there may be some lag).", "\nThe depth_to_pointcloud node drops that in half to ~15 fps for the PointCloud message. That surprises me. Is that conversion computation really that time consuming? Maybe we can make it faster using GPUs?", "I should be a little more clear that I am running all of this on a Pine 64 board (", "), so it won\u2019t be as fast as on an Intel chip.", "That being said, I don\u2019t really know how much CPU time this should take up.  My feeling is that there is room for improvement both in the astra driver and in depth_to_pointcloud, but I haven\u2019t profiled to find out where the bottlenecks are.  Before I went all the way to a GPU, I would profile, find the problem points, and try to optimize those first (this could be as simple as building with optimization and/or NEON turned on, neither of which I\u2019m doing at the moment).", "I just figured that the function is pretty basic. Iterate n^2 over an image and convert to PointCloud2 format. I\u2019m already compiling in Release, so optimization is all on. So maybe GPU is the only way to go for real acceleration.", "But all that aside, I did some performance analysis using the ", " tool and ran it against both the ROS1 and ROS2 versions of both ", " and ", ".", "\nIt looks like they both spend about the same time in the same functions, but the ROS2 variants spend extra time in ", ".", "Not sure if there are techniques, yet, that help to accelerate/overcome this new conversion step.", "\n[My next step is to verify that my ROS2 base was compiled in release mode\u2026]", "Ok. Verified my ROS2 base is in Release mode (it wasn\u2019t).", "\nI reran the profiling on the depth_to_pointcloud node, and this is the output:", "A little hard to read, but basically the worst offender is the convert_ros_to_dds on the PointCloud, which takes ~23%. Then comes convert_dds_to_ros on the Image, at ~6%, and then finally the actual conversion from Image to PointCloud2, at ~5%.", " maybe there\u2019s something we can fix in the CoreDX rmw wrapper that can help speed this up?", "Another idea I had was I could make a composition of the astra driver and this conversion and eliminate part of the slowdown (~6% for dds_to_ros conversion of Image).", "What about that ~40% going to wl? Is that the wireless driver? I guess that is the connection DDS would be using on my laptop\u2026", "Any other ideas?", "Nice debugging.  Out of curiousity, exactly how did you run operf (just so it is archived here for the future)?", "I was wondering about that wl myself.  The next time I have some time (probably next week), I might try this locally to see what happens when I\u2019m hooked to ethernet instead of wifi.  That would be interesting.", "I also notice that there is ~10% going to memmove and memcpy, unaligned.  I\u2019m guessing if we could find a way to ensure that that is aligned, that would also be faster.", "Other than those things, yeah, I\u2019d say we\u2019d have to concentrate on convert_ros_message_to_dds and see what we could improve there.", "I downloaded the oprofile package and installed it.", "\nTo use it, you have to have root privelages, so I created a bash script that I can run under ", ".", "\n", " does the trick.", "\nThe reason I chose oprofile instead of gprof (or other) is that you can kill the process and it still collects info.", "To generate the statistics you have to run:", "\n", "There could be something obvious that could significantly improve ", " for CoreDX, but it would be interesting to see if this is also the case with Connext (I\u2019m not sure FastRTPS would be apples to apples since it uses type support introspection to serialize/deserialize). We always knew that this function was wasted work, but to get rid of it, we need to have the ability in the middleware to publish and take serialized data directly rather than delegating serialization/deserialization to the middleware. That way we can do the \u201cROS message struct\u201d -> CDR serialized buffer directly skipping the convert function. I know we could do this in FastRTPS, but I\u2019m not sure about Connext. Also FastRTPS currently uses the introspection API which will be slower than statically generated code to do the conversions. We can likely get a huge speed up there by providing a static version of type support for FastRTPS.", "Ran the code in composition mode (both depth_to_pointcloud and astra_driver as libs, using UniquePtr to pass the Image), and that didn\u2019t seem to make much difference.", "Switched back to 2-node mode, and connected via Ethernet instead of Wifi (disabled Wifi, connected a dummy cable to another machine in the lab), and the performance was way better. So clearly that ~40% on ", " was getting in the way.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS1 driver delivers ~30 fps and consumes ~70% CPU.", "ROS2 driver, with CoreDX as rmw implementation, delivers ~5 fps and consumes ~170% CPU."], "url": "https://discourse.ros.org/t/astra-driver-for-ros2/1894"}
,{"title": "Execute code within a Node/Callbackgroup context", "thread_contents": ["Currently a node lives in a executor, and callbacks from timers and subscriptions are executed by that executor, taking callbackgroups into account.", "However, sometimes I have a trigger from somewhere else, not related to ROS, that I want to handle in that same context, This to prevent having to fiddle with mutexes etc.", "Basically something like this:", "\nrclcpp::Node::execute(CallbackT && callback, rclcpp::callback_group::CallbackGroup::SharedPtr group = nullptr);", "\nI then expect the callback to be executed as soon as possible in the same way as a subscription or timer callback.", "Now I do it by creating a one-off timer with a duration of 0, but it\u2019s not the nicest thing in the word. Would it be useful to have something like that in ros2?", "Yeah, that would be nice, and we\u2019ve thought about it before, but have always just done a timer will a small or zero timeout instead.", "Other similar systems have methods like ", ". Which just calls the callback as soon as it is convenient.", "Yeah it also works indeed, it\u2019s just that to make a one-off timer you need to store the timerhandle and make it accessible to the callback in order to make it one off.", "Just for reference, i now use the following utility:", "I solved a similar issue creating a custom objects that inherits from ", ".", "\nThis is registered similarly to subscription callbacks and it can  be triggered externally.  In my case it\u2019s triggered whenever data is added to a buffer.", "That\u2019s actually a cleaner solutions that should also introduce less overhead. Requires a bit more effort though but I\u2019ll look into it! Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["  gc_ = rcl_get_zero_initialized_guard_condition();", "  rcl_ret_t ret = rcl_guard_condition_init(", "    &gc_, context->get_rcl_context().get(), guard_condition_options);", "\n", "  if (RCL_RET_OK != ret) {", "    throw std::runtime_error(\"SubscriptionIntraProcess init error initializing guard condition\");", "  }", "}", "\n", "bool", "is_ready(rcl_wait_set_t * wait_set)", "{", "  (void)wait_set;", "  return buffer_->has_data();", "}", "\n", "void execute()", "{", "  if (any_callback_.use_take_shared_method()) {", "    ConstMessageSharedPtr msg = buffer_->consume_shared();", "    any_callback_.dispatch_intra_process(msg, rmw_message_info_t());"], "url": "https://discourse.ros.org/t/execute-code-within-a-node-callbackgroup-context/9406"}
,{"title": "Building ROS on Mac in CI", "thread_contents": ["I am considering building some ROS packages on Mac regularly for catching up broken build/tests on this platform.", "The first step would be to do a prototype (", " looks promising). If it goes OK, this needs to be integrated with ROS Buildfarm.", "What do you think - useful, doable?", "The main issue is the need for a reproducible build, which is hard to do because macOS doesn\u2019t have docker and no one (outside of Travis-CI that I\u2019m aware of) has automated use of VM\u2019s with checkpoints to emulate a \u201cclean starting state\u201d for each build. This is a crucial step for building packages in my opinion, but AFAIK (it could have changed) ", " only provides hosting on mac minis, not the automation for repeatable jobs. Travis-CI does provide this as continuous integration, and I believe some people are using (abusing?) this to build Homebrew bottles ", " might know more about that then me at this point.", "If you\u2019re interested in trying anyways, you\u2019ll want to catch up on the existing efforts:", "Those two links are the best summaries I can find, but efforts so far have stalled, as just keeping it building from source on macOS with Homebrew has become of a burden than the community appears to be willing to bear.", "In order to be integrated into the \u201cROS buildfarm\u201d (by this I assume you mean ", "), we\u2019ll need a way to provision machines, generate artifacts on a per package basis (bloom does this for debian and fedora packaging), and extend the build farm itself to create the necessary jenkins jobs to actually execute the packaging on the build machines and upload the result somewhere. Once you get to this point, ", " and/or ", " might have more to add.", "Just glancing at ", " again, it looks like they now support provisioning VM\u2019s on demand and use VMware under the hood:", "Also, I\u2019ll mention that Homebrew does it\u2019s packaging (last time I checked, again could have changed) not by having VM\u2019s, but by deleting the homebrew directory each time before a new build starts. So this is more like ensuring you completely cleanup the system before finishing a job, rather than discarding the changes and going back to a previous docker image or VM snapshot. This might also be possible, but in my experience it\u2019s harder for ROS since we not only install things from Homebrew, but also things from pip for Python dependencies (also possibly Ruby\u2019s gem, etc\u2026).", "I maintain the osrf/simulation tap where I build bottles for gazebo and its dependencies. We use a few mac minis with jenkins for the bottle builds.", "I have seen folks attempt to build bottles with travis-ci, though I\u2019m not sure what the current status is:", "\n", "\n", "The biggest challenge with maintaining homebrew bottles on a non-core tap is that your bottles can break at any time if a new bottle is built for one of your dependencies (such as boost or protobuf). I have a daily job that tests one of our gazebo bottles, and then we manually rebuild them if we notice that job failing due to an outdated dependency. We try to stay on top of it and don\u2019t see too much downtime, but this is not scalable. I wouldn\u2019t want to be in charge of keep homebrew bottles for all of ROS working.", "\n", "\n", "My initial focus is just building and testing.  Packaging will come later if building and testing is successful.  I believe both things is going to be useful - knowing that a particular change causes a build break or a test failure on Mac is a useful information for package maintainers.", "Yes, doing a clean (or more generally determistic) build is a concern for any CI process, and this work should get some data on the topic.", "If you want to avoid building \u201ceverything\u201d at once (including all recursive dependencies) you will need packaging to provide the binary packages for the dependencies.", "If you want to avoid building \u201ceverything\u201d at once (including all recursive dependencies) you will need packaging to provide the binary packages for the dependencies.", "Dirk is exactly right. Even if you only want to use travis for CI (which is easy enough to set up), once you have enough dependencies, your build will timeout if you don\u2019t install them from pre-compiled binaries (bottles in the homebrew case).", "According to ", "  the", "\njob\u2019s timeout is 50 minutes. When I was following ", "  on my computer", "\nfor ", " I think the whole process was done in less than 50 minutes and it includes quite a few packages being built.", "I plan to start with something small, see how it works, and then extend if successful.", " and/or ", " might have more to add.", "I think most of the key points have been covered. Any automated build and testing will benefit the project but in order to be widely useful, it needs to be reproducible and make efficient use of resources. You might find the ", " repository interesting. It shows how we build and test ROS 2 from source on ", " which has several windows and mac machines attached to it. As the project grows we\u2019re starting to feel the pain of this approach as CI times climb.", "If it goes OK, this needs to be integrated with ROS Buildfarm.", "We would definitely encourage contributions for MacOS support into the ros_buildfarm repository; giving community members the opportunity to deploy it. However since MacOS is not an officially supported platform, it is a small fraction of the user base, and costs for Apple hardware or compute time are significantly higher than other platforms, deploying MacOS to the official infrastructure is not a decision that can be made based solely on the technical capability to do so. Once we have the capability, if an organization would like to sponsor efforts to deploy and support the ongoing maintenance of MacOS infrastructure it could be reconsidered.", "I started with Travis, picked up ", ", and added the building and testing on Mac.  ", ". Then I opened ", " to discover that the last build in Travis happened 5 years ago.  That\u2019s many changes ago.", "Is Travis an option? Having VMs for free for OS projects sounds like a good deal to me.", "For smaller packages like rosdep Travis is definitely an option that\u2019s already in use. You\u2019ve found the Travis CI history for the old repository location. It was moved to", "rosdep multi-package manager system dependency tool - ros-infrastructure/rosdep", "So you can see the current status is here: ", "It looks like you\u2019ve already found it, but for others reference there\u2019s now a [PR under review}", ")  for rosdep testing on macOS.", "As the project grows we\u2019re starting to feel the pain of this approach as CI times climb.", "Do you have any information on how you plan to address this pain?", "Do you have any information on how you plan to address this pain?", "I think what ", " said: provide binary packages for the dependencies. You can see my other comments in this thread about that current status for macOS.", "I am focusing right now on building python projects on macOs.  Example: ", ".", " folder contains few things, and it will be pretty much the same for every project.  I am thinking about moving the content of .travis into a separate repo and adding it to existing repos as a submodule.", "What do you think about using submodules for this case?", "Normally I dislike using submodules, but given that it\u2019s only required for travis, I think it would be ok. At least I can speak for the repositories I maintain, other maintainers might not feel that same way.", "I too dislike submodules, and although they would only be necessary for travis if they are there by default it can be confusing/disruptive for developers. If it\u2019s only necessary for travis it would be simple to package that functionality into a resource that travis would fetch/install during the setup phase, like it does like any other testing/tooling dependencies which are installed via the .travis ", " segment.", "Thanks for the ideas.", "MacOS CI for ROS packages would be a great addition, even for core repositories like ros_comm (see e.g. ", ").", "You might want to have a look at ", ", which can be easily added to other repositories without submodules, just loke ", " mentioned. Maybe it even makes sense to integrate mac support there, since this is already in use by many packages it seems.", "You might want to have a look at ", ", which can be easily added to other repositories without submodules", "The same goes for ", ". Afaik neither of the two offers support for macOS atm. For ", " it is certainly planned to also test on macOS and Windows in order to satisfy the need in ROS 2. Once that works on Jenkins it should be feasible to run the same on Travis (with macOS) and AppVeyor.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/building-ros-on-mac-in-ci/4071"}
,{"title": "Generating 'dev' and runtime artefacts from ROS packages", "thread_contents": ["First: I\u2019m not entirely sure this belongs here, but I couldn\u2019t think of a better category. It\u2019s probably something that will touch the buildfarm if we can come up with something, hence why I posted it here.", "I\u2019ve been interested in reducing the size of deployments of ROS applications, both native and in Docker (and Singularity) images for some time now, and one thing that I have been curious about is whether it would be possible to generate separate runtime and ", " archives from ROS packages (to use the Debian/Ubuntu terminology).", "Some Googling led me to ", " on ROS Answers, which has an answer by ", " mentioning the support for automatic generation of ", " symbol packages (which has been active for Melodic and newer releases), but couldn\u2019t find additional discussions about the topic.", "Using multi-stage ", "s tremendous savings can be accomplished (going from 4GB+ to 200MB images), but this is non-trivial with ROS packages in the mix.", "Thinking about it, I can see two aspects which may make this either difficult or reduce the gains significantly:", "Pt 1 may not be too problematic. It\u2019s an assumption and would be something to figure out.", "Pt 2 could perhaps be approached very naively: assume everything exported by ", " in a ", " call (in ROS 1) ", " is local to the exporting package is what should go in a ", " package. Assume everything else would go into the runtime package. That information would somehow be transferred to Bloom when it populates the package structure and ", " templates.", "Another option could be to use CMake\u2019s support for ", " in ", " rules (as that could make it work for ROS 2 as well), but I have no idea whether that would help when generating Debian/RPM/something-else packages.", "I also realise that to do this right is probably going to be complex and will require quite some effort.", "This post here on Discourse is to find out whether others have thought about this as well or perhaps even have implemented something in this direction.", "This is definitely something that has been considered at various times. And it\u2019s been thought about recently by ", " who has been looking into generating rpms. As rpms have stricter linters.", "The automation of separating dbg symbols was eventually resolved with it becoming part of the standard debian toolchain:", "From that issue you can find links to various other discussions of adding ", " packages as far back as 2011. But on reflecting on it I think that the reason that we haven\u2019t found a solution for created them is that it doesn\u2019t make sense to \u201cautomatically\u201d split packages. There are many heuristics to say when to put something into the dev version or the runtime component. But in reality there\u2019s many corner cases that will never be met so the maintainer will basically need to list out resources to land in either regular or dev variants of the package. In the Debian world this is the role of the debian package maintainer. Each resource is placed into one or the other bucket. But more importantly the downstream package maintainer then has to add the right dependency on either the runtime or devel package. That downstream maintainer also splits their package into runtime and devel packages. But more importantly they also split the dependencies conditionally on the devel and runtime dependencies appropriately.", "Now you can see that we actually have 4 different cases for one dependency. There\u2019s both devel and runtime versions of both packages. And the dependencies declared on the downstream package have to be conditionally evaluated for the subset of the package that is being used to build or made available.", "We have a rich language of dependencies with at least 5 different versions of dependencies for a package including ", ", ", ", ", ", ", ", ", ". And that\u2019s excluding the extra test and doc dependencies as well as groups etc. We would then have to include a way to declare associations of each of those dependencies to the runtime or devel versions of a package.", "Looking at this from a little bit further away, the main reason for having ", " and ", " packages is because historically ", " was released as a tarball and then the debian maintainers would pick up the tarball, and select what went to each subpackage, and add the appropriate dependency declarations. Since then the process has not evolved much, though now often it\u2019s pulled directly from the source repository. But in general the upstream maintainers are not thinking about the impacts of releasing their software and packaging.", "By comparison in the ROS ecosystem core developers actively use packages as atomic units and leverage those packages even when developing, and they use and pay attention to the package dependencies. The efficiency gains from modular packaging that are usually gained by the debian maintainers work can be leveraged in the development environment too.", "As such I think that it might behoove us not to think of this as a problem to be fixed at the release stage, but as an area of efficiency that we can gain by structuring our upstream packages similiarly. The benefits of separating the build and runtime dependencies helps overall with compile times. Dependency trees do not grow as quickly if they are not the union of build and runtime dependencies. Parallel build tools can build faster when these potentials are paid attention to in the development environment.", "And I think that many of our common practices that the ROS community has developed such as recommending making message only packages gets a lot to this end. They are the higher level equivalent of headers and break apart the dependency trees by providing standard interfaces. And through encouraging small modular packages we get the ability to leverage the find grain dependency control.", "Specifically separating the build and runtime dependencies is something that has been shown to be quite effective that we\u2019re not actively doing. However, instead of trying to do that at release time. I\u2019d suggest that we consider making that a best practice for the ROS maintainers to create full packages that separate concerns into runtime and build time dependencies. This will give the developer the ability to fully specify the dependencies for each and where content goes is also clearly obvious. We could even pick up the naming convention of debian to name the content designed for build time to end with ", " however that would then conflict with the Fedora/RPM convention to name things with ", ".", "In ROS 2 we\u2019ve leveraged this separation of dependency types to enable swapping out the runtime libraries and hide the underlying implementations yet building them all against the same headers by defining a clear interface specification. There\u2019s always a matter of tradeoffs between making things more modular and keeping things as simple as possible.", "So in conclusion I think what I\u2019d like to suggest is that we already have support for all the bells and whistles needed to very clearly define the necessary dependencies. And that if we can identify places where separating the build and runtime dependencies would be valuable that we consider simply making separate packages and not try to find ways to automatically split a package and then find ways to refer to those partial packages in downstream packages. We\u2019re generating packages, we have packages. Keeping it one to one is basically the simplest solution.", "At work I am in a similar situation. We extensively work with docker images and found that ROS certainly doesn\u2019t make it easy to have small docker images. On the larger end they could easily reach 12GiB (although a lot of that was our own negligence). Even a base install of ROS resulted in close to a 800MiB compressed image.", "You\u2019re absolutely correct that there is an inherent limitation at the moment with ROS packages ", " including the dev artifacts and that has given us real trouble in a few areas. One of the things we did actually find however is that nine times out of ten it isn\u2019t actually the ROS packages themselves that are the problem - virtually all of the built debians contain just shared libraries and headers which is only going to be barely larger than a non-dev package. It\u2019s the upstream dependencies that they hold which are consistently problematic.", "As an example the ", " deb package, which is in any core installation of ROS, has a dependency on libboost-dev. This boost metapackage is ", " and has massive dependencies of its own, installing it on a fresh Ubuntu docker image installs 586MB worth of stuff even with ", ". Some of the dependencies that get pulled in are just plain useless for actually ", " ROS applications:", "To workaround this problem we came up with a novel solution. Before installing our ROS dependencies into our docker images we install custom, fake, metapackages. These metapackages are special in that they don\u2019t install anything themselves, they simply \u201cprovide\u201d (in the debian packaging sense) all of the upstream dev dependencies of which ever ROS packages we\u2019re intending to install (plus a subset of ROS packages that we definately don\u2019t need) without actually adding anything to the host system. That way when we later install the ROS dependencies it doesn\u2019t actually install the dev dependencies. It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "The positive news is that solving this specific problem in ROS packaging more generally is probably much easier than implementing ", " packages in their entirety. In theory all that is needed is to produce ", " packages that just have extra ", " dependencies of their own, no extra files.", "To a degree this should be straight forward as package.xml already has a mechanism for declaring these dev only transitive dependencies: ", ". The downside of this approach is that it involves fixing dependencies on basically every package in the ROS ecosystem since that\u2019s mainly used to export headers at the moment.", "A hackier approach would be on the rosdep side. At the moment the ", " rosdep key is declared as being the dev packages for every given OS: ", ". It should be possible to create a \u201cparallel\u201d rosdep file that declares the non-dev versions of packages which can then be automatically used by bloom to generate runtime packages with the non-dev dependencies.", "The final approach is as ", "  has suggested, explicitly create \u2018-dev\u2019 ROS packages. The downside of this approach is legacy, there is a substantial amount of legwork to implement this approach and tons of breaking changes to go with it. It also seems overkill since ROS already has most of the mechanisms in place to resolve this on the release side.", "Before installing our ROS dependencies into our docker images we install custom, fake, metapackages. These metapackages are special in that they don\u2019t install anything themselves, they simply \u201cprovide\u201d (in the debian packaging sense) all of the upstream dev dependencies of which ever ROS packages we\u2019re intending to install (plus a subset of ROS packages that we definately don\u2019t need) without actually adding anything to the host system", "Are these REP-127/140/149 metapackages, or the Debian variant? If the latter, are you using ", " for this?", "As an example the ", " deb package, which is in any core installation of ROS, has a dependency on libboost-dev. This boost metapackage is ", " and has massive dependencies of its own, installing it on a fresh Ubuntu docker image installs 586MB worth of stuff even with ", " .", "Yes, that was something I quickly ran into as well.", "That way when we later install the ROS dependencies it doesn\u2019t actually install the dev dependencies. It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "The positive news is that solving this specific problem in ROS packaging more generally is probably much easier than implementing ", " packages in their entirety. In theory all that is needed is to produce ", " packages that just have extra ", " dependencies of their own, no extra files.", "Could you go into a bit more detail about your approach (haven\u2019t had my coffee yet)? Are you referring to extra ROS packages here which package the dev dependencies?", "To a degree this should be straight forward as package.xml already has a mechanism for declaring these dev only transitive dependencies: ", " . The downside of this approach is that it involves fixing dependencies on basically every package in the ROS ecosystem since that\u2019s mainly used to export headers at the moment.", "So if I understand you correctly you\u2019d like to also use it to export \u201cother\u201d build dependencies?", "Thanks for the insight, this is quite a creative way to deal with this (although technically still a work-around ", " ).", "Are these REP-127/140/149 metapackages, or the Debian variant? If the latter, are you using ", " for this?", "Debian variant. When I prototyped it originally I did use ", " although we create them with custom tooling nowadays.", "Could you go into a bit more detail about your approach (haven\u2019t had my coffee yet)? Are you referring to extra ROS packages here which package the dev dependencies?", "No. Ultimately we only care about the debian packages. Certainly one approach to achieve that ", " to have separate dev ros packages but that involves overhauling basically every C++ package in the ROS ecosystem since they all invariably depend on boost in some way.", "So if I understand you correctly you\u2019d like to also use it to export \u201cother\u201d build dependencies?", "Yes, so for example I could say my ", " package has ", " on boost-dev, but ", " on ", ". This would then allow bloom to automatically generate two debian packages:", ": depends on boost-non-dev, contains all of the foo package itself", "\n", " depends on boost-dev and ", "This would only require a fairly procedural update to fix up everyone\u2019s package.xml", "It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "Nice! Do you have any Dockerfiles you could share publicly that demonstrate this approach?", "At the moment the ", " rosdep key is declared as being the dev packages for every given OS: ", ". It should be possible to create a \u201cparallel\u201d rosdep file that declares the non-dev versions of packages which can then be automatically used by bloom to generate runtime packages with the non-dev dependencies.", "This is something that came to mind multiple times when trying to reduce the amount of packages being pulled.", "\nWhile very convenient to have these \u201cblanket\u201d rosdep keys, it is very uncommon for any package to need \u201call of boost dev packages\u201d or \u201call opencv dev packages\u201d. A great deal of space would be saved if packages were to use more specific and targeted rosdep keys. While some stacks like OpenCV and PCL used to have a single/couple deb with everything, they now provide multiple debs and most likely we should leverage them. For some stacks there is already already a narrower set of keys available, the ", " and the ", " one: ", "In an ideal world, all packages would define separately their ", " with the ", " version of the libraries they use and the ", " with the runtime libs they really need to run the package.", "Given the number of packages using (or implicitly relying) on these blanket rules, it would be very challenging to actually apply this to an entire distro.", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Nice! Do you have any Dockerfiles you could share publicly that demonstrate this approach?", " The would be great and could be inspiration to reduce the size of the official ROS images that are ginormous.", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Hear! Hear! A fresh a proper slate, no time like the present to set the president. ", "So in conclusion I think what I\u2019d like to suggest is that we already have support for all the bells and whistles needed to very clearly define the necessary dependencies. And that if we can identify places where separating the build and runtime dependencies would be valuable that we consider simply making separate packages and not try to find ways to automatically split a package and then find ways to refer to those partial packages in downstream packages. We\u2019re generating packages, we have packages. Keeping it one to one is basically the simplest solution.", "This would be a nice way to go about it, but as ", " also wrote, that\u2019s going to be a lot of work (and will result in many more packages as well, which has an adverse affect of build times). For ROS 2 (as ", " suggested) this might be possible still.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "What ", " describes seems like a low-hanging fruit that could allow us to significantly reduce the footprint of ROS 1 (and potentially ROS 2) deployments without requiring too much work.", ": could you provide a little more detail on what you are doing with those metapackages such that we could replicate it? I\u2019m sure ", " would be able to figure out whether we could apply a similar approach to the official ", " and ", " Docker images.", "In an ideal world, all packages would define separately their ", " with the ", " version of the libraries they use and the ", " with the runtime libs they really need to run the package.", "Given the number of packages using (or implicitly relying) on these blanket rules, it would be very challenging to actually apply this to an entire distro.", "Maybe I or someone else can come up with a graphviz/dot script to identify the worst offenders in individual subsets of the packages? I\u2019ll try my luck with ogre/rviz", "There are quite a few tools available for visualising dependency trees (", " and ", " can do it fi).", " indeed brings in approx 580 MB of dependencies (when installed on a bare ", "). ", " is essentially almost singularly responsible for this (207 pkgs for ", " vs 210 pkgs for ", ").", " already lists very specific parts of Boost as dependencies, but as it depends on ", " (as ", " wrote), installing just ", " on a bare ", " image wants to install 655 MB of packages.", "Of this only 8.1 MB is actually placed inside ", ".", "Edit: ", ": I imagine you have some way of figuring out the runtime dependencies of particular ROS nodes / packages and then generate metapackages based on that information. Have you automated this, is this a manual process, or are you relying on package manifests?", " already lists very specific parts of Boost as dependencies", "I guess that\u2019s what I meant by \u201crelying on blanket rules\u201d. While roscpp ", "'s and uses specific parts of boost, it doesn\u2019t declare dependencies on any part of it in it\u2019s ", ", relying on lower level packages to provide all the parts of boost it needs.", "Not sure how the proper subset of boost packages appeared in the control file though\u2026", "Edit: ", ": I imagine you have some way of figuring out the runtime dependencies of particular ROS nodes / packages and then generate metapackages based on that information. Have you automated this, is this a manual process, or are you relying on package manifests?", "Definitely tools helping to extract this information will be very useful", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Hear! Hear! A fresh a proper slate, no time like the present to set the president.", "Yeah getting these split for ROS 2 and allow to install only the nondev version of ROS packages would be great.", "\nLooks like ROS 2 is facing other challenges as the core is completely boost-free and the ", " image ", ", my guess is from the way message packages are currently packaged.", "While very convenient to have these \u201cblanket\u201d rosdep keys, it is very uncommon for any package to need \u201call of boost dev packages\u201d or \u201call opencv dev packages\u201d. A great deal of space would be saved if packages were to use more specific and targeted rosdep keys.", "There is nothing holding us back to do exactly that already. All it takes is a minute to make a PR to update any packages which do use these \u201cblanket\u201d rosdep keys (see below for an example).", "Maybe it\u2019s something that could be encouraged for ROS 2 ?", "I don\u2019t see why ROS 1 packages couldn\u2019t be updated too. I doubt the number of packages (which use these \u201cblanket\u201d rosdep keys) and enough people care about is actually that high.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "ROS Noetic is not even released yet and will be supported for 5 years. I would think that is motivation enough to still update package you care about. Also it doesn\u2019t mean that the maintainer of a package has to do it. Anyone can create a pull request with this kind of change - the maintainer only has to accept it and roll a new release.", "And as mentioned above an example how easy it is to improve the dependency size: ", " uses specific rosdep keys (", ", ", ", ", ") instead of ", " (which maps to ", "). The effect on installing ", " (assuming you have ", " installed anyway):", "While dev/non-dev ROS packages would certainly get us further we could already do something today. So please consider to create PRs to replace the usage of \u201cblanket\u201d rosdep keys with something more specific.", "Our Dockerfiles by themselves aren\u2019t all that useful since most of what they do is install fake packages.", "The generation of the fake packages is the interesting part, I\u2019m not sure how much I can share so I\u2019ll describe it in high level terms. The entire process operates entirely within the realms of Debian packaging, nothing ROS specific.", "When we want to build an image we declare its high level debian dependencies, typically this will be a set of ROS packages (e.g. ros-kinetic-ros-core) and some other libraries that we want to use with it. We then have a Python script which takes this list and traverses the dependency graph. As it traverses the graph it will start to prune off dependencies based off of certain criteria, some of this will be hard coded rules based on package names but mostly it\u2019s automated heuristics. The main one is the one which eliminates packages based on their ", ". While all the ROS packages sit in the ", " section (since bloom doesn\u2019t know any better) the packages in the upstream Ubuntu repos are nicely split up which makes it easy to eliminate all the packages in the ", " section or the ", " section. As we prune off packages we add them to a list of packages to \u201cprovide\u201d in the metapackage we\u2019re going to generate.", "Along with this we also explicitly add certain non-dev dependencies to the images. For example we explicitly add ", " of the non-dev boost packages (which turn out to be not that big) to our images to make up for not having the boost-dev meta-package.", "Here\u2019s an example of the pruned graph for ros-kinetic-ros-core (nodes are only shown the first time they\u2019re found in the graph):", "Maybe it\u2019s something that could be encouraged for ROS 2 ?", "I don\u2019t see why ROS 1 packages couldn\u2019t be updated too. I doubt the number of packages (which use these \u201cblanket\u201d rosdep keys) and enough people care about is actually that high.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "ROS Noetic is not even released yet and will be supported for 5 years. I would think that is motivation enough to still update package you care about. Also it doesn\u2019t mean that the maintainer of a package has to do it. Anyone can create a pull request with this kind of change - the maintainer only has to accept it and roll a new release.", "And as mentioned above an example how easy it is to improve the dependency size: ", " uses specific rosdep keys (", ", ", ", ", ") instead of ", " (which maps to ", "). The effect on installing ", " (assuming you have ", " installed anyway):", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", ".", "This seems to have happened before (from: ", "):", "Before the ROS package ", " was bringing in the boost dependency transitively. Since ", " now uses the system package of ", " boost is not exposed as a transitive dependency anymore.", "Anyway the packages in ", " should state their direct dependencies explicitly anyway and not rely on transitive dependencies for this. Please see ", " for the proposed fix.", "Technically those downstream packages would need to be fixed of course. I just wasn\u2019t sure that was a feasible thing to do in ROS 1 still.", "That was all really. I\u2019m more than happy to start submitting PRs. But I don\u2019t like breaking things and potentially problematic changes like this have been met with hesitation from OR maintainers in the past, so it\u2019s not too strange for us to first look at approaches that would result in similar gains without causing too much trouble \u201cupstream\u201d.", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", " .", " Same reason here.", "\nIn the above example of ", " not declaring its\u2019 boost dependency. It\u2019s likely that this ", " PR broke ", " in the process.", "\nIn the case where the changes are targeting noetic only, it wouldn\u2019t be too much of an issue as maintainers would update their packages before releasing. But for people not having a dedicated ", " branch it may propagate build failures every time one layer fixes the dep declaration.", "If that\u2019s a road we\u2019re comfortable going down for Noetic, I\u2019m happy to help submitting PRs.", "It\u2019s likely that this ", " PR broke ", " in the process.", "Seeing as ", " hasn\u2019t been released yet for Noetic that isn\u2019t much of a problem right now.", "Noetic would seem like the time to start pruning these package manifests.", "re: replacing blanket ", "s: moving dependencies on keys like ", " to ", " et al. would keep builds working. The runtime dependencies would be much lighter though.", "Perhaps a two-step process could be used: first tighten up the ", "s, then change build dependencies.", "And a related PR for ", ":", "As it traverses the graph it will start to prune off dependencies based off of certain criteria, some of this will be hard coded rules based on package names but mostly it\u2019s automated heuristics. The main one is the one which eliminates packages based on their ", ". While all the ROS packages sit in the ", " section (since bloom doesn\u2019t know any better) the packages in the upstream Ubuntu repos are nicely split up which makes it easy to eliminate all the packages in the ", " section or the ", " section.", "Bloom\u2019s patch support could perhaps be used to change the default ", " to more appropriate values. That would require some work, but would seem doable for at least stuff in ", " and similar metapackages.", "But should we perhaps consider adding support for categories to ROS package manifests? Packages could be placed in ", " by default, but if the author/maintainer provides the metadata, Bloom could use that to populate the ", " field.", "RPM did have \u201cgroups\u201d, but those seem to have been deprecated some time ago.", "Here\u2019s an example of the pruned graph for ros-kinetic-ros-core (nodes are only shown the first time they\u2019re found in the graph):", "Entries without a ", " are pruned then, I gather? And the second column shows the cumulative sub total of the packages installed by (and for) that branch of the tree (but the trees only include a dependency if it wasn\u2019t added somewhere else earlier (so ", " shows up with 23MB, but that\u2019s probably because it depends on all sorts of Python dependencies which aren\u2019t shown a second time))?", "With some primitive scripting and a very manual process I\u2019d arrived at a similar set of packages, but I hadn\u2019t gotten ", " down to ", " MB yet.", "Perhaps a two-step process could be used: first tighten up the ", " s, then change build dependencies.", "I started part of the process and opened PRs to ", " and its\u2019 dependencies to get rid of the blanket ", " key (in both build and exec). This was pretty straight forward (though not automated) and did not include any audit/tightening of the runtime dependencies. This allowed to propagate the ~300MB space gain to the all of ", ".", "re: \u201cfirst tighten up the ", " s\u201d: Migrating all these packages to format 2 or 3 would also allow us to leverage the ", " versus ", " and tighten them accordingly. Bloom could then use that information to segregate ", " vs actual runtime dependencies.", "And a related PR for ", " :", "Great !", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", " .", "Assuming that almost all packages depend on e.g. ", " the only chance to breaking downstream packages is if all packages including and below ", " are switching from ", " to something more specific for an already released distro. If only one package limits the change to Noetic (since it already needs a separate branch for other reasons) downstream packages will still have ", " available in Kinetic / Melodic. Therefore I think the chance for regressions is fairly low.", "Perhaps a two-step process could be used: first tighten up the ", " s, then change build dependencies.", "What would be the advantage of that? The first step already affects downstream packages. The second step is local to the package and is easy to test and doesn\u2019t affect other packages - so why should it not be done at the same time?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS has (very) few leaf packages: many ROS packages export libraries, headers and other resources that get consumed by others as part of a build. This means that when a package is installed, it\u2019s most likely installed because it\u2019s being used by another. Main contributor to package size are the libraries (this is an assumption), which would always be present and thus having a separate runtime and ", " package doesn\u2019t offer much net gain.", "it\u2019s not necessarily something that can be automated: creating proper Debian packages can be ", ". Part of what makes it complex is knowing what should go where, and this is not always obvious either. A single source package could spawn multiple binary packages, and the rules that govern this are typically largely hand written.", "gcc", "perl", "python3", "Before it pulled in ~500MB", "Afterwards it only pulls in ~200MB", "Before it pulled in ~500MB", "Afterwards it only pulls in ~200MB"], "url": "https://discourse.ros.org/t/generating-dev-and-runtime-artefacts-from-ros-packages/12448"}
,{"title": "Announcement: Universal Robots launches ROS driver", "thread_contents": ["Hi ROS enthusiasts,", "We are delighted to announce, that we have ", "!", "Robot Operating System (ROS) is widely used within the industrial research and educational field, for exploring robot technology, or for developing advanced systems. Since the beginning of UR, clever minds in the research field have developed 3rd party drivers, enabling the ROS community to communicate with UR robots for various purposes. Impressively, more than 200 different forks of UR-compatible drivers filled up the ROS landscape. However UR has never supported nor developed any of these drivers. Until now!", "With so many community developed ROS-drivers in the landscape, it became difficult to find out which fork had the latest features, supported the right version of your UR software, or utilized the UR functionality in the best way. To mitigate these issues, and ensure that a solid driver utilizing all the best feats of the UR was available, we partnered with the German research institute ", " to cooperate on the development of an ROS driver that is backed by UR to ensure its stability and robustness.", "The new ROS driver provides a number of key benefits:", "The ROS driver is provided through UR\u2019s GitHub account, which allows for open collaboration within the ROS community.", "\nThere are two modes of operation for the driver:", "Go check out the new ROS driver today;", "\n", "Driver enabling ROS operation of UR robots. Contribute to UniversalRobots/Universal_Robots_ROS_Driver development by creating an account on GitHub.", "\n", "Furthermore, you can check out the two accompanying URCap implementations;", "\n", "Example implementation of how to use ROS driver on-demand in a URCap. - UniversalRobots/Universal_Robots_ExternalControl_URCap", "\n", "\n", "Utility to forward the UR Tool Communication Interface (RS-485) to a ROS remote PC. - UniversalRobots/Universal_Robots_ToolComm_Forwarder_URCap", "\n", " wrote up a good summary comparing the ", " to the new driver:", "\n", "\n", "If you encounter issues or bugs using the ROS driver, please use the Issue-reporting system for the respective GitHub repository.", "\nShould you have questions to using the ROS driver, or general support requests, please use the ", " in the ", ".", "Happy development,", "Your Universal Robots team   ", "Great achievement, lets hope that other manufacturers follow!", "The driver includes the specific robot\u2019s factory calibration data for improved accuracy.", "Does that include a way to calculate ik for the calibrated robot in an efficient way? We once had to resort to seeding a numerical ik-solver (trak-ik) with the solution from ikfast for the ideal robot as build quality was so bad.", "Does that include a way to calculate ik for the calibrated robot in an efficient way? We once had to resort to seeding a numerical ik-solver (trak-ik) with the solution from ikfast for the ideal robot as build quality was so bad.", "The xacro itself is updated (via a ", " file), so anything that uses the urdf to initialise the IK solver benefits.", "Somewhat off-topic, but:", "Great achievement, lets hope that other manufacturers follow!", "Congratulations to FZI & UR of course for this release, and we\u2019ve supported them gladly, but there are already quite a few (industrial) robot manufacturers that have released publicly supported ROS drivers. I won\u2019t name them here though, as this it not the place, but it is only fair to acknowledge their support as well.", "Ok, but nothing ur specific that could work in cases where ", " fails?", "nothing ur specific", "I\u2019m not sure what you mean by this, but there are currently no updates to ", " to take the calibration into account, if that is what you were asking.", "That would not be impossible though, I believe. But out of the scope for the FZI & UR collaboration at this point. Community contributions are most likely welcomed.", "I\u2019m not sure what you mean by this, but there are currently no updates to ", " to take the calibration into account, if that is what you were asking.", "That would not be impossible though, I believe.", "Well I assume they have something akin to ", " implemented on the controller which works event though the calibration might (?) break geometric assumptions made when deriving ", "? But that is most likely considered IP I guess.", "This might be a question best asked on the issue tracker.", "I can\u2019t really understand what you\u2019re writing here, but that\u2019s probably on me.", "To be clear: all control is still joint space based. There is no Cartesian interface (yet). It\u2019s a standard ", " based ", " which exposes the 6 revolute joints of the robot over RTDE in position mode. There is an experimental PR adding a velocity interface and the secondary and primary interfaces are also still used for some things (but not motion).", "That is at a very high level. As always there are many more details influencing this, but for those I\u2019d suggest posting either on the tracker or the UR support forum (which is behind a login).", "What ", " wrote applies to FK based on joint states: that will now take per-robot calibration into account, leading to orders of magnitude more precise results. On-controller compensation for things like gravity and link deflection is not part of that.", "However, as MoveIt (fi) for planning Cartesian goals is also using the same IK solver, it should affect control of the robot\u2019s EEF and positioning of that as well (as more accurate IK -> more accurate joint space goals).", "But I\u2019ll leave the rest to ", ", UR and FZI ", "Thank you  ", " for summing this up very well. Our focus was indeed on joint-based control and correct forward kinematics.", "Currently, we don\u2019t plan further developing ", " though.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\nFor ROS users, the driver provides an easy and plug\u2019n\u2019play use of UR robots.", "\n", "\nThe ROS driver utilizes all the key features of the UR robot to achieve state of the art performance.", "\nIt provides an industrial grade interface to the best extent allowed by current ROS practice.", "\nThe driver includes the specific robot\u2019s factory calibration data for improved accuracy.", "\n", "\nThe driver is built on stable and versioned API\u2019s, and is integrated into UR\u2019s continuous testing engine for assured quality.", "\n", "\nThe driver will remain open-source and rely on future community contributions.", "\n", "\n", ": easy programming research, embedding in OEM systems.", "\nIn this mode, the driver works like classic ROS drivers, assuming full remote control of the robot without interaction on the teach pendant.", "\n", "\n", ": Developers of vision systems, force/torque control, picking algorithms or similar products, which integrate with ROS.", "\nIn this mode, a sample URCap enables ROS control only where it is needed in the UR program. In this way it enables easier integration with ROS compatible devices into UR applications. It further enables commercialization of cutting edge technologies based on ROS."], "url": "https://discourse.ros.org/t/announcement-universal-robots-launches-ros-driver/10952"}
,{"title": "World ROS-I Day - July 11", "thread_contents": ["The ROS-I global community is going to be putting together the first World ROS-I Day, in the spirit of World MoveIt! day, but focusing on the repositories relevant to ROS-Industrial. We have 3-4 sites already signed up to host, as well as participating in virtual collaboration rooms, with continued participation from Asia, handing off to the EU, then finally to the Americas.", "We hope we can find some folks to jump in to do some clean up with us on the various repositories and packages, working on a predefined list of issues. Where hardware is needed for access we may seek particular collaboration partners, or have to table certain issues, though we are open to creative ideas.", "We will keep info up to date here as we work out the details, but we look forward to hosting this inaugural event and learn what we can do when we coordinate our efforts!", "Thanks in advance!", "Matt R.", "\nSwRI", "\nROS-I Consortium Americas Program Manager", "As a follow up to the above, ROS-I development leaders have started working up a list of repositories that will be the focus of the inaugural World ROS-Industrial Day on July 11th (WRID18). This will help with making for a manageable scope and allow for tracking of progress throughout the day.", "The repositories that are being included for WRID18 at this time are:", "On the ", " organisation:", "Regarding the ", " repository: this edition of World ROS-Industrial Day we will not be addressing anything related to drivers, pending engagement with Universal Robots. While we are aware of the fact that the current situation is far from ideal, the good news is that recent engagement with Universal Robots will enable improved driver performance in the future. As such, we will focus on the ", " packages in ", ", such as the robot support, kinematics, MoveIt and Gazebo packages. if there are members of the community that seek to improve current drivers and their performance, they are welcome to do so, but please contact the WRID18 organisers to discuss this.", "Outside of ", ", over in ", ":", "The maintainers of each of these repositories are in the process, and still in the process of being invited, to review issues within these repositories and label them as candidates for this event.", "Candidate issues are labelled with ", ", and later with a label to indicate the \u201clevel of effort\u201d that would be needed to address the issue so participants can choose tasks to work on based on this.", "Finally, all open tasks will be aggregated on a Github organisation-level project board which will be updated live during WRID18 to clearly show progress during the day.", "Obviously, if there are other issues you feel are worth considering, or repositories to be included, feel free to add to this thread!", "Thanks and looking forward to a great day on July 11th!", "We also have event info over at: ", ".", "Regards,", "Matt R.", "ROS-I Americas", "The ROS-I global community is going to be putting together the first World ROS-I Day, in the spirit of World MoveIt! day, but focusing on the repositories relevant to ROS-Industrial. We have 3-4 sites already signed up to host, as well as participating in virtual collaboration rooms, with continued participation from Asia, handing off to the EU, then finally to the Americas.", "Update!", "For those in the South Central Texas region, the Southwest Research Institute ROS-I Development team will be hosting an in-person World ROS-I Event from 8AM till 5PM at LiftOff located at 2014 S. Hackberry St, San Antonio, Texas 78210. Come and interact with local/regional developers, and work issues to improve the ROS-I repositories and continue to foster collaboration around industrial use-cases. We will have refreshments, lunch, and some souvenirs at the event.", "Thanks!", "Matt R.", "\nROS-I Americas Program Manager", "Another update!", "Issue board is coming together nicely!", "GitHub is where people build software. More than 28 million people use GitHub to discover, fork, and contribute to over 85 million projects.", "Streaming and Chat Services for Global participating in support of World ROS-I Day will be at:", "Streaming - ", "Live Chat Room (IRC): ", "Subject to change pending load testing.", "Hi, I organise ", " in Delhi, India. I was hoping to hold a meetup for ROS-I day. I already have a venue and logistics handled. Please let me know how to proceed?", "That is great to hear! Just jump in the virtual room and the IRC chat, you can also consult the GitHub issue board. Our friends in Singapore, that hub ROS-I AP, will be kicking things off. They should already be up and running, and moderating the chat over on IRC as well. ", " may be able to give more guidance to how they will be moderating the issue board during this time! Thanks for your interest and support of open-source robotics for industry!", "We would like to both welcome and thank Open Robotics for agreeing to host those in the Mountain View area looking to contribute to World ROS-I Day in a \u201cface to face\u201d fashion. We have added their location over at ", ", you can reply here, or tweet at Open Robotics for specifics as to when their doors are open! Thanks again ", " and the Open Robotics team for their continued support of ROS-I and open-source robotics for Industry!", " hi, could you please help with how will the issue board be managed? I have created an ", ". Also, I looked at some issues and shortlisted a few that I believe our community here can handle.", "Hi ", ", please log into the IRC chat / stream tomorrow. From there, we can begin assigning tasks from the issue board to your attendees that they wish to address. Looking forwards to it!", "Thanks, ", ", for the great initiative.", "We had a blast in Stuttgart, killed some candies and hopefully some bugs as well:", "\n", "\n", "Thilo", "Thanks to everyone who contributed to World ROS-I Day! We are doing a lot of \u201cpost-processing\u201d to be able to present what went down. We look forward to working the PR\u2019s and continuing to engage around the world!", "Please feel free to reply to this thread with ideas to improve this event and how we can make it more accessible/efficient to get work done!", "Thanks again!", "Matt R.", "\nROS-I Americas", "\nSwRI", "Great pic! And great working with the IPA team of course! ", " ", "Hey Sorry I wasn\u2019t able to reply, I was on leave, but looks like you managed to join which was amazing!", "Thanks for your good work!", "Cheers,", "\nMin Ling", "Hi!  I just have a question about the bar graphs found on ", ".  Are these statistics for July 13th and July 16th only? Or are they for a period of time?  Is there any reason those dates were chosen?  Thanks!", "The intent was to make sure we caught the activity on the 11th, and then try to get a sense if the activity sustained, since issues were assigned and still in process, so we wanted a snapshot for sustained activity. At least that was the goal for that particular write up.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " (except driver infrastructure)"], "url": "https://discourse.ros.org/t/world-ros-i-day-july-11/5087"}
,{"title": "Quality guide for ROS2", "thread_contents": ["You can find a ", " now. A point to note is the dependency of various quality attributes on general design considerations which will be addressed in a ", ". Both guides use a pattern format for efficient knowledge communication/transfer.", "Do you have any particular direction in mind you want to take these pages? Are you looking for community contributions?", "Sure. I encourage everyone to contribute. I just wanted to get something started when I stumbled over these first patterns a while ago.", "Thanks, certainly a useful effort. I am reading through this.  I think the goal appears somewhat similar to ", " (which is a project we are starting to work on). A number of thoughts/questions.", "It is not clear (yet) for me how ROS2 wiki will be organized (is it going to live on github?).", "Perhaps ", " and ", " should be maintained as sister websites linking-to-each other. OTOH, I think there will probably be large aspects of ROS quality that will be independent of ROS1 vs ROS2 division.  So I am a bit lost on how should we organize ourselves. I am particularly concerned with duplication of work, maintenance effort, and also decreased visibility if we distribute too much.", "Do you have any particular direction in mind you want to take these pages?", "To stick to a pattern format for a single \u201csuggestions/recommendation\u201d is the only thing one should consider right now w.r.t. contribution. (More and more books adapt to this format\u2026 even if you don\u2019t always recognize it. E.g. all the \u201ccookbook\u201d style books implement a lightweight kind of easy to read pattern format.) As soon as a reasonable aount of patterns have been collected they can be categorized for easier navigation. In my experience there are usually more than 1 categorization scheme but this can be addressed e.g. if one uses different categorization schemes as table of content style referencing. E.g. there could be a categorization scheme according to \u201cquality attributes\u201d (reliability, security, etc.) which could reference some same patterns as a categorization scheme \u201csoftware development lifecycle\u201d (requirement engineering phase, design phase, implementation phase, etc.).", "Thanks, certainly a useful effort. I am reading through this.  I think the goal appears somewhat similar to ", " (which is a project we are starting to work on). A number of thoughts/questions.", "Thanks. My intent for a ROS2 specific quality guide was to start something which is from a technical point of view specific to ROS2. I also wanted to avoid to create a second ROSIN quality hub website with a lot of background information because it could prevent potential readers (especially developers!) from reading more than the first view lines until they realize \u201cthere\u2019s nothing technical in there which I am interested in\u2026 I better watch out for some tutorials\u201d.", "It is not clear (yet) for me how ROS2 wiki will be organized (is it going to live on github?).", "I don\u2019t know as well. I placed it there because it was most reasonable for me back then when I started to add the patterns. However from a contribution/maintenance point of view ", " (not straight-forward to add images/diagrams, etc.).", "Perhaps ", " and ", " should be maintained as sister websites linking-to-each other. OTOH, I think there will probably be large aspects of ROS quality that will be independent of ROS1 vs ROS2 division.  So I am a bit lost on how should we organize ourselves. I am particularly concerned with duplication of work, maintenance effort, and also decreased visibility if we distribute too much.", "Yes, from my perspective a single entry point for both, ROS1 and ROS2 would be ideal. There is already divergence of various sources already (ROS1 wiki QA sites, ROSIN quality hub) which we should somehow cross-reference and merge if required.", "Another question is how to address different ROS2 user groups/domains packages are used in, applications packages are used in w.r.t. recommendations for quality related activities. (A lot of recommendations are just reasonable for the industrial domain.)", "Recommendations could e.g. be coupled to the ROS industrial package ", ": Experimental, DEVELOPMENTAL, PRODUCTION. (", " is e.g. EXPERIMENTAL.) This would allow to recommend activities with relation to the development state of a package.", "ADDITION:", "In my experience it is especially challenging to define activity recommendations w.r.t to the application software is used in. The higher level SW design and the idiomatic level can be heavily impacted by this. Developing a good approach which enables iterative migration to avoid the need for major re-factoring of software is challenging. We can probably discuss about this challenge in the upcoming \u201cROS Quality Assurance Working Group meeting - April 2018\u201d?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/quality-guide-for-ros2/3926"}
,{"title": "ROS Quality Assurance Working Group \u2013 May 2018 Meetings Minutes", "thread_contents": ["The implementation work of \u201cMake ROS packages quality visible\u201d has started. It will occur outside the group meetings and we will provide updates on the progress at each meeting and in the quality discourse. We have 2 volunteers so far for this work. Anyone who want to help, please let me know?", "\u201cAppoint ownership\u201d initiative has been discussed.", "\n1. It has been agreed that people who do the implementation are by default the owners of the initiative.", "\n2. \u201cOwnership\u201d is ensuring the continuity of the initiative.", "\n3. ROSIN FTPs were suggested as an option to finance part of the \u201cownership\u201d.", "\u201cEnergize the code review process\u201d initiative has been discussed.", "\n1. This initiative is to \u201cenergize\u201d the current code review process. The intent is to review the current process, make it more efficient and promote it.", "\n2. The current challenges with the current process were discussed:", "\n3. For core packages, it\u2019s a resourcing issue. Not enough volunteers available to review the code.", "\nSame issue for high level packages. Very small portion of the community shows interest in the review. It\u2019s usually just the maintainer who does the work.", "\n4. Two options have been discussed: (1) a dedicated Wiki page or portal for code review with a reward system for the reviewers (similar to the Karma system), (2) Each time someone submit a pull request, he/she is asked to review another pull request.", "\n5. The process also need to be more efficient", "\n6. Most participants think that a reward system linked to the review effort is a good idea.", "Thanks for the minutes ", ".", "Each time someone submit a pull request, he/she is asked to review another pull request. I.e. Your pull request doesn\u2019t get reviewed until you review someone else pull request.", "I don\u2019t believe option 2 was this strict: the suggestion was to ", " PR submitters to also review one, but not to make it a requirement.", "That would probably be too strong of a condition and deter people from contributing.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-quality-assurance-working-group-may-2018-meetings-minutes/4877"}
,{"title": "[URDF-NG] Next-generation robot descriptions", "thread_contents": ["There is some ", " on how to improve the current URDF format and parsers to fill some immediate needs.", "This topic is for questioning everything and discussing what the perfect way to share and reuse information about a robot might be.", "What needs are there? What assumptions can we make?", "What is this robot we are describing? What information do we need to share?", "What counts as a robot really? For the purposes of this discussion a robot is any physical device whose interaction with the world is sensed, processed, and acted upon in an intentional manner.", "For instance, some robots can be described as a set of rigid bodies and constraints on those bodies. Others include soft bodies or flexures. Nonlinear constraints (such as joint limits) and losses (transmission inefficiency) are often known or estimated well enough that we want to include them in the description. Sometimes the constraints are simplified to the joints between bodies and described in the manner of a graph.", "Myself, I use URDF only when forced to by the tools needed for a particular project. I prefer an embeddable scripting language (lua, python) for the description. Cleaner than XML for editing and one has callback functions for non-linear or heuristic information. If you are unfamiliar with this approach,", ".", "Sounds great right? However I don\u2019t get any of the benefits of a well designed and dessiminated format.", "I see enough frustration in the world with the limitations of URDF that it\u2019s clear that there is a need for something more. What does the next generation of robot description look like?", "I have a couple of observations.", "Does the use of a scripting language provide any other benefits, such as programmable features? That would be an interesting concept.", "The lack of tools for creating and editing URDF files in a more suitable interface is the problem, rather than XML, in my opinion.", "I agree that there are nicer, more suitable interfaces for creating and editing the robot information. However, ASCII text is a format that almost everyone on almost every platform can edit. No one has to wait for the editing tool and all it\u2019s dependencies to be brought to their platform.", "A YAML format for URDF would probably be as readableandcwritable as the RBDL format you mentioned.", "I prefer YAML to XML for static data files as well. The RBDL uses the Lua programming language. (The RBDL format is Lua).", "Does the use of a scripting language provide any other benefits, such as programmable features? That would be an interesting concept.", "Yes. If the definition file is used statically and only loaded from disk, then the language can be used to encapsulate concepts, deal with repitition nicely, and increase readability. For instance, if one wanted to specify their link parameters in terms of of Denavit-Hartenberg parameters instead of xyz-rpy, one would just insert a small function at the top of the file to do the conversion.", "If the definition file is used dynamically, then one can write functions in the programming language that are called by the code using the file. For instance, one could have a joint_friction(joint_number, joint_angle, joint_velocity) hook function defined in the config file that would get called during simulation steps.", "It would be nice to have URDF\u2019s in python, like how dynamic_configure files work.", "\nThat way it is not yet another language to learn.", "\nThe robot description would be much cleaner, you could support generating the description from parameters. This is useful for highly configurable robots such as the turtlebot, which currently hack this functionality into the existing xacro/urdf system.", "We should have the gazebo people and the ROS-I people as part of the discussion.", "\n", " and I been discussing this topic for a while.  It would be good to have one format for all of the ROS / Gazebo projects.  SDF was something that we thought could be the format.  There is a long thread on this on the old google groups.", " You can see how creating URDF\u2019s in python can work in practice with ", ". I wrote when I needed to deal with non-trivial URDF\u2019s. I find working with XML directly (editing the files) unpleasant. The idea that one would write a python script to parse and execute a language embedded into xml (ie xacro), rather than just using python directly, confuses me to no end. But I\u2019ve seen colleagues invest a lot of time and frustration using xacro to try to solve the problems they have with the URDF format so I guess it makes sense to some.", "Though perhaps I\u2019m misunderstanding you. Do you mean instead that you\u2019d like to see a dynamic_configure API? A robot_model node that you can dynamically update using dynamic_configure instead of a static xml text robot_model parameter?", "Any chance you can link that thread here? Any key outcomes of the thread?", "EDIT: I think ", ".", "EDIT: It seems the general consensus is to standardize on the SDF format. That seems like a sane iteration of the current situation with ROS and Gazebo.", "That looks awesome, thanks for the link.", "Something like that is what I was looking for. I would like to see something like that become the standard way to do robot description.", "EDIT: This is a response to your first message, I didn\u2019t see your second one.", "I started this thread hoping for some discussion on, well, next generation robot descriptions. Harmonizing current formats (URDF/SDF/etc) is a really important discussion for the community; but that\u2019s really about cleaning up the current generation.", "Anyone interested in the next generation?", "That\u2019s why I brought up the Python description format.", "ROS2 replaces XML launch files with Python ones, I think that it makes sense to do something similar with URDF the other source of XML pain in the ROS ecosystem.", "Looks like your stuff generates XML and sends it out to stdout and eventually into the robot_description parameter to be used by robot_state_publisher.", "I think for the next generation of URDF, I would like to get rid of the XML entirely (if possible) and have the Python code actually act as robot state publisher and send out the description data on a latching ", " topic, as well as tf data.", "As for the actual format, I like odio_urdf but I haven\u2019t looked at it very closely.", " ", "Can we get the Gazebo team on this thread?", "Hopefully the next gen of robot descriptions is the same between ROS2 and Gazebo.", "I believe they\u2019re already listening in.", "This category grew out of the ROS Live event with robot description formats", "\nas the topic.", " has put together a nice recording as well as lots of links can be", "\nfound at: ", "I\u2019ll call out specifically the ", "\nIf you weren\u2019t there or don\u2019t have time to listen at the moment.", "Tully", "ROS industrial needs to be part of it.  I looped Paul in on this thread.", "We started a document that some of the key players commented on.", "\nProbably should push it a bit harder now if there is interest.", "This is a big", "Looks like your stuff generates XML and sends it out to stdout and eventually into the robot_description parameter to be used by robot_state_publisher.", "Yes, that code is specifically for the URDF way of describing robots. The internal hierarchy of objects mirrors the URDF elements, and each object has a string representation that matches the URDF XML.", "I think for the next generation of URDF, I would like to get rid of the XML entirely (if possible) and have the Python code actually act as robot state publisher and send out the description data on a latching /robot_description topic, as well as tf data.", "I\u2019m having a hard time picturing why one would want to do this.", "There are many uses for a robot description that have no need for ROS IPC.", "Also one could argue that the state of the robot should not be part of it\u2019s description. The ", " and ", " documentation has some nice discussion of this, though with a focus on simulation and the simbody implementation.", "I\u2019m not sure python would be an ideal choice for describing a robot. Python was not designed to be a data exchange language. In addition, web applications, which can parse XML, would have a hard time with python.", "Hi all, it seems that there is interest in figuring out what to do ASAP for ROS2, which was not the point of this thread. I\u2019ve ", " and I\u2019ll link in the external discussions mentioned above so there is a good start for that.", "It will be cleaner and clearer to ", ".", "Describing flexible link robots? Reconfigurable robots? Soft robots? Able to wrap your head around configuration languages, or why a shared object file with an API might make a good reuseable, shareable robot description? Continue below.", "I\u2019ve always found a lot of value in a non-interpreted interchange format, like URDF or SDF or even HTML or JSON. So, for me I don\u2019t like the idea of a description format that exists only in an interpreted form (like as Python or Lua).", "It\u2019s convenient to have these tools when writing the descriptions, but very inconvenient when trying to consume the description in many different places. I think a good middle ground is like what we have with xacro + URDF. You might have issues with either xacro or URDF, but I think a good idea that came out of that is to pair a well established interchange format with \u201ctemplating-like\u201d tools to make it easier to create the description by hand or programmatically. This is reflected in things like web development, where the interchange format is HTML, but commonly the server will use a scripting language to generate the otherwise tedious to specify XML content.", "We were leaning towards SDF during our discussions.", "\nand at Modbot we are all about flexible / reconfigurable robots so that", "\npart interests us.", "I\u2019ve always found a lot of value in a non-interpreted interchange format, like URDF or SDF or even HTML or JSON. So, for me I don\u2019t like the idea of a description format that exists only in an interpreted form (like as Python or Lua).", "It\u2019s convenient to have these tools when writing the descriptions, but very inconvenient when trying to consume the description in many different places.", "I think a key part of an interchange format is that it\u2019s content is very clear and easy to grasp and process by all. A simple declarative text file certainly achieves that.", "The strength of that approach is that when in inevitable need for dealing with repetitive and formulaic declarations arises, you can use xacro and I can use assembly and everyone is happy.", "So I agree there is a strong case for a non-interpreted format.", "Can you elaborate on what about an interpreted/script interchange format worries you? The second you touch ROS (or xacro) you are loading the python library, so the memory/processing overhead can\u2019t be the issue. Configuration scripts usually have a minimum of \u2018code\u2019 in them (cmake being an ugly exception) and thus a minimum of patterns or idioms that are tricky for the new user, but perhaps that is the concern?", "It\u2019s convenient to have these tools when writing the descriptions, but very inconvenient when trying to consume the description in many different places.", "I think a good middle ground is like what we have with xacro + URDF. You might have issues with either xacro or URDF, but I think a good idea that came out of that is to pair a well established interchange format with \u201ctemplating-like\u201d tools to make it easier to create the description by hand or programmatically. This is reflected in things like web development, where the interchange format is HTML, but commonly the server will use a scripting language to generate the otherwise tedious to specify XML content.", "Is this what you would choose if you had a blank slate going forward? Is there anything you would want to see change or improve?", "Can you elaborate on what about an interpreted/script interchange format worries you?", "If the data exchange (previously I used interchange, which I think is not the correct terminology now) format is a script or interpreted then everyone who uses it needs to have an appropriate interpreter loaded and any context that might be used in the script. For example, RBDL requires everyone that needs the description to have a Lua interpreter. Lua is by no means a heavy requirement, but there are plenty of platforms where that would be inconvenient at best, for example loading the description in a webpage (couldn\u2019t find a Lua vm in javascript) or iterating through the links in Matlab (couldn\u2019t find one here either), neither of which have a native way to hook into a lua interpreter. You can always parse the description lua file in C++ and then serialize the in-memory \u201cobject\u201d to send it to a browser or to Matlab, but then you just have an unofficial data exchange format.", "Even if you manage to get an interpreter at each place where you want to use the description then you\u2019re likely to need some additional context to be available at each of those places too. Because the description script can have logic in it, it\u2019s a natural extension to want to use command line arguments or environment variables or something else to modify the resulting description. You could limit this functionality, perhaps RBDL does, but this is a core utility that people use in xacro and really need to make it easier to describe robots with lots of optional configurations. If you have no static intermediate format, then this context needs to go along with the description script so it can be given to the interpreter at each end point where the description is consumed.", "The second you touch ROS (or xacro) you are loading the python library, so the memory/processing overhead can\u2019t be the issue.", "Why do you need python to touch ROS? I can write a URDF without using xacro and load it with a pure C++ library. In fact I\u2019d argue that having an intermediate format that is data only would considerably reduce cpu and memory overhead compared to an data exchange format that is interpreted, especially when being consumed in several places.", "Is this what you would choose if you had a blank slate going forward? Is there anything you would want to see change or improve?", "I would prefer an explicitly defined data exchange format for the description that is data-only (probably XML or JSON) and then pair that with a prefered templating interface (xacro, erb, empy). That way consumers of the description can remain as simple as possible and the developers who are creating the descriptions have tools to help them with authoring. It would be great to have a \u201cprefered\u201d or \u201cprescripted\u201d template engine to use, that way tools could be built to understand them, but by leaving that unspecified developers get the freedom to use whatever template engine they prefer.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Save time by not having to think about and create a project specific description. (Accompanied by the risk that the description designers made provisions to handle all aspects of a description that your project needs.)", "Share a description for reuse by others. You and the peers you are sharing with must agree on the meaning of some subset of the information in the description.", "Share a description between different softwares. By using a single description, errors introduced by data entry mistakes are eliminated. ", ".", "what else?", "Easy to create and edit across all platforms.", "Minimize difficulty in integrating the description into new codes/languages/platforms.", "A starting set of description information that is useful and whos meaning is agreed upon.", "The ability to be extended as needed with project specific info.", "The ability to be formally extended by the community as progress maches forward.", "The descriptions are primarily created by people and the one tool in common across all platforms is the text editor.", "The descriptions are primarily consumed by software.", "Any given description is usually project specific (No standard object model)", "Only my internal software reads the description, so not a lot of reuse between software.", "No reuse between peers either. As a consultant an annoying amount of my work is confidential.", "XML was intended as a language primarily for computers to work with, with human readability as a bonus. The lack of tools for creating and editing URDF files in a more suitable interface is the problem, rather than XML, in my opinion.", "A YAML format for URDF would probably be as readableandcwritable as the RBDL format you mentioned."], "url": "https://discourse.ros.org/t/urdf-ng-next-generation-robot-descriptions/222"}
,{"title": "Support the PLY geometry format in URDF?", "thread_contents": ["Hello! I posted over ", ", but this place seems to be a bit more active.", "At the moment, the URDF spec only seems to mention Collada / DAE and STL file formats for the mesh node. STL, however, doesn\u2019t support a very robust set of data and Collada is pretty inefficient in terms of file size and pretty complicated to export. What are peoples thoughts on including something like the PLY format as an option for a mesh node file format?", "Thoughts? Thanks!", "\nGarrett", "Without being familiar with 3D modle formats, I\u2019d say someone would need to give the point of view from the libraries that use these files. I\u2019m thinking of:", "If they already have support for it, sounds great! If they don\u2019t, a newer version of URDF should at the same time support these big players I guess.", "Sounds reasonable. I\u2019m less familiar with the ROS codebase, myself, but I\u2019ve noticed some references to ", " around the ROS repos, and ", " (in the bullet physics repo?) has discussions around gazebo using assimp to load models, as well.", "If that\u2019s true, then this may be as simple as extending the spec to include another model format. I\u2019d almost suggest just allowing anything that assimp can load (if it\u2019s actually that easy), but I actually like the constrained list of formats because that makes it easier to consume the format on other platforms (we\u2019re visualizing these models using THREE.js and javascript, as well). To me, all that\u2019s missing is a simple to export, terse, binary geometry format.", "Thanks!", "\nGarrett", "It will not help to improve the state-of-the-art wrt supported features, but in some testing I did a few months ago it turned out that you can use just about anything that Assimp supports for meshes in urdfs. In the end it all gets converted to vertices & faces anyway, so as long as Assimp can load your mesh it should work.", "IIRC I even used ", " at some point.", "Note that this is really just about meshes, so none of the \u2018advanced\u2019 features formats like Collada and FBX support are used (bones, skins, kinematics, simulation properties, etc).", "I did not include Gazebo in these tests, so that may be different.", "Collada is pretty inefficient in terms of file size", "Collada has a variant where the whole file is compressed using gzip, the extension changes to ", " and it typically results in 90% reduction in file size. It supports all of the things you mentioned, but it is definitely not meant to be human readable and is not straightforward to export to without a library doing it for you.", "in some testing I did a few months ago it turned out that you can use just about anything that Assimp supports for meshes in urdfs", "Great! That\u2019s more or less what I was expecting. So it sounds like this is more a matter of documentation than anything else. Unless there\u2019s a serious reason not to expand the documented mesh formats, this could be as simple as just adding an extension to the list on the URDF wiki page!", "Collada has a variant where the whole file is compressed using gzip, the extension changes to .zae and it typically results in 90% reduction in file size.", "I\u2019d heard of ", " before, but it also wasn\u2019t listed on the URDF wiki as a suitable format. I\u2019d say it\u2019s worth adding that, as well, if it\u2019s considered a valid format. In a quick test, it looks like PLY is able to get down a bit lower than the zipped collada file, but I understand what you\u2019re getting at.", "The reason I\u2019m asking for documentation updates is because, like I said, we\u2019re looking into generating URDF models and visualizing them in the browser and other platforms outside of ROS because it\u2019s a nice, documented, and standard format that\u2019s easy to parse and export. I feel that the density, robustness, and simplicity of use of the PLY format scratches that last itch that isn\u2019t afforded by the other two mesh formats. I\u2019m open to other format suggestions that scratch that, too, though!", "Overall I\u2019m pretty happy with URDF \u2013 I\u2019m just trying to help the standard grow in a couple ways that accomodate my use cases! Especially if there\u2019s not a lot of work investment to make it happen.", "Thanks for your feedback!", "\nGarrett", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Supports a binary format type, which enables to smaller file sizes.", "Optionally supports texture coordinates so the ", " tag can be used for a surface texture.", "Optionally supports normals and vertex colors (and any other custom vertex attribute) for other color and shading.", "Extremely simple to write an importer / exporter for.", "\n", ". Haven\u2019t found much.", "\n", ". Their tutorial just talks about exporting to .dae/", "Collision computation libraries like ", " which I believe is used in MoveIt."], "url": "https://discourse.ros.org/t/support-the-ply-geometry-format-in-urdf/4325"}
,{"title": "[TB3] TurtleBot3 Tank", "thread_contents": ["Hi ", "TurtleBot3 has sprockets that enable anyone to easily change from general wheels to caterpillars.", "\nIf you want to replace it, you would just pull rubber wheels out of the sprockets and assemble pieces of caterpillars on it.", "\nIt makes to efficiently drive on piled woods or books rather than using general wheels.", "\nFurthermore, TurtleBot3 can ride on the off-road conveniently. (e.g. debris, grass, bushes and even rocks!!)", "\nThis example is made of two Dynamixels (XM430-W210-R), two Dynamixels dummy and 72 pieces for caterpillars. (The One side of caterpillars needs 36 pieces)", "Thanks.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-turtlebot3-tank/1169"}
,{"title": "OpenCR and encoders", "thread_contents": ["I want to use my OpenCR board to interface with two quadrature encoders for odometry and wanted to get your thoughts on different approaches.", "I did some searching and the STM32F7 is capable of reading quadrature encoders (I\u2019m not sure how many yet) but this may conflict with the current firmware settings on the board.", "\nAnyone know if the Low-power timer (LPTIM1) is being used for anything?", "\n", "My other option would be to use external counters and interface to them via SPI\u2026", "\n", "Other methods I should consider?", "I\u2019m leaning towards the external counters but  is there anything I\u2019m missing that would interfere with the above shield?", "Any suggestions would be greatly appreciated!!!", "Have you looked at the Turtlebot3 source? You can find it in the OpenCR code. The turtlebot3 uses encoders for odometry. It uses dynamixel\u2019s sdk, which you can definitely find the source for.", "I have slowly been working my way through the source, but haven\u2019t made it to the odometry section yet. The Dynamixel motors are \u201csmart\u201d servos and perform their own position/velocity control and just report this info back the the Opencr board.", "Sorry I wasn\u2019t very clear with my plans. I want to use the Opencr board exactly as the TurtleBot3 does, but swap out the Dynamixel\u2019s for larger motors with encoders. This will require the Opencr board to take on the servoing of the wheel motors and calculate odomerty on its own. I feel comfortable modifying the source to do this but wanted to see if anyone had advice or suggestions on adding encoders to into the mix.", "I have experimented with the STM32 processor and did want to use it\u2019s builtin ability to read quadrature encoders.   I had to give up because of a conflict over timber use.", "But I did find the CPU has zero trouble at all with VERY high interrupt rates  I have four motors each spins up to about 11,000 PRM and has 64 pulses per revolution.   The processor can handle the maximum rate and still do quite a lot else.  that would be (4 x 11,000 x 64)/60 interrupts per second.    That said the interrupt handler in C++ is very short and fast.", "You can further speed  up the  processing by a large factor if you remember the last time the motor changed directions and then you can ignore that the encoder is using quadrature.  And your handler becomes \u201ci = i + direction\u201d  Just one statement.", "So the simplest way to to do the encoder handing inside an interrupt handler in software.", "But if you can use the STM32 counter hardware that just seems so much more efficient but in my case I ran out of timers", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/opencr-and-encoders/2335"}
,{"title": "[TB3] Machine Learning tutorial", "thread_contents": ["Do you want to try the machine learning?", "We provide machine learning tutorial with TurtleBot3.", "First, we wanted to try machine learning, but we provided an installation tutorial for people who were frustrated at the installation stage.", "Second, This tutorial is reinforcement learning using DQN. We provide basic theory of DQN and algorithms that we use to help users understand reinforcement learning so that they can apply their own learning. What is important in learning and what to do to get the learning you want.", "Finally, In this tutorial, TurtleBot3 learns to go to the goal in gazebo maps:", "\nNo obstacle, Static obstacle, Moving obstacle, Combination obstacle", "Each map has different learning time and reward function.", "\nChallenge to create more efficient learning models and create agents with different goals in different maps.", "\nWe are by your side.", "For more information, please refer to the manuals below.", "\n[Source code] : ", "\n[Document] : ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-machine-learning-tutorial/5659"}
,{"title": "Experiences with Distributed ROS", "thread_contents": ["I am curious to know of any experience people have had running distributed ROS in all shapes and sizes. Have you worked with any of the following solutions (or anything else?), and how did it go? What thoughts or advice can you share?", "As for my personal interest, I have been doing some work with distributed ROS and UAV, and while it is all going well on the ground side, I\u2019m having some severe spikes in latency going through a cheaper wireless router. I\u2019m specifically interested if anyone has any ideas about reliable low-latency wireless links that are easy to interface with ROS.", "The most interesting setup I used was:", "This resulted in a ca. 100ms ping time. With this, I could teleop the robot\u2019s base and arms using the on-board Kinect, so OK data rate as well.", "Lessons learned:", "So I have worked with some rather large distributed systems and best practices are rather hard to say without understanding your topology.", "I have had the best success with routed architecture. Each node master then uses a static packet structure to communicate to the higher level controller (mainly due to the radio link).", "Your spikes are more than likely caused by the inefficiencies of your packet size vs your radio\u2019s frame size. For a test run iperf across your radio link at different packets size. You should be able to see what that most efficient packet sizes are.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Static ethernet network with some kind of switch", "Dynamic ethernet network with some kind of router", "WiFi and ethernet through a router", "WiFi-Only network with a common base station / access point", "WiFi Ad-Hoc network", "Some kind of mixture of anything else?", "a mobile robot with 2 PCs. One of these ran a OpenVPN server", "wired to a router", "that router bridged to a WLAN with multiple access points", "the care home\u2019s (in which the robot worked) network was configured to port-forward to the OpenVPN server", "A control station some 60-70km away VPNed into the robot.", "Tuning a multi-access point WLAN, where the robot roams between APs, is some work. Contrary to my intuition at the time, is that you do ", " want to have each AP at a high power but instead low. That way, the robot switches to the now-closest AP faster, yielding a better connection. WiFi is designed to stay \u2018attached\u2019 to the current AP as long as it gets a signal, even though it may be weak. Dropping a low signal early is better in that case.", "Running the VPN server on the robot is not the way to go\u2026 The server should be on a public IP, the robot behind a firewall."], "url": "https://discourse.ros.org/t/experiences-with-distributed-ros/1768"}
,{"title": "ROS_ERROR_STREAM in python", "thread_contents": ["I want to know the equivalent of ROS_ERROR_STREAM in python", "Please ask such help and debugging questions on ", " instead of here on Discourse. The Q&A website is optimized to provide answers as efficiently as possible while this site we focus on announcements and discussions scoped appropriately for each category. There are guidelines for asking questions at ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-error-stream-in-python/2545"}
,{"title": "Research about the battery in robots", "thread_contents": ["Dear community,", "We are researching on how to make robots programming more efficient and of a better quality. If you are into robot programming, specially drones, please help us by filling this survey. It takes no more than 3 minutes.", "Kind regards", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/research-about-the-battery-in-robots/5566"}
,{"title": "Best ARM board for ROS", "thread_contents": ["I would like to get an opinion on what people think is the best ARM board for ROS.", "I have tried the dragonboard, raspberry PI3 and beaglebone black.  beaglebone does not have the horsepower to run ROS (moveit,etc).  I think that a quad core A53 might be the minimum required.", "Anyway comments welcome.", "Shawn", "HI ", ",", "I\u2019ve tried all those and my advice for new projects would be to have a look at the new NVIDIA Jetson TX1 module. IMO it\u2019s by far the best ARM embedded board where to run ROS and friends.", " how is the TX1 compared to the TK1?", "I have used the TK1 myself, and it has the capabilities to be extremely powerful, however all of nvidia images are given out with low power settings so you have to configure it all yourself (i also noticed that it had a issue with IRQ balancing, which till this day I dont believe got fixed)", "Hi,", "Yes,  the TX1 would be a great board (or the TK1).  I should have clarified and said best board under 120.", "\nI am considering Pine64 but I will have to wait and see about it.  I am a bit disappointed by the RPI3\u2019s OS and lack of 64bit support.", "Hi ", ",", "\nI jumped directly to the TX1 so I\u2019m afraid i can\u2019t comment on that.", "Has anyone used an old flagship phone as a \u201cARM target\u201d? I\u2019d like to use some old android phones that still have a good 2GB of RAM + 64-bit ARMv8 that include a swath of radios, sensors, self contained power supply. Buying one retail might be above your $120 mark, but if you have one siting around in a relatives junk droor or with a cracked screen off ebay\u2026", "I know there is ROS for android with ROSJava, but flashing phones with a more common flavor of Linux and treating as a traditional embedded target has always been appealing to me. I think mobile device hardware support is a bit fractured thanks to device manufactures, so I\u2019ve only seen posts with Nexus and Ubuntu Touch, nothing like an old Samsung I have.", "\nRelevant ROS Answers post: ", "I would like to get an opinion on what people think is the best ARM board for ROS.", "I have tried the dragonboard, raspberry PI3 and beaglebone black. beaglebone does not have the horsepower to run ROS (moveit,etc).", "I was hoping to use the BBB for a mobile robot doing things like SLAM, localization, and navigation (move_base). So no need for image/video processing, only spinning LiDAR. From my testing, it doesn\u2019t look adequate even for that, unfortunately.", "How do people feel about the Odroids? ", "ODROID XU-4 is pretty awesome. It has a USB3 host, and if you have a USB3 peripheral that you need to talk to, I don\u2019t think there are many (any?) similarly-sized and similarly-priced options at the moment.", "+1 for the XU-4. It can run a surprisingly serious ROS setup (motion planning, depth image processing). I\u2019ve just bought a C2 as well; haven\u2019t had a chance to try it out yet though.", "spmaniato", "You can use the BBB but for me using MoveIt it the CPU ran around 80-90 percent during planning.  Which is not good.  Using the DB410C or RPI3 it runs around 60-70 percent.", "I have not used an Odroid, but have used SolidRun cubox and Radxa rock and TK1.", "Since you mentioned depth image processing: did you try connecting an Asus Xtion to the XU-4? Does it work?", "Nice discussion with interesting answers.", "Just to mention my experience:", "I also used ROS in rpi2 rpi3, bbb and Odroid XU4 successfuly in several", "\nprojects.", "\nI did not used MoveIt but I can tell that it is enough to support some SLAM", "\nsystems if the algorithms parameters are well tuned for efficiency.", "\nSpecifically odroid is quite powerful and it is able to execute this kind", "\nof heavy applications fluently.", "Kind Regards.", "we have used xu4 with turtlebot with asus camera", "I also used ROS in rpi2 rpi3, bbb and Odroid XU4 successfuly in severalprojects.I did not used MoveIt but I can tell that it is enough to support some SLAMsystems if the algorithms parameters are well tuned for efficiency.", ", have you ever tried running the navigation stack (i.e., ", " + ", ") on a BBB by any chance? I\u2019ve found that, even with relaxed parameters, it cannot handle it. But I may be doing something wrong. (It can definitely handle ", " plus the laser scan publisher and other drivers. It\u2019s ", " that takes it over the edge in my experience.)", "I also like the ODROID XU-4 for using USB 3.0. On the one hand \u2018Intel\u00ae RealSense\u2122 Robotic Development Kit\u2019 is now available. It is fantastic for the user using RealSense or other USB 3.0 device (but a little bit expensive than other SBC). How about it?", "\n", "check our\n3d model\nHigh performance and low power consumption features of the latest tablet technology", "\n", "Yeah, we also used it with an asus xtion. We set the depth resolution very low (QVGA or QQVGA) mainly because we didn\u2019t need the extra pixels. Overall It worked quite well though.", "i tried Pine64+ and installed Ros from source on it ros indigo with ubuntu 16.04", "\nit is very good", "\nyou can download image from", "\n", "PINE64-_ROSINDIGO_Ubuntu16.04lts_image - image for pine64+ installed on it ROS package From source and Distro Ubuntu 16..04 lts", "\n", " It\u2019s great to know that it works for you with Indigo on the Pine64.", "Thanks for taking the effort to share however I have to not recommend people grab it. Binary images of unknown provenance are potential security risks.", "If you wouldn\u2019t mind it would be great if you could share your experience bringing up Indigo on the Pine64 with as much info as you can remember in a new thread in this category. Also did you try using the Debian Jessie builds with Kinetic from debian packages?", "i tried Ubuntu 16.04 LTS with indigo and i will make it for  Kinetic also", "\ni tried  Kinetic with it but i stopped to make meta-ros yocto core-image-minimal for raspberry pi 2 and it works now i will upload it soon also and i will build Kinetic with Ubuntu16.04 LTS", "I have Kinetic running on Pine64 under debian.  works great.  Kinetic is still missing some packages that should be synced in two weeks.", "All of the ARM A53 boards perform similarly to me.  I do like the 96Boards form factor the best", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Intel\u00ae Atom\u2122 x5-Z8350 Processor (2M Cache, 1.44 GHz up to 1.92 GHz) CPU with 64 bit architecture; Quad Core", "Intel\u00ae HD 400 Graphics", "1~4G DDR3L", "USB 2.0 and USB 3.0"], "url": "https://discourse.ros.org/t/best-arm-board-for-ros/152"}
,{"title": "Support for Robots in meta-ros", "thread_contents": ["Dear meta-ros users,", "in the past, I have been contacted in private if meta-ros could include the kobuki_base packages.", "So far, there were no recipes for specific robots in meta-ros, such as the kobuki, turtlebot or PR2, mostly because I do not have any robot at my desk. However, I could spend some time creating the recipes. If there are no major issues to cross-compile the package, the recipes should be easy and quick to get done.", "Hence before getting started on that activity, I have two questions:", "We will of course start with the recipes for the robot that is most useful for the majority of the meta-ros users.", "Please let us know here, so that we do not duplicate our efforts.", "Best regards,", "Lukas", "Hi Lukas,", "I think it is a good idea to include robots, specially those used by many people as the Turtlebot. In the case of my company, we did a workaround to avoid writing recipes for the Turtlebot as we just wanted to do some quick tests.Having these would have been useful.", "However, I am not sure whether it should be included in the same meta-ros repository. On one side I think it makes sense, to avoid further synchronization problems, but I do not see any hardware-specific recipe in the repo.", "Best,", "\nJavier V. Gomez", "Dear Javier,", "thanks for your reply. From your post, I am confirmed that recipes for the turtlebot are a good starting point. Let\u2019s see what others think.", "To your question whether to include the recipes in the same meta-ros repository. I would stay with the pragmatic solution to include the recipes in the repository as long as they do not strictly require or modify any global changes in layer\u2019s configuration (which I currently do not foresee).", "\nIf all recipes are in one repository, it simplifies the configuration management compared to maintaining several repositories that depend on each other. The meta-ros community is not so large after all. Maintaining all content in one repository would not make the content and number of changes so large that we cannot handle the maintenance work efficiently.", "Best regards,", "Lukas", "Dear all, dear Javier,", "If you have been following the current development on the master branch of meta-ros, you might have noticed that we now finally provide a number of recipes for the kobuki robot.", "We provide recipes for 103 packages, including all packages from ecl_tools, yujin_ocs, rocon, kobuki_core and kobuki.", "We tested that all of those packages build and I checked that all runtime dependencies are declared as defined in the package.xml. However, we never checked if all of those tools and packages run on a cross-compiled image. So, we ask the potential users to report if there are any further issues that we are not aware of.", "As my spare time admits, I will continue to work on providing all recipes for turtlebot. A work-in-progress state is provided on the wip-turtlebot branch in my personal meta-ros fork. The currently most challenging part is to get openni2-camera cross-compiling. The speed of progress on that overall activity will however be largely driven by the number of user requests for turtlebot. Currently, some meta-ros users and contributors are asking for assistance on other interesting topics for meta-ros. So, progress on turtlebot support is currently not on my personal priority list.", "Best regards,", "Lukas", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Are you interested to have support for a specific robot?", "Have you already developed some recipes for some robot?"], "url": "https://discourse.ros.org/t/support-for-robots-in-meta-ros/729"}
,{"title": "Compressed PointCloud2?", "thread_contents": ["Hi,", "I had a thought that I had on the back of my mind for a while and I would like to share it with the community.", "We know that PointClouds messages can be quite large.", "\nThe image transport allow us to send compressed images. This has great benefits when we are connecting remotely to a robot over Wifi or when recording with a rosbag.", "But we miss a similar functionality iwith even large messages, i.e. poit clouds.", "I wonder if anyone has addressed this issue already or if we can propose to have an \u201cofficial\u201d solution to this problem.", "In my mind, a potential simplistic solution would be to:", "I would personally use a very fast compression algorithm such as LZ4 (we don\u2019t want to add too much latency).", "What do you think?", "Davide", "Do you have any data on the compression efficiency for point clouds?", "\nConsidering that they are usually highly unstructured data, I\u2019m not sure if off the shelf compression algorithms perform well.", "\nI haven\u2019t tried it, though.", "In general, I think compressing point clouds is a good idea.", "\nMaybe even with a lossy option for tasks that do not need accurate data such as visualization.", "This might be worth checking out:", "\n", "Draco is a library for compressing and decompressing 3D geometric meshes and point clouds. It is intended to improve the storage and transmission of 3D graphics. - google/draco", "\n", "Pointcloud compression is an active field of research, and there are quite a few algorithms available (at least in papers). PCL already supports some aspects: ", ".", "In ROS contexts the topic has come up earlier: ", " on ROS Answers for instance, and ", "  on the ", " tracker itself.", "Up till Groovy the ", " stack even contained a ", " package: ", ".", "The ROS Answers Q&A has a comment that links to ", " which seems to be a package that implements a nr of the things you suggest ", ". Might be worth a look.", "I am thinking about lossless compression ONLY.", "I hacked a solution one year ago and my experience is that LZ4 is so fast that the time wasted to do compression and decompression is by FAR lower than the time wasted to transmit a large message over Wifi.", "Rephrasing, not only we reduced the bandwidth used, but the latency was actually better.", "We also made a solution for this last year.", "\nAlthough it\u2019s not entirely lossless, one can change voxel size to individual needs. In our lab, we run the solution on six jetson TX2 nodes that map a space of 10x10x4m in 4cm voxels ", ". We get 40.5 in compression ratio and use the point-count per voxel to calculate an intensity value.", "\nYou can read about it here: ", " (free) and try the ROS-package here: ", "Atle", "Just to mention a ", " solution to this issue we came up with a few years ago.", "While developing communication solutions for the NASA Space Robotics Challenge and its limited bandwidth (team Olympus), we investigated the possibility to convert point clouds to range images and back-- PCL <----> OpenCV. The point cloud transport would then take the shape of a usual image. If I recall correctly, this image could be further compressed as a sensor_msgs/CompressedImage.", "Given that we were converting sparse point clouds from sensor reading -as opposed to dense object mesh- this solution worked really great in terms of compression ratio and reconstruction accuracy (not so great in term of CPU consumption ^^)", "The package is not in good shape but one might be able to scavenge some code from it: ", ".", "Maybe ", " could provide some more info, possibly numbers?", "I\u2019d also suggest working on the base of ", ". This package has been written by a student of Czech Technical University a few months ago and the goal was exactly to mimick the behavior of image_transport. It implements the very same plugin behavior.", "What is already implemented is the lossy compression with draco library.", "If you want some lossless solution, it should be pretty easy to implement it as a plugin into this framework.", "As for support in RViz - there is none yet, but you can launch a node similar to image republisher (also implemented in the package) which converts the compressed pointclouds to raw PointCloud2.", "I don\u2019t have the exact numbers of the original pointcloud size. But by transporting a 26KB depth image and a 85KB rgb image we were able to reconstruct the original pointcloud.", "As a matter of fact, I wrote something similar that can turn a PCL into a depth image. Its original purpose was to merge depth images from different perspectives into one as it would be seen from an arbitrary perspective around the point cloud. Worked pretty well, albeit rather slow and costly wrt. calculation time.", "If you\u2019re interested in the code, it can be found there: ", "This seems like the most elegant solution. Nice!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Define a new Message type, let\u2019s call it ", "   ", ".", "Add this message to drivers, such as the Velodyne or RealSense-ROS ones. Compression and publication can be skipped in no one subscribe to the compressed topic.", "Create an official C++ and Python library to convert the compressed type to a PointCloud2 message or, better, a pcl::PointCloud.", "Add this new compressed cloud to RViz"], "url": "https://discourse.ros.org/t/compressed-pointcloud2/10616"}
,{"title": "Featured ROSCon 2018 Talks", "thread_contents": ["As we start to get ready for ", " we will be featuring talks from the ", " to help inspire people to submit their own talks as well as give people a sense of what they can look forward to at ROSCon 2019!", "\n", " ", " ", "          ", "\n", "Astrobee is a free-flying robot designed by NASA to operate alongside astronauts inside the International Space Station, where it will carry out scientific and surveying tasks in microgravity. The robot can autonomously mate with a docking station to recharge, as well as perch to existing ISS handrails using a three degrees of freedom arm. Its open source flight software stack is built on ROS Kinetic, uses a delay-tolerant DDS bridge for space-to-ground communication, and is accompanied by a Gazebo simulator that enables researchers to develop and test behavioral algorithms. This presentation covers the the software architecture, challenges faced during the development process, facilities for testing prototype hardware, and broad lessons we have learned over the last three years.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "          ", "\n", "Over the past 10 years, ROS 1 has proven itself to be the framework of choice for prototyping and developing large robotic applications. Many limitations preventing ROS 1 from being used in production applications have been discovered over the years, and after significant prototyping, ROS 2 makes great headway in making ROS 2 suitable for production. Autonomous driving is the next great technology waiting to be realized and transform our society. A full autonomous driving stack is inarguably a large robotic system, and consequently ROS 2 is the right framework upon which to develop a full autonomous driving stack. To prove this, we have developed a part of the autonomous driving stack based on ROS 2.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "          ", "\n", "Over the past half year we have been working on the new launch system for ROS 2 based on roslaunch from ROS 1. The goal of this presentation is to summarize the state of the design document for launch, describe the current state of the reference implementation, and dive into the rationale behind some of the design decisions. The attendee should come away with a deeper understanding of the differences between roslaunch from ROS 1 and the new ros2 launch tool. We\u2019ll also have some short demonstrations to show how these differences might be useful to everyday users.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "Over the past couple of years, Open Robotics collaborated with NASA Ames Intelligent Robotics Group to develop the Resource Prospector lunar driving simulator. In particular, a strong focus was placed on generating high quality visual images from the cameras on the simulated rover. This resulted in in several improvements made to Gazebo, including support of high resolution digital elevation maps, improved shadows, integration of custom material shaders, and a new lens flare plugin. This presentation will show the resulting lunar terrain environment created in Gazebo and discuss some of the challenges in modeling the environment.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "In this talk we show the current state of ROS 2 features through hands-on demonstrations. We guide the developer through a set of features and functionalities of ROS 2 beginning with creating a simple \u201chello world\u201d package and consecutively increasing functionality. We show how to launch multiple nodes, how lifecycle nodes can be used to bootstrap a complete system, through to how the ROS 2 security features can be utilized to secure applications. This talk is meant to highlight the newly available features and tools of ROS 2 and aims to incline developers to start working with it.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "FIRST, or For The Inspiration and Recognition of Science and Technology, is an international organization focused on engaging students through STEM. This year, our team - The Zebracorns - was the first in the high school FIRST Robotics Competition (FRC) to control our robot entirely using ROS. In our presentation, we\u2019ll introduce the unique challenges presented by FRC with restricted hardware options, time, and resources. We\u2019ll talk about our motivation for implementing ROS, the specific application within FRC, and our ambitions for the future of ROS within the FRC community.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "Cruise Automation\u2019s self driving car runs on top on ROS. This talk will share some of the lessons we learned while scaling up the ROS stack to a very complex Robotics problem and 500+ engineers. We will talk about performance, reliability, code organization and health, and the ways we have found ROS to excel or fall short.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", "\n", "Reliable, life-long mapping and localization is an essential component for mobile robotics in continuously changing warehouse environments. We present a system based on Cartographer in which robots run finite-history SLAM for low-latency localization and continuously stream local map updates to a cloud service. The cloud component assembles and optimizes a globally consistent pose graph out of the streaming data of the agents. Magazino piloted cloud-based Cartographer in a customer warehouse using its fleet of mobile picking robots. By sharing the local map changes among each other, the robots were able to maintain their localization accuracy while dealing with dynamic environments.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "On January 11, 2018, Sony Corporation released aibo (", "). aibo that is back on market beyond the time of 12 years constructed via robotics framework named ROS. In this presentation, we introduce examples of development in aibo from the point of view of ROS, starting with introduction of aibo, architecture, embedded technology, real-time optimization, robot development environment, simulation etc.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "RViz is now available for ROS 2, including most of its features. This talk will look back at the migration of RViz from ROS 1 to ROS 2, and present the main challenges and changes performed during the migration. It will focus on a new package, rviz_visual_testing_framework, that makes writing automated UI tests - including the 3D rendering part of RViz \u2013 possible.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "\n", "The  ", "  stack is one of the core components of the ROS Ecosystem. This talk will present lessons learned from maintaining the navigation stack over the past five years, and discuss a new generation of interfaces for increased functionality. This talk will cover: the new  ", "  interfaces, with a new  ", " interface; the locomotor package which replaces move_base and demonstrates new functionality; and ROS 2.0 + Navigation.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "This presentation discusses the building of a task-driven, message-synchronized execution abstraction layer on top of ROS\u2019s message passing, aiming to mimic a deterministic system while continuing to use a non real time operating system. We discuss the overall structure of the framework, as well as the details of various capture and synchronization policies, and the profiling and diagnostics of task executions. We further discuss methods for using simulators and bagfiles in conjunction with the synchronization framework for repeatable testing and issue reproduction.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "In this talk we give a short introduction to Mozilla rr, a powerful C/C++ debugging tool for Linux, and show how to more effectively debug ROS nodes by republishing messages from rr recordings. Mozilla rr serves as a gdb (GNU debugger) replacement which efficiently records the execution of a process and then provides repeatable deterministic debugging of the recording, enabling a very powerful debugging experience with reverse execution.", "Thank you again to our Platinum Sponsor and Gold Sponsors for supporting ROSCon.", "\n", "\n", "\n", "This talk gives an introduction on how to contribute to ROS 2. Specifically this talk shows you how to take a ROS2 feature from a design discussion, to a concept document , to a series of pull requests, through to a released feature. The talk also discusses the process of patching bugs: moving from an ", " bug, through trouble shooting, bug report generation, and finally submitting a patch. If you\u2019ve never contributed to open source patching your own bug is a great place to start and this talk will guide you through the process.", "ROSCon registration is ", "! The deadline for regular price admission is just two weeks away. Get your tickets now before the prices go up.", "                    ", "\n\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops closes today. Submit ", ".", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Early registration is ", "! The deadline for early registration is August 24th, 2019 (or until the limited supply of early registrations sells out, whichever comes first).", "Early registration is ", "! The deadline for early registration is August 24th, 2019 (or until the limited supply of early registrations sells out, whichever comes first)."], "url": "https://discourse.ros.org/t/featured-roscon-2018-talks/9071"}
,{"title": "ROS Installer Tooling", "thread_contents": ["Hello fellow humans and robots,", "We have recently been working on a new ", " and have now finally written enough documentation for an official release of the build toolchain for the installer.", " [", "] is a modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file. It has been tested with Ubuntu 16.04 and 18.04 and has significant automation for caching and tracking upstream releases.", "As an example, the ", " is being used to generate an ISO installer with Ubuntu 16.04 + ROS Kinetic + drivers + desktop and system enhancements.", "Along the way we have developed a few ROS packages, that should be available in the next Kinetic sync, which we would like to highlight for potential collaborators. Melodic support should be done within a few weeks.", " [", "] provides automatic configuration of the ", ", ", ", and ", " for most use cases by introspecting the kernel routing tables and network interfaces for the current IP address. This allows mobile robots running on a wireless network to automagically connect to a workstation running ROS. It is designed to support fixed interfaces, Zeroconf, DHCP and/or a VPN.", "Caveat emptor: It may also make it easier to access your robot from the internet.", " [", "] builds on a broad survey of previous work starting ROS at bootup. This package should work with most robots that have Systemd installed.", "For example to run ", " at bootup", " provides an efficient GUI interface for locally starting and stopping services enabled by the ", " package.", "Please let me know if you have any feedback!", "Thanks,", "\nBil", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-installer-tooling/11461"}
,{"title": "Which board/microprocessor should I use? I want to use ROS Kinetic on Ubuntu 16.04, along with packages like move_base, laser_scan_matcher for indoor autonomous navigation", "thread_contents": ["I am confused as to which board will be able to handle the necessary computation to run autonomous navigation for my turtlebot-like-robot.", "\nI want to run Ubuntu 16.04 OS and ROS Kinetic, and need packages like move_base, laser_scan_matcher.", "\nI\u2019ve heard that Raspberry Pi 3 and its Arm Cortex A53 isn\u2019t enough for the purpose.", "\nWhat are your views?", "That highly depends on your budget, software requirements and space limitations.", "\nThe packages alone also don\u2019t say  that much about the required computation power.", "\nE.g., what kind of LIDAR are you using (how many data points per second does it produce?), what other kinds of sensor (e.g., cameras) are you using?", "\nFor cameras etc. the USB bandwidth might be a limiting factor that has to be kept in mind.", "Without more information, I can only suggest some generally viable small form factor options depending on your computation power requirements:", "\n", " You could try the Pi 4 which is a lot faster than the Pi 3. The 4GB RAM might not be enough depending on your use case but I assume it should be able to handle autonomous navigation with the kind of LIDAR you would put on a Turtlebot-like robot.", "\n", " A mini-PC such as an Intel NUC would surely be enough to handle almost anything you throw at it.", "what kind of LIDAR are you using (how many data points per second does it produce?)", "Hey ", ", I\u2019m using RPLidar A1, and using it at 5Hz with 2000 data points.", "\nNUC seems too bulky, actually. Looking for something much more compact like Pi.", "\nWould you recommend Beaglebone or Odroid XU4?", "Hi, if you don\u2019t have budget constraint you can try UP board or jetson nano or Beaglebone. Working with ROS , Lidar and some kind of application that is to be done like turtlebot alike of robot if designed.", "I agree with ", " on the UP board if budget is not a constraint.", "\nI wouldn\u2019t recommend a Beaglebone for this use case (I assume you want something simple to work with and develop on) because they have very little RAM (unless there\u2019s a model I missed) and that can lead to frustrating issues for inexperienced developers, e.g., you may not be able to compile directly on the board but will have to cross-compile on your host machine and deploy the binaries manually.", "\nA jetson nano really only makes sense if you also have a camera and want to use graphics accelerated algorithms or deep learning otherwise the Pi 4 is faster IIRC.", "\nHowever, if I were you I would try a Pi 4 since you\u2019re not dealing with a lot of data and if the Pi is not enough, it\u2019s a cheap mistake to make and maybe you can still use it for something else.", "\nAlso, the Pi is widely used and has the highest chance of finding someone who can help if you run into issues.", "\nI don\u2019t know how the Odroid compares to a Pi 4.", "Later on, you can still switch to something that is smaller / more energy efficient but harder to work with.", "Looks like you mised the ", " (Cortex A15, 1GB RAM) and the ", " (Cortex A15, 2GB RAM)", "I have had great luck using the TX2 with the ", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a Q&A site which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/which-board-microprocessor-should-i-use-i-want-to-use-ros-kinetic-on-ubuntu-16-04-along-with-packages-like-move-base-laser-scan-matcher-for-indoor-autonomous-navigation/11488"}
,{"title": "Announcing MARV Robotics, a Powerful Data Management Platform", "thread_contents": ["Hi ROS folks,", "\nwe are pleased to announce MARV Robotics, a tool that will make management of gazillion of your bags nothing but joy. With MARV robotics no bags will get lost anymore. You can conveniently search, analyze, view, tag and comment bag files. MARV Robotics enables you to efficiently develop your algorithms and is a place where your continuous integration starts.", "Get jump started by:", "For documentation and support consult: ", ".", "MARV Robotics will also be presented at ROSCon 2016 on Sunday afternoon. Talk to Dejan for details.", "Dejan, Daniel, Marko, Florian", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["checking out the demo instance: ", ",", "installing it: ", "."], "url": "https://discourse.ros.org/t/announcing-marv-robotics-a-powerful-data-management-platform/609"}
,{"title": "ROS Navigation course", "thread_contents": ["Dear Roboticists,", "I would like to introduce the ROS Navigation in a single week course which will be provided on February 13th.", "This courses is ideal for beginner/intermediate roboticists aiming to become proficient in ROS.", "Entirely practical ROS Navigation training which integrates theory and practice, teaching with exercises and real ROS Navigation project. Comprehensive exercises with diverse simulations, help you get into the ROS development in a faster and more efficient way.", "Live Class:", "\nA single week of ROS Navigation learning in the sunshine city of Barcelona.", "\nExclusive for ONLY 5 individuals", "\nExam and ROS Navigation Certification: There will be a test at the end of the course. Those who pass the test will receive a ROS Navigation certification.", "\nDuration: February, 13th to 18th", "\nLocation: Gran Via de les Corts Catalanes, 608, 3\u00baD, 08007 Barcelona, SPAI", "\nVirtual Class:", "\nIf you cannot attend to Barcelona you have the option to attend the class online. Through a web transmission on real time, with full access to the teacher and other attendants to the class.", "\nAvailable Places: 25", "*Monday: Navigation Basic concepts", "\n*Tuesday: Mapping", "\n*Wednesday: Localisation", "\n*Thursday: Path Planning", "\n*Friday: Path Planning + Obstacle Avoidance", "\n*Saturday: ROS Navigation Exam", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Basic knowledge of Linux shell", "Basic knowledge of Python programming", "A laptop (can have any operating system)", "Required basic knowledge of ROS", "You can enroll the course here: ", "\n", "You can contact us with questions and doubts here: ", "\n"], "url": "https://discourse.ros.org/t/ros-navigation-course/1105"}
,{"title": "Last Call for ROS Navigation Course", "thread_contents": ["*** Apologies if you have received this call for participation more than once  ***", "\nROS Navigation in a single week", "Dear Roboticists,", "The ROS Navigation in a single week course which will be starting on February 13th. This courses is ideal for beginner/intermediate roboticists aiming to become proficient in ROS.", "Entirely practical ROS Navigation training which integrates theory and practice, teaching with exercises and real ROS Navigation project. Comprehensive exercises with diverse simulations, help you get into the ROS development in a faster and more efficient way.", "Live Class:", "\nA single week of ROS Navigation learning in the sunshine city of Barcelona.", "\nStill 3 spots available", "\nExam and ROS Navigation Certification: There will be a test at the end of the course. Those who pass the test will receive a ROS Navigation certification.", "\nDuration: February, 13th to 18th", "\nLocation: Gran Via de les Corts Catalanes, 608, 3\u00baD, 08007 Barcelona, SPAI", "\nVirtual Class:", "\nIf you cannot attend to Barcelona you have the option to attend the class online. Through a web transmission on real time, with full access to the teacher and other attendants to the class.", "\nAvailable Places: 15", "*Monday: Navigation Basic concepts", "\nWhat a full navigation system for a robot must contain", "\nHow to launch a full ROS navigation system", "\nHow to debug a ROS navigation system", "*Tuesday: Mapping", "\nHow to create maps of environments", "\nHow to use those maps for navigation", "\nHow to tune robot and software for map creation", "\nHow to connect ROS programs to the mapping system", "*Wednesday: Localisation", "\nHow to localize a robot in an indoor environment", "\nHow to tune the localization system", "\nHow to connect ROS programs to the localization system", "*Thursday: Path Planning", "\nHow to make the robot move autonomously on an environment", "\nHow to use the different planners to create a path", "\nHow to configure the costmaps for optimal path planning", "\nHow to connect ROS programs to the path planning system", "*Friday: Path Planning with Obstacle Avoidance", "\nHow to make the robot avoid obstacles", "\nHow to use different local planners to avoid obstacles", "\nHow to configure the costmaps for optimal obstacle avoidance", "\nHow to connect ROS programs to the obstacle avoidance system", "*Saturday: ROS Navigation Exam", "\u2013", "\nYUHONG LIN", "\nBusiness Assistant of R&D", "\nThe Construct", "\n", "Master ROS Skills and Create Your Own Robot Programs. Use online ROS Tutorials to learn ROS fast. Build and Test Your ROS Based Robot Programs.", "\n", "\u2013", "\nYUHONG LIN", "\nBusiness Assistant of R&D", "\nThe Construct", "\n", "Master ROS Skills and Create Your Own Robot Programs. Use online ROS Tutorials to learn ROS fast. Build and Test Your ROS Based Robot Programs.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Basic knowledge of Linux shell", "Basic knowledge of Python programming", "A laptop (can have any operating system)", "Required basic knowledge of ROS", "You can enroll the course here:", "\n", "You can contact us with questions and doubts here: ", "\n"], "url": "https://discourse.ros.org/t/last-call-for-ros-navigation-course/1252"}
,{"title": "Servo limits/mapping", "thread_contents": ["Hi,", "I\u2019m just getting started with ROS, and was finally able to get ROS installed on Linux Mint 17.1 rather then in VirtualBox or on some older slower HW I had laying around.", "I\u2019ve been working with MyRobotLab but have had a lot of issues with Java not wanting to run well on 64bit Linux Mint.  Lots of random crashes and running it on a Win10 box there were other issues, especially when trying to use the OpenCV service and tracking.", "Ultimately I want to do some experimentation with physical object manipulation and computer vision, but I\u2019m taking baby steps at first.", "I have a head constructed with dual cameras for eyes, and using MyRobotLab I\u2019ve been able to do some basic object tracking (on a good day with the Buffalo running on the west side of the mountain:)).  But due to the Java issues I\u2019ve been experiencing as well as the shifting MRL APIs it is difficult to get the reliability I need.", "So I\u2019ve moved to ROS, and have done some of the earlier tutorials.  I\u2019ve written some Arduino code for controlling the servos in the head and have successfully tested that using rostopic.", "In MyRobotLab the Servo service had the ability to specify MinMax values for limiting the servo output, as well as a mapping function that can be used to map 0 - 180 degrees to the physical limits.  For example in the head I may need to limit eye movement in the Y axis to a fairly small range.", "I\u2019m assuming that there may be some code already available in ROS for this type of implementation but I haven\u2019t located it yet.", "So before I reinvent the wheel I thought I would ask if there might be some generic code for implementing a mapping layer to manage the servo(s) and then the output from that mapping layer would be sent to the Arduino code via rosserial to handle the low level movement of the servos.", "Thanks,", "\nBurt", "Hi Burt,", "It\u2019s great you\u2019re getting involved with ROS. However we ask that help and debugging questions be asked on ", " instead of here on discourse. The Q&A website is optimized to provide answers as efficiently as possible while this site we focus on announcements and discussions scoped appropriately for each category.", "A ", " shows that there are already several questions with answers about servos and rosserial on arduinos.", "For more informatoin please take a look at the guidelines for asking questions are at ", " The more complete you can ask your question the better answer you can get.", "Thanks\u2026  I checked ", " and while there are some arduino related servo questions answered I didn\u2019t seen anything related to what was asking.  I\u2019ll check one more time, and if I still can\u2019t find an answer I\u2019ll try posing the question there.", "Burt", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/servo-limits-mapping/1657"}
,{"title": "ARIAC Finals results announced", "thread_contents": ["We are happy to announce the final results of the Agile Robotics for Industrial Automation Competition (ARIAC).", "ARIAC is a simulation-based competition designed to promote agility in industrial robot systems by utilizing the latest advances in artificial intelligence and robot planning. The goal is to enable industrial robots on the shop floors to be more productive, more autonomous, and more responsive to the needs of shop floor workers. The virtual nature of the competition enabled participation of teams affiliated with companies and research institutions from across three continents.", "While autonomously completing pick-and-place kit assembly tasks, teams were presented with various agility challenges developed based on input from industry representatives. These challenges include failing suction grippers, notification of faulty parts, and reception of high-priority orders that would prompt teams to decide whether or not to reuse existing in-progress kits.", "Teams had control over their system\u2019s suite of sensors positioned throughout the workcell, made up of laser scanners, intelligent vision sensors, quality control sensors and interruptible photoelectric break-beams. Each team participating in the finals chose a unique sensor configuration with varying associated costs and impact on the team\u2019s strategy.", "The diversity in the teams\u2019 strategies and the impact of their sensor configurations can be seen in the video of highlights from the finals:", "Scoring was performed based on a combination of performance, efficiency and cost metrics over 15 trials. The overall standings of the top teams are as follows.", ", Center for Advanced Manufacturing, University of Southern California", ", Pernambuco Federal Institute of Education, Science, and Technology / Federal University of Pernambuco", ", Case Western Reserve University", "Top-performing teams will be presenting at IROS 2017 in Vancouver, Canada in a workshop held on Sunday, September 24th. Details for interested parties are available at ", "The IROS workshop is open to all, even those that did not compete. In addition to having presentations about approaches used in the competition, we will also be exploring plans for future competitions. If you would like to give a presentation about agility challenges you would like to see in future competitions, please contact Craig Schlenoff (", ").", "Congratulations to all teams that participated in the competition. We look forward to seeing you in Vancouver!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ariac-finals-results-announced/2127"}
,{"title": "Announcing Secure ROS", "thread_contents": ["Hello:", "I\u2019d like to announce the Secure ROS project (", "). Secure ROS is a \u201cfork\u201d of core ROS packages to enable secure communication among ROS nodes. The main goal of Secure ROS is to enable secure communication for regular users of ROS.", "Secure ROS uses IPSec in transport mode and modified versions of the ROS master, ", " module and ", " library to ensure secure communication. At run time, the user can specify authorized subscribers and publishers to topics, setters and getters to parameters and providers (servers) and requesters (clients) of services in the form of a YAML configuration file for the ROS master. Secure ROS allows only authorized nodes to connect to topics, services and parameters listed in the configuration file.", "The goal of Secure ROS is similar to SROS which was announced last year, but the approach is different. Secure ROS allows easy re-use of existing ROS nodes and packages.", "You can find more detailed documentation, check out examples and download packages at ", ".", "Cheers,", "\nAravind.", " Could u please share some information for the following questions:", "\nwhat\u2019s the difference btw Secure ROS and SROS ?", "\nwill SROS and Secure ROS be developed in parallel or lean to one of them for ROS?", "\nROS2 is up and it\u2019s totally different infrastructure than ROS, so the security on ROS will be kept forever or obsolete when ROS2 is adopted/used widely in the future ?", "Hi Roser:", "I\u2019ve been traveling and sorry for the delayed reply. Secure ROS has the same goals as SROS, but uses security at the IP address level. This means that you can have a config file which can list all the authorized accessors to various topics, services, parameters, etc. IP address spoofing is prevented using IPsec at the kernel level and is independent of ROS.", "There is currently a PR for Secure ROS and it is conceivable for both Secure ROS and SROS to be present leaving the user to choose which he wants.", "ROS2 is a different kettle of fish and is not within the scope of Secure ROS.", "I hope this explains it. Let me know if you have any other questions.", "Thanks for your sharing ", ". It\u2019s more or less a P2P session with IPSec for secure ROS, what\u2019s your suggestion for multicast security with IPSec?", ": I don\u2019t know if IPsec can be used for multicast security in transport mode. Is there such a use case in ROS? In any case, just to be clear, IPsec is external to and independent from Secure ROS - but Secure ROS relies on the features that IPsec provides.", "okay. thanks.what I am thinking is that the multicast will be more efficient while exchanging data with multi-nodes, anyway, it seems to reasonably consider that in ROS2 ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcing-secure-ros/1744"}
,{"title": "Zenoh: zero network overhead protocol", "thread_contents": ["Dear All,", "I wanted to let you know that we have just released under ", " a peer-to-peer implementation of our   ", " protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating ", " as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", " For those of you that are familiar with XRCE, ", " is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as ", ".", "A+", "Great job releasing it under the Apache 2 license!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/zenoh-zero-network-overhead-protocol/3148"}
,{"title": "Spatio-Temporal Voxel Layer [Experimental]", "thread_contents": ["Hi!", "I\u2019m the maintainer and a senior robotics engineer at Simbe Robotics working on a new package I\u2019ve dubbed the Spatio-Temporal voxel layer. The goal of this package is to be a 1:1 replacement for the voxel layer improving on raytracing, marking, and efficient searching the configuration space. It also includes work for decaying voxel values over time and infrastructure to do rudimentary dynamic obstacle tracking.", "I am hoping to get some community contributions, bug reports, and features. It\u2019s important to get community buy-in in the development process. It is a highly experimental project in its infancy, the wiki page in the repository posted below contains descriptions of features in development and installation instructions to get involved.", "A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations - SteveMacenski/spatio_temporal_voxel_layer", "Please feel free to reach out to me at stevenmacenski [at] gmail [dot] com if you or your company are especially interested in it or assisting in development or feature requests and you want to touch base about how to get involved.", "Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/spatio-temporal-voxel-layer-experimental/3708"}
,{"title": "Provide official Raspbian packages", "thread_contents": ["There seems to be a lot of interest in using ROS on Raspberry Pis. The solutions that I have seen so far are either to compile ROS manually for Raspbian, which is not straightforward, or to just use Ubuntu on Raspberry Pi 2 or 3.", "\nHowever, Raspbian still provides the best support for all Raspberry Pi versions, including version 1 and Zero, and I think that officially supporting Raspbian by providing Debian packages would help a lot of people to get started with ROS on Raspberry Pi.", "What are the blocking issues that prevent the build farm to build Debian packages for ", "? The only issue I see are the drivers for low-level camera and media-codec access, which are not available on other distributions. Apart from this, Raspbian Stretch contains all necessary packages to build the full ROS desktop stack.", "Specifically to your question I do not believe that there are any blockers for supporting Raspbian Stretch. For ROS purposes it\u2019s basically the same as Debian Stretch. And we have built packages for raspbian in the past too.", "Targeting all the way back to Raspberry Pi 1 is noteably harder as it does not fully support the armv7 architecture and you must compile ", " or target the slower armel(armv6) instruction set. Setting up infrastructure to support that very specific hardware would be non-trivial. In the past I know that people have actually resorted to building packages natively on the rapsberry pi. And some packages took close to a week to compile. This would require a very large cluster and a lot of maintenance and supervision that I would not recommend.", "We have general support for armhf(armv7) and aarch64(armv8) builds on the build farm as well as support for both Debian and Ubuntu. The buildfarm can be configured to support the full matrix of architectures, distros (both Debian and Ubuntu). However if we turned on all of those architectures it would take a lot more computation time as well as monitoring and maintenance to keep track of the builds. In the past we\u2019ve supported as many as 8 simultaneous architecture/distro combos for a single rosdistro and with our overlapping rosdistro support cycles we can end up with quite a few. (For example we\u2019re currently building for 18 at the moment. Across Indigo, Kinetic, Lunar, and Melodic) If we supported the full matrix, 13 distro/rosdistro targets across 4 architectures, we would be building more than double the buildfarm size. And the arm builds incur extra costs as they either need to be run in emulation or on separately maintained ARM based servers. The emulated builds take close to 10x as long as x86 builds which also incurs more costs.", "The main reason that we are not building armhf for stretch is resource availability. There is both a direct cost of running the computers as well as maintenance and monitoring of the buildfarm and all the computational resources it uses. As well as release management and build failure tracking. In the interest of being efficient with our available resources instead of trying to build all targets we choose to pick a spanning set that will cover as many users as possible with the resources we have available.", "You can find the targeted distros are recorded in ", " and we review trends in the computers we see people using as well as our ", " to prioritize which distro-arch combinations to target.", "Related to the Rasbperry Pi we specifically made sure that we had at least one armhf target that would be able to run on the Raspberry Pi. The Ubuntu Xenial armhf which is a commonly used image for the Raspberry Pi. These same packages can also be used on all other armhf boards the support Ubuntu instead of only working on the Rasbperry Pi which is why we picked the generic Ubuntu target rather than the more specific raspbian target.", "If you have a specific use case and want to just support local needs, building your own packages is possible. In developing the build farm infrastructure we have worked hard to make sure that what we do on ", " is reproducable by the community. If you start here: ", " there\u2019s full documentation of how to setup and run your own buildfarm and if you do so you can pick the architectures of your choice. We did this to allow support for specific use cases where a project wants to have a specific potentially non-standard target so that they can do a localized effort instead of needing to ask for specific support on the main buildfarm for a smaller use case.", "As an aside, with additional support Open Robotics can enable more architectures on the main buildfarm. If anyone would be interested in sponsoring support for a specific platform please contact me directly.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/provide-official-raspbian-packages/3990"}
,{"title": "Spatio-Temporal Voxel Layer", "thread_contents": ["Hi!", "I\u2019m proud to announce the first beta of a working new replacement for the ROS voxel_layer and voxel_grid. It uses a number of new technologies and techniques that I think prove effective for a number of indoor and outdoor robotics solutions.", "We use OpenVDB as the efficient data structure and access of the voxel grid and use \u201cfrustum acceleration\u201d to clear points within the depth camera measurement frustum of a depth sensor rather than raycasting (in development, current support for immediate clearing, recommended use at 1-3hz. Will be released soon and would be effective around 20hz.) and global voxel decay allowing for decaying old information in highly dynamic environments (outdoors, retail, warehouse, etc).", "Check out the project below with full configuration guides. I have benchmarked this on my robot at being ", " the CPU usage of the traditional voxel_layer. I have also used it for generating 3D maps of 60,000 sq.ft. environments with ease. Videos are in the readme.", "As always, looking for collaborators to make this awesome! Feel free to shoot me an email, my address is in the package.xml. I hope to support 3D lidars and optional raycasting solutions for sensors that it makes computational sense for (i.e. planar lidars).", "A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations - SteveMacenski/spatio_temporal_voxel_layer", "This is awesome!", "I recently started using voxel_grid and I am looking forward for testing your alternative implementation.", "Davide", "Awesome! Let me know if you have any issues. A colleague PRed the acceleration work and I\u2019m working on independently evaluating before landing it. For now, instantaneous decay is configured out of the box and due to its nature, it\u2019s most effective at lower rates.", "Steve", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/spatio-temporal-voxel-layer/4119"}
,{"title": "Melodic Morenia Buildfarm Available", "thread_contents": ["Greetings ROS users,", "We\u2019re excited to announce the availability of the buildfarm for the next distribution of ROS, Melodic Morenia!", "Current Status of the release:", "\nThe targeted platforms and minimum requirements are specified in ", ". As of today 94 packages have been released into Melodic with ", " for Ubuntu Bionic and Artful as well as Debian Stretch.", "To make the release as convenient as possible for our beloved maintainers, we have a ", " keeping track of which repositories have been released compared to ROS Lunar. This page allows you to see if your repositories are releasable, and also shows what dependencies are not satisfied (if any) and all the repositories depending on your repositories.", "Call for action to all maintainers:", "\nIf the status page shows that you can already release your package, please consider getting a Melodic release out soon.  If your package is blocked, please use the ", " to coordinate with the maintainers of packages blocking your releases.", "\nMake sure that you have the latest version python-bloom and python-rosdistro before starting your release.", "Please record any API changes or major behavioral changes in your package", "\non the ", ".", "Once your package has been added to Melodic in rosdistro, you can generate", "\na prerelease command on ", ". Before running the prerelease make sure that python-ros-buildfarm is up to date.", "The Melodic release date is May 2018, but it\u2019s good to get started early (especially if many packages depend, directly or indirectly, on yours)!", "Your friendly ROS Team", "I\u2019m not sure where exactly to ask so I\u2019ll just piggyback here:", "Are there plans to add a kinetic builds for Bionic Beaver? Or are these expected to be compatible due to the usual backward compatibility on linux? We run our software which is based on MoveIt! and rviz on kinetic on xenial and I would love to switch to bionic as soon as possible. ie not wait until all our dependencies have been released for melodic.", "Unfortunately, no supporting new Ubuntu release with existing ROS distributions is an explicit anti-goal. This is  because the changes in dependency versions between Xenial and Bionic would often require changes in our packages and new releases of those packages, which is essential the majority of the effort involved in releasing a new ROS distribution.", "As an example, the process of getting rviz or moveit released into melodic versus released in Kinetic but on Bionic, is basically the same:", "The only real difference is that with melodic you need to adjust your code to adapt to changes in your dependencies since the last ROS release, but we try to keep these changes small in the core. So a preference is placed on doing the new ROS distribution rather than forward porting an existing ROS distribution.", "As ", " mentioned there\u2019s specifically not plans for Kinetic to be built on Bionic Beaver. Target platforms are listed in ", "If you want to use Kinetic on Bionic however in general it will work if you build from source.", "As an early adopter on an unsupported platform you will likely find some of those issues that the maintainers will triage during the Melodic release cycle.", "Another option you may want to look at is running some of your system in a Docker container. ", " for things that are not yet released into Melodic.", "What is the reason for supporting non-LTS releases of Ubuntu which are only supported for another 2 months after releasing a long-term supported ROS version? I.e. if ROS melodic gets release in May and Ubuntu Artful (17.10) EOL is July, are there actually people using a LTS ROS on a non-LTS Ubuntu during these 2 months?", "I actually think that supporting non-LTS Ubuntu versions (build farm resources, forum support) is not very efficient and other long-term Linux distributions or versions could be supported instead (CentOS, \u2026).", "I think this topic of releasing for non-LTSs and supporting alternate distro has been touched on a few times. I recall ", " discussing this last year, so I\u2019ll just link to that thread here for reference:", "In my lab, we\u2019ve never really use non-LTS ROS or non-LTS ubuntu releases, let alone other combination of with. Although, we do use previous ROS LTS with a newer Ubuntu LTS rather often, such as when working with or Fetch robots still locked at 14.04-Ubuntu/Indigo-ROS (due to closed source driver) from our labs  workstations running 16.04-Ubuntu for better desktop software. As ", " suggested, we\u2019ve manly been getting by using Docker containers to cross over the distro gab.", "In fact, recently we\u2019ve taken to containerising every ROS node on the robot as so we could upgrade to 16.04 on our fetch robots, then use 14.04 imaged container to just run the binaries for fetch hardware drivers. For the rest of the fetch stack that is open source, we use a docker registry on a local lab server to automatically rebuild and deploy those packaged images nightly on 16.04 images to streamline with the rest of our custom packages we maintain from 16.04 workstations.", "I would be interested in a community survey as to the distribution of users across ROS/OS comonations, just to see what the current landscape is at large. The last time this came up, I think one of the better justifications for non-LTS releases was to afford maintainers time to test and migrate to newer upstream libraries to coincide with Ubuntu\u2019s tick-tock cycle. Of course maintainers can always build everything from source, but with robots, testing your stack sometime requires testing your package against everything else you need to run the robot. Having a ROS disto binaries released into the OS your developing with can help reduce the debug time in packages that are not your own.", "I attach past discussion related to this topic. I re-read past ", ", ", " and ", "\u2019s answer, and I agree with their opinion and it is plausible, but the opportunity cost is much higher than the benefit. Let\u2019s try to discuss this topic again. ", "Hi ", "Thanks for your questions.", "\nHowever ROS Discourse is for news and general interest discussions. ROS Answers", "\n", " provides a forum which can be filtered by tags to", "\nmake sure the relevant people can find and/or answer the question, and not", "\noverload everyone with hundreds of posts. So we recommend users to ask their questions there following our support guidelines: ", ".", "\nYou can also find valuable resources to get started with ROS on the ", " that should answers most/all the questions above.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["you need all your dependencies to be {in melodic, re-released into kinetic with bionic support}", "release your package and make sure it works {in melodic, in kinetic on bionic}"], "url": "https://discourse.ros.org/t/melodic-morenia-buildfarm-available/4248"}
,{"title": "Release of towr v1.4 - Trajectory Optimization Library for Walking Robots", "thread_contents": ["Dear ROS community,", "We are excited to announce that ", " ", " has been released!", " is a light-weight and extensible C++ library for trajectory optimization for legged robots. A base-set of variables, costs and constraints that can be combined and extended to formulate trajectory optimization problems for legged systems. These implementations have been used to generate a variety of motions such as monoped hopping, biped walking, or a complete quadruped trotting cycle, while optimizing over the gait and step durations in less than 100ms (", ").", "The thoroughly improved version ", " can now easily switch between different robots using the GUI, visualize ", " type of terrain with terrain patches, directly plot the produced motions using ", " and more. We also improved the efficiency of the underlying Eigen-based NLP wrapper ", " (video below not sped up!).", "Try it out:", "\n", "For more information, visit ", ".", "Enjoy!", "\nAlex @ ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/release-of-towr-v1-4-trajectory-optimization-library-for-walking-robots/5551"}
,{"title": "Learn about Laser Range Finders with ROS in my Udemy Course", "thread_contents": ["Hi", "I am pleased to announce a new major update for course on ROS on Udemy course entitled", "\n", "I have added a new section on how to work with Laser Range Finders (i.e. laser scanner) with ROS. I have demonstrated examples with two real devices namely the Asus Live Pro RGBD camera, and Hokuyo laser range finder.", "The content of the new section includes the following:", "I am planning to add a intrusion detection system project using laser scanner with ROS.", "Also, previously a complete new section on OpenCV was introduced and demonstrated 13 different topics.", "Next major update will be presentation of transformations with tf package in ROS.", "This course is a shortcut for any new ROS user to learn ROS by demonstration and in efficient way.", "You can enroll to the course with a discount on", "\n", "Thanks", "\nAnis", "Hi!", "Congratulation for new course Professor Koubaa.", "I go there to see.", "Thanks.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Background information on laser range finders and their characteristics", "Overview of main commercially available laser range finder used with ROS", "How to connect an Asus Live Pro camera with ROS using openni2 drivers and how to convert a depth image into a laser scan topic, and how to visualize it with rviz", "How to connect a Hokuyo laser range finder using urg_node driver and how to create a static transform to visualize with rviz", "How to record scan topic data with rosbag, and how to replay and visualize it with rviz", "How to create a subscriber ROS node to scan topic and process scan message to find max, min and average ranges (Python, (and C++ soon))", "How to configure the parameters of laser range finder with ROS (coming soon)"], "url": "https://discourse.ros.org/t/learn-about-laser-range-finders-with-ros-in-my-udemy-course/5717"}
,{"title": "A toolkit for Reinforcement Learning using ROS and Gazebo", "thread_contents": ["For those interested in Reinforcement Learning, here\u2019s some recent results obtained at at Erle.", "Briefly,", "While the paper gets published in arXiv you can temporarily access a summary of this work at ", "hi, very cool, but why/for-what do you use openai.gym ? Do you know ", "hi ", "but why/for-what do you use openai.gym ?", "Given the recent popularity of the OpenAI gym, we\u2019ve used the ", " to facilitate a common interface for RL problems in robotics. That is, ", ".", "Do you have code for this ? I am trying to use Gazebo for reinforcement learning to do hand object grasping with DQN. It would be great if you can share how you coded binding Gazebo and open AI Gym", "Do you have code for this ?", "If I\u2019m not wrong, the friends of Erle have the code in github: ", "Thank you very much. This is really helpful.", "I would strongly recommend looking into DDPG, TRPO, A3C, or FAN for a grasping task. DQN doesn\u2019t perform well for robotics tasks due to its discrete action space and poor data efficiency.", "I would strongly recommend looking into DDPG, TRPO, A3C, or FAN for a grasping task. DQN doesn\u2019t perform well for robotics tasks due to its discrete action space and poor data efficiency.", ", that\u2019s interesting, thanks for sharing.", "\nCan you point out any benchmark or results that compares DQN with these different techniques? A  pointer to a paper would also do.", "hi ", ", that sounds interresting. I build a robotic humanoid hand (", ") and i am also interrested into building a ai-software for hand grasping. Do you have already any code and where? thanks", "No I am working on it. And I was not able to find some related work. I don\u2019t have a code yet.", "Thank you for your idea.", " What is FAN stands for? Could you point out links or full name of FAN?", "perhaps your interested as start in code ", " . Its a ROS node with tensorflow. The code projects the angle of a phalanx into a picture and then feeds a DQN.", "Sure, here are some references and reading material.", "A benchmark that sadly doesn\u2019t include DQN, but does include TRPO and DDPG:", "\n", "1199.79 KB", "\n", "DDPG:", "\n", "648.14 KB", "\n", "Asynchronous RL learning showing improved performance of asynchronous actor critic over asynchronous Q learning.", "\n", "2.20 MB", "\n", " Apologies, it\u2019s NAF not FAN. It was designed for robotic manipulation and outperforms DDPG. Paper is here:", "\n", "2.26 MB", "\n", "Hi Victor,", "is your code connecting ROS/Gazebo to OpenAI gym easy to extend to other robots and tasks? I would like to use the openAI interface with a Robotis Mini robot that I am simulating in Gazebo.", "Thanks!", "\u2013roberto", "Also, I saw this other project: ", "\nDid you know about it?", "For those interested, a follow up work on this topic that will be presented at ROSCon this year:", "\n", "Full article available at ", ".", "That training speed up sounds very interesting.", "\nI tried searching for the framework but couldn\u2019t find it tho, is it published yet?", "Hello ", ",", "The proposal of the paper named \u201crobot_gym\u201d is built upon ", ".  There\u2019re no plans to release our particular setup (which is what\u2019s discussed in this paper).", "If you\u2019re interested, you should be able to reproduce such setup yourself using gym-gazebo and customize it to your needs. There\u2019re some ", " that will facilitate the process.", "Hi Victor,", "\ndoes your presentation have any relation with the ROS package ", " or is it something different?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-toolkit-for-reinforcement-learning-using-ros-and-gazebo/442"}
,{"title": "Final call: ROS Summer Course 2018 - Online & Certificate", "thread_contents": ["==============================================================", "\nIntensive ROS Summer Online Course", "Date: August 20 - September 1, 2018", "Duration: 2 Weeks", "Apply here: ", "==============================================================", "Available spots: 30 students", "Languages: English", "ROS certification: Provided", "Practice-based intensive ROS learning", "A completely ROS course that combines theory and practice. We will give you the basic tools and knowledge to understand and create basics ROS related project fast. With comprehensive exercises with diverse simulations, you will get fast into the ROS development world in a more efficient way.", "You will learn ROS by executing code and using diverse robot simulations in a visual and efficient way", "You will develop to projects where you will apply what you have learned in each unit", "You will perform 2 exams to test the knowledge that you have acquired along the week. Only those students that get 8 points out of 10 will get the certification issued by The Construct", "Monday", "Unit 1: How ROS Basic Structure works", "Start Unit 2: ROS Topics - Work on Project -1 Section 1", "Tuesday", "Finish Unit 2: ROS Topics", "Unit 5: ROS Debugging Tools", "Fish Project -1 Section 1", "Wednesday", "Start Unit 3: ROS Services", "Fish Project -1 Section 2", "Thursday", "Finish Unit 3: ROS Services", "Start Unit 4: ROS Actions", "Fish Project -1 Section 3", "Friday", "Finish Unit 4: ROS Actions", "Fish Project -1 Section 4", "Saturday", "Monday", "Unit 6: Mapping", "Unit 7: Localization", "Unit 8: Path Planning", "Work on Project -2 Section 1", "Tuesday", "Unit 9: Face recognition", "Unit 10: People detection", "Work on Project -2 Section 2", "Wednesday", "Unit 11: ROS Control 1", "Unit 12: ROS Control 2", "Unit 13: ROS Control 3", "Work on Project -2 Section 3", "Thursday", "Unit 14: ROS Industrial 1", "Unit 15: ROS Industria 2", "Unit 16: ROS Industria 3", "Work on Project -2 Section 4", "Friday", "Unit 17: OpenAI ROS 1", "Unit 18: OpenAI ROS 2", "Unit 19: OpenAI ROS 3", "Work on Project -2 Section 5", "Saturday", "Husky", "BB-8", "WAM ARM", "Parrot AR.Drone", "Sphero", "Turtlebot 2", "Jibo", "Mira robot", "Fetch", "UR5", "FULL DAY DEDICATION", "Basic knowledge of Linux shell", "Basic knowledge of Python programming", "A laptop ( can have any operating system)", "NO PRIOR KNOWLEDGE OF ROS REQUIRED", "The Construct ", "An online integrated platform for learning and developing ROS-based Robots.", "You can contact us with questions and doubts here: ", "What does this certification actually mean? Has anyone in industry or OSRF given it a rubber stamp to indicate that this is a useful course and gives enough context to be meaningful in a hiring cycle?", "I think this topic has been brought up before without much as to a conclusion or actions from The Construct to validate that the industry wants this or that this particular course offers what is needed for me as a hiring manager to look at this and say \u201cthis person knows enough ROS to satisfy my needs\u201d for a ~$600 USD pricetag at the expense of students trying to get a job in the field.", "I\u2019d love to hear some feedback from the construct as to who they\u2019ve chatted with as accepting this certification as they advertise on the page that is makes them \u201cjob ready\u201d.", "Thanks for the comment Steve,", "\nwe have not got a third party certification. The whole course is based in our experience as workers, researchers and teachers in the ROS world for several years.", "Since you work for a robotics company and you are a ROS expert, we would be very happy to have a live chat with you and hear your opinion about which requirements must a ROS course have in order to be certifiable as \u2018job ready\u2019.", "No attachment from your side! Just we would like to know your opinion and start building a DB of experts in the field opinions, so the next time that somebody asks this question, we can at least, provide a statistic from the experts.", "Does that suit you?", "P.S. Sorry for the late reply. But we believe that better late than never.", "Hello ", ",", "Rather than asking who\u2019s certifying the ROS courses, the actual question that matters to us at Acutronic Robotics is what do these profiles that include an item listed as ", " or ", " actually mean with that.", "That simply they\u2019ve installed ROS and went through the ", " tutorials? That it was part of one of their grad or undergrad courses? That they\u2019ve contributed to the navigation stack? Not likely this last one since those that have contributed to relevant packages, tend to list it. At least that\u2019s my experience hiring.", "Of course, you could claim that you somehow figure that out through questioning and exercises but we particularly would benefit a lot optimizing this step. Having people sending their applications with small certifications (at this stage, all the ROS courses are self-certified including ", "\u2019s but also the ones from other sources such as Udacity, EdX or others) allow us to go through the ROS-specific syllabus and understand what these profiles \u201cshould know\u201d.", "Looking at ", " courses, IMHO ", " is starting to become wide enough to cover relevant topics of ROS.", "Thank you Victor for stating so clear what is actually relevant to address on a ROS certificate.", "Let\u2019s use your comments to ignite the creation of a open definition of ROS certification levels, defining the subjects and exercises that need to be mastered on them. Also let\u2019s include how the certification process should be ensured, so any academy implementing it can actually certify that their students are really certified on those terms.", "We are going to create a group here in Discourse so anybody from the industry, research and ROS development world can have a say and help create a common syllabus, and testing procedure for a certification for any academy in the world.", "We can call this an open source certification ", "I think we might be going in a different direction than I intended. It\u2019s not that I think its important there\u2019s some generalized set of things someone must learn to qualify for a given \u201ccertification level\u201d, its the fact that the curriculum seems to not have been created with informed opinions from industry about elements that should be included to represent a real hiring advantage by having the certification you are trying to provide. If this was branded as just a course to learn ROS that\u2019s one thing, but that\u2019s not how the webpages read.", "When you make claims like \u201cjob ready\u201d without actually talking to folks that are hiring roboticists/ROS users, it seems like false advertising and some mis-information for non-robotics folks/graduate students - potentially leading to false expectations of their skill level leaving or the marketability of a certification. A certification won\u2019t mean much (or anything) to anyone hiring roboticists if they didn\u2019t have a say in what was covered ", " even knowing a list of things this certification would let me expect them to know. Udacity can do what they do because the coursework is widely available and I can see what it is someone with a nano-degree is expected to know and generally the skillsets taught in a course like that are reasonably in-depth (as far as you can in a few months) for the ~800 price tag.", "I don\u2019t think any organization - except by a larger decree from OR or similar status organization - should define what it means to be certified in ROS or the surrounding ecosystem. However if you made aware what your certification does, and it actually represents a significant volume of knowledge beyond that of a graduate student\u2019s messing around with research for a few years, that could represent value to hiring folks. But that\u2019s still not really enough to claim \u201cjob ready\u201d, I think that claim requires some type of partnership or extensive polling of robotics companies to make this certification carry weight to be a meaningful influencer in hiring cycles.", "tl;dr  this seems to be a more extended version of what ", " mentioned, with an additional dive into the \u201cjob ready\u201d claim that I take issue with.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Registration deadline: August 16, 2018", "\n", "\n", "\n", "\n", "\n", "\n", "Learn ROS by programming robots:", "Apply to ROS-Projects:", "Exam & ROS Certification:", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Exam 1", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Exam 2", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/final-call-ros-summer-course-2018-online-certificate/5606"}
,{"title": "Announcing SolidWorks to URDF Exporter 1.5", "thread_contents": ["It is my pleasure to announce that we\u2019ve made a huge update to the SolidWorks URDF Exporter. A big thank you is due to ", " for inspiring and supporting the work and also to the team at ", ".", "From Verb Surgical:", "These new features identified by Verb are included in over 61 new merged Pull Requests. Some of the biggest:", "We\u2019ve also taken this opportunity to migrate from BitBucket to Github hosted under the ROS organization.", "SolidWorks to URDF Exporter. Contribute to ros/solidworks_urdf_exporter development by creating an account on GitHub.", "Please see the wiki for a download link and more information.", "Again, thank you to Verb Surgical and PickNik, but also thank you to the large number of users who continue to use this tool. I never could have imagined my Willow Garage intern project would be so widely used today!", "\nVerb Surgical, along with physicians and administrators, is creating the future of surgery. A future with improved patient outcomes, better information and greater hospital efficiency around the world. Our mission is to \u201cdemocratize surgery\u201d globally. Verbs are known to connect someone who does with something that needs doing. In the future, our actions will connect surgeons to an end-to-end platform for surgery, including pre-operative planning, intra-operative decision making and post-operative care.", "\nPickNik is supporting the worldwide open source robotics movement through community building, consulting expertise, and the development of highly-capable motion planning software. PickNik combines world-class robotics expertise and state of the art open source robotics frameworks to save you time and money.", "\nStephen Brawner is the original developer and current maintainer of the SolidWorks URDF Exporter. An experienced researcher in robotics and AI with code deployed on real-world robots, Stephen offers the know-how and insight to tackle a wide array of projects.", "Great work. This will  be very useful.", "I waited a long time for it! Great work!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["All custom inputs you add to your URDF configuration are now saved internally. No longer will you have to re-input joint limits or other hand-entered items.", "Optionally export the URDF without creating meshes. Have some meshes that you already like? Now you can just quickly update the URDF with a handy new button!", "Import/Export from a CSV file: For some larger robot models, it\u2019s not practical to track all properties in the SolidWorks model. Now you can import specific properties, like existing meshes, mass/inertia properties or joint information from a CSV file.", "Unit and integration testing: New tests will help ensure that new code is validated against existing SW model examples.", "Tons of bug fixes!"], "url": "https://discourse.ros.org/t/announcing-solidworks-to-urdf-exporter-1-5/9242"}
,{"title": "ROS acceleration using Xilinx AI Edge computing", "thread_contents": ["Future Robots are smarter and capable of making decisions in real-time. Decision making requires the use of AI for performing classification and semantic segmentation. Such AI algorithms require the use of dedicated hardware for making decisions at the edge. This group is meant for discussing subjects related to the integration of the:", "The following slack workspace (", ") is available for online discussing.", "\nXilinx Edge AI framework,", "The Xilinx Edge AI Platform provides comprehensive tools and models which utilize unique deep compression and hardware-accelerated Deep Learning technology.", "The platform provides efficient, convenient and economical inference deployments for embedded-CPU-based FPGAs.", "The Xilinx AI team consists of renowned researchers and experienced professionals known for their pioneering work in the field of deep learning.", "I would tempted to encourage people start by looking at Ultra96.", "Thanks ", ". I have experience using the following boards:", "I would like to encourage you to share your experience using the Ultra96", "\nKind Regards", "\nPedro", "The Ultra96-V2 updates and refreshes the Ultra96 product that was released in 2018. Like Ultra96, the Ultra96-V2 is an Arm-based, Xilinx Zynq UltraScale+ \u2122 MPSoC development board based on the Linaro", "\n", "\n", "The Ultra96-V2 updates and refreshes the Ultra96 product that was released in 2018. Like Ultra96, the Ultra96-V2 is an Arm-based, Xilinx Zynq UltraScale+ \u2122 MPSoC development board based on the Linaro 96Boards Consumer Edition (CE) specification....", "\n", "\n", "Avnet has announced the new Ultra96 development board, compatible with the 96Boards Consumer Edition specification from Linaro. The Ultra96\u2026", "\n", "\n", "Ultra96\u2122 is an ARM-based, Xilinx Zynq UltraScale+\u2122 MPSoC development board based on the Linaro 96Boards specification. The 96Boards\u2019 specifications are open and define a standard board layout for development platforms that can be used by software...", "\n", "\n", "One of the best Edge AI platforms out there today following 96Boards specification.", "For full disclosure  - I am the author of 96Boards specification.", ",", "\nHave you come across any Ultra96 project where ROS is used? If so can you share it here?", "\u2026 I am also totally biased, as I created ", " platform.", "We wanted to use the EV version of the Zynq MPSoC to allow the \u201cEdge-AI\u201d to benefit from the integrated codecs, etc. + give more fabric and more memory. The VCS platform is also in Open Source, but has a much higher cost, intended for industrial application.", "Even Xilinx like Sundance\u2019s \u2018story\u2019 and efforts - see ", " - and", "The Ultra96 is a \u2018gift\u2019 from Farnell/Avnet to help R&D and I can only recommend buying one to learn about Xilinx Vitis and Edge-AI.", "i got some experience on ", ", including ROS1 to publish the sensor data.", "Hi ", ",", "\nCan I kindly ask you to share your experience here? Perhaps a simple tutorial.", "In my experience the xilinx workflow works really well, IF the model you are trying to deploy was built with the same version of caffe or tensorflow as your DECENT and DNNC. Trying to deploy pre-trained models you find online can be tricky and time-consuming (unless they come from the AI model zoo of course ", " then you are one step removed from a DPU kernel). Deploying your own models should be fine if you align your environments.", "I\u2019m also interested to see if and when pytorch support arrives. I\u2019m currently working with DNNDK on the ZCU102, maybe the newer VITIS AI (built on top of DNNDK) has support for more frameworks and easier integration of models built with/against different versions of tensorflow and caffe?", "Hi ", ",", "\nPlease join our growing community on slack ", ".", "\nLets make our life easier.", "\nBw,", "\nPedro", "we did internally put some login in PL for doing pre-process for imaging (not so much i can share), and the CPU receives that pre-processed data and publishes as ROS topic. this is long time ago, but if i remember correctly, i did use ROS kinetic.", "hope that helps you.", "Hi ", ",", "\nCan you please join our slack group ", "Hi ", ", thank you for the invite, however we got a ZCU102 loaner to do some testing for a project. We will be handing that back over soon. So I am not sure if Ill be handling any FPGAs for the foreseeable future. I am interested in the subject in general, but I dont think I\u2019ll be contributing much.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS", "ROS2", "OpenCV4", "Librealsense2", "Xilinx DPU TRM"], "url": "https://discourse.ros.org/t/ros-acceleration-using-xilinx-ai-edge-computing/11995"}
,{"title": "ROS2 Real-time Working Group Online Meeting 5 - Nov 13, 2019, 7AM PDT (UTC-7)", "thread_contents": ["Hi ROS2 RTWG members, our next meeting will be in 8 hours (sorry for the late notice).", "Agenda:", "Please add if you have more topics.", "Call coordinates are in the calendar invite for RTWG and below:", "Time: Nov 13, 2019 07:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "Meeting ID: 171 629 184", "\nPassword: 803967", "One tap mobile", "\n+16699006833,171629184# US (San Jose)", "\n+19294362866,171629184# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 171 629 184", "\nFind your local number: ", "i wish i could join, but cannot make it cz of the time difference.", "\ncould you share minutes later as usual?", "thanks,", "unfortunately, I could not attend this meeting as I was on a business trip. In the next meeting, we (", ", ", ", ", ") would like to present and discuss the LET executor for rcl and the corresponding PR: ", "A small suggesting for the announcement of the WG meetings: Could you please add them to the ROS 2 WGs calender at ", " ?", "In the next meeting, we (", ", ", ", ", ") would like to present and discuss the LET executor for rcl and the corresponding PR: ", " great. How much time do you need for the presentation and what would be the objective that you\u2019d like to achieve after the presentation (asking just to organize and make the meeting efficient)?", "Could you please add them to the ROS 2 WGs calender at ", "I have the meeting added to the Events calendar as suggested by ", " here: ", " (see at the bottom).", "I see that this calendar ", " is not being used anymore. ", " is that correct?", ", thank you for the quick reply. Let us stick with the calendar ", " that you already used.", "Regarding the presentation on November 27th: I propose to plan 5 to 10min presentation + 10min discussion. The goal is to decide whether the LET Executor functionality should be included into the rcl repo and how to organize it with respect to the package structure.", " where do I find the meeting minutes of yesterdays RTWG meeting?", ", I should have talked to my colleague ", "  first ", " I was not aware that he already presented the LET Executor in the meeting on November 13. Nevertheless, at the next meeting, we should please discuss with the WG participants and in particular ", " where and how to integrate the LET Executor and the C convenience functions for RCL, cf. ", ". Several options are conceivable (rcl, rclc, \u2026). As the next meeting will be after the Eloquent release, that\u2019s probably the right point in time to discuss this PR.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Recap workshop RosCon 19", "Update on WIP", "Performance measurement\n", "Erik says that there are different and biased performance test benchmarks for DDS floating around.", "\n", "\n", "\n", "Bosch and Nobleo are running the experiments and also measuring the performance with this code:\n", "testbench Generator: ", "\n", "testbench for the static rclcpp executor (nobleo): ", "\n", "With above packages you can: split up benchmark, configure the number of publishers, subscribers, essentially it is a benchmark generator, simple way to generate test cases & can see overhead in DDS, rmw, \u2026", "\n", "\n", "Different contributors to the ROS 2 ecosystem use benchmarks & tools that are either all different - and thus cannot be compared - or simply provide invalid results.\n", "ADLINK\n", "benchmark for Cyclone DDS", "\n", "eProsima\n", "benchmark for Fast-RTPS", "\n", "Bosch & Nobleo\n", "benchmarks/analyses for:\n", "Static executor", "LET ", " executor", "\n", "using:\n", "\n", " + ", " (instrumentation, tracing, and analysis/visualization)", "\n", "\n", "New ROS 2 real-time inverted pendulum demo (from Carlos)\n", "looking at memory allocation, page faults, context switches, etc.", "using ", " + ", " (instrumentation, tracing, and analysis/visualization)", "\n", "\n", "LET rcl executor\n", "Bosch created a pull request on ", " which includes the LET rcl executor and some helper functions (rcl_ext) (to simplify Setting up rcl handles (like Publishers, Subscribers, timers ) on C API", "\n", " says that it should  go into a different repository. Possibly rclc could be revived.", "\n", "Single-process, real-time rmw\n", "\n", " writing of GitHub issues", "\n", "\n", " => ", "  and ", "  discussed inclusion of LTTng tracing\n", "\n", "  Common package for real-time settings would be nice => memlock, setting scheduling policies, cpu affinity", "\n", "  thinks that there are 2 objectives for such a package: to either make it easier to use or portable.", "\n", "AI: Dejan to write a longer Discourse post on how to measure performance correctly", "AI2: Silexica to implement their Performance Testing Platform for ROS community", "AI3: ", " to finalize the issues for  single-process, real-time rmw", "\n", " LET executor: ", "\n", "\n", "  single-process, real-time rmw"], "url": "https://discourse.ros.org/t/ros2-real-time-working-group-online-meeting-5-nov-13-2019-7am-pdt-utc-7/11460"}
,{"title": "Supporting / maintaining SLAM in ROS2 - input requested", "thread_contents": ["Our Nav2 WG has been using a ROS2 ported fork of ", " for doing SLAM.", " has been kindly keeping this updated but it is forked from the upstream ", " project, which doesn\u2019t have a ros2 branch. My previous attempt to broach the subject there was ", " which basically boomeranged back to me to create an RFC and take it to the google cartographer ", ", which hasn\u2019t met in 7 months, in order to get it approved and a ROS2 branch created. This is obviously a hassle, but I\u2019d be potentially willing to do it, except it doesn\u2019t look like that meeting is happening anymore and I\u2019m not even sure that repo is being maintained anymore, based on the last commit having been back in May.", "If anyone ", " what is going on with cartographer and whether it is still being supported / maintained, I\u2019d love to hear that.", "BUT, based on what I\u2019m seeing, I believe we need a well-supported, maintained SLAM for ROS2, and I\u2019m opening the discussion here for suggestions as to who and what that should be going forward.", "Is this meant to be just 2D SLAM or also 3D? Also does it make sense to simply fork Cartographer and maintain it separately until (if) we see any more activity from the original project (though like, it\u2019s a Google project so there\u2019s every reason to believe that it\u2019s just dead now). What about Karto?", "I\u2019d also like to shamelessly plug one of my projects as a possible contender. I\u2019ve been working on what is essentially a rewrite of open_karto but built to be \u201cpython first\u201d and more flexible (e.g. supply scan matching for a new kind of sensor and the SLAM would come for free). It\u2019s also faster than Karto based on what I\u2019ve seen and the ultimate goal is to unlock life long mapping. It\u2019s almost ready for primetime (just finished the ROS 1 node). I\u2019m committed to working on it and maintaining for at least a few more years and I\u2019ve deliberately kept the code as simple as possible. I believe I can explain the code fully to another engineer in a single afternoon (in fact I did that for a colleague just a few days ago). This should help with maintainability. If there\u2019s interest I can accelerate work on a ROS 2 wrapper.", "A complete 2D and 3D graph SLAM implementation using plagiarized code from Karto - safijari/yag-slam", "From the weekly, I finally (one armed, no less ", " motorcycles are dangerous, don\u2019t get one) ported slam toolbox to ROS2. You\u2019ll get mapping, localization, and lifelong mapping capabilities in complete if I did everything right. The only non-ported items are related to the rviz plugin and interactive markers which I have tickets open in the appropriate repos.", "Slam Toolbox for lifelong mapping and localization in potentially massive maps - SteveMacenski/slam_toolbox", "Building in build farm as we speak and should be installable in the next dashing sync. I\u2019m not sure if anyone at Intel has the cycles to play with it, but expect a similar level of support for this project as I give navigation2.", "So I guess I should clarify, I\u2019d really like something that meets these criteria:", "Basically, something that would be a workhorse for ROS2, always reliable and very good performance, and well-maintained.", "I also think it\u2019s a good thing if there are more than one that meet those criteria.", "I think that\u2019s both incredibly reasonable and unrealistic for the moment. Different options meet different bullets. I\u2019d say my work meets the bullets of that that are most important: efficient, documented, debians, and open (sure, not apache, but I make no terms against commercial use, just to give back, which I find very rational given the time organizations I\u2019ve been in have dumped into it). Gmapping if ported would be 4 of those. Karto 3.", "Karto nor gmapping provided PR testing or have really been maintained at all in the last 5 years. Gmapping for the majority of its life was non-commercial and Karto is GPL. The only example I could give that had all that is cartographer@ros1. I would like very much so to have gmapping die in ROS1, its really past its prime by really any other available option.", "Do you see another non-cartographer example in ROS1 that provided those that we could look to port? These are important points but tempering with past projects is useful.", "I\u2019m not even sure that repo is being maintained anymore, based on the last commit having been back in May.", "It looks like the official print cloud project for cartographer has been rather active as of late: 11hr ago", "View billions of points in your browser. Contribute to googlecartographer/point_cloud_viewer development by creating an account on GitHub.", "But I don\u2019t see a large overlap in active viewer maintainers and original cartographer_ros maintainers.", " - I\u2019m not excluding your slam toolbox as the potential right solution, just trying to clarify what I think requirements are. I\u2019m sure others have their opinions as to what is needed, I\u2019m trying to get some of those as input too.", "I don\u2019t know of any other non-cartographer example in ROS1 that meets all the criteria I listed, which is partly why I started this thread. I\u2019d really like to see someone in the community or TSC step up and volunteer to take ownership. It seems like there are quite a few SLAM implementations out there but they\u2019re generally released as part of a conference or academic paper, maintained for a little while, then abandoned. This is the problem I see that I\u2019d like to see solved for ROS2.", " - thanks for the info on the point cloud repo. I still have my concerns however, until I hear some commitment from the maintainers themselves.", "When ", " I discussed this earlier, I think we came to the conclusion:", "I believe that we left off with wanting to propose this to the ", " developers, but as you said, there hasn\u2019t been much traction there.", "I guess I\u2019m confused as to why the wrapper needs to be upstream and can\u2019t be a repo under OSRF.", "You are correct, and it could be.  I think ideally it would be maintained in the same place that the ", " wrapper is maintained, so that improvements can be ported across the versions.  That saves a bit of effort in shuttling things back and forth across forks.", "Another thread mentioned that the OSRF ros2 port doesn\u2019t contain recent changes which the cartographer folks say make substantial  improvements. I think Matt\u2019s goal is to have it committed to and maintained by Google so OR isn\u2019t constantly chasing changes in Cartographer/ros1 wrapper that they (probably?) don\u2019t have time or long term resources to do if we can get Google to do it.", " my goal with that comment wasn\u2019t to push ST as much as point out that our options with that check list are very limited and we might want to temper expectations from looking at history unless there\u2019s someone standing up saying they\u2019ll do the work & maintain under a ROS org long term. With that list of requirements, I see 2 most reasonable options: we reapproach Google folks again on ROS2 and re-explain the importance or we port Gmapping and write the testing infrastructure ourselves. With that said, the last substantive change to gmapping was years ago so I\u2019m not entirely sure the automated testing pipeline is totally necessary with the small cadence of changes.", "I\u2019d prefer option 1 since I really hoped to leave gmapping behind. Then there are 3rd party options like what Jari and I have presented amongst others, I can create an overview of the other ones I know about that are more or less equivalent (REP105 frames + lidar) but I wouldn\u2019t place much hope there will be a long term support plan as many are as you mentioned for a paper and widely untouched afterwards. Something used by companies in production is more attractive to me since there\u2019s a group that has invested interest in it working, which at minimum Jari and I\u2019s would have.", "To add to this: having used gmapping, and then Karto and Slam Toolbox in production I\u2019m 100% with ", " on letting gmapping die with ROS 1. Karto and Slam Toolbox work infinitely better and are more lightweight*.", "*And the more I look into Cartographer the more I\u2019m convinced the original framework presented by Karto is much more straight forward (trying to follow code paths in Cartographer make my head spin) and seemingly just as flexible as the one present in Cartographer. I feel like this matters for maintainability even if the original maintainers drop off.", "trying to follow code paths in Cartographer make my head spin", "Couldn\u2019t agree more. I\u2019ve found reading the standard template library to be easier than that. SLAM Toolbox has 50% less code in entirety than Cartographers ", ". Reading their paper I\u2019m in love with their ideas but I can\u2019t match that up to the actual code that exists. Not to say our stuff is trivial to understand, but its at least readable and well commented.", "Edit: sorry this is here nor there. I think Cartographer is a reasonable option regardless. I\u2019d just like to narrow down to finite options and discuss what\u2019s the best direction amongst them to move forward with. I think there will be some compromises any way we go- but step 1 is what are the options. So far I\u2019m hearing: cartographer, karto, gmapping, slam toolbox, and yag", "I\u2019m surprised that nobody suggested the idea of starting a ROS2 SLAM effort from scratch. Therefore let me do so.", "My SLAM experience is mostly ROS1 based and over the years I got to \u2018play\u2019 with many - if not most - of the SLAM frameworks out there. During that time I always had a feeling of frustration because (choose any, non exhaustive) the framework \u2026", "I believe all these cons already somewhat make the case for developing a ROS2 SLAM framework.", "Notice that I am not suggesting to re-implement karto, or cartographer, or ORB-SLAM but a framework organizing the data-flow much like ", " with defined API, base classes and tooling. Such framework would be fully ROS2 based and emphasize modularity and flexibility. As for the initial demo, a simple 2D pose-graph (karto-like) could be implemented.", "I do realize this is a substantial effort, likely one for a dedicated WG, thus I\u2019m only mentioning the idea to see the interest and feedback it gathers.", "Post post edit: I fear we may be leaving the domain of the original discussion.", "I can\u2019t comment on if it would be beneficial to set up SLAM similar to the nav stack but all the reasons you mention are the reasons I wrote my own SLAM package (or rather, rewrote most of Karto, ", "). Most SLAM/VIO/VSLAM/LOAM implementations seem to ultimately come down to", "So, yeah. If those 4 pieces are sufficiently abstracted the choice of hardware should be meaningless as long as folks can supply 1 and 3, I think (I\u2019m not very good at this stuff so maybe there\u2019s nuance I\u2019m missing).", "Agreed, I think in concept its totally possible. I\u2019ve spent some time thinking about that for about a year but never had really the motivation or resources to \u201cdo it right\u201d so I haven\u2019t even started.", "Would that be awesome? Yes, absolutely, and I\u2019d love to be part of it as its happening, but for the more immediate need of a ROS2 reliable SLAM solution that\u2019s going to be looked after, we should steer back towards that. A modular-SLAM metapackage would be at least a 6-12 month undertaking.", "Post post edit: I fear we may be leaving the domain of the original discussion.", "As per the fairly open title of this thread, I don\u2019t think we are. But I\u2019d be happy to start a new thread if people prefer to.", " has essentially summarized the 4 \u2018modules\u2019 of any regular SLAM out there (others include graph-sparsification, planning and whatnot). This would be one of the layer of abstraction. Others can be devised, especially from the implementation perspective.", ", glad to see that you would like to be part of such project! I have been thinking about it for a while too and I think that ROS2 now offers pretty much all the basic pieces to build an awesome framework on top, action/srv, components etc.", "\nI agree this would be more of a mid to long-term project and does not respond to the immediate need for a mapping solution in ROS2. But you know, the earlier we talk about it \u2026", "This also assumes graph slam remains the flavor of the day, which I think will be true for awhile, but has to be extendable beyond it as well if we want it to be useful into the general future. Probably have a PF unit and plugins for deep learning units while making sure that its super performant with all that generalization.", "Mapping is one of the robot tasks I think new flavors of deep learning could eventually outshine our meaty human brains in, especially since its a non-safety critical task there\u2019s alot of leeway it can be given", "My current assumption is indeed that such framework would rely on graph-based SLAM as it is currently the ", " standard formulation. However since the framework would essentially organise data and its production, the solving aspect is therefore just another module, possibly one that allows for different formulation. But this aspect of the modularity is not quite clear to me yet.", "I do agree with you that machine learning is growing in SLAM. My view is that it \u2018only\u2019 going to be a particular implementation of a particular task (e.g. feature detection/extraction, place-recognition, odometry etc\u2026) rather than a larger (partially) end-to-end thing. In this context I\u2019m not worried about fitting those algorithms.", "\u2018only\u2019 going to be a particular implementation of a particular task", "IG, I agree with you when it comes to general perception tasks as it relates to ", " robotics. IG, I agree that most slam implementations that use deep learning use it as a feature extractor or for one of the specific tasks. IG, I think its hard for even something like an RNN or similar to have the memory to accurately accumulate data as to remove structure emposed by a graph or tree.", "But I\u2019m seeing work that is slowly changing that. I think there\u2019s going to be some structural elements to aid a DL approach, but I think we\u2019ll be seeing effective methods that aren\u2019t structured so rigidly as graphs in the near term future.", "*in general", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["High quality & performance mapping (obviously)", "Liberally licensed for use in production (BSD 3-clause or Apache 2.0 preferred)", "In a mainline ROS github organization such as \u2018ros2\u2019 or \u2018ros-perception\u2019, similar to slam_karto and openslam_gmapping", "Maintained by more than one maintainer, with a commitment to keep it current with new ROS2 releases and respond in a timely fashion to issues", "Released as debian packages", "Well documented ROS2 topic / services interfaces, tutorials", "With maintained CI, including testing pull requests, to maintain quality", "High quality & performance mapping (obviously)", "Liberally licensed for use in production (BSD 3-clause or Apache 2.0 preferred)", "In a mainline ROS github organization such as \u2018ros2\u2019 or \u2018ros-perception\u2019, similar to slam_karto and openslam_gmapping", "Maintained by more than one maintainer, with a commitment to keep it current with new ROS2 releases and respond in a timely fashion to issues", "Released as debian packages", "Well documented ROS2 topic / services interfaces, tutorials", "With maintained CI, including testing pull requests, to maintain quality", "\n", " itself is completely independant from ROS/ROS 2, so the upstream source should be valid for both.", "\n", " needs (and currently has) a port for ROS 2, but the ideal scenario would be to get it upstream into a ", " branch or as part of a unified repo (a single repo that could build for ROS/ROS2).", "\u2026 feels outdated, be it the code and/or the actual algorithm", "\u2026 has no flexibility, extending it essentially means re-writing a substantial portion of it", "\u2026 has little to no modularity. SLAM is a set of algorithms put together, but often there are no easy way to only change a chunk of it", "\u2026 does not have a somewhat standardized API that would make comparison easier", "\u2026 is too task specific, 2D-only/vision-only etc.", "\u2026 is a pain to set up (I\u2019m not pointing a finger at cartographer)", "Do some form of odom correction or odom calculation, compute uncertainty", "Build graph that encodes uncertainty", "Look for loops", "Optimize graph given some condition"], "url": "https://discourse.ros.org/t/supporting-maintaining-slam-in-ros2-input-requested/10986"}
,{"title": "SingleThreadedExecutor creates a high CPU overhead in ROS 2", "thread_contents": ["Hello,", "We are looking into the performance of ROS 2 on Embedded boards and we find out that ROS 2 consumes high CPU because of the overhead introduced by SingleThreadedExecutor. We did some tests to profile the CPU usage and we observed that if we run 20 publishers and 200 subscribers in one ROS node, 70% of the CPU is consumed by SingleThreadedExecutor and 20% of the CPU is consumed by DDS implementation.", "By running the same example in Fast RTPS directly, it consumes 3.5 times less CPU as compared to ROS 2. The tests that we have performed along with their results can be found in this link: ", ".", "Is anyone else is also looking into measuring the CPU usage of ROS 2? Please share your findings here and let us know if we are doing something wrong.", "Our current analysis suggests that the SingleThreadedExecutor needs to be optimized otherwise normal ROS 2 cannot work properly on \u2018ARM A-class\u2019 embedded boards. We are willing to look more into this problem and can help by performing more tests and providing feedback to improvements. Please let us know if there is any other way to contribute to this.", "Thank you,", "\nIshu Goel", " and myself are also currently looking at this, but I\u2019m going to give him some time to work more on it by replying in his stead ", " He has looked into this based on his tracing work, see ", " It uses LTTng to directly instrument rclcpp and rcl.", "First of all, many thanks for describing your results so openly and so early, particularly for providing the initial benchmark programs. This makes it much easier to compare and combine results.", "In general, what I\u2019ve heard from the OSRF and others is that people are somewhat aware of the inefficiencies in the executor, but nobody had exact numbers so far, and therefore this problem was so far not prioritized. I think this has now changed ", "Regarding your analysis, one thing I would caution is that \u201cperf record\u201d is a sampling approach. This means it can miss executions which are too short. I don\u2019t think this compromises your results, but since you were asking, I wanted to mention it.", "Therefore, in our work, we use LTTng, which integrates both perf event and userspace tracepoints. We have tried both instrumenting every function automatically (which has noticeable overhead), and manual instrumentation of just the most relevant functions. The latter is a bit more work, but also gives more precise results.", "About the single-threaded executor, one thing that I noticed is that it operates in the following way:", "Could the overhead of the executor be due to the fact that after every message it checks again all the entities? this can potentially be very expensive in cases like the one that you tested (200 subscriptions in the same executor)", "Moreover, the ", " function in ", " is marked as ", ".", "\n", " do you already have any idea on how it should be improved?", " Yes, that is a big part of the overhead.", "Since the wait_set only really needs to be update whenever there is a change to the entity list, it is likely that some of this effort could be avoided, or made less expensive. However, without having had a more serious look at the design, I cannot currently say what the best option would be. Maybe ", " or ", " have some ideas.", "I noticed it operates in the follow way (although I could have missed something):", "There is a list of nodes, a node has multiple callbackgroups, a group has multiple executables (eg timer, subscription, client, service, any). So basically a tree: node -> group -> executable", "The tree can be quite large, and is walked often. There seems to be room for improvement by walking/copying/searching the list of executables less (mainly step 5 & 6).", "It\u2019s all weak_ptr by design, but it must keep the memory valid (shared_ptr) as long as it\u2019s in rcl_wait, which is a bit conflicting. It\u2019s also the reason for lots of lookups in the original tree: the only way to see if something disappeared is to rebuild it.", "Is there some information on the design somewhere? Typically a executor works by just submitting executables/callables/callback to a thread(pool), where the executor just maintains a queue of work to do. This is a more complicated design that is implemented in three different layers (rclcpp/rcl/rmw).", "I also saw something on the roadmap about changing the relation between nodes/groups en refactoring the executor. Are there already more concrete ideas about this?", "Could the overhead of the executor be due to the fact that after every message it checks again all the entities?", "i had the same concern with quick code scan and tried the following patch if it affects cpu consumption,", "so far it DOES NOT reduce cpu consumption.", "my environment is", "\ndocker: 18.09.8 ros:dashing", "\nHost: Ubuntu 16.04.6 LTS / Intel\u00ae Core\u2122 i7-4790 CPU @ 3.60GHz", "Hi all,", "what do you think about this?", "already checked if i can reduce cpu consumption, it does some but not a big deal\u2026", "will dig deeper.", "Hi ", ",", "Thanks a lot for your efforts. We are also working on creating a static scheduler to see how much performance gain can be achieved. We will share our result as soon as we complete our work. Please keep sharing the results of your work.", "got it, thanks!", "\nwe will do the same!", "tomoya", "Sorry for the delay! Here are the results of the investigation done by ", " and me.", "We could replicate the earlier results, showing that the Executor consumes a lot of CPU. In that, we could distinguish two cases:", "Compared to earlier work with similar results, we took care to minimize overhead and only count time spent actually on the CPU. Therefore, we consider not just the qualitative result, but also the absolute numbers to be trustworthy.", "This has been non-trivial, because the executor calls very many, very short functions (mainly to do with ", "s). This causes problems both for traditional profiling (which adds lots of overhead) and for sampling-based profiling (which may not notice these). Just to give an idea, initialising the nodes took at least a good 10 seconds when using ", "! Without profiling, it takes ~100 ms.", "To achieve this, we use 1) explicit instrumentation of only the relevant high-level functions and 2) we capture scheduling events. This allows us to sum CPU time only when the thread is actually executing on the CPU.", "Specifically, we only looked at the main ", " functions:", "See our executor instrumentation ", ".", "As mentioned before, based on scheduling information, we only count CPU time when the thread is running on the CPU, not when it is blocked.", "We chose the ", " test case, since it has the highest CPU usage. We traced it for a few seconds. The thread itself has a CPU usage of 55.87% (this is less than the 70% overall CPU usage reported earlier, because it does not include time spent in the dedicated middleware threads).", "In our first analysis, we looked at ", " in some detail, because of the high overhead numbers reported earlier.", "As you can see, from the \u201cfunction\u201d bar, the core ", " function indeed only takes ~32% CPU, the rest is Executor overhead. However, as you can also see from the \u201cthread\u201d bar, the whole method only makes up ~18% of the CPU usage of overall thread. This means that other parts of the Executor are more important.", "Therefore, we took at step back and looked at the two high-level functions in ", ": ", " and ", ".", "The ON CPU time for each function is compared to the whole thread and to the parent function. In this case, 79.21% of the CPU time for the whole thread is spent in ", " vs. 8.22% for ", ". These numbers are similar to ", ".", "Since ", " is likely dominated by running user code, we took a closer look at the functions inside ", ": ", " and ", ".", "Here, ", " represents 67.02% of ", "'s CPU time, and 53.09% of ", " the actual CPU time for the thread!", "Looking at the code, ", " checks its lists of timers/subscriptions/services/clients/waitables and returns once it has found one that is ready to execute. As a side note, having to loop over all the lists would explain the large CPU usage difference between the ", " test case and the ", " test case, since the latter has only one node.", "If we look at the CPU usage for each function individually, we can see that ", " is indeed the most CPU-intensive function.", "The full data is below.", "In conclusion, the executor should be optimized. Figuring out if \u2013 and which \u2013 executable is ready seems to take ", " of CPU time.", "We used LTTng and the ", " & ", " packages. The Jupyter notebook which was used to get the results above can be found ", ". This post can also be found ", ", which also shows how profiling overhead can really mess with the results.", "Thank you Christophe, very nice results. Good to see that we came to the same conclusions, this makes our case even stronger. I\u2019m currently working on posting an issue on the rclcpp github where I will reference this discussion. I think your findings will be very helpful!", "Edit: The issue is now available here: ", "btw, for reference with respect to the changes ", " did: No single call is to blame. The main issue is that, for every single timer-invocation or message, the whole internal representation is traversed. In contrast, if you use the middleware directly, you can attach a listener directly to each communication objects. This avoids traversal ", ".", "However, the listener approach has the problem that we have very little control over when which message is being executed. That\u2019s precisely why ROS 2 adds executors, and can even have different ones.", "IMHO, it would help to look at the interface between rmw and the executor, to pass more information across and thus avoid traversal.", "The main issue is that, for every single timer-invocation or message, the whole internal representation is traversed. In contrast, if you use the middleware directly, you can attach a listener directly to each communication objects. This avoids traversal ", " .", "i need more time to dig deeper but i do agree on this.", "besides, since this is optimization, we might as well define reasonable goal to achieve.", "tomoya", "Just FYI,", "\ncreate \u201cexecute_any_executable_list\u201d and \u201cget_next_ready_executable_list\u201d to reap the executable event as much as possible in single iteration. (that is said if the multiple executables are ready to fire, number of iteration to reap the executables will be much less.)", "so far, we do not see much improvement.", ", and all", "could you take a look at the following PR?", "\n", "\n", "thanks,", "\nTomoya", "Hello everyone,", "Our first POC for a Static version of the Executor can be found here ", " . This version works with the latest stable release of dashing giving the following results:", "\n", "Our StaticExecutor has been added to rclcpp in such a way that the old functionality remains intact. To use our executor please follow the README. The package also contains dockerfiles to quickly inspect the CPU usage on your PC for different executors (the LET executor created by Bosch for micro-ROS is also included in this comparison).", "If you try out the docker example please share your results. It would be even better if you could use our executor with your own source code. This way it can be tested for more use-cases. If you run into bugs, please let us know! We did make some assumptions with respect to the source code, given that this is a POC (assumptions are mentioned in the README).", "We think this POC is a good first step to highlight possible performance gains. The final goal is to get an optimized Executor with proper scheduling mechanics in the core ros2 stack. We are currently working on a fork from ros2 master to create a proper PR for this version. We will keep you updated on the PR progress here.", "Rather than a fork, you could probably provide your new executor as a separate library.", "Great!!! we will look into that.", "Rather than a fork, you could probably provide your new executor as a separate library.", "+1 on this.", "thanks", "Hello everyone,", "For now we created this PR for rclcpp ", " . We are considering making the code a separate library. Having the static executor as an optional package would prevent the bloating of ROS2. However, the package would also require a maintainer. Since we are a relatively small team that plans on doing more work (creating more packages in the future), we have to consider if and what packages we want to maintain. The static executor is a relatively small package, so we could consider picking it up (this is an internal discussion we are yet to have).", "Please leave your comments and thoughts on the code under the PR. Even if the PR does not get approved, we hope to at least draw attention to the CPU overhead of the current implementation.", "Small update: We updated the dashing version of our static executor to be semi-dynamic. The node guard_conditions are used as event trigger to rebuild the wait-set and executable list. This means that when a subscriber, timer etc. is added during spin(), the executor will notice (by checking the guard_condition) and rebuild, making the use of the static executor less restrictive.", "This updated version can (still) be found here ", ".", "We will create a master (eloquent) version of this, but we first want to fix some Jenkins linter errors and do some clean up on our PR.", "If you try out our code please share your results here. Please report any bugs you find. Possible optimizations are best posted on the PR when we apply the changes there.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a node starts spinning", "receives a message (awakening from spin)", "the executor calls ", " to retrieve the entity that has to handle the message", "the message is handled by the subscription", "the executor checks again all the registered entities", "if no entities have work to do, the executor goes to sleep again", "It populates a list of all executables by walking the mentioned tree into a memorystrategy. (promote weak to shared_ptr)", "The memory strategy is then converted into a wait-set (to call the rcl)", "The wait set is waited upon. Implementation is all the way down into the RMW layer. It differs per RMW implementation.", "After the wait, only ready executables are left in the waitset (not null).", "The memory strategy is updated with this list (remove all that are not ready, to allow weak_ptr to cease)", "For a ready executable, the group is retrieved from the original tree by searching the entire tree.", "Execute", "Go back to step 6, if more executables where ready, otherwise go to step 1", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "when there are few or no messages (e.g., for a timer-driven node), then the ", " method causes the majority of the overhead, with 70% of its time spent in ", " and only 30% (excluding waiting) spent in the RMW layer and below. We determined this using the \u201cnopub\u201d, pure timer benchmark.", "when there are many messages, the majority of the CPU usage \u2013 up to about ", " in our tests \u2013 is caused by the ", " function. This is pure ", ". We determined this using scg\u2019s \u201cros\u201d benchmark, which sends 10000 small messages per second.", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/singlethreadedexecutor-creates-a-high-cpu-overhead-in-ros-2/10077"}
,{"title": "ROS2 bagging w.r.t. services and actions", "thread_contents": ["Hi, I was hoping to get a bit of clarity on something that\u2019s been in the back of my mind regarding a migration to ROS2. At one point long ago, I remember reading that there was some consideration for building services over DDS pub/sub, and I thought \u2018great, that means we\u2019ll probably get first-class support for service bagging\u2019. Lack of services is a huge pain point with using bags as a system logging mechanism in ROS1.", "At some point, the landscape shifted and ROS2 services were implemented via DDS-RPC instead of pub/sub, which I imagine precludes using a side-channel recording mechanism like bagging. Sadly, with ROS2 actions being (rightfully) implemented via services, this means neither services nor actions are baggable in ROS2. That\u2019s really a shame - I\u2019m sure I\u2019m not the only one who occasionally used ROS1 ", " in some capacity just because they were baggable.", "Understanding that ROS2 development is a sea of shifting priorities - is this something that would even be possible to resolve without fundamentally upending the design? Can it be \u2018solved\u2019 at the rmw implementation layer, or would it require more fundamental changes?", "I had never even thought about bagging services and actions. Now I also want this!", "At some point, the landscape shifted and ROS2 services were implemented via DDS-RPC instead of pub/sub,", "DDS-RPC uses topics to implement services.", "And other implementations, like OpenSplice, use our own version of this on based on topics.", "However, Services in ROS 2 do not have to implemented with Topics, as you pointed out. They are their own concept in the rmw API, this was done to allow for optimizations for Services if desired. Making them on top of Topics always would perhaps not be the most efficient thing to do.", "What\u2019s prevented us from recording them is having some rmw API for observing the exchanges between a client and server by a third party (like rosbag). We could add this API, though it may make it hard for future rmw\u2019s which don\u2019t use a one/many to many mechanism to implement services, which presumably would be where the efficiency gains would come from (by using a one to one comm pattern like gRPC/HTTP2 or something).", "One thought, that wouldn\u2019t require changes to ", ", is that we could simply republish requests and responses on a well known topic name. For example, if the service were called ", ", there might be a ", " and ", ", publish to by the client and service respectively. They could be activated selectively (to keep overhead low, activating this by default for all services would be expensive I think), and that would enable recording of the services.", "I think recording is the only thing that makes sense though. I don\u2019t see how replaying requests or responses directly makes sense.", "One thought, that wouldn\u2019t require changes to ", " , is that we could simply republish requests and responses on a well known topic name", "That\u2019s an interesting suggestion. I would potentially take that one step further and introduce a new generic topic similar to ", " or ", " that has all the service and action request, response, and feedback logged to it. I think that could actually have a variety of useful consequences.", "For example the topic may have the information like action/service name, type, timestamp, a string/serialized-version of the request/response/feedback and the caller (assuming that\u2019s available, which I think it is). The downside clearly is in trying to make a general message to go over that topic, the request/response/feedback wont be in their native types but as strings or serialized blobs. Though it could be conceivable that with the type and string/serialized a ", " tool could convert them into real types for playback/reading.", "there might be a  ", "  and  ", " , publish to by the client and service respectively. They could be activated selectively (to keep overhead low, activating this by default for all services would be expensive I think)", "That\u2019s a great approach! What would be the expensive part? I imagine if a bag recorder exists that whitelists ", " for recording, it could just subscribe to those topics. If no bag recorder exists, isn\u2019t it a performance no-op to create a topic with no subscribers?", "I think recording is the only thing that makes sense though. I don\u2019t see how replaying requests or responses directly makes sense.", "I imagine there\u2019s some esoteric cases where someone may want a bag playback to emit service calls to an external node, but I can\u2019t think of anytime I\u2019ve seen a practical application.", "new generic topic similar to  ", "  or  ", "This would probably not be as useful as the namespaced version proposed by ", ". A topic like that would be an all-or-nothing firehose, and potentially prohibitive to subscribe to from a large node\u2026 short of a DDS mechanism like keyed topics, which may not even be exposed in ROS2 (yet).", "I would personally just find it annoying to have to enumerate all the servers to bag up, ", ", ", ", ", " for ", " servers. I perfer the one-stop-shop firehouse that I can pare down on debug.", "If no bag recorder exists, isn\u2019t it a performance no-op to create a topic with no subscribers?", "There\u2019s always overhead to having more things, even if there\u2019s no actual match. There\u2019s discovery traffic and memory usage at least. And since every node will likely have a few services the impact is large even if the individual fixed cost is small.", "I would potentially take that one step further and introduce a new generic topic similar to ", " or ", " that has all the service and action request, response, and feedback logged to it. I think that could actually have a variety of useful consequences.", "How would you log them? As a string? You cannot have different types on the same topic (or you shouldn\u2019t). If you\u2019re logging them as a string, you could just use ", ".", "I would personally just find it annoying to have to enumerate all the servers to bag up, ", " , ", " , ", " for ", " servers. I perfer the one-stop-shop firehouse that I can pare down on debug.", "A parameter to rosbag could be used to toggle recording all topics matching ", " and ", ". It wouldn\u2019t require the user to manually enumerate all services to record.", "I guess action feedback can already be recorded since it is a topic.", "I had never even thought about bagging services and actions. Now I also want this!", "That\u2019s\u2026 surprising. ", "How about bagging-via-tracing?", "It\u2019s a bit of a wild idea, and make it feasible, we would need to support the Windows and MacOS tracing frameworks, in addition to Linux, and we would need to look at making this location transparent, but it ", " an existing, low overhead, tunable, configurable, data capture mechanism which can support any source.", "That\u2019s an intriguing idea, but I\u2019m curious about how low the overhead would really be? One of the advantages of capturing via subscription is that you don\u2019t directly impact the execution time of the sender (assuming you allow for enough computing resources, etc., of course). How much would capturing large data via tracing impact the execution time? Or is there zero impact?", "How about bagging-via-tracing?", "That honestly sounds like a great idea, although probably a completely separate discussion :). Between that or something like bagging-via-pcap, it would be great to have an option to record data without having to depend on the underlying connections to function reliably.", "It looks like the rosbag2 storage backend is pluggable, but unless I\u2019m missing something, transport is not.", "How much would capturing large data via tracing impact the execution time? Or is there zero impact?", "In general, I would expect it to have less impact than the current rosbag approach. Before going down that route, we should probably test this hypothesis, however.", "The way this works is as follows: The tracing frameworks I\u2019ve looked at make a copy of the data and store it in a lock-free ring-buffer inside the process for later retrieval by the capture process. That happens at the point where the tracepoint inserted, and it will block until the data has been copied.", "The later retrieval and disk storage by the capture process happens asynchronously, however, so \u2013 apart from consuming CPU and disk bandwidth \u2013 storage does not impact the traced process.", "Of course, the exact impact also depends on where you\u2019re putting the tracepoint. The easiest way would be trace messages when they are serialized. That way, you don\u2019t have to deal with the exact type, and just store a byte array. This would be the same pathway that also sends data to rosbag via subscription.", "Now, when you have an application that sends around images intra-process, they would not normally be serialized. Adding ", " kind of recording will change that, and thus impact the system. With tracing, you could, in principle, generate a message-type specific tracepoint, however, and avoid (DDS) serialization. That\u2019s advanced stuff, but it could be done.", "What I\u2019m not sure about is how this all compares, performance-wise, to the new shared-memory transports that are currently being introduced.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-bagging-w-r-t-services-and-actions/11618"}
,{"title": "Intraprocess communication with services", "thread_contents": ["I was wondering if anyone can tell me if a ros2.0 service server and client exist in the same process can they share data with shared memory as publishers and subscribers can?  The ", " gives a great example of sharing pointers between publishers and subscribers but does not mention if services can support this.", "Thanks", "Unfortunately no, we only have support for pub/sub intraprocess. Though I believe it is possible to support Services (the implementation would be slightly different from pub/sub), we just haven\u2019t had the time to implement it yet.", "Why is it necessary for a intra-process service to share memory ? I thought services are mainly used to call specific functions of a sub/pub in a node.", "Why is it necessary for a intra-process service to share memory ?", "Well, they don\u2019t need to share memory, but that\u2019s the most efficient thing to do when dealing with intra-process.", "I thought services are mainly used to call specific functions of a sub/pub in a node.", "Services may or may not be implemented with pub/sub, but that\u2019s up to the middleware implementation. Even if services are implemented with pub/sub, that implementation would occur below where the intra-process optimizations for pub/sub occur.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/intraprocess-communication-with-services/2655"}
,{"title": "Dashboard of ROS2 next focus and high priority for upcoming release or next direction", "thread_contents": ["The ros2 beta3 has been released recently, and the community ultimately know what\u2019s included in it now and thank u for your great contribution ", " .", "\nHowever, what\u2019s the features which are planned for the next release on Dec 13th, 2017 ? I\u2019m curious whether there\u2019s a place to make clear what\u2019s the current focus and high priority for the next of ROS2 for ROS users,  I try to find and unfortunately I failed to get this kind of information ", " , so how about a dashboard to get the ROS users/developers aligned about the ROS2 current focus and high priority to be more efficient to contribute on the same page ?  and this also benefits the newcomer to spend a lot time/effort to make clear the ROS2 next direction from various scattered information while ROS community gets larger and larger, for example, from the long discussion thread of each component of ROS2 github.", "However, what\u2019s the features which are planned for the next release on Dec 13th, 2017 ?", "As the roadmap already states: ", "The feature list for the next version will be filled out shortly after the Beta 3 release.", "thanks, that\u2019s okay if there\u2019s a bulletin for the feature list in the roadmap.", "\none more question, you know, ros is made up with different component packages respectively. do you think that it\u2019s necessary to brief a summary to the current focus and high priority (maybe design change / technical direction or decisions) of a component ?  the change is dynamic and maybe it\u2019s hard to clarify everything and requires some discussion, but these kind of information is valuable.", "By the way, can the community developer apply the access to ", " ?", "do you think that it\u2019s necessary to brief a summary to the current focus and high priority (maybe design change / technical direction or decisions) of a component ?  the change is dynamic and maybe it\u2019s hard to clarify everything and requires some discussion, but these kind of information is valuable.", "I am sorry, but I don\u2019t understand the question. Can you maybe elaborate on this a bit more?", "By the way, can the community developer apply the access to ", " ?", "The ROS 2 Jira board is not public atm. All relevant information should be available in public tickets on GitHub.", "do you think that it\u2019s necessary to brief a summary to the current focus and high priority (maybe design change / technical direction or decisions) of a component ?  the change is dynamic and maybe it\u2019s hard to clarify everything and requires some discussion, but these kind of information is valuable.", "I am sorry, but I don\u2019t understand the question. Can you maybe elaborate on this a bit more?", "Now ROS has 2 trunks, one is ROS and the other is ROS2, and each has many different components. maybe the ROS2 will be ultimately the final in the future.", "\nFor ROS2, there\u2019re new feature development or component porting from ROS, so\u2026is it possible to summarize some information regarding what the current focus or priority for a component ? maybe it\u2019s long to elaborate every details for a component, and that\u2019s waste of time, so the short key message with key points is enough for the newcomers get the status of a component, for example:", "Based on the easy to make clear the status of a component, it helps the efficiency for ROS users to employ it or get hands dirty to further contribute it in the same page while ROS community gets larger and larger ", "This is actually a question each and every maintainer of a ROS packages has to answer.", "We (at OSRF) are currently working on a lot of \u201cfoundational\u201d stuff: basically what is enumerated on the roadmap. While we try to demonstrate the usability and improvements using demos we don\u2019t maintain many of the higher level packages in ROS.", "E.g. for ", " we are trying to rewrite the existing code base to support both ROS versions (see ", "). But each component (navigation, moveit, etc.) will have to come up with their own plan. For other packages like ", " we haven\u2019t even started to plan how to support ROS 2 with it.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["the key features to develop", "the key blocker to use that component in ROS2,", "the totally technical redesign info from ROS", "the next direction if the above info is undetermined", "\nand so on\u2026", "\nFor a specific instance here, ROS2 navigation, maybe it\u2019s really hard to know what\u2019s going on in ROS2 for most community users."], "url": "https://discourse.ros.org/t/dashboard-of-ros2-next-focus-and-high-priority-for-upcoming-release-or-next-direction/2700"}
,{"title": "Multi threaded subscription callbacks", "thread_contents": ["I\u2019m hoping someone can solve a debate we\u2019re having.", "Does each subscription callback run in its own thread? Or do they round-robin in a single thread based on the data order received?", "I guess we have assumed that a node with multiple subscriptions were spawning those callbacks in their own thread, which makes things performant. We\u2019ve even been implementing some data protectors on shared data because of this assumption.", "If this isn\u2019t the case, then our callbacks must be fast enough, and our algorithms na\u00efve enough to hide the fact that data is being processed sequentially.", "Does each subscription callback run in its own thread? Or do they round-robin in a single thread based on the data order received?", "Depends ", "In both C++ and Python we have this concept of Executors. A single threaded executor (the default if you\u2019re using ", ") is merely round-robin in the blocking spin call (in the thread in which you call spin). There is also a multi-threaded executor which will create a set of threads which will be dispatched work in round-robin fashion.", "Right this second there is no way to have a separate thread per subscription (which was possible in ROS 1 with \u201ccallback queue\u2019s\u201d), but we want to change that. Right now, the most granular you can get is one executor per node (and therefore one thread per node in the case of a single threaded executor).", "Have a look at the single threaded executors, they just wait for something to do, do it, and then loop:", "The multi-threaded executors do something like: acquire a lock, wait for something to do, claim it, release lock, do it, loop. The C++ one will do this in each thread (each thread is identical), but the thread you call ", " on will just wait for the other threads to join (sort of wasted atm):", "But the Python one works differently, it uses the thread in which ", " is called to wait for work, then dispatches that work to a \u201cthread pool executor\u201d (Python concept, not ours) via the ", " method (", ") which actually executes the user\u2019s callback:", "You\u2019ll notice there is nothing special about these executors, and you can create your own which let you have complete control over how many threads and how they are utilized.", "I guess we have assumed that a node with multiple subscriptions were spawning those callbacks in their own thread, which makes things performant.", "That is not the case, and whether or not that\u2019s true depends on your definition of performant ", "If you want to reduce overhead and latency, then creating threads for each callback would be very inefficient. For utilizing multicore systems as much as possible (another definition of performant), then you\u2019ll want at least some threads, but still you\u2019d ideally want to only have as few threads as possible and reuse them rather than create them frequently.", "Lucky we give you all the tools required to control threading, you can use a single thread or multiple threads or even create your own executor and do what ever you want.", "If this isn\u2019t the case, then our callbacks must be fast enough, and our algorithms na\u00efve enough to hide the fact that data is being processed sequentially.", "This is where the concept of \u201ccallback groups\u201d comes in, I think. The idea of a callback group is that everything with a callback belongs to one (timers, subscriptions, service clients/servers) and the type of callback group determines how the executor will treat them. For instance, two callbacks (same from two different timers) in a single \u201cmutually exclusive\u201d callback group will never be executed at the same time as one another by the executor. However, if you place them in separate mutually exclusive callback groups, then they could be executed at the same time as one another (and therefore data shared between them needs to be protected). There is also a \u201creentrant\u201d callback group, which means that not only can a callback be called at the same time as other callbacks, it can be called multiple times concurrently.", "The purpose of the callback groups is to let the user describe the synchronization coupling between different callbacks without tying that to the threading model. For instance, you can describe a callback as reentrant, but that does not mean it will be called concurrently, because if you use a single threaded executor it will not, but if you used a multithreaded executor it might (if for example there is more than one message to be processed simultaneously).", "Like the executors you can create your own callback groups to express any constraint you might have.", "The Python ones are probably the easiest to understand:", "Sorry for the lack of documentation, but we\u2019ve been trying to make sure this all makes sense before finishing it off and documenting it throughly. Any feedback on the pattern is welcome.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["C++:", "auto any_exec = get_next_executable();", "execute_any_executable(any_exec);", "Python:", "handler, entity, node = next(self.wait_for_ready_callbacks(timeout_sec=timeout_sec))", "handler()", "executor::AnyExecutable::SharedPtr any_exec;", "{", "  std::lock_guard<std::mutex> wait_lock(wait_mutex_);", "  if (!rclcpp::utilities::ok() || !spinning.load()) {", "    return;", "  }", "  any_exec = get_next_executable();", "}", "execute_any_executable(any_exec);", "handler, entity, node = next(self.wait_for_ready_callbacks(timeout_sec=timeout_sec))", "self._executor.submit(handler)", "class ReentrantCallbackGroup(CallbackGroup):", "\"\"\"Allow callbacks to be executed in parallel without restriction.\"\"\"", "\n", "def can_execute(self, entity):", "    return True", "\n", "def beginning_execution(self, entity):", "    return True", "\n", "def ending_execution(self, entity):", "    pass", "\n", "\n", "class MutuallyExclusiveCallbackGroup(CallbackGroup):", "\"\"\"Allow only one callback to be executing at a time.\"\"\"", "\n", "def __init__(self):", "    super().__init__()", "    self._active_entity = None", "    self._lock = Lock()"], "url": "https://discourse.ros.org/t/multi-threaded-subscription-callbacks/2863"}
,{"title": "ROS2: anything equivalent to topic_tools:ShapeShifter?", "thread_contents": ["Hi,", "I am start looking at the source code of ROS2 to understand how message are serialized.", "\nMy goal is to update ", " to ROS2.", "I GUESS that serialization details are defined by the middleware (OpenSplice, eProsilica, etc), aren\u2019t them?", "Is there anything equivalent to ", ".? Is it even possible to have something that works similarly?", "Is there any way in ROS2 to introspect the content of a message at run-time?", "Cheers", "Davide", "No, unfortunately we don\u2019t have that yet. ", " was working on it for this release, but we ran into some issues figuring out how to most efficiently get data in and out of the underlying DDS vendors without (de)serializing. The API we\u2019ll provide will be relatively simple, something like ", " and ", " and some wrapper like ", " in our C++ API, but it\u2019s the \u201chow\u201d that\u2019s held us up.", "But we\u2019re still looking at this issue and it\u2019s in our critical path as we need it to implement rosbag and to implement \u201ctype masquerading\u201d.", "I\u2019ll try to remember to reply here when we get the first version of that new API figured out.", "The other thing I suppose you\u2019ll need is a way to parse the \u201cROS IDL\u201d files, e.g. ", "? We don\u2019t currently have that in anything but Python. I see that you have one for ROS 1 in C++:", "As part of your package, which is awesome ", ", but it would likely need to be updated to support the new syntax we support in ROS 2 (see ", ", specifically the part about bounded strings and arrays).", "Also, it would be awesome to have a separate library for parsing the various interface files in C/C++, so it could be reused everywhere.", "There\u2019s also the possibility to have another, more machine friendly, version of the interfaces. For example, as part of generation at build time we could generate an xml or json file which represents the contents of the corresponding ", " file. Then you could lookup and load that file instead (with an xml or json library), which would save you from needing to writing parsing code, and instead you would only need to interpret the contents correctly.", "This is a feature I\u2019d really like to see well supported in both ROS 1 and ROS 2, it would be especially helpful in making the bridge between the two more convenient to use with non-standard types. So, once we get you unblocked by providing something like ShapeShifter, I\u2019d be really interested in helping you with anything you\u2019re missing to implement it.", "Awesome, I am looking forward for it.", "The \u201cmachine friendly\u201d format you mentioned would (of course) be VERY welcome but not strictly necessary, since I have done already most of the heavy lifting in terms of parsing.", "One quick question. My runtime deserialization was created doing a \u201creverse engineering\u201d of how ROS1 messages are created.", "\nIn DDS is the serialization protocol standardized or middleware dependent? Since DDS is known to have good interoperability, I am tempted to think that the former is true\u2026", "The DDS serialization protocol is indeed standardized. It uses a format called Extended Common Data Representation (XCDR). The most up-to-date reference is the DDS-XTYPES specification version 1.2. See ", "Well, we\u2019re going to need to provide a function which can serialize and deserialize a ", " given an output structure. For example, with ", " it might be like:", "The implementation of this imagined ", " function may end up calling an implementation specific function, or at the very least how it is deserialized should be an implementation detail.", "But of course what you want is more like:", "We have the beginnings of this in ", ", but I\u2019m 100% certain how much is still needed to support this at the user level. We might not be able to use any of that package at the user level, since it isn\u2019t always used in the implementation, but it is at least representing the structure of a message as C++ structs and objects which is part of what we\u2019d need to do. My initial guess is that we would need to have a way to generate the same typesupport structures but given an input string or structure which describes the type.", "So I\u2019d actually say we should try to avoid your package needing to reverse engineer how to deserialize, instead that should be provided by our abstraction layer. This allows us to protect the stuff that comes on top from underlying changes, e.g. if we changed from one version of CDR (the serialization standard used in DDS and ROS 2) to another.", "Just two side notes:", "The roadmap currently contains a task to revisit the message definition format. The ", " format currently used with some minor extensions from ROS 1 might be replaced with something more powerful since the format can\u2019t handle several of the pending feature requests. Just to be consider when spending effort on implementing the current format in different languages.", "While the serialization protocol of the DDS implementations follows a single standard you can\u2019t rely on the rmw impl. to be DDS. There has e.g. be efforts to implement the rmw interface with other middlewares like OPC UA.", "Summarizing:", "THINGS WE MIGHT DO:", "A) I guess that the first step is to add to ROSIDL the equivalent of ", ". Preferably expressed as JSON or XML format.", "B) It must be possible to subscribe to a topic in a \u201cgeneric\u201d way, i.e. bypassing the deserialization step and accessign the raw bytes of the message.", "C) To implement a run-time deserializer, I don\u2019t see any solution but implementing a different code for each protocol ", " No magic wands.", "A) I guess that the first step is to add to ROSIDL the equivalent of ros::message_traits::Definition< Type >::value(). Preferably expressed as JSON or XML format.", "I assume the value should contain the message description in a readable format. Why should that be provided through a language specific symbol? I would suggest to provide this information through the ament resource index which would make it available across languages.", "what I meant is that ros::message_traits::Definition< Type >::value() return a ", "But this static C string contains the information in a easy to parse format that any language can easily parse.", "Something like this ", "what I meant is that ros::message_traits::Definition< Type >::value() return a const char*", "I think exposing this information (even a preprocessed version to actually store e.g. yaml) through the ament resource index would be better in order to make it available to any language.", "ok, I got your point.", "Well, it seems that at least for eProsima, there are some open source pieces of software that can help me with the this task. But it will take me time to dig into the source code", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "Type erased messages are needed for applications such as rosbag, rqt_plot, PlotJuggler, MATLAB importers, etc. Any generic topic subscriber indeed.", "ShapeShifter had all the informations needed to do this: raw bytes buffer, Message definition and MD5Sum.", "ROS1 had just one protocol, whilst ROS2 can potentially have DDS or others."], "url": "https://discourse.ros.org/t/ros2-anything-equivalent-to-topic-tools-shapeshifter/3393"}
,{"title": "Discussion about uint8[] type transport in ROS2 python", "thread_contents": ["Hi , I\u2019m trying to start a discussion about what type should ", " be converted to", "Background:", "Test comparison result:", "issue description :", "Our attempts", "So if the attempt 1 seems good to you , we\u2019d like just use the attempt 1 in our code .", "\nBut if the ", " is important for some upper application or algorithm in existing codes , then we will try to finish attempt 2", "link to the ", "Opinion Invitation:", "\n", "\n", "Appreciating your opinion !", "Just a few suggestions on maybe how to deal with this:", "Given this is kinda a\u2026 corner case, just by monkeypatching the method to use a converting method that\u2019s already prepared for the usecase:", "Of course, this implies generating the the thing I called ", ". A better name would come up from the experts, I\u2019m sure. Or there could be a function to just enable that in a more friendly way:", "Or make an option to choose to use the old list way when instantiating a message that contains this specific problem.", "Or generate two messages when a field of type uint8[] appears:", "Or even allow for every field the mode of conversion in a generic way (keeping the default the most efficient):", "Well, just throwing some ideas around. The monkeypatching way, with maybe a good method name to make it more friendly is probably my favorite.", "Thanks \uff01 I\u2019ll try your suggestion !", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/discussion-about-uint8-type-transport-in-ros2-python/4633"}
,{"title": "Publication: Security and Performance Considerations in ROS 2: A Balancing Act", "thread_contents": ["A joint US-Australia effort resulted in this publication I wanted to share with the ROS 2 community.  Full link below, and abstract here:", "Robot Operating System (ROS) 2 is a ground-up re-design of ROS 1 to support performance critical cyber-physical systems (CPSs) using the Data Distribution Service (DDS) middleware. Accordingly, the security of ROS 2 is highly reliant on the security of its DDS communication protocol. However, finding a balance between the performance and security is non-trivial task. Inappropriate security implementations may cause not only significant loss on performance of the system, but also security failures in the system. In this paper, we provide an analysis of the DDS security protocol as well as an overview on how to find the balance between performance and security. To accomplish this, we evaluate the latency and throughput of the communication protocols of ROS 2 in both wired and wireless networks, and measure the efficiency loss caused by the enabling of security protocols such as Virtual Private Network (VPN) and DDS security protocol in ROS 2 in both network setups. The result can be directly used by robotics developers to find the optimal and balanced settings of ROS 2 applications. Additionally, we analyzed the security specification of DDS using existing security standards and tested the implementation of the DDS protocol by performing static analysis. The results of this work can be used to enhance the security of ROS 2.", "Full link:", "\n", ", you\u2019d be probably be interested on this.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/publication-security-and-performance-considerations-in-ros-2-a-balancing-act/6212"}
,{"title": "Latency and throughput in ROS2", "thread_contents": ["The paper ", " evaluates the latency of ROS2 (different node configurations, different DDS implementations, different QoS policies, etc.).", "Does someone know if these results have been updated by some more recent work(s)?", "We will release a plugin-based tool that will let you evaluate latency and memory resource while altering number of pubs/subs, QoS, RT settings, large and small message types, publishing frequency, security settings, inter and intra process communication, etc. We need 2-3 more weeks, stay tuned.", "That\u2019s fantastic ", ", looking forward to have a look.", "On our side, we recently released a first tech report (", ") that\u2019s part of a series that hopefully will characterize better the latencies and throughput in ROS 2 while taking in consideration Real-Time aspects. This first tech report treats OSI layers 1 and 2.", "Hi all, we released the performance_test tool: ", " to e.g. benchmark latency, jitter, lost samples, etc. in different DDS implementations.", "\nCurrently supported benchmarking is for communication mean over:", "We plan to extend it to use ROS1 comms as well.", "Hope you find it useful, any feedaback is more than welcome.", "D.", "Hello !", "Nice tool ", " !", "What are the requirements on the RMW layer to be tested ?", "\nI have a ", ", but it currently only support simple pub/sub.", "\nIt would be interesting to measure performance with your tool and compare it to DDS implementations.", "Also (for latency), is it only measuring the end-to-end latency, or can it be more comprehensive ? For example measuring the time spent in each layer (RCL/RMW/DDS/Network).", "Hi ", ",", "I am the maintainer of the performance test tool.", "If you already have RMW implementation which supports pub/sub you should be able to directly test your communication mean without any additional work. You just need to set the proper environment variable ", " before starting the tool.", "It can not messure latency based on application layers unfortunately, for this it would have to be invasive in all these layers.", "But you can create a new communication plugin for the NDN transport as I did for FastRTPS here:  ", ".", "This will allow you to compare only the communication frameworks performance and will also give you some insight over the overhead the various RMW layers introduce.", "If you run into issues implementing the plugin I will be glad to support you.", "Thank you for your answer ", ",", "I was lacking some features (typesupport, proper management of multithreading, etc\u2026), but in the end I was able to test my stack with your package. Also, I already implemented the invasive solution for measuring the latency of each layers. I will ask you if I need help, thank you again.", "Hi ", ",", "Also, I already implemented the invasive solution for measuring the latency of each layers. I will ask you if I need help, thank you again.", "Would you mind sharing some more details about your solution for measuring latency in each layer? Maybe we could even integrate it into the performance_test itself.", "I didn\u2019t do anything complex, but since it is an invasive way of measuring, I don\u2019t know if it is easy to integrate into the performance_test. ", ", and then post processed with a python script. It is not very precise, but since I don\u2019t want the real latency (only being able to compare the two implementations), it is ok for me. To avoid the print extra cost during the experiment, you can register the events+timestamp in a (pre-allocated) table, and print everything at the end.", "Very nice. I was thinking if someone could do a  ros message extension to add-in the latencies at each layers in a private area of the message at each layer (encode as a BLOB) and could be retrieved at the subscriber to measure-in.", "For those interested, a new technical report studying this topic is available: ", ".", "Here\u2019s another update: ", "In this work we present an experimental setup to show the suitability of ROS 2.0 for real-time robotic applications. We disclose an evaluation of ROS 2.0 communications in a robotic inter-component (hardware) communication case on top of Linux. We benchmark and study the worst case latencies and missed deadlines to characterize ROS 2.0 communications for real-time applications. We demonstrate experimentally how computation and network congestion impacts the communication latencies and ultimately, propose a setup that, under certain conditions, mitigates these delays and obtains bounded traffic.", "Compared to other results:", "Hello Victor, thanks a lot for your reports, I\u2019m reading them right now and I came to a sentence that I don\u2019t understand. On ", " on the 4th page you say:", "Additionally, as Ethernet is asynchronous, the high priority frames sharing the same link can content between them.", "What does ", " mean in that context?", "Sorry if it\u2019s a bit of a picky question.", "Oops, later on I found the usage of ", " which now makes sense to me. I guess that one was a typo ", "Hey there ", "! It certainly sounds like a typo. Many thanks for reporting. Let us review it internally and report back if our mistakes go beyond that.", "Cheers!", "An another one:", "A new generation of robot systems which are modular, flexible and safe for human-robot interaction are needed. Existing cobots seem to meet only the later and require a modular approach to improve their reconfigurability and interoperability. We propose a new sub-class of cobots named M-cobots which tackle these problems. In particular, we discuss the relevance of synchronization for these systems, analyze it and demonstrate how with a properly configured M-cobot, we are able to obtain a) distributed sub-microsecond clock synchronization accuracy among modules, b) timestamping accuracy of ROS 2.0 messages under 100 microseconds and c) millisecond-level end-to-end communication latencies, even when disturbed with networking overloads of up to 90 % of the network capacity.", "Read the tech report at ", "Hi Victor,", "Thank you for the reports related to latency using RT_PREMPT linux and ROS 2 with various network settings. It was very interesting to read.", "Have couple of questions.", "In the base RT_PREEMPT linux kernel report (", "), I understand Table-III and Table-IV is what matters. But, while looking at, Table-II (Roundtrip latency results with RT normal), was curious if you know what might be the reason for TX traffic at 100Mbps, the MAX latency is considerably high at 25ms? I would expect latency to be high when RX traffic at 100Mbps", "In the ROS 2 evaluation report (", "), in Fig 5-a, when the system is idle, DDS2 has high MAX latency(4ms) compared to others. Was just curious which DDS implementation is this and what might be the reason?", "In the Fog 6-f of the ROS 2 evaluation report, at 80Mbps, where it cannot meet the deadlines and dropped packets, was curious, is ksoftirqd processing the packets the primary cause for the latency or can it be the DDS layer causing the latency. Also, about packets being dropped, would setting the size of kernel socket buffers ( net.core.rmem*, net.core.wmem*) would help too.", "Thanks", "\nAnup", " see 3 questions above by folks from Apex.", "Hi ", ", ", "Thanks for your feedback, I will try to clarify some of your doubts:", "Both TX and RX paths are suffering from the context change to the ksoftirqd threads but in a different way. In the transmission path both streams are going trough the same Qdisc queue. When there are packets pending to be transmitted in the Qdisc queue they are sent from the ksoftirqd context. At some point the fair scheduler decides that the ksoftirqd thread has consumed enough CPU and it is preempted. During this time, packets are accumulated and we observe high latencies in the order of milliseconds. For 100 Mbps it looks like the RX path, packet are processed more efficiently. This is probably because the Ksoftirqd context is not triggered all the time and part of these packets are processed in the Ethernet IRQ thread which has real-time priority. However, when we increased the network load of the concurrent traffic (>200Mbps) we observed also high latencies even in the RT normal case.", "For fig 5a and 5b we were using the default configuration of each DDS. In the case of that DDS the default configuration might not be optimized for low bounded latencies but for other purposes. However, when for the real-time settings (fig 5c and 5d) we customized the configuration of that DDS and the problem was solved.", "In this case we had 80 Mbps non-ROS 2.0 concurrent traffic with the ROS 2.0 round-trip traffic. As there is no contention in the DDS layers the problem was very likely caused in the kernel level. Posterior analysis tracing the kernel confirmed our suspects. Changing the socket queues may prevent packet drop but would not solve the root of the problem which is going to cause latency. The real problem is caused by how the net processing is deferred to ksoftirqd context. For the moment we can only mitigate these problems and expect this is solved in the new kernel releases.", "My colleague ", " just answered ", ". Ping me if you guys are around IROS and would like to discuss this face to face.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["FastRTPS directly", "ROS 2 rmw layer (and thus any supported rmw_* implementation)", "Connext DDS Micro directly", "All the measurements have been made in embedded devices.", "We measure latencies in a inter-component scenario. Given the lack of synchronization mechanisms (in this particular work we did not set them up), we use round-trip (ping-pong).", "Previous work focuses on the measurement of local latencies while we measure distributed ones.", "We measure how communications are affected in stressed conditions. This is the best way to show if the communication stack is well configured for real-time (which connects to previous work ", " and ", ").", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/latency-and-throughput-in-ros2/4367"}
,{"title": "ROS graph information tools implementation discussion", "thread_contents": ["The stretch goals for Beta 1 (due in mid-December) include rostopic. rostopic is a faily vital tool when developing with ROS, and is probably one of the most-used tools in the ROS-based robot developer\u2019s toolkit. However, there are some challenges in developing rostopic for ROS 2, particularly the ", " command.", "In ROS 1, rostopic can easily go off and get the list of known topics from the master, because the master Knows All. This approach is not so straight forward in ROS 2. The use of distributed discovery in DDS means that, by default, no one knows for certain the entire state of the DDS network. Furthermore, when a new participant starts up, it has absolutely no knowledge of the state of the network (beyond what may be hard-coded in) and has to wait for the discovery process to being producing results. This process may be very quick for things on the local machine, but for remote computers it is likely to take a significant time.", "If we implement ", " by making it start up, wait for a given length of time, then print the result, there is a good chance that it will not give a complete picture of the ROS graph and so will not be a useful tool.", "The purpose of this topic is to decide how to implement rostopic such that it provides the same rapid response as ROS 1, while dealing with the distributed nature of the graph in ROS2, and with as complete information as possible given the limitations of distributed discovery.", "To get right into it, my proposal is:", "The daemon mentioned above could be easily extended to also provide information for rosnode, rosservice, and similar commands.", "If we use the above approach, I would like to fix the ROS topics provided by the deamon as well as any other protocol interfaces/ports, and data formats (e.g. YAML) so that they are useful by other tools and tool implementors, such as rviz and rqt_graph.", "There has also been work done on rostopic and friends by the fine folks at Erle Robotics:", "Their implementation I think uses a listener waiting for information to pour in on a specific topic. However rather than trying to summarise their work myself, it would be great if ", " could drop by and give us a description himself.", "Hey ", ",", "Totally agree with your reasoning above. Good hearing that there\u2019s someone else interested on this :).", "The issue you pointed out above summarizes our work pretty nicely I believe. We had an initial (more complex) implementation for OpenSplice and then switched to FastRTPS. As for now, we\u2019ve got simple  ", " and ", " functionalities published and available. Changes are needed in ", ", ", ", ", ", ", ".", "Motivated by your comments I just went ahead and submitted a set of pull requests to integrate the changes needed upstream. Here they are:", ", i think you\u2019ll be interested in this.", "\nCheers!", "A dedicated information service seems to be a good approach, and I\u2019m not overly worried about introducing a point of failure there.", "However, when googling for this, some patents pooped up! Checkout ", " and ", "  They are not exact matches, but particularly \u201cnetwork assisted peer discovery\u201d is pretty close.", "btw, from the first patent, you can find a number of other patents related to DDS. I know this is tangential to your question, but it has me a bit worried here.", "Could you give a brief overview of your approach, for a basis of discussion? How much does it differ from my proposal above?", " That\u2019s a little disturbing. There\u2019s probably prior art (I basically described DNS), but it could put off companies.", "Could you give a brief overview of your approach, for a basis of discussion? How much does it differ from my proposal above?", ", the current proposal from our side (shared in the PRs above) uses the FastRTPS primitives to inspect the existing DDS participants/topics and report those using the ROS tooling. This simple approach did the job for us but we understand that a more generic and DDS-vendor agnostic layer might be put in place at some point.", "Hope that helped clarifying our approach.", "I am very interested in this discussion, as we at ASI have been playing around with ROS2 a lot. We had implemented our own version of ", " by using ", "We haven\u2019t had the time to really dig into the rcl and rmw layers to understand fully what\u2019s going on, but using this call has worked out fairly well. I will attest to ", " statement that using this call comes at the cost of time. We have to let the tool spin for many seconds (currently around 15 seconds) just to be sure we\u2019ve collected info from everyone in the system. Pretty inconvenient from a timing perspective, but still better than nothing!", "Also, I\u2019ve noticed a new flaw in this approach which I think may be related to my previous discussion ", ". When we set the Participant Index for OpenSplice to \u2018none\u2019 I think it\u2019s causing the get_topic_names_and_types() to come up empty-handed. Or at least, it\u2019s coming up empty in a sporadic way. I still have to investigate that some more.", "As an aside, we\u2019ve also got a rostopic echo working on most basic messages. It\u2019s a simple python hack-fest where we have a python script that does:", "This has helped tremendously, even as hacky as it is.", "I could post the code if anyone thinks it\u2019ll be useful.", "I think the idea of a daemon process to optionally provide the requested information faster is a good approach ", "I would like to comment on several aspects mentioned in the thread:", "Imo all of the following cases should work:", "The command line tools are only one use case of that interface. It should be possible to write code on-top of it which accesses the information without caring about the internal optimization of having a daemon. So all the functionality should be exposed in an API which is then also being used to implement the cli.", "I am not convinced that the daemon should have a ROS-based ", " order to call its service the client would still need to wait for some discovery phase. Even if that is local it will take additional time which we want to avoid.", "\nTherefore I think the daemon should provide its interface using a different protocol. That protocol needs to work without any \u201cdiscover\u201d and be available across all targeted platforms.", "With the choice of the protocol comes also the decision how that daemon is being identified. It could be one single daemon per ROS graph or it could be one daemon per host to make the queries \u201clocal\u201d. If the daemon would e.g. expose its interface via d-bus you would probably run it on each host. Someone mentioned a REST service it could be either a single global one or a local one per host. Anyway we need to determine how the daemon is being referred to in the context of running to separate ROS systems on a single machine.", "Any of these tools and interfaces should be independent of any specific rmw implementation. If the rmw interface is being implemented by the newest and hotest discovery / marshalling / transport solution the tools should continue to work. The rmw interface might need to be extended to provide all the necessary information.", "Non-Roboticist outsider looking in.", "Service Discovery in a distributed environment while not necessarily a solved problem has many implementations currently in use in production in a variety of companies and deployments.", "Of the top of my head I\u2019m thinking about EtcD and Consul.", "Has anything like that been considered?", "Glad to see that many people manifested interest in this discussion.", "\nAs expected these tools are needed by everybody and it resulted on a lot of prototype solutions being created by each and everyone of us. Hence the need for this discussion to provide a global rmw agnostic solution for these tools.", "Thanks ", " for this great summary.", "A couple notes / open questions on top of what has been stated:", "Agreed on ", " comment that the daemon should be seen as a way to provide information faster rather than as a single-point of failure. The feature should hence work without it (at a performance cost)", " on the fact that the ROS side of things should be rmw_implementation agnostic and allow users to develop their own tools or daemons using the protocol of their choice.", "Protocol-wise I\u2019d be curious to know what the preference of all of you is. It seems to me that REST services became very popular now that most people provide web interfaces to their systems but would it be your preferred choice ?", "In the case of multiple juxtaposed ROS systems, it comes down to how do we identify \u201cdifferent\u201d ROS systems. An approach could be to have the daemon aggregates all graph information for a given DDS Domain. And thus create a deamon for each DDS Domain used. What other approach for differentiating ROS systems could be used ?", "Another point being addressed is the tradeoff between duplicating daemons (and information storage) on each host vs having a single global daemon and service. If the default protocol allows it, this could be left up to the end user by providing an option to specify when and how to launch the daemon. And the way to query daemon information should be adapted accordingly. We will still need to specify the default behavior for this.", "Imo all of the following cases should work:", "The daemon got started before (by launch, the system, wherever, this should be an implementation detail).", "The daemon gets started on demand by the first invocation of a command line tool needing it (slower on first call).", "No daemon is there but the tool should still provide reasonable results (trade off between wait time and completeness).", "The command line tools are only one use case of that interface. It should be possible to write code on-top of it which accesses the information without caring about the internal optimization of having a daemon. So all the functionality should be exposed in an API which is then also being used to implement the cli.", "Thanks for enumerating those. I agree whole-heartedly with all of them. (Especially the last! I implement my tools like that and it\u2019s been very useful.)", "I am not convinced that the daemon should have a ROS-based ", " order to call its service the client would still need to wait for some discovery phase. Even if that is local it will take additional time which we want to avoid.", "\nTherefore I think the daemon should provide its interface using a different protocol. That protocol needs to work without any \u201cdiscover\u201d and be available across all targeted platforms.", "I strongly support having a non-ROS based interface. You\u2019ve listed the advantages already. However perhaps it could be useful to have a ROS-based interface in case someone wants to use the functionality from a node? Perhaps this should be a lower-priority feature unless we find an actual use case for it.", "With the choice of the protocol comes also the decision how that daemon is being identified. It could be one single daemon per ROS graph or it could be one daemon per host to make the queries \u201clocal\u201d. If the daemon would e.g. expose its interface via d-bus you would probably run it on each host. Someone mentioned a REST service it could be either a single global one or a local one per host. Anyway we need to determine how the daemon is being referred to in the context of running to separate ROS systems on a single machine.", "Yes, there are several ways that this could go. I think that if we use an environment variable that we can build an automatic mechanism without too much trouble. For example, the rostopic tool/library could look for an environment variable defining the graph information daemon\u2019s address, and if it isn\u2019t defined attempt to contact one on the local machine. If that doesn\u2019t work then it could request the daemon be launched on the local machine.", "Any of these tools and interfaces should be independent of any specific rmw implementation. If the rmw interface is being implemented by the newest and hotest discovery / marshalling / transport solution the tools should continue to work. The rmw interface might need to be extended to provide all the necessary information.", "Absolutely agree. This is another benefit of hiding it behind a well-known service description, I think: It becomes easier to abstract away the implementation. Then we can look into using existing technologies such as those ", " mentioned and see if they do what we need already.", "Protocol-wise I\u2019d be curious to know what the preference of all of you is. It seems to me that REST services became very popular now that most people provide web interfaces to their systems but would it be your preferred choice ?", "I\u2019m in favour of a REST service as the initial goal. I\u2019ve also been told by someone locally who does a lot of tool implementations, using the browser as an interface, that he wants \u201ceverything\u201d to be available over REST. I\u2019m assuming he\u2019s talking about introspection facilities, not absolutely everything.", "However, I think the information should be available on other protocols from the same daemon if someone wants it that way. If we specify the data structure and the protocol used to access it separately then it should be relatively easy to add additional access methods.", "In the case of multiple juxtaposed ROS systems, it comes down to how do we identify \u201cdifferent\u201d ROS systems. An approach could be to have the daemon aggregates all graph information for a given DDS Domain. And thus create a deamon for each DDS Domain used. What other approach for differentiating ROS systems could be used ?", "Being able to configure which domains the daemon aggregates data for, and run different daemons for different domains, sounds like a sensible use case. Especially with the potential for using domains for security zone partitioning.", "I would check whether ", " a non-ROS (which, in this case I take to mean a non-DDS) interface is strictly necessary. Maybe we can avoid the discovery phase by other means, e.g. by pre-supplying the necessary information, assuming that the daemon runs locally (which other means would also have to assume).", "My reason for that is to avoid duplicating communication code and functionality. While at first it might appear simple to open a TCP connection and exchange some data, you have to handle edge cases, like when the port is blocked by something else, or supporting in-process communication, and so on. I think the reliance on XML-RPC in ROS1 was a cause of complexity, and one of the advantages of using DDS is that we can avoid that.", "It would still be possible, of course, to offer a non-ROS interface ", ", e.g. for use by web-based services. This would not necessarily have to run on every machine, however, but only when necessary.", "Demand starting is a tricky business. I would prefer that, in the first instance, the launch procedure starts the daemon, the client implementations falls back to DDS discovery when the daemon is not present. Also, I would prefer that when demand-starting is implemented, we do this through a minimal daemon process which loads the \u201creal\u201d process upon first connection. This again keeps demand starting functionality out of the core libraries.", "Regarding independence of the specific rmw implementation: The interface should be independent, but the internal implementation could be vendor-specific. Several DDS vendors offer this functionality already (including FastRTPS, as far as I can tell), and this would be very good to re-use. I would be worried that we\u2019re inefficient otherwise, when we bypass the vendor.", "I would check whether requiring a non-ROS (which, in this case I take to mean a non-DDS) interface is strictly necessary. Maybe we can avoid the discovery phase by other means, e.g. by pre-supplying the necessary information, assuming that the daemon runs locally (which other means would also have to assume).", "So we\u2019ve got a vote here for the ROS interface being the only interface.", "I think it is reasonable to have the same information available over different interfaces. Certain tools (e.g. web-based tools) would benefit from not needing to use the DDS stack.", "Which interfaces are ", ", and which interface is considered the \u201cdefault\u201d interface by tools, are the questions that we need to settle.", "How fast a tool using a ROS topic can start up and get the information it wants is something I hope to test today.", "My reason for that is to avoid duplicating communication code and functionality. While at first it might appear simple to open a TCP connection and exchange some data, you have to handle edge cases, like when the port is blocked by something else, or supporting in-process communication, and so on. I think the reliance on XML-RPC in ROS1 was a cause of complexity, and one of the advantages of using DDS is that we can avoid that.", "I think this overstates the difficulty. If we implement using Python, as an example, there are good libraries available for providing web services, such as Django, CherryPy and several lower-level-but-still-abstracted libraries included in Python 3.", "The counter-point there is that, like XML-RPC in ROS1, we would be increasing the number of dependencies if we use something like Django or CherryPi.", "I think this overstates the difficulty.", "I meant not just implementation difficulty, but also use-time complexity. While not entirely comparable, consider in ROS1, the use of XML-RPC was the root cause of all the mess with ROS_NAME/ROS_IP, and the various DNS resolution issues that led to. At least in my experience, lots of people struggled with that.", "You\u2019re taking in an entirely new communications stack, with all the side effects that has.", "Sure, you can try and avoid that, for this special case, but that\u2019s just why I\u2019m suggesting that we investigate whether it\u2019s really necessary first, instead of barging headlong into an implementation.", "You are right about use-time difficulty; I hadn\u2019t thought of that. The less that can go wrong, the better, from that perspective.", "I agree that investigation is necessary. I originally envisioned the ROS tools like rostopic using the DDS stack to get the information, with the REST interface being a convenience thing for makers of tools that benefit from not using the whole ROS stack. ", " would prefer it be the other way around, I think.", "Anyway, I spent some time today hacking a wholely-unscientific benchmark into the add_two_ints_client example. Here are my results, based on 100 runs on a 2015 macbook pro (because for some reason alpha8 just point blank doesn\u2019t run on my Ubuntu desktop and I haven\u2019t had time to figure out why yet). Times are in seconds.", "I would like to second the argument of ", " about use-time complexity of multiple communication mechanisms, from a different point of view: I have a student working on developer understanding of ROS and the current variety of communication mechanisms, and the consensus about the persons being interviewed is that there are too many of them.", "I am all in favor of keeping it simple, and implementation-wise, the original proposal using a daemon and the existing ROS communication mechanism looks very attractive to me. The daemon always runs and has the information available as a ROS service call, plus uses a topic to broadcast changes to the graph such that long-running tools are informed of this.", "If later on it turns out that more is needed (e.g. REST API), this can simply be implemented as extra functionality on top of this layer.", "I just want to reiterate: the reason why the current command line tools are slow is because they have to wait for the discovery phase to finish before they can query the desired information. The idea of the daemon is that it is already running before and it has already accumulated the information.", "If the command line tool wants to use the ROS interface to request information from the daemon it again needs to wait for the discovery phase to finish before it can do so. It might only need to wait for the availability of that daemon but still this implies a significant overhead in waiting time for the user. I don\u2019t think a command line tool (which e.g. is often also used for completion) can have that time penalty. While I am certainly not a big fan of having a different transport mechanism I don\u2019t see how the requirement of the lowest latency possible can be fulfilled with using a distributed peer-to-peer system like DDS.", "I guess ", " point was to use DDS communication, but find a way to avoid the discovery phase in the specific case of connecting to the daemon. It seems to me that letting a tool like rostopic know how to directly connect to the daemon with DDS is not a much different problem from letting the tool know how to connect to the daemon through some other protocol.", "Thanks to ", " for making my point better than I did ", " Yes, that is exactly what I was tryint to suggest.", "if the command line tool wants to use the ROS interface to request", "\ninformation from the daemon it again needs to wait for the discovery", "\nphase to finish before it can do so.", "Not necessarily. See ", ", for example.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A daemon is started by  something, e.g. roslaunch, or on system start up, or when starting the terminal and sourcing setup.bash, or some kind of ", " command.", "This daemon\u2019s job is to listen for changes in the graph. It records all changes and stores the current state of the graph. The recent work on listening for graph changes by ", " would be used.", "The daemon provides a ROS topic on a well-known topic, e.g. ", " (based on the idea that the ", " namespace is reserved for infrastructure topics). Connecting to this topic will provide the current state of the graph.", "This topic probably needs to be a service to avoid wasting resources broadcasting something only needed intermittently.", "If it is a service, there is still value in having a topic that broadcasts on change for long-running tools to listen to.", "It may be worth considering having a separate domain for the infrastructure topics.", "Each computer being used should run a copy of the daemon so that connecting to it does not involve traversing the network and so is fast.", "The ", " command, upon starting, connects to the daemon\u2019s topic, gets the current graph info, and prints it out.", "If rostopic cannot find the expected topic, then it prints a warning, then waits a configurable length of time (with a suitable default), running the DDS discovery process and listening for graph information. After the time limit it prints the result.", "If you see that warning, you should understand that something is broken in your ROS system.", "This is a downside of the daemon approach: It is a single point of failure for the rostopic functionality.", "The daemon can also provide the same information over any other useful protocols. One particularly useful one would be a REST service, so that tools could connect and get the information without needing to use DDS or potentially be a part of the ROS graph. For example, a web-based tool.", "takes commandline input for topic name, message type and qos", "does some smart file searching for .msg files", "does some string-replacing on a ", " file", "executes that newly generated ", ".", "\n", "\n", "The daemon got started before (by launch, the system, wherever, this should be an implementation detail).", "The daemon gets started on demand by the first invocation of a command line tool needing it (slower on first call).", "No daemon is there but the tool should still provide reasonable results (trade off between wait time and completeness).", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-graph-information-tools-implementation-discussion/674"}
,{"title": "Rosbag Backward Compatibility", "thread_contents": ["I\u2019m not sure if ROS 2 has reached this point in its development, but has there been any discussion about maintaining some level of backwards compatibility with bag files from ROS 1? I\u2019m specifically thinking about the scenario where I have a lot of data collected using ROS 1 that I would like to replay into ROS 2 systems.", "ROS 1 bags store the actual message data with the same format as it is being used on the wire (plus some additional header and meta information). This is done so that recording / replaying bags is efficient.", "Since ROS 2 will use a different marshaling / transport its wire format will be different. Therefore I would expect the ROS 2 \u201cbags\u201d to be stored in a different format.", "But there are two ways moving forward:", "I\u2019d also say that, as you pointed out, we\u2019re not to the point where we\u2019ve decided on the bag format. In ROSBag version 2 (which is the current version used in ROS 1: ", "), it assumes ROS 1\u2019s serialization and ROS 1\u2019s topic properties (like latching = true/false). One option would be to do a ROSBag version 3 which would have the ability to store arbitrary connection information and serialized data. Then you\u2019d just need the right plugin to read or write a particular format for the data. If we did that, then we\u2019d probably consider trying to backport this to ROS 1 and create migration tools like we did with ROSBag version 1 -> version 2 (", ").", "Another option would be to use some other existing data format scheme like hdf5 (", ") or something like that. We\u2019re keeping an open mind about reusing more broadly used solutions, but the rosbag format has familiarity and commonality with ROS 1 in its favor. Until we come up with a concrete solution for ROS 2, I think you have to assume the format might be different from what\u2019s in ROS 1. Either way, we\u2019re aware that people have loads of rosbags already and will need to either have them work natively in ROS 2 (through the bridge or a plugin maybe) or have them easily converted to the ROS 2 format.", "Thanks for the quick response, it sounds like there\u2019s a number of viable solutions.", "Several years ago I did a bit of work on developing and implementing a possible new format for ROSBag heavily based on the Matroska container format (.mkv). It uses EBML and is very flexible, supporting pretty much any data and metadata you can dream of, chapters, streaming, chunking across multiple files, etc. It has compatibility with the tools you can use to modify and manage Matroska files. You could even conceivably store a ROSBag 2 format file in this container format, although I doubt that would be very efficient or worthwhile.", "Although I finished the format design, I never finished the implementation (changes at work got in the way). The half-finished implementation is still online:", "tawara - Stream-based data recording and playback library", "The specification is probably more useful than half an implementation:", "I\u2019m not suggesting that this format is the way ROSBag should go in ROS 2. Never finishing the implementation meant I never tested its viability and speed, for example. But I thought I\u2019d mention it if anyone is interested. ", " was also involved in this work.", " also did work in his previous lab with a variation of the ROSBag format. I\u2019m sure he will chime in with the results of that work.", "At the same time as the above work, I also looked at HDF5. I haven\u2019t looked at it since, but back then my impression was that it\u2019s great for certain kinds of data, like lots of raw numbers coming from things like strain sensors and temperature sensors, but it was not an ideal fit for more structurally complex data. However, that may have changed and the library and tools were good. Being able to make use of existing tools would be nice. The people at KULeuven have done a lot of work with HDF5.", "Phew, been a while ", "What I can say about my own work on the format ", " mentions is that it was motivated not completely, but significantly, by internal politics. At my old lab, we had a different middleware developed internally, and it was not acceptable (politically) to use ROS. I thought that maybe others had the same problem, but exchanging data would be very useful, so we tried to create something which didn\u2019t have \u201cROS\u201d in the name, but was conceptually quite close, so ROS could eventually migrate. There was not a lot of interest either in the ROS camp, nor in the other camp, though, so this went nowhere, unfortunately.", "Otherwise I probably wouldn\u2019t have felt the need to change anything fundamentally ", "My colleagues here also tell me that they some of the use cases of rosbag2 are problematic, e.g., when a rosbag is split, there is no information about this in the bag file. You have to infer either using external information, or a heuristic based on the file-name. We would prefer some information on this to be in the library.", "Maybe I\u2019ll post some more later, if I remember anything more.", "Having spoken with a number of colleagues about this now, we primarily see a few necessary feature extensions. This would actually apply to the current rosbag already:", "The Matroska-based format I mentioned above can do all those things with ease. With the exception of the last point, I expect it wouldn\u2019t be too hard to add then to the existing format, too.", "Hi Dirk,", "I am trying the first approach i.e getting rosbag data in ros2 using ros1 bridge.", "\nBut I am facing some issues here.", "\nThe callback for the subscription of rosbag data is not consistently happening.", "\nI suspect that this is somewhat related to the clock. Please let me know your suggestions on the same.", "\nI did not play bag with --clock as I am not sure that how to use use_sim_time with my ros2 nodes.", "steps i followed in each terminal:-", "\nroscore", "\nstart the dynamic bridge :-  ros2 run ros1_bridge dynamic_bridge", "\nrospbag play", "\nros2 run  ros2-node", "Thanks,", "\nPoonam", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You can simply use the ROS 1 rosbag application to playback a ROS 1 bag file and use the ", " to bridge the messages to a ROS 2 system.", "As soon as a ROS 2 specific bag solution exists someone could write a \u201cplugin\u201d to enable also reading files with the ROS 1 bag format.", "The use of textual data-type definitions in rosbag2 was the most problematic thing, and we discussed about that quite a bit.", "I recall I found some of the arbitrarily sized meta-data structures to be a bit annoying to parse.", "Other than that, yes, the format is non-standard, but it\u2019s not overly complicated. To my knowledge, there are no generic data-writing libraries out there, because it appears that almost everybody his or her own format. This is a bit annoying, but it also suggests that finding the right abstractions is not trivial.", "continuation information. when a bag is split, there is no information about this in the file. You have to guess from the file-name. It would be awesome if there would be a marker at the end of the file, to let readers know where to continue.", "information on parameters: currently completely missing, so you can\u2019t reconstruct parameter settings, nor can you detect run-time changes", "latching. there is no information that I know of that a topic is latched", "playing multiple bags: when loading multiple bags, I\u2019m told that rosbag indexes all of them in a time-consuming fashion to determine whether they are to be played sequentially or interleaved. maybe we could avoid that."], "url": "https://discourse.ros.org/t/rosbag-backward-compatibility/773"}
,{"title": "Design process of ROS 2", "thread_contents": ["During today\u2019s navigation WG meeting, we had some more discussion about the use of life cycle nodes and actions in the ROS 2 navigation code. This discussion came back again to issues that several people have with the design of the APIs for these features, which then further got into the fact that there is not much discussion about the detailed design and implementation of many features. Several of us feel that time pressures are leading to designs that are driven by implementation, features that are not fully fleshed-out before being implemented, or what are assumed to be prototype implementations ending up not changing and then being used by an increasing number of users, making them hard to change. We would prefer to see long-term design that aims for \u201cwe will eventually get to here\u201d. Even if the implementation does not get there immediately, at least everyone will know where we are going and that any current design or implementation is not the final result.", "The design repository is good and has allowed many people outside the OSRF, myself included, to contribute to designing concepts for ROS 2 and in some cases put quite a lot of detail into specific parts (the launch facilities come to mind). But the documents in this repository more often than not do not go into detail on things like APIs or library organisation, which are important details that impact how developers can use the feature being discussed. The two most prominent examples that I have seen are the life cycle nodes being implemented as a separate object type, which requires duplication of much of the API and means life cycle nodes can\u2019t be used in some parts of the ROS 2 API, and the ", " while topics and services are included in the Node class, which contrasts with what many were expecting would happen before implementation would begin. I have also seen the design of topic names often come up in security-related discussions, such as ", ".", "We recognise that there are limited resources being put into ROS 2 development, that those resources are also being asked to maintain and develop ROS 1, create new ROS 1 releases, maintain a build farm, and so on in addition to developing ROS 2 under huge time pressure. However the main problem is not so much the lack of time being put into detailed design of core ROS 2 features as it is the lack of an opportunity for people to comment on the details until a pull request with a completed implementation comes along - by which time it is difficult to say \u201cchange the whole concept of how this is implemented\u201d. When detailed discussions do happen in GitHub issues, it is usually after-the-fact and frequently the discussion thread just stops without a resolution - probably because the relevant people have so much on their plates to deal with. It was also mentioned today that these discussions often end up going circular due to the limitations of talking in text all the time. This is in contrast to there being working groups for non-core features that meet regularly (weekly, in the case of the navigation WG) and talk about requirements, goals, etc. that are driving how many things will work. These weekly meetings help push things along and help prevent discussions just stopping.", "The purpose of this thread is not to accuse anyone of ignoring detailed design. The conversations on the issues linked above show that the Open Robotics developers put a lot of thought into how they design APIs and implement features. I do feel that in general the core ROS 2 libraries are well-designed, but there are a growing number of places where I have concerns, and I know that others have places that concern them, too. What I think we need is:", "I agree strongly. I have the same concerns, particularly about Actions and Lifecycle nodes, which we\u2019re trying to implement within Navigation2 in the near future (Dashing release). I would really like to see a \u201cDesign Forum\u201d or working group formed to discuss topics such as these.", "I think there are two opposing goals about the development process here: on the one hand it should be as thoroughly designed and implemented as possible and on the other hand the community wants many new features as quickly as possible. We have done more of the former in the beginning of the ROS 2 development and then more recently - based on the feedback of the community - changed more towards the later. The desire for a fast development pace comes with the side effect that a first implementation of a concept might not be the final perfect solution. But I think that is absolutely fine. That is why ROS distributions exist. While the API in each distribution stays stable a newer ROS distribution has the freedom to break them in order to move forward.", "For ROS 1 - which is around for 10+ years - it is reasonable to expect that \u201ccore\u201d API doesn\u2019t usually change  between distros - or if it does that it aims for a tick tock cycle. For ROS 2 which is much less mature I don\u2019t think this is required at all. Just because a certain feature has been implemented doesn\u2019t imply it is final by any means. ROS 2 is too young to assume everything is stable / mature and not changing. As long as changes are clearly documented it can be expected from packages using the API to follow those instructions (e.g. for ", ") in order to support future ROS distros.", "The two features mentioned - lifecycle and actions - are good example of the process. There has been a significant amount of discussion and iteration on the design documents themselves as well as on the pull requests implementing these features. But most of the current concerns have not been raised during that multi-months time. These kind of feedback often only comes up way later when a bigger audience is trying to use the new functionality which I think is natural. Would it be great of these aspects would have been brought up and addressed earlier in the process? For sure. But the fact that they didn\u2019t come up earlier shouldn\u2019t prevent us from iterating on the design as well as the implementation in the future as needed. If this would imply \u201cchange the whole concept of how this is implemented\u201d that would be unfortunate but if the reasoning behind that need is sound it should probably be done.", "Regarding the question about the venue / forum for this kind of discussions: imo these already exist in various forms. There is the design repo with already merged, pending or to-be-created issues and PR, the actual tickets implementing specific features as well as this Discourse category. All these can be used to discuss topics like this. You referenced two specific tickets where this exact kind of discussion is actually happening. The main step missing from these discussions is imo a follow up in terms of someone taking the lead to act on the discovered shortcomings and working towards improving them. And ultimately this is very often limited due to resource constraints as well as lack of initiative.", " What kind of forum do you have in mind? As ", " mentions, there are already ample asynchronous communication tools at our fingertips, but they don\u2019t appear to meet your (or ", "\u2019s) needs. Is the problem from your perspective the lack of a regular synchronous interaction?", "Maybe I\u2019m just tired from a long week (yay Saturday!) but I don\u2019t seem to be as worried about this as I was yesterday. ", " Also I kinda agree with all but the first half of the last paragraph of Dirk\u2019s post. I like an agile, iterative approach to development like that ROS2 is following (or at least trying to).", "My post above was an amalgamation of the points raised in the navigation WG meeting, so I don\u2019t want to speak out of turn for all those points of view. I think for me the main problem is that although we expect and need to iterate on features, that iteration is not necessarily happening. Dirk alluded to this in his last paragraph. For example, the life cycle nodes implementation is still mostly unchanged (in terms of how it is implemented) from the first implementation 2 (?) years ago. Yet now tools, libraries and major projects are starting to be built on it. The larger the mass of users the greater the force against major changes, and at some point each feature will reach a critical mass of use where the pushback against change is greater than the push for change, and we end up stuck where we are. Isn\u2019t this why ROS2 began in the first place?", "But as everyone keeps pointing out, this lack of iteration likely comes from a lack of resources. As each feature gets done, developers move on to the next feature being demanded by the community and lose track of something that they probably originally intended to go over again. I know I\u2019m guilty of this over the years. (Doing new stuff is always more fun than reworking old stuff!)", "In regard to what kind of forum might be needed, I don\u2019t like the way it is hard to see into the thought processes of why a feature is implemented the way it is, and where it is intended to go. We are not all in the same office so it is hard to just roll over in my chair (too lazy to walk) and ask. Yes, we have many textual forums available, but text is slow and easy to misunderstand while simultaneously hard to correct. This is why I like the idea of having a regularly scheduled teleconference where we can ask about something and get a rapid, easily-clarified answer. A question for the TSC, perhaps?", "Another improvement could be to introduce epics into the feature management process. That might make it easier to track how a feature is being iterated on, what the current end goal is and how far along towards that goal we are. There are some issues that track major features with a checklist of smaller issues, but these are usually \u201cimplement large feature X for next release\u201d. Epics are traditionally used across many release cycles and take a longer-term view. I\u2019m not sure how easy it would be to get epics into GitHub\u2019s or Waffle board\u2019s facilities, but I feel like this could be a relatively low-resource way to improve visibility of the longer-term development goals and process. I\u2019ve added a task to my list to look into if it is technically possible with our current tools in the next week.", "I think lack of time to iterate and the lack of a democratic process for contentious design choices are issues here, but instead of solving those difficult issues here and now, I think better communication practices could help in the meantime.", "tl;dr I think we should:", "I think the discussion we had about the actions API suffered due to the nature of email/discourse discussions being so delayed. You end up with a lot of interleaved responses and it\u2019s hard to iterate on disagreements.", "To that point, I think one thing we\u2019re missing is ", " instantaneous chat.", "We have an IRC channel (", ") which is ", " chat and is in my opinion just ok.", "\nI am usually on IRC but rarely use it.", "\nHowever, I recently had some design related discussions with a community member (", " sorry to single you out) and it was honestly much more productive than back and forth on GitHub or discourse are usually. So I\u2019d like to see more of this kind of chat to see if it helps these cases.", "However, IRC is just ", " and we lack a convenient way to do higher bandwidth communication like voice or video without scheduling a meeting (for what ever reason we don\u2019t use IRC to agree to do a Google meet or skype or w/e). I really enjoyed having a google hangout \u201csituation room\u201d during the crystal release. I think it was helpful for community members trying to get things into the release to be able to just hop in and ask a question, or just listen.", "I know ", " likes to say \u201cdifferent tools will not solve the problem\u201d (and I tend to agree), usually meaning that when there\u2019s an underlying issue like lack of time to read and respond to design questions different tools won\u2019t help, but in this case I think a chat system that\u2019s more modern than IRC might be helpful, especially if it has a voice and video option which would make it casual to start up a conversation.", "I think using text chat (whether IRC or something else) would help, but honestly it will cause other issues. Usually, if I don\u2019t answer a question about design things on discourse or GitHub immediately it\u2019s because I\u2019m busy. If someone hits me up on IRC with a very specific question, they\u2019re more likely to get an immediate response out of me, and that\u2019s good for both me and them, but it also makes it harder for me to focus on long running tasks. It\u2019s all too easy for me to loose an entire day to reviews or design questions or help with bugs (whether they come from the OR slack or IRC or GitHub, etc\u2026). So if we spend more time talking on these high bandwidth channels, I think it\u2019s going to improve others ability to work with us, but it\u2019s also going to consequentially impact our productivity.", "I\u2019ve seen this issue with content creators in the video game industry, and the way they balance this is by having community moderation, where issues bubble up through levels of access and insulate people who are easily overwhelmed. Email/GitHub/Discourse are kind of good for this, because it makes it easier for me to ignore conversations until I have time, but I still see them and often fall into reading them, thinking about them, or responding to them. I think a more casual setting might encourage community members to respond to each other before I even have time to read it. For what ever reason, I find this is more common on something like IRC than in forums like email. Perhaps because the perceived cost of a response is lower.", "I\u2019ve been thinking about suggesting that we emulate these communities for some time, but ironically I haven\u2019t had the time to put together a proposal. But briefly, I think we could learn from their use of community moderation, and the tools they use to accomplish that. One pretty common thing is that almost all of them use discord, which is a free service that provides text, voice, and video chat in channels (sort of like slack). However, it is geared towards gaming and gaming communities, but it seems to excel at community management, and it provides a lot of tools and automation for this purpose. There are a few projects using it already, and they have an Open Source outreach page: ", " Other communities have similar solutions with different tools, choosing to use slack or gitter or one of the other options. I\u2019m not tied to discord, but I do think it\u2019s a good solution. Obviously there\u2019s always a discussion to be had about memorandum and the benefits of asynchronous communication like email, but I personally think there\u2019s a place for these more modern solutions to facilitate ephemeral discussions.", "Currently we don\u2019t use the services we already have (IRC) to full effect, but I think if we had a more modern tool like discord or slack, and more of us committed to being available on it (IRC is pretty dead most of the time), then we might increase our use of it. This would help with communication. We could start with no moderation or hierarchy, i.e. everyone has \u201caccess\u201d to everyone else, and if it becomes an issue we can start to add some structure to prevent overwhelming individuals.", "No matter what we do (in terms of mitigation techniques like community moderation), this will negatively impact some people\u2019s productivity more than others, but it will also hopefully let us better leverage other contributors who are blocked or frustrated otherwise. So I think we just need to be aware of this trade-off and (this isn\u2019t a new request from me) also take this time spent collaborating into account when scheduling feature development. Personally I think this point needs discussion and support from the TSC level, but that\u2019s just my opinion.", "Maybe comparing some political models for open-source communities might be helpful.", "Such as", "The main trade-off maybe being: speeding up any process until an outcome vs. improving how well the outcome covers the needs of different (sub-)communities.", "However, I recently had some design related discussions with a community member (", " sorry to single you out) and it was honestly much more productive than back and forth on GitHub or discourse are usually. So I\u2019d like to see more of this kind of chat to see if it helps these cases.", "I was coming to say pretty much exactly this. IRC isn\u2019t perfect, although we\u2019ve been using it successfully for ", " in Ubuntu. If we were starting today would we use something else? Maybe, but I don\u2019t think the choice of tech matters so much as long as it\u2019s open to everyone and consistently used and attended (including Open Robotics folks). We have rooms with hundreds of people in it, and our community knows that we Canonical employees are always there. How do they know? Because we have our own design discussions there, live, in front of everyone. Some of us actually used Telegram for about a year because we were starting to tire of IRC, but we ended up finding that having design discussions and making architectural decisions out of the public eye ended up making our community feel a little alienated and out of the loop. This conversation sounds familiar ", " .", "I totally get that being on [chat platform] and being pinged causes a context switch that quickly eats up time, but delegation is key here-- if everyone is there, we can all share the load. And I\u2019ve actually told people \u201cI\u2019m sorry, I\u2019d love to help but I\u2019m slammed right now, can you come back tomorrow maybe an hour earlier in the day?\u201d and they will. Or \u201cI can\u2019t help at the moment, but [other person] was working on that yesterday.\u201d", "Not every community works like Ubuntu, I get that. One of the reasons we\u2019re able to have design discussions in front of everyone is that Canonical is a mainly remote company, and we have to have those conversations ", ". We might as well have them in public, and that has worked really well for us. As I understand it, Open Robotics is not a remote company, but you\u2019re working with a large community who is. Having discussions in chat instead of in-person would probably slow you down, but I wanted to share my perspective.", "I\u2019m total outsider here.  But with 20+ years experience in the aerospace software world.   I would not suggest adopting methods used in my industry, they result in very slow progress.   But I would make one hard change in your process.", "Note the observation about \u201clack of an opportunity for people to comment on the details until a pull request with a completed\u2026\u201d    this is natural, people don\u2019t notice until finally the change is made.", "One change in your software process might be that \u201cNO CODE IS WRITTEN UNTIL A X DAYS AFTER A DETAILED DESIGN IN CHECKED-IN\u201d  This will cause some slow down but that everyone sees the pull request for the change in the design documents and comments THEN,  not after the code is already written.  There is a natural tendency to NOT change working code.", "Basically take advantage of that observation that no one cares until after the pull request.  So make the pull an early step.", "Just saying \u201cwe should talk more\u201d rarely actually happens,  Make a rule that forces more talking.", "The goal should be to make changes early not \u201clater\u201d,", "With the increasing addition of remote engineering resources thanks to the efforts of the TSC, there must be some kind of instantaneous chat going on. ", " mentioned an Open Robotics slack; having an internal slack is not something I\u2019m going to argue against because I agree it is needed. As everyone seems to agree, what is lacking is the equivalent for the whole community that is ", ". I gave up trying to maintain my connection to ", " because was 95% no activity, 4% someone making a random statement along the lines of \u201cHi there! I\u2019m learning ROS! Yay\u201d, and the remaining 1% was interesting discussion. This not-being-used thing is a problem. As ", " said, having to hold your design discussions in a public channel that supports rapid, lightweight response is better all around.", "What technical solution to use for instantaneous chat is another thread now, but I am all in favour of trying to transform things a bit in this way.", "Plus my collection of Slacks is not nearly big enough yet.", "I don\u2019t think the choice of tech matters so much as long as it\u2019s open to everyone and consistently used and attended (including Open Robotics folks)", "I agree with this.  I\u2019ll go to whichever chat platform is used.  I think it would be useful to examine what particular issue causes the IRC to be inactive.  My first thought is that more activity from Open Robotics folks would be helpful.", "I\u2019m not sure I agree that adding slack or IRC is going to resolve this any better than Github and Discourse has. I think its just going to add more confusion, not less, IMO. I already get 10-30 github notifications, dozens of emails, and 5-10 Discourse notifications each day. Adding IRC/Slack will only make things less focused, not more. I think something more like the model that Google Cartographer is using for their ", " is a better way to discuss the key design issues in real-time.", "I would also like to add that I do understand the iterative approach, but without knowing what the long-term design goal is, often the first iteration becomes the only implementation (as noted above by ", "). At least if the goal is stated clearly in a design document (or an Epic as he suggested), then it can be made clear that there will be changes upcoming to reach that final design. I don\u2019t think we have that clarity right now. It might be clear to the implementers that it was only meant to be a first implementation, not the final one, but to the community, it\u2019s not clear. See the thread on ", " that was mentioned above for a good example of this.", "I\u2019m not sure I agree that adding slack or IRC is going to resolve this any better than Github and Discourse has. I think its just going to add more confusion, not less, IMO.", "My observation was that of two of my recent design interactions with people outside our office, one was on GitHub and one was on GitHub but with short bursts on IRC, and that the latter was much more productive. It certainly doesn\u2019t address issues where we have insufficiently written down what\u2019s in our heads so that external people can more clearly see what we envision for the API or design, nor does it address the fact that we\u2019re all already overwhelmed with notifications.", "To be clear we already have IRC, so we don\u2019t need to add it, though I suppose you mean \u201cadd it to our workflow\u201d, in which case my experience is that it would help, but again that\u2019s just my opinion.", "I think something more like the model that Google Cartographer is using for their ", " is a better way to discuss the key design issues in real-time.", "In my opinion, that\u2019s just using instantaneous chat on a regular schedule with meeting notes. Another way to address this short fall of \u201cbeing on the same page\u201d would be to have more meetings where we sync with one another, though I\u2019m not sure if that\u2019s more or less efficient that ad-hoc discussions in something like IRC or discord (or a voip conversation arranged in either). Personally, I\u2019m at my capacity for meetings already.", "I would also like to add that I do understand the iterative approach, but without knowing what the long-term design goal is, often the first iteration becomes the only implementation (as noted above by ", "). At least if the goal is stated clearly in a design document (or an Epic as he suggested), then it can be made clear that there will be changes upcoming to reach that final design.", "I\u2019ve always been a proponent of more detailed design and finding as many issues as possible before starting work (to that point I created the ", " website and tried to encourage its use), but I\u2019ve always been encouraged to just jump in and get started or to avoid things like \u201cdecision paralysis\u201d, \u201cpremature optimization\u201d, or \u201cover-engineering\u201d. I think those are valid concerns, but at the same time I anticipated issues related to not having enough of our design written down in order to involve external team members. I also started to make a ROS 2 version of the \u201cConceptual Overview\u201d from ROS 1 in order to convey more of this, but I\u2019ve not had time to keep it up, nor has any project come along that working on that seemed to fit with:", " (note this is out of date by now)", "Perhaps someone who is more on the side of \u201cget things done\u201d could speak up here.", "I would also like to add that I do understand the iterative approach, but without knowing what the long-term design goal is, often the first iteration becomes the only implementation (as noted above by ", ").", "I think it is important to discuss this point in particular.  I don\u2019t think that ", " in ROS 2 is written in stone.  The decisions that have been made are generally made with the best information at the time, but as we gain more experience and use cases, these can all evolve.  So things that are still on \u201cfirst iteration\u201d are mostly there because nobody has had the time to evolve them to where they need to be.", "Perhaps someone who is more on the side of \u201cget things done\u201d could speak up here.", "(clipping some good points by William)", "I am typically more on the \u201clet\u2019s get something in and iterate later\u201d side, but when building a middleware system, some advance thought does need to be put in.  While my point above about pieces evolving is true, we can\u2019t always improve them ", " (since we generally guarantee API compatibility).  So my opinion is that there should be some design work done up-front, but then things should move along and we can iterate towards a better solution later.", "Circling back towards whether instantaneous chat helps or hurts here, I have also seen it work on previous projects.  But it will mean that everyone involved (including at Open Robotics) needs to make a dedicated effort to be on that chat and to conduct design discussions on that chat, even when they are with people in the same building.", "because nobody has had the time to evolve them to where they need to be.", "It always comes back to the resources problem. I\u2019m curious if someone (", "?) is able to shed some light on how much development resources ROS2 currently has, how much help the TSC idea has been in increasing them, and how much development resources are expected to grow in the near future. It won\u2019t really help this discussion but it would put some things in a better context.", "even when they are with people in the same building.", "I was reminded of this need again this morning by ", ". I\u2019m not going to argue against wanting to gather around a whiteboard and discuss things in detail. That would be stupid. It\u2019s by far the most efficient approach (for the people there). But I wonder, were an instantaneous chat platform and instant teleconferencing available and used, if it would be feasible for when such a discussion begins to announce it in the chat and throw up a teleconference for anyone interested and available at that time to join in. I\u2019m grateful that notes were taken and posted to the pull request but if I had the opportunity I would have liked to be involved in that discussion and hear the context for and detail of the comments made.", "Another idea is, when a new design pull request or major feature implementation pull request goes up, to schedule a day or two later a teleconference to talk about it as part of the review process. Give people some time to read the document or the code, perhaps start their GitHub-based reviews, but also give a forum after that short period for having a chat about it to clear up more complex concerns.", "A couple thoughts, as an outsider (slash interested student).", "This is why I like the idea of having a regularly scheduled teleconference where we can ask about something and get a rapid, easily-clarified answer.", "Another idea is, when a new design pull request or major feature implementation pull request goes up, to schedule a day or two later a teleconference to talk about it as part of the review process.", "Something like PX4\u2019s ", " would help outsiders like me keep up with development and slowly get involved. It would also make development more transparent and open to the community. There\u2019s already a couple WGs for specific areas (e.g. navigation), but there are none for core ROS2 stuff (AFAIK) other than the (closed-ish) TSC.", "I think it would be useful to examine what particular issue causes the IRC to be inactive.", "Too many options just gets confusing, even if each one has its specific uses (discourse vs. answers vs. GitHub vs. IRC).", "Something like PX4\u2019s ", " would help outsiders like me keep up with development and slowly get involved. It would also make development more transparent and open to the community. There\u2019s already a couple WGs for specific areas (e.g. navigation), but there are none for core ROS2 stuff (AFAIK) other than the (closed-ish) TSC.", "This is something I\u2019m really in favour of, even though I have too many meetings already.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A venue to have regular discussion of detailed design and implementation issues before we get presented with an implementation, and not buried amongst the dozens of other issues that come across our GitHub notification lists every day.", "Active participation in this venue by anyone working on a core ROS 2 feature.", "use instantaneous chat more often", "consider some tools to facilitate that (like discord ", " or slack)", "try to mitigate overwhelming core contributors with community moderation (as needed)", "Do-ocracy (e.g. explained here: ", ")", "(Benevolent) Dictatorship", "Timocracy", "Meritocracy", "Democracy"], "url": "https://discourse.ros.org/t/design-process-of-ros-2/7782"}
,{"title": "ROS2 Navigation - Input requested", "thread_contents": ["Hello ROS and ROS2 users and developers! I have been in discussion with David Lu and have reached out to OSRF to start an effort to develop the ROS2 Navigation stack. We are currently investigating the needs for the next-generation Navigation stack in ROS2.", "I\u2019d like to invite you to reply with things that:", "I have my own wish list but I don\u2019t want to seed the discussion, I want to hear other users thoughts.", "Also, if you are an active community member and want to be involved in the development effort, please let us know that also. We have a small team right now, but desire to do this in the open with community involvement.", "How to be involved in the development ?", "Happy to know this kickoff. ", "Per my experience of using ROS Navigation Stack, I would like to add below items into the wish list:", "Thanks for the feedback, a couple questions.", "Peter, I sent you an email, we can talk offline", "This is a rather ambitious list, and perhaps some of the navigation tasks applicable to these maps types may be too domain specific, but I\u2019ll just float some far ideas here:", "I suppose I\u2019d like to see navigation planners that could interoperate map format types that are more memory efficient, compressible, dynamic, human relatable, e.g. less pruly metric based like voxels or occupancy grids. I\u2019d also to see ROS navigation planners generalize beyond the classic 2.5D mobile robot on a planer workspaces or perhaps appropriate other environment data in a map as navigational heuristics like for packbots, quadrotors, ROVs that climb, fly or swim in 6DoF.", "Any dedicated Discourse/Slack or other mean for getting involved ??", "One of the main frustration I experienced with ROS nav stack is the lack of flexibility (e.g. ", " inner state machine) that eventually got partially addressed by the community later on - e.g. ", " let you use the state-machine of your choice under the hood.", "\nSimilarly, as it has been mentioned already, the possibility to use other types of map representation would be awesome.", "I\u2019m interested in a similar desire to extend the typical map layers into more semantic meanings. Would add wifi, ble beacons and other rf land marks to the annotated affordances idea mentioned be ", "I haven\u2019t used the navigation stack, but I understood David Lu to say at the last ROSCon that it doesn\u2019t support Ackermann-steered vehicles. That is an important use case.", "I agree with ", ", decouping the move_base in separate modules in a state machine would greatly improve its flexibility.", "Regarding features, a nice little thing to have IMO would be to be able to pass the goal tolerance (xy and yaw) in the goal message to move base.", "Ok, let me see\u2026", "Things to keep about the current navigation stack:", "Things that I will change:", "\nPlease, correct me if I am wrong. Right now, if you want to use your own planner (local or global), you have to use the C++ API. The class that you provided was extremely convenient to minimize the flow of information and speed up the full Navigation Stack work.", "\nHowever, ROS2 should be more efficient while handling messages right? Is it fast enough to deal with the idea of using a node for each planner? This node could receive all the information throw messages or is too much?", "\nIf this is the case, we should study the possibility of detaching the nav_core and the planners.", "\nMaybe the community will be more comfortable with a publish/subscribing paradigm. Is just a thought, but this will make everything even more modular right?", "Is the navigation stack too dependent on the position or is just my impression? If the robot does not start in his position, everything goes really crazy. The local planner and the local cost map can be more independent of the positioning system, right? Is a bad configuration of my navigation or is like that?", "Things that I will add:", "\nA simple system that tracks common problems. E.g: Hey, I am waiting for a map and it is not coming! Hey, I am sending cmd_vel commands but the position does not change! Things like that.", "A simple undocking algorithm; I do not want my robot to move backward unless a recovery behavior specifies so.  A tiny algorithm at the beginning that moves the robot backward x meters could be handy. Moreover, this can be attached to a simple boolean topic. There, a sensor can publish if the robot is at the docking station or not. If the robot is at the docker, you execute the undocking before moving, if not, just move as usual.", "+1 to the idea of specifying the goal tolerance.", "I\u2019m not an expert navigation researcher but I am a user and an experimenter.", "For me, the biggest desire is for navstack2 to be a navigation framework built using ROS2 principles rather than a monolithic navigation solution, as it is in ROS1. This means defining the separate components (local planner, global planner, local map probider, global map provider, planner supporters such as costmap providers, rescuers, path follower, and so on), defining the interfaces between them, and specifying those as ROS2 messages, services and actions. We should be able to say \u201ca node (or set of nodes) that provides a global planner compatible with navstack2 should publish/subscribe/use these topics, services and actions and provide these parameters, at a minimum\u201d. The key thing here becomes defining the APIs between the different parts of the navigation stack.", "Navstack2 should take advantage of the capabilities of ROS 2 to make things nodes but then keep them in the same process so that message passing is nearly cost-free.", "Being possible to specify via a configuration file what the global planner is, what the local planner is, etc. would also be possible. This is to have it not just be a bunch of nodes with defined interfaces, but be more of a framework where it is simple to build a complete navigation stack without feeling like you are plugging things together manually.", "If we do this, then we can achieve the goal of allowing different planners to be plugged in, etc. that is frequently stated in this thread.", "It will also ensure that it is inherently easy to introspect the internal navigation process, because we can intercept all the messages flying around between the different parts.", "Building on this, navstack2 should then provide a default configuration that provides an equivalent or better navigation functionality to the ROS1 navigation stack.", "As someone who has a strong desire to participate but is something like 1000% over-committed in time, I would also like to see a central place where I can comment on design decisions and implementation choices made when I have time, without losing track of where everything is. Please make a github project so we can make issues and discuss them, if you haven\u2019t already.", "A simple undocking algorithm", "I think this is too application-dependent. But, it should be ", " easy make things like this default behaviour for your robot in navstack2.", "wow,  very detailed, and right, it\u2019s what I meant\u2026 ", "All, thanks for the input so far, keep it coming!", "A few comments.", "Keep the feedback coming!", "To enable autonomous navigation, you have to allow the robots to sense the environment around to create its map and enable collision avoidance, In other words, sensor data input to any algorithm is important.", "\nNow there are various sensor solutions serving for this purpose, for example, Sonar, Lidar, MMW, Vision and so on they serves different preference and it\u2019s possible to sensor fusion with them in the real autonomous navigation. Free to think ROS2 navigation can consider more:", "Late to the party, but here\u2019s my $0.02.  Generally, +1 to all of Geoff Biggs\u2019 coimments.  I\u2019d like to see:", "I\u2019d be interested in having our group here at Oregon State help with some of this, depending on where it goes.", "cheers", "\u2013 Bill", "Not really a navigation stack user myself, so just passing by, but I was surprised ", " wasn\u2019t / isn\u2019t mentioned more. Only ", " mentioned it once earlier in this thread.", "From the ", " at ROSCon and ", " it seems it\u2019s gone into the direction that ", ", ", " and some others sketch:", "Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the ", " and the ", ". For navigation on costmaps see ", " and ", ".", "Would seem to be a good idea to get some input from its maintainers (", " et al.).", "Yes, I was overjoyed when I saw ", " announced at ROSCon last year. I consider it a ", " step in the direction I want to see the navigation stack go. I haven\u2019t had time to try it out myself yet, but I agree with ", " that any effort to develop a new navstack for ROS2 should consider it the biggest input into design.", "Hi all,", "\nWe\u2019ve created a repo for the ROS2 Navigation project here:", "\n", "ROS2 Navigation. Contribute to ros-planning/navigation2 development by creating an account on GitHub.", "\n", "We\u2019re going to collect all design inputs, here, starting with high level use cases and requirements:", "\n", "Please submit your use cases and requirements via pull requests so we can have design discussions there.", "Thanks,", "\nMatt", "A bit late, but I figured I would chime in (having been a maintainer of the navigation stack for going on 5 years now).", "First, I concur with the several comments about better modularity. I\u2019d almost suggest that the ROS2 navigation stack shouldn\u2019t include any planners in the main repo (as is the case with MoveIt). There have been several newer (and probably better) planners developed \u2013 but users assume they should use only the \u201cdefault\u201d planners. At the same time, maintaining a code base that includes \u201call the planners\u201d is just not feasible. Having better modularity, and having things like local and global planners exist in other repos, makes it far easier to have more maintainers involved, and for development to proceed quicker (if you don\u2019t like planner X, go write and release planner Y).", "While splitting those things out to other repos, I would suggest providing some basic/core code to build planners on. At some point a Willow Garage intern started to refactor base_local_planner in that direction \u2013 but it was never really finished.", "On the subject of 2d/3d \u2013 I think there is a fine balance to walk here. While most research is pushing more in the 3d direction, commercialization tends to push towards cheaper/smaller processing power \u2013 and some of the optimization in terms of 2d/2.5d in the navigation stack is important here. While a full 3d mode is awesome, requiring the whole system to always act as 6Dof pose + 3d terrain may make it unusable on smaller platforms like Turtlebot.", "With regards to not being monolithic \u2013 I think this will be a serious challenge. One of the things that ROS1 does a really poor job of is synchronizing the operations in multiple nodes. I\u2019m not sure how much ROS2 really helps in that regard.", "But here\u2019s my most important feedback: we need better testing. One of the reasons we have a hard time merging things in the current navigation stack is that there is just almost NO test code (similar issues with MoveIt). I have spent an enormous amount of time physically testing code in simulation or on real robots to try and be sure something contributed works \u2013 only to find out that it actually breaks some particular feature that someone was using. If you\u2019re going to largely overhaul/rewrite things \u2013 do it in a test-driven way, and make sure those tests are meaningful so that the system can actually be maintained.", "Has anyone looked at what other ROS2 dependencies are missing? AFAIK, there is no equivalent of actionlib yet (which is probably a pre-req to actually building most robot applications in ROS2). I\u2019m also not sure the status of things like parameter management or dynamic reconfigure (highly required for people to actually tune a navigation setup in a reasonable amount of time).", "-Fergs", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You like about the current ROS Navigation stack and would like to see kept the same (or equivalent functionality)", "Things you think can be improved on and would like to see done differently, aka. your wish list", "make a more flexible mechanism for plugin implementations, especially for recovery plugins.", "Support multi-thread and heterogeneous computing", "Adopt AI gene (e.g. Reinforcement learning) into path planning and collision avoidance.", "Support more map types", "it\u2019s better to support 3D path planning and CA", "I think you mean AI Gym, right? If so, are you aware of the ", " project? I have been using it for some RL work in this space. I believe it could be ported to ROS2 fairly easily also.", "What map types are you thinking in particular? Google Maps? Non-2D occupation grid maps?", "3D is on my wish list too, for applications like drones.", "What map types are you thinking in particular? Google Maps? Non-2D occupation grid maps?", "Semantically oriented Maps\n", "Indexed Points of Interest\n", "rondevu points", "moving goals", "\n", "Labeled region boundaries\n", "property borders", "tolls or crossing costs", "exclusion zones", "\n", "Annotated Affordances\n", "Doos, Elevators, Appliances, Chargers", "Departments, Faculties", "\n", "\n", "Vector Maps\n", "Floor plans or 3D scale models\n", "Google maps/earth", "Architectural blueprints", "\n", "Roadways maps\n", "Turning lanes, intersections, crosswalks, etc", "Congestion, Traffic density", "\n", "\n", "Geo Maps\n", "Topological\n", "Elevation and grade", "Underwater terrain", "\n", "Weather\n", "wind and tide velocities", "Dynamic time series forecasts", "\n", "Approximate at scale\n", "WGS84 vs local cartesian", "Alternate ", "\n", "\n", "\n", "All the dynamic reconfigure for sure.", "All the representations in rviz (necessary for tunning as well)", "I will keep the algorithms that are currently implemented in ROS1, if possible with the same parameters. This will make the transition smooth.", "I plan to create a ", " repo for this (hopefully under ", " namepace, working on that), and will start capturing the wish list items and requirements, as well as documenting design decisions, etc. This thread is just the primer for that.", "The many different types of maps requested, to me means we need a more abstract data type for maps, that can represent many different types of maps that might be inputs to the system. I\u2019m not sure what that will be exactly yet, but to me, this illustrates that we need to decouple the map type as much as possible from the system.", "I also agree on the use of ROS2 nodes for the low level plug-ins like global and local planning. That will improve decoupling and make it easier to replace those components with other algorithms, and will also ease the debug effort as pointed out before. We can do this using shared memory pointer passing so that the performance overhead is small.", "general flexibility to smoothly use their output in different phase of autonomous navigation", "how to adapt their combination or sensor fusion well while engaging with ros2 navigation stack", "consideration to certain solution with upcoming trend or innovation to extend, for example, for vision based, it may not require to create map firstly to navigation in the future.", "More pluggability in the elements of the nav stack, and the ability to hot-swap implementations.", "A common API that will allow people to add their own underlying representations.", "The ability to handle time.  I\u2019ve been toying with the idea of a map that changes throughout the day (as corridors become congested, and such), so the ability to integrate this into the system would be important to me as a specific use case.", "The ability to run more than one algorithm at a time, compare the results and mux them.  This is important for localization, where it can be used to compare the performance of two algorithms, or to use different algorithms at different time.", "Factoring things up as finely as possible.  As Geoff pointed out, this should be more lightweight in ROS2", "Ability to develop in Python or C++.", "Use floats or doubles as the underlying representation, not some fixed-point hack.  Actually, it might be nice to use arbitrary underlying representations (of probabilities).", "Some more modern algorithms from the literature.", "Being able to swap maps in and out of core seamlessly, so that I don\u2019t have to keep all of campus in memory at the same time.", "Multiple floors in a building.  Hybrids to let me get from one building to another."], "url": "https://discourse.ros.org/t/ros2-navigation-input-requested/4884"}
,{"title": "Robot DevKit for ROS2", "thread_contents": ["This is the initial release of Intel\u00ae ", " open source project, which enables developers to easily and efficiently create, customize, optimize, and deploy a robot software stack to an Autonomous Mobile Robot (AMR) platform based on the Robot Operating System 2 (ROS2) framework. The Robot DevKit supports Ubuntu 18.04 and has been verified on Intel NUC (CPU: Intel i7-6700HQ ", ", Mem:16G) and ADLink Neuron Board (CPU: Intel i7-6700TE ", ", Mem:16G) with TurtleBot3 Waffle chassis. Installation includes the software components below that are made available in your development environment to build your own Robot.", "ROS2 core for key packages of ROS2 framework", "ROS2 Intel\u00ae", "\u2122 with CPU, GPU and VPU acceleration for open visual inference and neural network optimization providing easy to use face detection, person identification, age and gender recognition, and more!", "ROS2 Object Analytics with Intel\u00ae", "\u2122 for object detection, tracking and 3D localization.", "ROS2 navigation and SLAM for map generation and robot navigation with the map", "Rviz2 visualization tool and gazebo-ros2 simulator", "Intel\u00ae", "\u2122 for 3D vision, point cloud creation, SLAM and navigation", "for key components", "The Robot DevKit provides off-the-shelf features to developers, like avoiding obstacles while navigating, using models in OpenVINO model zoo for object or people face detection, optimizing your models to get improved neural network performance on a variety of Intel\u00ae processors (CPU, Intel\u00ae Processor Graphics, VPU), etc. In the near future, we\u2019ll be adding robot arm features with motion planning and control, perception for grasping, hand-eye calibration, etc. Stay tuned for more features!", "Your feedback and suggestions are highly appreciated. Feel free to provide feedback or report issues in", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/robot-devkit-for-ros2/8900"}
,{"title": "Is there any DDS implementation using shared memory inside system?", "thread_contents": ["Hi all,", "im just curious on this, i can see some vendors working on dds implementation.", "\nbut is there any dds implementation supports shared memory? or working?", "thanks in advance,", "\nTomoya", "A question like this is better for ", ", but I\u2019ll give you a quick answer here.", "thanks, i will look into them.", "confirmed that RTI Connext DDS used shared memory, actually it maps shm in the process space. but so far our internal performance test tells us it is not so good for latency. connext dds implementation is provided as binary so we are not sure what\u2019s going on. is there any specific room we should have this conversation. maybe just asking for RTI\u2019s help?", "What sort of latency are you seeing? Could the data marshalling and unmarshalling cover it?", "sorry to be late to get back here,", "Publisher:Subscriber=1:1, skylake, Ubuntu16.04", "i was expecting much faster since it uses shared memory.", "(*) Latency\u2026(end - start), start is right before publish msg, end is subscriber callback fired. so this is just for latency for communication.", "we are considering we should use ", " instead of ours, we will check how it works.", " I\u2019d recommend checking out the ", " script if you want to run a comprehensive batch of experiments with ", ".", "thanks for the tip in advance, will check that out.", "we did try out ", ", the result comes up the following.", "skylake ubuntu16.04 Pub:Sub=1:1, QoS(BestEffort, Volatile), Latency Mean [ms]", "not sure what\u2019s going on with connext more than 32KB, and there is no way to investigate since it\u2019s binary release. but if we use the shared memory precisely, this will be much much faster and also throughput. is there specific configuration required to connext? maybe we are not using the precise configuration? someone could you give us some help here?", "thanks,", " DDS configurations is a rather large topic, and would be difficult to even introduce with all the appropriate context in a single post.", "You\u2019re right that using shared memory is going to improve ECU- or PC-local latency performance. There are other considerations to make WRT message size, such as packet fragmentation and how that affects the network layers, and network buffer sizes \u2013 this is more applicable when you\u2019re ", " using shared memory features, which may help explain some of the results you posted for larger message sizes.", "The ", " numbers may not be too far off from expected due to the packet fragmentation and serialization/deserialization delays, which are especially troublesome with nested data structures. I cannot confidently say the same about the ", " results, those seem off to me from a cursory review.", "Were you able to rerun the performance test experiments with shared memory enabled?", "appreciate for your help and comments on this thread, i understand what you mean that taking care of protocol layer and network, that are the difficulties to adjust for shared memory transport.", "not sure if we could actually use shared memory or not, as far as i can tell is that shared memory is mapped into the process space, but taking really long time to transmit.", "Continental published eCAL this week - a publish-subscribe framework that is designed for high data throughput with minimal latency.", "\nIt\u2018s using shared memory for inter process communication and can use different serialization protocols like google protobuf, flatbuffers or capnproto as well for highest performance.", "\nCheck ", "Hi Tomoya, ADLINK Technology\u2019s ", " has shared memory support for dashing for significant reduction in latency for ROS2 modules running on the same compute, with especially large improvement when doing 1:many. ", ", use ", ". Would be happy to help.", "Sorry to jump in here so late\u2026 I think there are some known inefficiencies in the way the ROS/rmw is using RTI Connext DDS in that it makes some extra memory copies and allocations.", "RTI\u2019s public benchmarks produce much better performance over shared memory. You can see the results here: ", ".", "You can reproduce this in your own platform using the open source RTI perftest tool: ", "\nyou can also download the binary distribution here: ", "To send data larger than 64KB you need to enable asynchronous publishing. The RTI perftest will do that automatically for you.  You can look at the source code of perftest to see how to do it.", "thanks for the information, OpenSplice DDS implementation is provided as binary? it has to be commercial version once it comes to the actual product, am i right? (i do not read the license yet, though) besides, do you have any benchmark result with comparison?", "thanks,", "hi Tomoya, OpenSplice is open source and commercial. Both work with dashing and use same rmw. Shared memory is a feature of commercial which is ", ", other is ", ".", "thanks, any benchmark result then?", "We have some briefly benchmark result for your reference.", "\nOpenSplice community v.s OpenSplice commercial", "Latency test:", "\nOpenSplice community edition + ROS2 rclcpp + rmw_opensplice_cpp :", "\n1k array: ~273 us", "\n4k array: ~299 us", "OpenSplice commercial edition + ROS2 rclcpp + rmw_opensplice_cpp + shared memory :", "\n1k array: ~105us", "\n4k array: ~129us", "Throughput test:", "\nThroughput can up to 1.7Gbps.", "\nFor example: a 8K array ROS message publishing / subscribing rate can up to ~25200 hz", "Notice\u2026", "thanks for the information, that\u2019s really interesting!!!", "BTW, May I just ask a few questions based on the result?", "thanks in advance, cheers", "all test use a tuned opensplice configuration", "What was it tuned for?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The RTI Connext DDS implementation definitely has the ability to use shared memory for DDS clients running on the same computing node, and as I recall will do so automatically. However, it will still marshal data because it needs to do so for many of the features of Connext DDS, such as logging and introspection.", "eProsima\u2019s FastRTPS apparently does not use shared memory yet but ", ".", "OpenSplice DDS does use shared memory internally. I don\u2019t know if this implementation marshals data for shared memory.", "all latency and throughput tests use optimized build .", "all message array memory are pre-allocated before sending to the rmw layer.", "all test use a tuned opensplice configuration. Different configuration could have different performance.", "what is the platform device and ROS2 version?", "How many subscribers on the Reader side, if not a problem?", "Is there any chance to support much bigger data, such as MB order images?"], "url": "https://discourse.ros.org/t/is-there-any-dds-implementation-using-shared-memory-inside-system/7609"}
,{"title": "All of parameter activity via \"parameter_events\" topic", "thread_contents": ["Hi All,", "we would like to hear some comments on this. any idea will do good, so let us hear what you guys are thinking about this. also any milestone for extension are planned or not.", "currently any parameter events such as changed or deleted parameter using ", " will be notified via \u201cparameter_events\u201d. this single topic will be used for all of parameter activities.", "it is really simple enough to use only single topic to notify parameter events, but from user perspective we like to receive the event that we are interested in only. we can filter the event from parameter_events topic but we must receive all of the events anyway. we do not actually want to receive the event if not necessary.", "there would be some options i guess,", "thanks,", "\ntomoya", "This would seem to be a similar situation as with TF: clients receive all broadcasts, but may only be interested in a subset. Clients are shielded from this by the listeners and buffers in TF, to which the filtering you mention is delegated (sort-of: the Buffer does store everything that comes in, but clients only query the buffer for information they are interested in when they have a need).", "there would be some options i guess,", "This reminds me of the bus vs channel communication patterns. Either you receive everything and filter yourself (bus), or you make connections with just those entities producing messages you are (certain you are) interested in. Both are limiting you, as you cannot easily switch.", "To avoid implementing something like the TF client library for parameter events, and looking just at DDS, perhaps content-based subscriptions could be used. They would allow you to express interest in only subsets of messages published on certain topics. Filtering would be delegated to DDS, and callbacks would only be called whenever the filter predicate matches.", "I think content-based filtering at the DDS level would be the most efficient way to handle this, but a nice API at the parameter/node level would make it more user-friendly and future-proof.", "\n", "thanks for the thoughts, i think that DDS contents filtered topic would work too.", "the question is any DDS implementation supports \u201ccontents filtered topic\u201d?", "\nas far as i know, contents filtered topic is one of the extension, so not sure what vendor supports that feature right now.", "and more information is welcome!", "thanks,", "\ntomoya", "the question is any DDS implementation supports \u201ccontents filtered topic\u201d?", "\nas far as i know, contents filtered topic is one of the extension, so not sure what vendor supports that feature right now.", "A quick check would seem to indicate the following vendors have at least some support for content based subscriptions (or filtering):", "a nice API at the parameter/node level would make it more user-friendly and future-proof.", "Certainly. This needs to be exposed at the ROS level in some way or other.", "Not just for parameter events, but subscriptions in general I would say.", "To keep things connected: here is an old ROS Answers Q&A about this: ", ".", "From user perspective I believe the best would be to have a node which I can roslaunch in only in case I need parameter updates (events). So this node can be configurable on start and send events only for those parameters I\u2019m interested in", "\ne.g. roslaunch prameter_events default.launch update_list:=param1,param2", "I had submitted a PR to the rclcpp repository to add a ParameterEventsSubscriber class which I\u2019d hoped would address some of the issues mentioned here. Namely, the class subscribes to one or more parameter events topics (on either your current namespace or remote namespace), can filter events by parameter name and node, and can set a custom callback per parameter.", "Here is the current PR: ", "This week I am addressing the current feedback, so feel free to offer any additional comments.", "thanks i will look into the PR and leave the feedback.", "the class subscribes to one or more parameter events topics (on either your current namespace or remote namespace), can filter events by parameter name and node, and can set a custom callback per parameter.", "That sounds like an analogue to the TF Listener/Buffer approach.", "i think that we need to consider DDS contents filter topic to see if we can actually do what we want to do. and then bring up those interfaces up to ROS generic common API, so that parameter_events can be modified with new nice API. that is just idea and keep posted if we got anything.", "We will do feasibility for DDS Contents Filtered Topic 1st and then come up with high level design.", "seems like that ", " supports ContentFilteredTopics, we are gonna use this for feasibility check.", "I\u2019m somewhat hesitant about content filtered topics, as they introduce quite a lot of complexity in the rmw layer. The mechanism for expressing filters is complex and would be a huge burden for non-DDS rmw implementations.", "I\u2019m not 100% convinced about the motivation, as I do not think the bus approach (receiving all and filtering for what you want) is going to be problematic in practice. It\u2019s actually worse for tf as data is usually streaming continuously (especially before tf_static and with things like moving robot arms). Though it is not disallowed nor technically wrong, I don\u2019t foresee parameters change so much that you cannot subscribe to all events and filter down. It will be most intense during startup of a large system occurs. And I don\u2019t think many things will need to subscribe to this topic (mostly logging tools and introspection tools).", "Additionally, there are other ways to address this problem which may not be as elegant, but are still sufficient in my opinion. For instance, we could have a composable node that you could attach to any process which would subscribe intra-process to ", " and then filter as you configured it to do, and republish it on a new topic for external consumption. This solution keeps the middleware simple (and therefore more portable), and it should achieve similar performance to the content filtered topics.", "Also, though many of the implementations support content filtered topics, I believe many will not do publish side filtering (this is not a requirement of the feature in DDS, AFAIK). And therefore it will simply be doing what you the user could do (or a library on your behalf) and receiving all data, but filtering it before delivering it to you.", "That all being said, some significant value could be gained by using content filtered topics with keys and instances. We do technically allow key\u2019ed types via IDL now, but we lack the API support to make use of this effectively right now.", "So it\u2019s worth investigating still, but I just wanted to temper the interest in content filtered topics slightly, and/or prepare you guys for the fact that adding them to the middleware API is a significant undertaking.", "thanks for sharing practical thoughts!", "And I don\u2019t think many things will need to subscribe to this topic", "but as long as parameters are there, user wants to do something via parameter events. i believe that this is reasonable. and mostly user is interested in specific parameter for each node, it will receive all of the parameter events anyway. of course that is always depending on use cases.", "would be a huge burden for non-DDS rmw implementations.", "possibly, yes", "I\u2019m not 100% convinced about the motivation", "me neither, we are just trying to figure out what is suitable for.", "I believe many will not do publish side filtering", "So do I, but this is the key for improvement we want.", "\nincluding this, i believe that it is worth to investigate the feasibility.", "thanks,", "\ntomoya", "ParameterEventsSubscriber is introduced in ", "\ni think that we could improve that using Contents Filtered Topics base on ParameterEventsSubscriber.", " ", " ", " , and all", "We tried to use ContentFilteredTopic with opensplice, i see this could be really useful to optimize /parameter_events to be filtered with user interests. opensplice supports ContentFilteredTopic but only on  DataReader side filtering which is not efficient enough for network transport.", "Does anyone have plan to support ContentFilterTopic on DataWriter side with multiple expressions and array types? i just trying to get more information around here.", "thanks in advance.", "Hi ", " ,", "Does anyone have plan to support ContentFilterTopic on DataWriter side with multiple expressions and array types? i just trying to get more information around here.", "Yes \u2014 but, there\u2019s no ETA yet.", "but partition is produced", "All DDS implementations support partitions, and there are quite a few tricks one can play with them (you can publish in multiple partitions, subscribe in multiple partitions and use wildcards, too). Currently ROS2 doesn\u2019t rely on partitions so one would think there is a way to use them for this. It\u2019d be worth a try at least.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["all parameter events via single topic \u201cparameter_events\u201d (current)", "parameter events via topics for each node (user can specify whose parameter is interesting)", "each parameter via each topic (user can specify which parameter is interesting)", "all parameter events via single topic \u201cparameter_events\u201d (current)", "parameter events via topics for each node (user can specify whose parameter is interesting)", "each parameter via each topic (user can specify which parameter is interesting)", "Twinoaks", "RTI", "Prismtech", "OpenDDS", "eProsima"], "url": "https://discourse.ros.org/t/all-of-parameter-activity-via-parameter-events-topic/10767"}
,{"title": "ROS 2 Tooling Working Group Announcement", "thread_contents": ["Hello,", "\nThe AWS RoboMaker team is pleased to announce the creation of the ", " .", "The objective of this working group will be to develop and maintain ROS 2 tools. A tool, in this context, is software supporting developer productivity or robot operation. Examples of existing tools under this definition are rviz, rqt, and rosbag.", "Please note that we will try a more structured approach with this working group. Specifically, this working group will manage the roadmap, contribution, and ownership of subprojects. All owned GitHub repositories will live in the working group GitHub organization: ", ". The working group will start by owning the ", ", to try out and iron the new process.", "The first meeting of the ROS 2 Tooling Working Group will be on ", " \u2192 ", ". We will present the ROS 2 WG governance model and discuss improvements to the cross-compilation tool.", "If you have any question about the process, please reach out to me: ", "Thank you!", "Hello Thomas,", "I am happy to see the ROS 2 Tooling WG is formed and thank you for driving the first meeting. In additions to the date and time, do you mind to share the link of the online meeting? Thanks!", "I would like to join, thanks for leading.", "Examples of existing tools under this definition are rviz, rqt, and rosbag.", "i believe that those tools are mandatory for developers and has to be easy, quick and efficient. so at some part.", "Would CLI also fall in this?", "Any particular desire to move rviz to the web?", "Any particular desire to move rviz to the web?", " itself is a Qt application and that won\u2019t change.", "That being said there are multiple web-based alternatives which aim to provide similar functionality as ", " (one example is ", " but there are certainly more and probably more feature rich ones). Both approaches will likely coexist since they satisfy different use cases.", "Is there a particular technical reason why the \u201cprimary\u201d (and I know that\u2019s a wish-y wash-y definition - but the main supported/used tool) robot visualization tool couldn\u2019t/shouldn\u2019t be in the webrviz? Long term, I could in concept think that rviz would sunset for rvizweb.", "I\u2019ve seen other companies with web rviz like things (Cruise, amongst a few others) but none are well supported or full featured to rely on or bother to finish development myself due to lack of long-term support.", "This isn\u2019t probably the place for this conversation, but I\u2019ve been thinking about this recently and a tooling WG seems like a good a place as any I\u2019ve seen.", "Is there a particular technical reason why the \u201cprimary\u201d (and I know that\u2019s a wish-y wash-y definition - but the main supported/used tool) robot visualization tool couldn\u2019t/shouldn\u2019t be in the webrviz?", " is simply the longest existing, most feature rich, most frequently used tool which exists at the moment. That is probably the only reason why you can consider it to be the \u201cprimary\u201d tool. While the web-based alternatives all have there benefits there are simply cases where rviz outperforms them (feature wise, performance wise, or in some other aspect).", "Nothing prevents any other tool to become more used and feature rich. But as you mentioned yourself none are well support or full features at the moment - so some people have to commit the resources to make it a superior choice.", "All good points, I\u2019m just throwing that out there as a potential candidate then for the tooling group should anyone else have a desire for it. I\u2019m not a web developer but I would love to see webrviz/similar be the defacto standard, in lets say, 3 years. In browser seems to be the way things are going.", " Future direction for RViz is an excellent topic of discussion for the working group as we move forward. However, lets not derail this thread by diving too deep into it here ", "The scope is starting small and the plan is to evolve over time as the governance and ownership model for the WG evolve. We want to set the WG up for successful work toward feature development by being careful about focus.", "Hi ", ", is there a google calendar group for this to get automatic invites?", "Thanks", "You should have received an invite Dragan. We\u2019ll be using the same system than ROS 2 Security.", "Anyone can join the ros2-working-groups-calendar-invites Google Group to get an invite: ", "Thanks got it ", ", the odd thing is the top post says it\u2019s on the 18th(Friday), but the invite I received says it\u2019s on the 20th(Sunday).", "Is it on the 18th or the 20th?", "This is a mistake, this Discourse post is correct, the event is on the 18th (Friday)", "I will never understand why anyone could ever desire to have rviz in a web browser\u2026", "I will never understand why anyone could ever desire to have rviz in a web browser\u2026", "Because then I can make at a front end developer to make something for me rather than making me do it ", "User-facing \u201cthis is what the car is seeing\u201d UIs.", "sorry i cannot make it to be there, is there minutes available?", " I personally don\u2019t have the bandwidth to join this working group, but I wanted to point out that the time you have chosen completely excludes anyone in Asia from joining. Please consider either rotating the meeting time through the time zones (so that everyone has a turn at the sucky time), or at least shifting it a couple of hours earlier or few hours later so that it falls outside the midnight-to-6am range for Asia. Probably a couple of hours earlier would work best for America and Europe.", " the minutes are there - ", " I added your suggestion to the agenda for tomorrow.", "The agenda for tomorrow\u2019s meeting is on the doc too. I ran out of time to properly announce this one, we\u2019ll do better next time!", " I would like to suggest, to include in your TWG, the tools used for real-time testing and validation. A set of tools for that was presented at the real-time workshop at ROSCon 19: ", " by Ben from Silexica.", "I do not have the bandwidth to drive that but if you would consider this inclusion, then I can connect you to Ben (if you haven\u2019t been yet).", "D.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-2-tooling-working-group-announcement/10970"}
,{"title": "ROS 2 for Consumer Robotics", "thread_contents": ["Hi all,", "\nI\u2019m Alberto, from iRobot.", "\nAs you may know, my colleague Juan and I gave a presentation at ROSCon about using ROS 2 in consumer robotics application.", "\nIf you missed it, you can find it here ", "The purpose of our talk was to discuss about a performance gap in ROS 2 that was preventing it from being effectively used on a particular class of platforms:", "\nwe thought that Linux-based, low cost, embedded platforms (as RaspberryPi) were not taken enough into consideration.", "During our talk we highlighted the most relevant problems of the current (Dashing) release and we suggested some improvements.", "\nAll the experiments presented in the talk or in this blog post have been performed using our open-source performance evaluation framework: it\u2019s a very simple framework that measures some fundamental performance metrics; its peculiarity is that it allows to simulate any type of arbitrarily complex ROS 2 system through a JSON file.", "\nYou can find it here ", ".", "We want to thank all of you for the interest you showed in the problems that we presented and we are extremely happy to see all the work that is going on to address these issues.", "First of all, we can have a look at the default performance of the current ROS 2 master.", "\n", "The performance are already a lot better than what we measured for Dashing, in particular the CPU usage and the latency improved and we have a reduction of ~40 Mb of RAM. We have no more \u201ctoo late\u201d or \u201clost\u201d messages (we are in a single process scenario, so this is highly desirable!)", "However, what is really interesting is all the work that is currently going on and will be part of the next releases", "CycloneDDS", "\nRefactor of the serialization procedure: this made the code cleaner and it reduced latency and CPU usage, especially for big messages.", "\n", " (results ", ")", "\nThis has been recently merged to master.", "\nImproving the deserialization procedure will be the next step and many more features are coming since this DDS has been added to the list of ROS 2 stable sources.", "FastRTPS", "\nSubstantial improvements on the memory side: before most of the memory allocation was happening at startup to target real-time use cases, while now the default behavior will be more memory efficient in scenario where real-time constraints are not present.", "\nThe RAM usage in our benchmark goes from 116 Mb to 33 Mb.", "\nPart of this is already available in master, while other works will be ready for the next Eloquent relase.", "\n", "\n", "RMW Iceoryx", "\nA new RMW, completely based on a shared memory layer in order to ensure best performances for inter-process communication.", "\nThis is already available and it\u2019s receiving a lot of interst and support.", "\n", "Static Executor", "\nA first step towards the refactor of executors in ", ". This new executor performs the best when most of the nodes are already available at startup, but is still able to recognize new publisher/subscriptions.", "\nUnfortunately, it is not updated to the current master, so I haven\u2019t been able to test it extensively, but the CPU usage looks definitely better!", "\nIt\u2019s targeting the next ROS 2 release, Foxy.", "\n", "\n", "With all this work going on, I think that we can say that if Dashing and Eloquent were the releases that added to ROS 2 most of the required features, with the next release, Foxy, the ROS 2 performances will get a big improvement!", "Let\u2019s keep going in this direction, to make ROS 2 a success!", "Hi Alberto,", "As you said, Fast RTPS and rmw_fastrtps default settings are designed for ", ":", "1.- ", ": We allocate some memory at startup to avoid dynamic allocations. While this is good for many applications, it is not if you are looking for minimum memory usage.", "2.- ", ": The user thread is not publishing directly the message but copies the data to a buffer, and a middleware thread does the actual publication. This is good for real-time determinism as the user thread returns immediately. But is not good if you are looking for minimum latency.", "Also, the Fast RTPS for dashing didn\u2019t count with an intra-process mechanism.", "Therefore in your initial comparison, you were comparing:", "But, if you setup Fast RTPS with the recommended settings for this case and use the latest version, you get:", "This experiment includes the whitelist mechanism you recommended in your presentation (10 mb less of memory usage). A lot better, and matching your requirements.", "The documentation to setup the rmw of fast RTPS is in the readme of the rmw: ", "Also is worth to comment ROS2 is creating a DDS participant per publication or subscription, leading to a lot of participants. That is not the recommended use of DDS, and in this case leads to a lot of participants. For the next ROS2 release this behaviour is going to be fixed:", "We did an experiment using your framework with an equivalent topology, but just a single DDS participant. In that case:", "With 1-to-1 mapping, both the memory and the CPU usage improve a lot, as it is the recommended way to use DDS.", ":", "Also, to easily reproduce these results and the different experiments, we created a complete Rosject:", "Get your copy of the ROSject iRobot performance test dds comparison by @2960bd0df664", "Important Note: iRobot scenario uses a Raspberry Pi with raspbian. The rosject creates a cloud instance using linux, and the memory page size is bigger than in a raspberry PI, so the memory measures are higher for any configuration, but it is useful to understand the differences.", "As conclusion, ROS2 with Fast RTPS is highly configurable, ", ", and for Foxy we will focus on performance, so as you predict, it will be even better.", "For a how-to on how to change the basics:", "\n", " (readme)", "\nAnd for detailed documentation of all the available options, see:", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Fast RTPS: Static Allocations + Async Pub + Loopback (Dashing)", "Cyclone DDS: Dynamic Allocations + Sync Pub + Intraprocess (Latest)"], "url": "https://discourse.ros.org/t/ros-2-for-consumer-robotics/11860"}
,{"title": "ROS2 TypeScript client library available", "thread_contents": ["A new version of the ROS2 JavaScript client library ", " (v. 0.11.1) has been released. Key enhancements include:", "For TypeScript developers, the introduction of TypeScript declaration files (*.d.ts) enables smart coding tools such as VS Code and Eclipse CodeMix to help improve their coding efficiency and quality. If you want to stick with classic JavaScript no problem as the base library api is based on JavaScript ES6.", "One key requirement to note before installing the library from npm is that it requires a node.js version between [6.x - 10.x].", "To learn more visit:", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Support for TypeScript coding", "Compatibility with the latest ROS2, Eloquent release"], "url": "https://discourse.ros.org/t/ros2-typescript-client-library-available/11980"}
,{"title": "ROS 2 TSC Meeting Minutes: 2020-01-16", "thread_contents": ["Family names! We\u2019re getting really professional now! ", "Just a small modification: In Spain they have two last names (e.g. Martin Losa), so either use the first name \u201cJaime\u201d, or use the complete last name \u201cMartin Losa\u201d. Other alternative is: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees\n", "Toffee Albina from TRI is out sick - sorry to be missing the meeting", "Aaron Blasdel from Amazon", "Thomas Moulard from Amazon", "Joe Speed from ADLINK", "Lokesh Kumar Goel from LG Electronics - replacing Brian Shin as TSC rep", "Kyle Fazzari from Canonical", "Jaime Martin from eProsima", "Karsten Knese from Bosch", "Dejan Pangercic from Apex.AI", "Sean Yen from Microsoft", "Will Son from ROBOTIS", "Brian, Dirk, Louise, Tully, William from Open Robotics", "Steve Macenski Samsung Research", "Geoffrey Biggs from Tier IV", "Matthew Hansen + Harold Yang, Intel", "\n", "Preliminaries\n", "[2 mins][Hansen] Hand-off of Intel\u2019s TSC rep\n", "New Intel rep: Harold Yang", "\n", "\n", "Old business\n", "[15 mins][Gerkey] Introduce and briefly discuss ", " (n\u00e9e ROS 2 Essentials)\n", "Encourage all TSC members to weigh in.", "Reach consensus on using this list as the definition of \u201cthe project\u201d for the purposes of evaluating whether new TSC applicants and current TSC members are \u201ccontribut[ing] materially to the project\u201d (from our charter).", "Agree on feedback that Gerkey will provide to two pending applicants.", "\n", "\n", "New business\n", "[Hansen] Request for help with Navigation2 going forward\n", "Foxy Release Feature \u201cWish List\u201d - ", "\n", "Public request posted. Looking for more TSC member help.", "\n", "[Biggs] ROSCon JP will be held on September 8 in Tokyo", "[Blasdel] PAL & Amazon are talking about a ros control working group to get ros control ported to ROS2.", "Process for creating TSC working group. Present proposal to TSC meeting for a approval", "[Speed] OpenSplice RMW to Eclipse Cyclone DDS, fewer RMWs to support. Targeting Foxy\n", "Help OpenSplice users move to Eclipse Cyclone DDS which is easier, faster, smaller and more efficient than OpenSplice. Both come from the same development team so Eclipse Cyclone DDS will seem easy & familiar to OpenSplice users. ADLINK will give technical support to help OpenSplice users make the transition\u2026", "All parties will stop building or supporting rmw_opensplice for Foxy (Eloquent and earlier still maintained).\n", "Speed to ", " (DONE) and update rmw_opensplice README.", "D.Thomas to update REP-2000", "\n", "\n", "[Karsten] Status of rclcpp: Wish to refactor for more RT and execution management.\n", "Bosch may put together PRs that refactor things; will need review", "Woodall may be able to help with both development and review", "Apex.AI planning to make a number of contributions, via internal resources and at Open Robotics", "Plan: use real time WG to determine relative priority of different options (loaned messages vs. take N message vs. refactoring rclcpp vs. changing how executors work)\n", "[Biggs] WG should consolidate and summarize status and priorities\n", "Pangercic to create a page; look at safety WG as an example", "\n", "\n", "[Biggs] What about rcl? (For people who don\u2019t want to use C++)\n", "[Woodall] Current efforts focus on rclcpp but will benefit rcl. And rcl isn\u2019t meant to be user-facing.", "\n", "\n", "[Pangercic] ROS 2 User Survey?\n", "[Foote] No current plan for a survey. We usually do it on demand when we know what we\u2019re trying to learn.", "[Pangercic] Question(s) to ask:\n", "Is ROS 2 becoming mainstream?", "\n", "Pangercic to determine what Apex.AI wants to learn and then come back with some proposed questions.", "\n", "[Macenski] Samsung may be able to contribute more engineering resources with experience on static and dynamic analysis; what should they work on?\n", "[Fazzari] Security-related analyses", "[Woodall] Quality-level analyses", "\n", "\n", "Recurring business\n", "Next ROS 2 distro release - Foxy\n", "[5 mins][D. Thomas] Update on status, roadmap, changes since last time\n", "\n", "\n", "As mentioned in the last meeting: decision to use C++17 (e.g. filesystem) pending, will be made soon if there is no further input", "\n", "\n", " which will accumulate changes / migration notes over time", "\n", " has only sparse information - please comment with your contributions!\n", "Ongoing OR work items:\n", "Change mapping of ROS nodes to DDS participants, instead allow multiple ROS nodes within the same context to share one DDS participant\n", "[Jaime] Major DDS implementations are designed to share the participant, including OpenSplice, RTI, CoreDX, OpenDDS, and Fast RTPS, So I recommend to really push this change.\n", "Specifically recommend to use one DDS participant per process", "\n", "[Speed] Avoid this change to ROS 2? Users can choose RMW that fits their use case w/o changing ROS 2, see ", " (specific feedback from Erik/Joe ", " and ", "). Eclipse Cyclone DDS handles this use case w/o any change to ROS 2. Dirk asks if Eclipse Cyclone DDS team can show others how they could can make other RMWs do this.", "[D.Thomas] We\u2019re open to alternative proposals", "[Biggs] This discussion is a good argument to form a Middleware WG; DThomas to follow up", "\n", "Support use case of using xacro to perform substitutions that can be passed to parameters", "Python API for iterating over messages in a bag", "\n", "Many items on the ", " would be important but require resources (some random unordered examples):\n", "Check uniqueness of node names", "Configure logging (by file and/or at runtime)", "Introspect QoS settings", "Notify about graph changes (add/remove nodes/pub/sub/service/client)", "Executor performance problems", "Interoperability between native DDS entities and ROS entities", "rosbag: clock / simtime support", "Common way to configure RMW", "Fix custom message allocators with intra-process comm", "\n", "\n", "\n", "\n", "Working groups [5 mins each]\n", "[Knese] Embedded\n", "No updates", "\n", "[Macenski] Navigation\n", "\u201cDefault SLAM\u201d Navigation working group result - SLAM Toolbox\n", "In coming weeks talking about management of the repo and adding relevant other maintainers", "Macenski to announce on Discourse", "\n", "Working on a working group IROS paper comparable to the \u201cOffice Marathon\u201d paper", "\n", "[Pangercic] Real-time\n", "Meeting Jan 8: ", "\n", "LET executor: Working on addressing comments with new PR on rclc (within the next week)", "Silexica Performance Testing Platform\n", "AWS ECU resources are requested (Kai and Aaron were just introduced)", "\n", "Performance Testing white paper is in progress", "[DThomas] some PR pending, Apex will take a look", "\n", "\n", "[Biggs] Safety\n", "No updates", "\n", "[Fazzari] Security\n", "Discussed changing the 1-1 node to DDS participant mapping and assigned action items for security-related features", "Vulnerability disclosure policy being drafted", "Next meeting scheduled for 1/28", "[Woodall] Relevant REP covering quality level definitions: ", "\n", "\n", "[Blasdel] Tooling\n", "Cross Compile tool now supports ROS1 targets", "Moving Cross Compile tool to generic python pip-installable package like colcon", "System Metrics collector close to first release (it was also forked by a community member for ROS1 backporting!)", "ROSbag2 per-bagfile compression will be landed in the next week", "[DThomas] Users are reporting problems logging topics based on QoS settings / playing back with the same QoS. Also they can\u2019t play back simtime/clock.", "\n", "[Speed] Edge AI\n", "kick-off discussion next week", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-2-tsc-meeting-minutes-2020-01-16/12382"}
,{"title": "Release of ifopt v2.0 - Interface to NLP solvers (Ipopt, Snopt)", "thread_contents": ["Dear ROS community,", "We\u2019re happy to announce the release of version ", " of ifopt.", "It combines the advantages of ", " and ", ":", "For more information, visit ", ".", "Best!", "\nAlex @ ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Solver independent formulation of variables and constraints with Eigen (highly efficient)", "Automatic index management by formulation of ", "\n", "Integration: pure cmake ", " or catkin / ROS", "light-weight (~2k lines of code) makes it easy to use and extend"], "url": "https://discourse.ros.org/t/release-of-ifopt-v2-0-interface-to-nlp-solvers-ipopt-snopt/5553"}
,{"title": "TSC Meeting Minutes 2019-04-18", "thread_contents": ["Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees:\n", "Allison Thackston (TRI)", "Brian Gerkey (Open Robotics)", "Brian Shin (LG Electronics)", "Dejan Pangercic (Apex.AI)", "Dirk Thomas (Open Robotics)", "Filipe Rinaldi (Arm)", "Geoffrey Biggs (Tier IV)", "Jaime Martin Losa (eProsima)", "Jonathon Smereka (TARDEC)", "Karsten Knese (Bosch)", "Louise Poubel (Open Robotics)", "Matthew K Hansen (Intel)", "Rutvik Hora (Amazon)", "Seonman Kim (LG Electronics)", "Tully Foote (Open Robotics)", "Victor Mayoral Vilches (Acutronic Robotics)", "\n", "Old business:\n", "[2 min] [Brian] ROS trademark update\n", "Brian providing usage specimens to lawyers for filing.", "TSC: volunteers to form small group to work on usage policy?\n", "Geoff", "Jaime", "Brian", "\n", "\n", "[2 min] [Brian] Elevator pitch\n", "Brian iterating on draft with layout designer. Should have something ready in the next week or so.\n", "Brian to integrate links (comment from Matt H.)", "\n", "\n", "[2 min] [Brian] TSC content in ROS 2 docs\n", "Live: ", "\n", "Update Safety lead to Geoff\n", "PR: ", "\n", "\n", "\n", "\n", "[2 min] [Dirk] Transition plan from ", "\n", "\n", " shuts down May 16 (", ")\n", "Want public state to determine the status of the board.", "Also want multiple organizations in a single board, but might not be achievable.", "\n", "\n", "\n", "New business:\n", "Working group document storage and communication channel\n", "We\u2019ll continue a thread in Discourse, tentatively looking at adding discourse categories to collect working groups in a hierarchy.", "\n", "[2 min] [Brian] Open Robotics hiring Evangelist\n", "\n", "[15 min] [Brian] Dashing release communication plan\n", "Planned Support Cycle:\n", "LTS: 2 years", "\n", "Features: need to accumulate key feature list\n", "Google doc for commenting open to everyone", "Brian to set that up", "\n", "Demos: who has demos to include?\n", "Several TSC members planning to include demos, aiming for screenshots, videos, narrative text, and/or tutorials describing how to replicate the work.", "Demos from outside the TSC also welcome!", "\n", "\n", "\n", "Standing updates:\n", "[15 min] [All] Release planning - dashing update:\n", "Apex.AI\n", "Testing and launch improvements\n", "\n", "Performance Testing", "\n", "Acutronic Robotics\n", "Real-Time\n", "Work already started, expecting a first version with selected platforms for June (Dashing)", "\n", "Information models\n", "Coliza released ", "\n", "\n", "Motion planning progress: ", "\n", "Articles describing the process and raising awareness: ", "\n", "CI ", "\n", "Docker containers (for CI purposes) ", "\n", "\n", "Install instructions ", "\n", "\n", "Bosch\n", "Ongoing work with diagnostics. Waiting for PRs to be merged.", "Preparing buildfarm CI job to build rosbag2.", "\n", "Amazon\n", "QoS PRs in progress. Working end to end with RTI & OpenSplice", "Testing in progress against FastRTPS v1.8 that supports these QoS", "Security threat model is merged. Contributions from Acutronic for MARA", "ASAN/TSAN jobs are running. Our team is working on stack ranking and fixing issues.", "\n", "eProsima\n", "Deadline, lifespan, and liveliness QoS Supported on Fast RTPS 1.8", "Static Memory allocations. It is a WIP, but discovery allocations will be all static in Fast RTPS 1.8, and many others.", "\n", "Open Robotics\n", "IDL support landed\n", "3rdparty generators need updates", "Rospy generated content now leverages numpy might be API changes", "\n", "Intraprocess efficiency/performance improvements", "C++ API for composable nodes", "Launch support for loadable components", "New generation of Launch API", "Read Only Parameters", "XML frontend for Launch(might fall back)", "Range support for Parameters, in process by at risk for feature freeze deadline.", "Fixed rclpy memory leak, expect backport", "rosbridge_suite port is ongoing", "Infrastructure and CI jobs improvements", "Support for generating RPM spec files expected", "\n", "LG Electronics\n", "In Progress: OpenEmbedded Platform support", "Merged PRs to rosdep and rospkg that were needed for OpenEmbedded", "Submitted PR for superflore upstream, under review", "Have reference HW demo on Raspberry Pi 3 with set of Crystal packages using superflore automatically generated recipes", "Working on extended Crystal package support (currently 275 packages)", "Will build Dashing packages now that Dashing distribution index has been released", "\n", "Microsoft\n", "Working with Open Robotics on ROS 2 release logistics for the Windows side, including how to host chocolatey packages, ROS bridge and vcpkg integration.", "Stood up a colcon build on Azure DevOps", "We\u2019re going to add an installer to rosdeps so that when a chocolatey package is not available, rosdep will consult vcpkg to see if a recipe is available and offer to download, build and install.", "Working on a vcpkg feature to export chocolatey packages", "\n", "\n", "Working groups:\n", "[5 min] [Matt H] Navigation\n", "Added Obstacle Layer - still doing some tuning", "Converting nodes to lifecycle nodes, for improved bring up / shutdown reliability - in code review", "Converting to using Actions across the stack", "Adding recovery behaviors via behavior tree", "Adding parallel planning", "Moving to using BehaviorTree_CPP v3.0", "Support integration of system test into ROS2 build farm? (need to discuss)\n", "Issue: ", "\n", "Line: CI for simulation-based testing of navigation", "\n", "\n", "[5 min] [Rutvik] Security\n", "Threat Model is merged, Turtlebot and MARA models", "Setting a 2 week cadence for ongoing meetings", "\n", "[5 min] [Dejan] Real-time and safety\n", "Reference hardware platforms\n", "Using the ", "\n", "\n", "\n", "[5 min] [Karsten] Embedded\n", "No updates as for now. Preliminary work has to be done before the next meeting, presumingly in May.", "\n", "[5 min] [V\u00edctor] Motion planning WG Suggestion\n", "Discussion of plans and planned actions, input from TSC", "\n", "\n", "Late new business:\n", "[Rutvik] armhf (32-bit ARM) support\n", "Seeing demand for armhf", "Looking to improve the toolchain", "ARM interested in supporting this", "Also of interest to Apex to avoid requiring hardware, all platforms are arm based", "Target Tier 3 to start, want to push it up to at least Tier 2", "\n", "[Brian Shin] Definition and requirements for new platform support\n", "Rep-2000: ", "\n", "OpenEmbedded as Tier 3 platform\n", "Would like more definitions for what the requirements are for the tiers.", "A process for being selected/supported would be valuable", "\n", "\n", "[Karsten] State of real time within ROS 2\n", "State of executor\n", "How can we approach proposing changes to this?\n", "Design Doc or PR proposals are best or go through working doc", "\n", "\n", "ROS 2 and Real-time discourse post", "\n", "New TSC member inquiries\n", "Looking for ways to engage, how can they get involved? What can we send them? They\u2019re looking for chunks of work to pick up to demonstrate interest.", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/tsc-meeting-minutes-2019-04-18/8832"}
,{"title": "Map Library for Autoware.Auto", "thread_contents": ["In the ", ", attendees(including myself) have agreed that Lanelet2 was only realistic solution for map library.", "Discussion was very short and the consensus was that  ", " .", "As a result, Autoware.ai would start supporting Lanelet2 library from v1.13. However, now that we have more understanding about the library, I would like to discuss once more if we should also Lanelet2 library for Autoware.Auto.", "Here are my positive and negative feedback about the library.", "\n", "Overall, I thought the library was easy to use once I get used to it if we want to just use their API. However, underlying implementation is quite complex, and it would be hard to track down the issue if we find a bug. Also, ", ". For example, we have a report that Lanelet2 cannot be built on arm architecture due to mrt-cmake-modules package which Lanelet2 depends on, but the author have no intention of removing the dependency.", "Therefore, I think that we should create our own simpler library and maintain in the AWF gitlab repository, and I think we should be starting soon for Hackathon in April.", "\nAny thoughts?", "I have been working with Lanelet2 over the past few weeks (including the pre-release autoware integration) and generally agree with the Pro/Con breakdown by ", ". I have found the tagging mechanism needs more documentation/standardization for it to be used without the potential for error. One way to mitigate that may be for the AWF to define what set of tags are to be used in Autoware as well as create the TrafficRules objects for the US/UK/Japan as needed (Germany is the available default).", "However, I think we should be careful about rushing too quickly into building our library from scratch. That was essentially the idea originally proposed for Autoware.ai here ", "\nThat received a large amount of push back from the community which wanted a standard format to be used.", "\nSome points to consider", "I just went through the process of manually creating a small lanelet2 map last week in JOSM. I then upgraded the map according to ", " to conform to the autoware lanelet2 extension (Great job ", " on that doc btw).", "Here is my feedback from the process:", "Here is my take on some of the cons you list:", "Expression of road objects are geometry driven. i.e.) geometry is not attribute of road objects, but object type is attribute of geometry. You cannot tell the object class unless you look into tags.", "Can you elaborate on why that is a con? Does it make code more complicated, less efficient, etc?", "Since all attributes are defined as optional tags, developers must be aware of what tags are present.", "I don\u2019t see this as an issue, there are validators that exist and can be extended to check for more things/tags.", "Overall I agree with ", "\u2019s hesitation to abandon lanelet2.", "\nMy suggestions would be to try to upstream as many improvements as we can back to lanelet2 with the support of the autoware community. If 20 github users from autoware comment on a feature request to make arm compilation easier, and open a PR to make it happen, I doubt they\u2019d really want to decline it.", "First, I post general comments on Lanelet2, and later, will write about concerns by ", " and ", ".", "I think Lanelet2 is a good library as discussed in ", ".", "\nI know and agree with this conclusion, but it doesn\u2019t mean Lanelet2 the best.", "It said just that Lanelet2 is better than Autoware Maps Format, and it was for Autoware.AI, NOT for Autoware.Auto.", "\nWe must accomplish the production level in Autoware.Auto, but I feel we can\u2019t do it with Lanelet2(including Lanelet2 as backend).", "Of course, we shouldn\u2019t be in a hurry, we have to list up enough motivations to move to a new format.", "\nHowever, I think we also need to consider for what reasons we continue to use Lanelet2.", "I mean, we have to consider:", "I believe this is the Maps WG\u2019s mission.", "\nCurrently, I feel everybody is saying that \u201cLanelet2 looks good\u201d just with their feelings, and that the core discussions are not taken place.", "\nLet\u2019s consider them more deeply together!", "And as for a new format, I\u2019d like to develop a prototype because we probably can\u2019t judge good or bad without a concrete example.", "\nI believe I can show you in a few weeks, so please give me feedback then.", "It would be:", "and in the future, targeting:", "Thank you for your feedback.", "Would an AWF implemented library still use the lanelet2 conceptual format (lanelets/areas/regulatory elements etc.)? I would prefer that over switching to an alternative without a very good reason.", "\nWould forking the Lanelet2 repo be sufficient for making the kinds of changes which might be needed?", "I think the overall concept behind the library could be kept (like separating road objects layers and routing modules). Major change that I would like to have is slimming out implementation of core primitives. About forking, since the part I am concerned is core primitives of the library, we wouldn\u2019t get much merits from forking.", "The AWF probably represents the single largest user community which would want to work with Lanelet2. Can we use that to get the lanelet2 developers directly involved?", "If we are going to use Lanelet2 for Autoware.auto, I think asking lanelet2 developers to join the WG would be nice.", "Regardless of the backend library, it would be nice if the interface used by downstream components could be largely preserved to ease porting of Autoware.ai components to Autoware.Auto. What is the practicality of doing that?", "I agree that ideally the interface of the library used by downstream should be independent of backend library. (That\u2019s one of the reasons why we wanted to list up all the functional requirements we need.)", "\nAbout porting the Autoware.ai components to Autoware.Auto, I don\u2019t think we want to do that in first place. Many of the components(lane_rule, velocity_set, decision_maker) that use HD Maps have very bad implementation, and they need refactoring anyways.", "Are there any functional requirements missing from lanelet2 which cannot be added without reworking the library? Pretty much all of the \u201ccons\u201d listed are surface level issues which represent a challenge in learning the lanelet2 API but do not represent bugs or blockers on usage of the library.", "I believe all of the functional requirements we have seen so far is achievable using lanelet2. My major concern is the challenge to learn the core implementation behind the API. Maybe it\u2019s only me, but I had hard time looking deep into them. (e.g. How they implement attributeMap? What\u2019s the logic behind registration of new Regulatory Elements?)", "\nIt might be good to come up with non functional requirements first.", "\nThank you for your feedback.", "Expression of road objects are geometry driven. i.e.) geometry is not attribute of road objects, but object type is attribute of geometry. You cannot tell the object class unless you look into tags.", "Can you elaborate on why that is a con? Does it make code more complicated, less efficient, etc?", "IMO, it was more intuitive to have geometry as an attribute of road objects not vice versa, and I thought this could be an entry barrier for most of the developers and map makers. But I guess having good guide could solve the problem.", "Since all attributes are defined as optional tags, developers must be aware of what tags are present.", "I don\u2019t see this as an issue, there are validators that exist and can be extended to check for more things/tags.", "Can we assume that all users of Autoware.Auto to run the validator? I guess we can automatically run validation at loader, but I still want to have less ambiguity when accessing to tags(AttributeMaps) of an object. Otherwise, we would have to check existence every time we access to tags.", "If 20 github users from autoware comment on a feature request to make arm compilation easier, and open a PR to make it happen, I doubt they\u2019d really want to decline it.", "That maybe true, but we probably don\u2019t want to do it every time we need a critical change to the library.", "Thanks for providing your responses. Here are some more thoughts.", "Would an AWF implemented library still use the lanelet2 conceptual format (lanelets/areas/regulatory elements etc.)? I would prefer that over switching to an alternative without a very good reason.", "\nWould forking the Lanelet2 repo be sufficient for making the kinds of changes which might be needed?", "I think the overall concept behind the library could be kept (like separating road objects layers and routing modules). Major change that I would like to have is slimming out implementation of core primitives. About forking, since the part I am concerned is core primitives of the library, we wouldn\u2019t get much merits from forking.", "I am more concerned with how the road is represented then the library structure. Will we still divide lanes into lanelets which are built from 2 linestrings which are built from points following the lanelet2 standard. Or are you proposing that we rework that structure as well? It is important that we are clear when discussing road description standard, the file format, or the access library.", "I believe all of the functional requirements we have seen so far is achievable using lanelet2. My major concern is the challenge to learn the core implementation behind the API.", "I agree that the way the library is implemented is hard to follow (lots and lots of templates). Is this the reason you believe it should not be used in Autoware.Auto?", "It said just that Lanelet2 is better than Autoware Maps Format, and it was for Autoware.AI, NOT for Autoware.Auto.", "\nWe must accomplish the production level in Autoware.Auto, but I feel we can\u2019t do it with Lanelet2(including Lanelet2 as backend).", "Can you elaborate on why exactly Lanelet2 cannot be used? Is it because the library implementation is hard to follow?", "GIS-based, which is more standard than OSM", "OSM is GIS based as each point is described by lat/lon, so I am not sure what you mean by this point. OSM is also just the file format used to store lanelet2 maps and does not directly relate to how the lanelet2 road description standard is defined.", "OpenDRIVE subset(+ some extensions?)", "Are you suggesting we use the OpenDRIVE road description standard instead of lanelet2? That is a fundamentally different way of representing road geometry.", "Sorry if this is a late interference. This is an awesome discussion. I\u2019ve read that with great curiosity.", "\nI\u2019d like to point out a couple of potential problems I see with lanelet2. I\u2019m speaking from personal experience as we are using in-house made mapping system very similar to lanelet.", "\nLanelet is a geo-points based solution, meaning that map lines (road sides, road marking lines, stop lines e.t.c.) represented by a set of points (", ").", "\nWith this kind of representation performance of some basic map operations will not be satisfactory.", "\nLet me give examples:", "If I have task to redesign the system from scratch now. I\u2019d seriously consider using vector(", ") representation of linestring instead of approximated breadcrumb points. It\u2019s more complicated math for sure, but if done right, I believe it can give you O(1) for those cases I described.", "Hope it make sense and can help on final solution.", "Can you elaborate on why exactly Lanelet2 cannot be used? Is it because the library implementation is hard to follow?", "Exactly.", "\nIt supports many use cases and well tested, so it has no problem as long as we use basic features.", "\nHowever, once we start to use advanced features, to customize it, or to guarantee the code quality, it will be very hard without the maintainers\u2019 help.", "\nWhat if the Lanelet2 maintainers stop supporting?", "\nIf we continue to use Lanelet2 in Autoware.Auto, I think we must invite the maintainers to Autoware Foundation, at least.", "OSM is GIS based as each point is described by lat/lon, so I am not sure what you mean by this point. OSM is also just the file format used to store lanelet2 maps and does not directly relate to how the lanelet2 road description standard is defined.", "OSM can be said a kind of GIS, but its expression is quite different from the usual GIS formats. As ", " says, OSM is geometry-driven, but GIS is feature-driven.", "\nOne problem of that is OSM requires special editors. Please see ", "If we switch to GIS-base, we can use other editors like QGIS or ArcGIS, which I think are more useful than JOSM. Also, we can use many libraries, for example, GDAL, geopandas(shapely, fiona).", "Are you suggesting we use the OpenDRIVE road description standard instead of lanelet2? That is a fundamentally different way of representing road geometry.", "Partly yes, but I\u2019m not saying that we should you the \u201creference path + offset\u201d way.", "\nAs discussed in the Maps WG, we must support OpenDRIVE to integrate with simulators.", "\nWhat I\u2019d like to do is to create more general and AD-friendly data structures, and to develop a loader/converter from OpenDRIVE to the data structures.", "\nOf course, it can be done with Lanelet2, but it needs to be extended(e.g. lane-connection because Lanenet2 re-calculates it in runtime).", "I am more concerned with how the road is represented then the library structure. Will we still divide lanes into lanelets which are built from 2 linestrings which are built from points following the lanelet2 standard.", "I don\u2019t think we have to change the data structure that much, but apparently ", " has a comment on that. I haven\u2019t done much investigation about computation time as he has done.", "I agree that the way the library is implemented is hard to follow (lots and lots of templates). Is this the reason you believe it should not be used in Autoware.Auto?", "Yes. I am not sure if we want to use the library that we can\u2019t fully understand. If we notice something going wrong in the library, I\u2019m not confident to track down the root cause.", "Hello everyone,", "\nI just finished addition lanelet2 load/save functions to ", " map editing tool. feedback is strongly appreciated.", "Practically using lanelet2 library is painful:", "From the semantic point of view:", "From coordinates system point of view:", "About keeping lanelet2 or creating a custom format for Autoware.Auto, I am in favor of creating our simple format that satisfy the current Autoware functionality efficiently, and develop a converter for each standard (lanelet2, OpenDRIVE, Vector map) in which we can be sure that the output map meets both functional/nonfunctional requirements for Autoware.ai and Autoware.auto.", "I will try to summarize these points in tomorrow\u2019s meeting.", "Expression of road objects are geometry driven. i.e.) geometry is not attribute of road objects, but object type is attribute of geometry. You cannot tell the object class unless you look into tags.", "Can you elaborate on why that is a con? Does it make code more complicated, less efficient, etc?", "If it\u2019s hidden in the IO layer, it\u2019s not a big problem for application layers. However, it makes the loader unnecessarily complicated and inefficient.", "\nPlease see the source code. I think it\u2019s a sophisticated C++ code, but I think it can\u2019t be said as \u201cpractical\u201d.", "Since all attributes are defined as optional tags, developers must be aware of what tags are present.", "I don\u2019t see this as an issue, there are validators that exist and can be extended to check for more things/tags.", "Which do you think is better? I prefer the latter.", "For example, if we use an Database-based way, we can realize the method 2.", "Adding a complement to ", " 's comment.", "As ", " says, we can keep the overall concept.", "\nHowever, as ", " says, if we use Lanelet2, applications depend on the lanelet2 structure, so we should avoid directly using Lanelet, RegulatoryElement, and so on.", "We can remove dependencies on Lanelet2 data structures by defining APIs. (i.e. Using Lanelet2 as a backend)", "\nHowever, I believe that creating a simple library from scratch is much easier and better than doing that.", "Since it requires to change significant amount of lanelet2_core, I think forking is not a good idea.", "\nIt can\u2019t be done without the maintainers\u2019s help.", " Thank you very much for your great feedback!", "\nI strongly agree with your thought, but could you explain these points a bit more detail?", "Have you tried loading/converting OpenDRIVE?", "\nIf so, it would be very useful if you share the experience with us. (but in another thread?)", "I\u2019m sorry, but I couldn\u2019t understand this part: \u201cwe can prevent people from creating .osm map using JSOM.\u201d", "\nWhat do you mean?", "\nSure, I want to share that experience.", "\ncurrently I am trying to have different format of the same map , which can all work with Autoware.", "\nfor example:", "Until now conversion is not correct due to:", "I\u2019m sorry, but I couldn\u2019t understand this part: \u201cwe can prevent people from creating .osm map using JSOM.\u201d", "\nWhat do you mean?", "sorry , I mean we can\u2019t prevent people from creating .osm maps that don\u2019t follow our standard ! and once they load it to Autoware they face all kind of troubles, and start complaining.", "sorry , I mean we can\u2019t prevent people from creating .osm maps that don\u2019t follow our standard ! and once they load it to Autoware they face all kind of troubles, and start complaining.", "Thank you, I understand.", "\nI think we should regulate the digitizing process more strictly so that people can\u2019t create wrong maps.", "\nTo do that, I propose moving to GIS-based way, which can limit the tag names and values(e.g. enum).", "In addition to that, we need to check more complicated things,", "\nfor example, \u201cwhether adjacent lanes are aligned\u201d.", "\nI think this is the role of the validator.", "The Arm build issue is fixed in ", " and merged to master. The SHA will soon be propagated to autoware.ai.", "Thank you! Very much appreciated!", "This is only a long term concern for Autoware, but I asked a question on the lanelet2 repo about updating the lanelet map geometry after loading and apparently there is no process at the moment. ", "Thank you. I think this is the workflow of map updating after loading.", "The important point is, as you may know, that we can\u2019t simply replace only the changed objects because they are related to other objects.", "\nWe usually need to replace them in some sets of objects(e.g. mesh) to keep the consistency.", "However currently, since the map is small in most use cases, I think it\u2019s enough to do by \u201cwhole map replacement\u201d.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["map data structure is flexible. (Adding new tags is very simple)", "The library is organized into modules(packages) for different objectives (core, routing, io, etc.)", "Documents and sample code exists explain how to customize the library. (e.g. adding new parser, adding regulatory element, etc)", "It seems to cover minimal uses cases so far as we were able to replace Autoware.ai functionaity.(see also ", ")", "Expression of road objects are geometry driven. i.e.) geometry is not attribute of road objects, but object type is attribute of geometry. You cannot tell the object class unless you look into tags.", "Since all attributes are defined as optional tags, developers must be aware of what tags are present.", "Some objects have alternative ways of expression its geometry. (e.g. trafficlights can be polygon or linestring) This makes code to be relatively complex.", "IMO there are excess abstraction done for the primitive objects. Linestrings3d has 5 inheritance for example. This makes harder to track down the underlying data when debugging.", "Would an AWF implemented library still use the lanelet2 conceptual format (lanelets/areas/regulatory elements etc.)? I would prefer that over switching to an alternative without a very good reason.", "Would forking the Lanelet2 repo be sufficient for making the kinds of changes which might be needed?", "The AWF probably represents the single largest user community which would want to work with Lanelet2. Can we use that to get the lanelet2 developers directly involved?", "Regardless of the backend library, it would be nice if the interface used by downstream components could be largely preserved to ease porting of Autoware.ai components to Autoware.Auto. What is the practicality of doing that?", "Are there any functional requirements missing from lanelet2 which cannot be added without reworking the library? Pretty much all of the \u201ccons\u201d listed are surface level issues which represent a challenge in learning the lanelet2 API but do not represent bugs or blockers on usage of the library.", "It was a little complicated to learn all the tags/relations and how to apply them in JOSM (took me maybe 4-6 hours to fully grasp it all during the process of map creation), but once you understand, it\u2019s pretty straightforward. A walk-through guide explaining the map creation procedure would make this much simpler and faster. (I may create this guide if I have time, but no guarantees).", "Overall it was an easy experience since JOSM has so many existing features/plugins for map creations.", "I did not look into the library source code at all, so I can\u2019t comment on the complexity of that.", "What are the requirements for the map of Autoware.Auto?", "Does Lanelet2 meet the requirements, or can meet them at low cost?", "If not, how will we meet the requirements?", "GIS-based, which is more standard than OSM", "Easy to extend(probably than Lanelet2)", "OpenDRIVE subset(+ some extensions?)", "At least Lanelet2 equivalent libraries(e.g. geographic, routing)", "OpenDRIVE converter(at least, from OpenDRIVE)", "One of basic operations: Defining if a geo-point is within specific lanelet. This task boils down to the checking if the point is inside the polygon defined by points of linestrings forming lanelet. This is a standard algorithm and it requires O(N) operations where N is a number of points in polygon. So checking single point is O(N) and in typical driving situation such check happens hundreds of times per second which loads CPU badly. We certainly do some precomuptation/caching to mitigate the load but problem is still there.", "Another basic operation: For a geo-point, find closest point on the linestring. In general case it requires iterating all the points inside the linestring, meaning O(N) and again this operation is executed very often while driving.", "It took me one week to get used to the deep layered interfaces and templates.", "There is multiple ways to access traffic objects. don\u2019t know which is efficient and safe and which is not.", "There is a lot of hidden functionality.", "I wasted several hours facing a bug , when creating a new object with LineStrings3d with and id, it allocates huge amount of memory. (I need to formulate a bug and report it to lenelet2)", "Because the map editor internal map structure (OpenPlanner RoadNetwok map) is center line based. - which means lanes are represented as center line waypoints -, exact conversion is really difficult , what I did is approximation. Meaning:", "\n\u2013 modules that uses the map such as (planning, localization, \u2026 )) should follow the lanelet2 structure. and this is very ", "\n\u2013 Loading vector maps, OpenDRIVE and convert them to lanelet2 will face lots of problems, main issue will be the contextual representation difference.", "lanelet2 need manual setting of the Origin lat/long from the GPS grid. even if we add a tag in the autoware extension, we can prevent people from creating .osm map using JSOM.", "Force every developer to use a validator", "Prevent inputting invalid tags in the digitizing phase, and developers don\u2019t need to care about simple mistakes", "Would an AWF implemented library still use the lanelet2 conceptual format (lanelets/areas/regulatory elements etc.)? I would prefer that over switching to an alternative without a very good reason.", "Would forking the Lanelet2 repo be sufficient for making the kinds of changes which might be needed?", "\u2013 Loading vector maps, OpenDRIVE and convert them to lanelet2 will face lots of problems, main issue will be the contextual representation difference.", "lanelet2 need manual setting of the Origin lat/long from the GPS grid. even if we add a tag in the autoware extension, we can prevent people from creating .osm map using JSOM.", "Moriyama vector map -> lanelet2.", "CARLA OpenDRIVE maps -> lanelet2.", "GPS coordinates conversion , and how that can ", " with the ", " maps.", "Missing some basic information such as ", " (which is essential for the creation of lanelet)", "A vehicle senses a construction zone", "Upload the sensing information to cloud and fix map\n", "Or, vehicles might do this by itself?", "\n", "Publish new map(probably mesh?) to vehicles", "Replace mesh and reconstruct the map"], "url": "https://discourse.ros.org/t/map-library-for-autoware-auto/11720"}
,{"title": "Autoware Map WG: HD Map functional requirements", "thread_contents": ["I would like to discuss developing the initial Use Cases for HD Maps scenarios the AW Map WG proposed, and the requirements based cases from last meeting, into a functional description of the use cases - starting to define what functions a map API would need in order to realise the use cases.", "As part of defining requirements for an HD map API it is also useful to think about the minimal performance levels we assume from other AD modules. The purpose of an HD maps is support the perception, decision making and motion planning of AVs and although we would like to define a best case HD map, it must be made with respect to a minimal level of performance of an AV and its AD modules.", "So I propose for each use case scenario list 1) any assumptions we are making about the AD systems , and 2) the functional requirements.", "First off, focusing on Valet Parking, and in extension to the general AD challenge.", "The general use cases/requirements of HD map were determined to be (please add any):", "Valet Parking: Use cases", "To start:", "Requirements for traffic light use case:", "Any thoughts on this (it\u2019s just a starting point for discussion), or on any of the use cases, or on the requirements process in general?", "For Reference:", "past work on defining use cases:", "and their requirements:", "Thank you for your suggestion. (I\u2019m sorry for my late reaction.)", "\nI believe considering what functions we will need is so important for defining an efficient map model.", "So I considered some use cases.", "\nLet\u2019s continue this work and make things more concrete!", "Note:", "\nAt this stage, I don\u2019t want to make things so strict, so please ignore little things.", "\nAlso, the purpose of this post is NOT to list up all functions but to list up major ones and to utilize it for considering the map model for Autoware.Auto.", "The vehicle should recognize where(which lane) it is driving on the route, based on the localized pose.", "The vehicle should detect traffic lights associated with the current route. (not only the nearest one but also the next one in some cases)", "Similarly, we would need functions for finding other features from a Lane.", "If we just want to use features for localization, finding near features can be useful.", "\n(It\u2019s hard to use features that were found by distance for planning.)", "The vehicle should decide where to stop according to stop lines.", "\nIf it stops due to red traffic lights or crosswalks, it should know which is the associated stop line.", "Sometimes crosswalks don\u2019t have the stop lines, so it has to calculate where to stop from the shape of crosswalks.", "\nHowever, I think it\u2019s an application-level matter, not a map level one.", "I think it\u2019s good for us to take these next actions:", "List up more functions using more use cases.", "Classify each function", "Write pseudo-code for each use case", "I have organized the requirements into one document. Please commend if there is anything else to add. ", "2019/11/13 update", "\n", " (56.7 KB)", "Also, here are some of the non functional requirements for map library. (discussed with ", ")", "Regarding the requirement that the map should provide curvature and slope information for the motion planner - I agree it is useful information, but I would like to clarify is there an assumption that the map should provide a smooth reference path for a motion planner to follow? As it stands, one weakness of lanelet2 is that it doesn\u2019t have curvature information - just a piece-wise center line. If fine grained enough curvture can be estimated (much like current autoware waypoint path).", "How accurate should the curavture/slope information be?", "Personally, I don\u2019t think the map-data should provide them, because it will make the digitizing-process much complex.", "\nSo I propose that the map-data will provide as much accurate 3D pose as possible, and the map-library(or planner-library) will calculate curvature/slope.", "\nHowever, when we switch to OpenDRIVE, I think we can support detailed curvature/slope as optional parameters.", "How accurate should the curavture/slope information be?", "I\u2019m not sure but assume we don\u2019t need so high accuracy.", "I have updated the document and added columns to briefly indicate which function in Lanelet2 supports each requirement.", "\n", "\n", "I will also share the link to google drive documents in case anyone wants to add comments.", "\n", "\u30b7\u30fc\u30c81\n\nMap Functional Requirements,Pseudo function,Availability in Lanelet\nOverall Requiremnt,Requirements for each Module,Detailed Requirements,Function,Remarks\nMap API can provide enough information to all Autoware Modules,Map provides enough...", "\n", "Thank you very much for summarizing.", "It seems Lanelet2 has enough capability, but sometimes", "\nit gets complex even to accomplish simple features.", "\nI think it\u2019s better if we have a bit higher-level APIs.", "Also, if we create an abstraction layer for future backend(Lanelet2) replacement, we need to define general data structures.", "\nThe reason is that if we directly depend on Lanelet2 data structures such as TrafficLight/TrafficSign/RightOfWay, it will result in many dependencies: boost and other Lanelet2 specific classes.", "However in my intuition, to convert Lanelet2 into general data structures is too hard and it will increase the maintenance cost.", "\nTherefore, I think we should definitely define new-and-simple Autoware-flavored data structures and use only them.", "\nOtherwise, we will probably take a huge technical debt\u2026", "Yes, thanks Mitsudome-san.", "A couple of comments:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Driving according to traffic light", "General traffic signs perception and use for regulation", "Stop lines and required behavior, links to other elements", "Detection of pedestrians in crosswalk regions", "Definition of drivable areas, their regulations, associated regulatory elements", "Planning (route, lane, local)", "Parking in a given parking location", "Localisation", "Assumes", "camera based traffic light detection module requiring image region of interest of traffic lights linked to a given lane", "Functional Requirements", "Given poses along current path (range?) return lane/lanes which will be travelled", "Given a lane return any associated traffic lights", "Get 3D geometry of traffic lights (point,polygon)", "Project 3D geometry into 2D camera plane (map function? support query?)", "For a specific traffic light, return stop line.", "For a stop line, return related traffic lights if any?", "\n", "\n", "\n", "\n", "can be efficiently/inefficiently implemented with Lanelet2", "can be efficiently/inefficiently implemented with Lanelet2 if we extend it", "cannot be implemented with Lanelet2", "\n", "\n", "\n", "If it\u2019s already implemented in Autoware.AI, evaluate the efficiency", "\n", "There is quality assurance for map library", "Map library works in real-time", "HD map library works with very large maps", "OSS developers can easily understand map format and map api", "For Global planner - requirement mentions series of \u201croads\u201d detailed requirements only takes about \u201clanes\u201d. Is this loose semantics or does a  lane level planner equate to a \u201croad\u201d planner? I guess answer depends on architecture of planning module.", "One functional requirement that isn\u2019t listed, is the requirement to enable context aware prediction of other road users: given map and traffic rules - which are the possible valid future paths of an observed car/pedestrian etc."], "url": "https://discourse.ros.org/t/autoware-map-wg-hd-map-functional-requirements/10788"}
,{"title": "Autoware TSC meeting minutes for March 20th, 2019", "thread_contents": ["March 20, 2019, 7 AM Tokyo (+1day), 10 PM London, 2 PM California", "Nothing in particular.", "We need to settle on the tool chain to be used by Autoware.Auto for development. This encompasses tools used for management of the development, tools used by the open community for accessing the source code, filing bugs, and making contributions, and tools used for quality management.", "There are several options being considered. Whichever one we choose, the AWF will need to pay for the cost of use.", " (312.7 KB)", "Tier IV has a Velodyne VLS128 available for use by the foundation. What should we do with it?", "Questions from Apex.AI regarding the LGSVL simulator and its license (", ")", "4 posts were merged into an existing topic: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees\n", "Geoffrey Biggs (Tier IV)", "Antonis Skardasis (StreetDrone)", "Dejan Pangercic (Apex.AI)", "Dmitry Zelenkovskiy (LG)", "Esteve Fernandez (Apex.AI)", "Jan Becker (Apex.AI/Autoware Foundation board)", "Joe Buckner (AutonomousStuff)", "John Buszek (AutonomousStuff)", "Kenji Funaoka (Tier IV)", "Nikos Michalakis (TRI-AD)", "Paul Sastrasinh (TRI-AD)", "Shinpei Kato (Tier IV/Autoware Foundation board)", "Stephane Strahm (Kalray)", "Victor Duan (Linaro/96Boards)", "Yang Zhang (Linaro/96Boards/Autoware Foundation board)", "\n", "Minutes: Geoffrey Biggs (Tier IV)", "Opening remarks and new member introductions [2 minutes]", "Projects update: Autoware.Auto/Autoware.AI (Tier IV/Apex.AI) [10 minutes]", "Autoware.Auto toolchain (Tier IV/Apex.AI/TRI-AD) [20 minutes]", "Automotive Grade Linux cooperation (Tier IV) [10 minutes]", "Mapping system and format (Apex.AI) [15 minutes]", "Autoware projects use case (Apex.AI) [15 minutes]", "Heterogeneous computing platform for Autoware (Linaro) [10 minutes]", "Velodyne VLS128 usage proposals (Tier IV) [5 minutes]", "TSC member FTE work (All) [15 minutes]", "Release naming [5 minutes]", "Simulator licensing (Apex.AI/LG) [Remaining minutes]", "Review opinions on the tool chain and information provided by TRI-AD and make a decision on what to use\n", "Geoff, Esteve", "\n", "Coordinate with the Linux Foundation/AGL on their efforts to improve safety certification practices of open-source software\n", "Geoff to take point", "ARM, Linaro, TRI-AD, Tier IV, Kalray to participate as interested", "\n", "Lead improvement of mapping format used by Autoware\n", "Brian Holt (Parkopedia)", "\n", "Produce some proposals for milestone demos that we could set to drive the development of Autoware.Auto and direct resources (choice and ordering to be decided in TSC meeting ", ")\n", "Geoff", "\n", "Coordinate with Tier IV and Kalray to bring their work on algorithm acceleration into the computing platform effort\n", "Yang", "\n", "List planned contributions that go towards Foundation work and can be used to meet the milestone demos (once decided)\n", "All premium members", "\n", "Follow up with premium members who have never attended a TSC meeting\n", "Yang", "\n", "Hire a lawyer with experience in open source licenses to look at licenses for all potential simulators\n", "AWF board", "\n", "1.11 is due to be released on Thursday March 21st.", "Repositories, processes, etc. are being improved for 1.12 and on. See the relevant ROS Discourse threads.\n", "\n", "The Autoware code will be moved to a new GitHub organisation called \u201c", "\u201d", "Branches need to be pruned; many of them have stuff we want to keep so the pruning must be done carefully.\n", "In particular, Apex and AutonomousStuff each have one branch that should be merged.", "Four-way intersection work will be merged from the Apex branch into the AutonomousStuff branch.", "\n", "Updated ADE to ROS2 Crystal", "Ported internal Apex.Auto ray ground filter to Autoware.Auto\n", "\n", "Vehicle interface design discussion\n", "\n", " is possibly also relevant", "All interested in the vehicle interface, please read the above issues and participate in the discussion.", "\n", "Covers repository, issues, task management, CI", "Currently used by Apex.AI\u2019s initial project setup", "Cost-prohibitive and pricing model is not compatible with an open project.\n", "Special program for Non-Profit Organizations", "\n", "\n", "We may qualify for this despite the rule against making money because the foundation itself is non-profit.", "\n", "Apex.AI has experience with this toolset.", "Esteve (Apex.AI) has experience with this toolset.", "Use of GitHub maintains the existing Autoware community and keeps us close to the existing ROS community.", "TRI-AD proposes this. They are willing to maintain it.\n", "\n", " (369.1 KB)", "\n", "Use of GitHub maintains the existing Autoware community and keeps us close to the existing ROS community.", "JIRA is very powerful for managing complex projects.", "JIRA can integrate with GitHub such that the average user won\u2019t need to touch it (we can limit it to maintainers only), so they won\u2019t need to see a complex tool and can work through the GitHub interface.", "GitLab CI is a good tool and integrates well with GitHub", "Use of GitHub maintains the existing Autoware community and keeps us close to the existing ROS community.", "No CI tool of its own.", "Task management tools are poor; the best third-party replacement (", ") is shutting down in May.", "Linaro: How will we deal with ARM-native CI?", "TRI-AD is going to talk to a GitLab rep about costs; Apex.AI to join that meeting.", "TRI-AD wants the decision to be based on the merits of the tools rather than the need to maintain it or not; they will do any setup and maintenance we need.", "TRI-AD has put together a document describing their work on tool chains based around infrastructure-as-code.\n", "\n", "Esteve is concerned about JIRA being an entry point for users; it\u2019s fine for management but it\u2019s too much for the average contributor. The Apache Foundation used to use it, but everyone hated it and as soon as the foundation allowed Github use, everyone moved there.", "Dejan feels that option 3 is the worst option because of the combination of tools due to the issues syncing between tools. For example syncing between GitHub and JIRA issues is nearly impossible because of the different semantic models. Also problems switching between tools for different tasks.", "Dejan has found that Gitlab (used alone) has produced the happiest engineers.", "AWF board states that they are happy to cover CI cost, but would like a member to cover other costs if possible.", "Tier IV met the Linux Foundation people responsible for their Automotive Grade Linux project recently.", "They have had success in producing a stripped down Linux for use in the entertainment stack in cars.", "They are starting their next two projects now:\n", "The instrumentation stack.", "Unified Autonomous Driving Platform (UADP).", "\n", "They want to cooperate with Autoware and Apollo.", "UADP has significant involvement from Toyota, Honda, Mazda, Suzuki and Subaru. Volkswagen Group and Hyundai have just joined. Component suppliers such as Denso, Harmin and Panasonic are also involved.", "Non-traditional companies are also involved, such as Amazon Web Services and Adobe.", "Two expert groups currently working:\n", "Vehicle to cloud, working on over-the-air updates, map provisioning, etc.", "Vehicle to road infrastructure, working on smart infrastructure.", "\n", "They are also related to/involved in/running ELISA, the project to produce a safety-certified Linux.\n", "BMW and Toyota are major supporters of ELISA.", "\n", "The Linux Foundation and the AGL project want an improved method for certifying open-source software. They believe ISO 26262 is out of date.\n", "Tier IV is interested in coordinating on this for use for Autoware.Auto.", "Probably TRI-AD is as well.", "\n", "They will consider if AGL should join the Autoware Foundation in the summer, once they have figured out their concrete project goals.", "The Autoware Foundation needs to decide if it wants to join the Linux Foundation and participate in AGL and ELISA.", "ARM, Linaro, TRI-AD, Tier IV and Kalray will coordinate on working with their safety-critical efforts.", "I recently talked to several mapping providers: Atlatec, Mandli, Carmera, Parkopedia (Parkopedia in the TSC). Between them Atlatec and Mandli already provide maps as Lanelet2 format and Carmera also is thinking about trying to do that.", "I also concluded, after talking to some mapping experts, ", ".", "On the other hand, Tier IV is creating a Lanelet2 to Autoware vector map format translator (in their recent work for AutonomousStuff at least).", "So I feel that we are getting enough traction between mapping players such that they would finally all converge to a common format.", "But we need someone in the AWF to a) make sure that Lanelet2 is really ok, b) to then come up with the plan and c) to start integrating this into Autoware.", "Questions:\n", "So Brian do you have time to start driving this?", "Are you in a position to talk to above players since potentially you are competitors? Then I can also introduce you.", "Shinpei, Geoff are you OK to start ditching vector map in favor of something that others use (e.g. Lanelet2)?", "\n", "Apex.AI had a lot of problems with the existing Autoware format, for things like not being able to represent certain features, and being inefficient at runtime.", "We need someone from the TSC to take on management of the mapping format stuff.\n", "Shinpei proposes that Parkopedia takes the lead, due to it being their core competency, and Tier IV provides back up due its experience with mapping.", "\n", "Tier IV and AutonomousStuff have got Lanelets2 working with Autoware. OpenDrive is already supported.\n", "Dejan is concerned that this support is only a translator, not using the Lanelet2 format as core.", "Shinpei thinks that being based on open standards is good, but Lanelet2 has information in it that Autoware does not use. So Lanelet2 is a good frontend but we need to figure out how it will be used internally.", "\n", "Mandli is probably going to join the AWF, so they may be able to contribute to this work.", "Several AWF partners are working on the use cases/demos. For instance:\n", "AutonomousStuff/Tier IV on City-pilot", "Parkopedia/StreetDrone/Apex on Autonomous Valet Parking", "TRI-AD on autonomous shuttles/taxis?", "Tier IV on CES demo", "AutoCore/96Boards/Robosense autoware demo on heterogeneous platform (<20 km and avp)", "\n", "It would make sense to focus on one use case where at least the compute ECU and sensors are unified. This will make us faster and allow us to build a really good foundation to then expand into other use cases", "Can we agree on one use case and all work towards it?", "When ROS 1 was first being developed Willow Garage put significant effort into achieving a series of specific, well-defined use cases, the peak of which was the PR2 running a marathon. The recent ROS 2 user stories report identified that the lack of such milestones has hampered development and marketing of ROS 2 - without an eye-catching demo it is harder to demonstrate to users that ROS 2 is robust and useful and mature enough. Autoware must avoid falling into the same trap for Autoware.Auto. Having a single use case at a time that all of us are working toward would unify resources and improve quality.", "We need a series of increasingly complex, capable and robust use cases that each builds on the previous to demonstrate improvements in capabilities and robustness. The initial set of milestones proposed by Apex.AI is a good starting point.", "If we can publish these milestones and hold public demos when they are achieved, this would be good marketing.", "Apex.AI, Parkopedia and LG want to focus on closed environments (valet parking) as the first goal. Tier IV has worked on this area already, so probably interested. StreetDrone also has a project similar to the valet parking demo so probably interested.", "AutonomousStuff wants to stay on the public roads of their CityPilot project.", "TRI-AD does not necessarily want to stay on public roads; it would be nice but they are happy to scale down if necessary. Having something that works end-to-end is better than having a specific use case.", "Tier IV is also working with Kalray on acceleration and parallelisation of algorithms.", "Tier IV can provide hardware logic (FPGA implementations) for some algorithms, such as MVP and Yolo, which uses much less power than GPU-based implementations.", "Yang feels that Tier IV\u2019s work on accelerating the compute aspects is complimentary to their work on the ECU.", "Shinpei wants to get his university (not an AWF member) involved in the work on algorithm acceleration.", "Stephane: What use case is the computing platform project working on?\n", "Linaro: Parking and street driving. Minimum 40km/h driving. This is driving the necessary sensor and computing capabilities.", "\n", "Linaro: If ROS driver and Autoware integration is ready and no one else is applying for then we can use it for demo in China in May probably more realistic", "Apex.AI: Please mount it and drive around, then provide the data.", "What do FTEs from TSC members work on? A while ago there was a following suggestion being made:\n", "AS/Street Drone provides vehicle, urdf, dbw, testing on parking lots", "AutoCore provides a target ECU NOT including BSP and OS - to be confirmed", "Linaro working on aarch64 CI for Arm native development, testing, vehicle interface", "LG provides an integrated simulation environment with the car and the parking lot world model", "Arm provides security", "TRI-AD provides data storage in the cloud and data labeling services", "Parkopedia provides map of the parking lot", "Velodyne provides a vlp128 sensor", "Apex and Tier IV provide all other SW", "\n", "When we spoke to the Linux Foundation about the Autoware Foundation, something they very strongly insisted on was that using FTEs as a measure of commitment to the foundation and/or as a way to pay for membership is a very bad idea. They stated that they used to use this approach, but ran it as an honour system at first. When they started trying to track it, they found that it was very hard to accurately track, and more importantly, most members were not actually providing the FTEs they had promised.", "Apex.AI feels it is time to start tracking results of FTEs, such as code.", "Tier IV proposes assigning member companies to specific projects.", "There are two or three companies who are premium members who have never joined the TSC meetings, so are they actually contributing?\n", "Yang to find out what these companies are doing.", "\n", "LG wants to confirm the parking lot and the car to be used for the valet parking demo so they can produce an accurate simulation.\n", "Esteve and Geoff to coordinate this.", "\n", "\n", "\n", "The proposed milestones need to be taken into account. Contributions should be related to the Foundation\u2019s work and goals on these milestones.", "\n", "We could consider giving each release of Autoware.Auto a name, similar to ROS and Ubuntu release names.", "Options proposed include famous car models, famous racing drivers, and car parts.\n", "The first two may be legally problematic.", "\n", "It seems that if we want to use LGSVL free of cost (i.e. the Personal tier), it\u2019s only possible if the company/organization that develops it has a revenue of less that 100K a year (", "). If the simulator were to be developed under the umbrella of the Autoware Foundation, we\u2019d have the problem, since its finances exceed 100K.", "\n", "\n", "It seems that the license has certain limitations that make it not open source (e.g. it restricts use to ROS1 and ROS2 only (\"(1) build source code or use binaries with ROS1 and ROS2 systems\"), forbids reverse engineering, etc.).", "\n", "\nApex.AI: This is unclear and it is hard to define what a ROS1/ROS2 based system is. e.g. what about rosjava, which is effectively a port of ROS1?", "Do the assets you provide have a separate license? Is it Creative Commons or an open source license?", "\n", "\n", "This sentence in your license: \"Your goods and services (\u201cProduct\u201d) integrated with the Licensed Materials may be publicly demonstrated or exhibited for non-commercial purposes\": this means that an AWF member could NOT take e.g. ", ", integrate it with lgsvl simulator and show the resulting point cloud at CES and try to sell it?", "\n", "\n", "\"(3) application only to files created by LG, skipping default assets and code provided by Unity3D as project scaffolding\" => how can one in this case self integrate a map that e.g. Parkopedia did?", "\n", "\n", "\"application to 3D assets\" - what does this mean?", "\n", "\n", "\u201cThis Agreement governs the use of the confidential, non-public, pre-release LG Simulator Software (the \u201cSoftware\u201d)\u201d - I am not sure why words confidential and non-public are here? Isn\u2019t everything in ", " non-confidential and public?", "\n", "\n", "The license refers to a PRE-RELEASE (section 1.1).\n", "When will there be a final release?", "What will be the license of the final release?", "\n", "\n", "\n", "\"irrevocable license solely for the purposes of ", " using, testing, evaluation, simulation and validation of the Licensed Material (the \u201cPurpose\u201d)\" vs what is further below: Notwithstanding the above, Your goods and services (\u201cProduct\u201d) integrated with the Licensed Materials may be ", " demonstrated or exhibited for non-commercial purposes => these are 2 conflicting statements.", "\n", "\n", "LGSVL wants to offers AV simulator for Autonomous Development for free", "LGSVL intention is to help Autoware Foundation with development process while keeping an opportunity to offer additional premium features in the future (possibly running simulation in the cloud as a service). As a ATF member LGSVL commits resources to provide foundation the best possible simulation tools for free and make sure there are no impediments for using it. Licence limitation are only related to immediate commercial usage: aka selling modified version of simulator or related derivative work (data, models and etc.) See LGSVL Simulator License Terms.\n", "\n", " (57.9 KB)", "\n", "LGSVL will review current license agreement and use own legal adviser to make sure current license is FULLY reflecting intentions.", "See also proposed modifications and clarifications to the license agreement: ", "\n", "LG wants to support developers of autonomous driving systems, but also make sure they have a way to monetise in the future. LG feels that offering the simulator to developers now is important to enable future monetisation.", "LG had ideas for what they wanted to achieve, and they asked their legal team to produce a license agreement that enabled them.", "LG wants people to use the simulator freely except for changing it. They intend for things like maps and so on to be completely changeable without needing to change the simulator itself.", "LG does not want people selling data generated from the simulator, such as LIDAR scans and simulated camera images. However, this does not extend to works produced using the simulator, such as DNN trainings.", "LG believes that the Autoware Foundation is about making the self-driving stack, not the simulator, so they want to keep any potential commercial uses of the simulator for themselves.", "Apex.AI: Why not just use an existing open source license? Using a custom license means that the foundation has to spend money and time getting a lawyer to check the license is OK. We want to use the simulator and LG wants the foundation to use it, so using an existing license would make it easy for all concerned.\n", "LG: Because LG management believes there is a future for the simulator. If they choose the wrong open source license, the project might become unfundable. Most of the tools used and especially the game engine at the core of the simulator are expensive.", "LG is interested in hearing about existing licenses that may achieve what they want but also be easily accepted by users.", "\n", "Apex.AI: The license is not open source. It is shared source.", "LG: We want people to use the simulator, but we want the right to decide how people make money from it.", "The reason LG wants to distribute the code is because they believe that users being able to look at the code and understand how it works (and why it may not be working the way they expect) is important.", "Dejan: The AWF needs to be protected against violating license terms they don\u2019t understand.", "Shinpei: We need to understand what LG intended, so they need to provide that information and answer the above questions by text. We also need to understand if the license is usable by the AWF."], "url": "https://discourse.ros.org/t/autoware-tsc-meeting-minutes-for-march-20th-2019/8515"}
,{"title": "Lanelet2 ROS Message Design", "thread_contents": ["Currently, there is no ROS message defined for Lanelet2. The official Lanelet2 seems to take some time to define them. ", "However, it is urgent to replace current vector map format with Lanelet2 since the closed format is slowing down the development of other modules (e.g. perception and planning). Therefore, I suggest we define ROS message with Autoware community and use it for the meantime. Perhaps we can create PR to official repository if it works well within Autoware.", "For Lanelet2 ROS message definition, I have considered three approach so far.", "I prefer the third approach since I never used ", " to track down the issue related to vector map in my experience. I have used to check if the topic is published or not, but I never used it to actually check the content of topic data.", "Any suggestions or feedback would be appreciated.", "Thanks for opening the discussion Mitsudome-san.", "I agree on your comments, working with the binary topic has been the most easy so far.", "For 2) there is also the need to project back into long/lat to write the xml file, and then on the subscriber to project into the lanelet map structure again. Not such a big problem, but for MGRS projection there was some issue with the MGRS base code sharing for reverse projection? Anyway, it introduces a layer of processing not needed by binary format.", "For 3) is it an option to publish the full map, and nodes that desire subsets of the map (either geometric or primitive subsets) could use a ROS service to ask for a defined subset - the map loader could then construct a purpose built lanelet map of the subset, and send in single topic format?", "Similarly for 1) rather than define individual message types and filling out all information, subset maps (containing for instance just point primitive layer) could be generated and published as subset lanelet maps and recombined if necessary at the subscriber. This would simplify the message structure and take advantage of the existing API to construct messages.", "Thank you for the detailed explanation,", "\nI agree with you, option 3 is good.", "\nalso lets consider that in the future map information will be updated regularly, such as dynamic maps , so we semantic based ROS topic.", "\ncurrently in OpenPlanner I use option one, although we can solve the multiple topics problem by making one message holding the whole map data (or part of the map) .", "\nthe processing cost here is only rebuilding the network to achieved faster search.", "\nexample of the functions needed for planning:", "it will be good if we can build the road network and send the map as ros topic, one problem with that is data size. but if we consider a map server who send small part of the map frequently then this is the best solution in my opinion.", "Please consider in your options:", "Please also state your assumptions, requirements, and detailed use cases that will drive the decision on what sort of messages to create.", "Thank you for your comment.", "\nHere are some of the information that I can think of", "Assumptions:", "Requirements:", "Detailed Use Case:", "I overall agree, but need to add a couple of things", "Assumptions:", "Detailed Use Case:", "Some thoughts on Geoff\u2019s suggestions.", "Regarding Structured vs Unstructured messages: I initially started implementing structured messages with each lanelet2 primitive having its own message type. The only problem at the time was that it was convoluted accessing the underlying data from the lanelet2 API to build each message, but now we have more experience with the API it should be relatively easy. As far as usage goes however, I don\u2019t know if it makes sense to structure the data in messages.", "Regarding an ideal structure for map messages oriented towards use, I conceptually agree, but in practice, how we use data is largely defined by the internal format. The messages being passed are being consumed through the internal format. If it makes sense to build a ideal message format, then why not build an ideal map format?", "I disagree that much code duplication would occur even if every node that utilized map data in an unstructured Lanelet2 format has to do some sort of decoding - this is the purpose of shared libraries. A single encode and a single decode function should exist (or be created) in a shared library which can be linked against by any nodes needing to handle Lanelet2 format data. The decode function would return a class structure holding the in-memory representation of the map (or subset of the map) which can be manipulated as-needed by the consuming node and the encode function would simply return the encoded, unstructured data which can be published. Minimal code duplication.", "On a related note, I agree with ", " - a structured representation of the data being passed around doesn\u2019t serve much practical purpose with the design of Lanelet2 having such a strong decoupling of the primitives from what a grouping of primitives represent. The overhead inherent in encoding/decoding them as structured messages and the additional bandwidth required for the metadata are also non-trivial. Given that network I/O is second only to disk I/O in order of the most expensive operations you can perform on a modern computer, I lean heavily toward unstructured transport. I also think the actual implementation of how we access the map or a subset of the map (making requests to a server, segmenting the map, etc.) are somewhat irrelevant to representation of the data at the transport layer. Whether we split this up into multiple nodes or have a single server handling the map, they are still passing around the same basic datum and, in the case of Lanelet2, those datum are not easily reconstructed into the objects they represent without the structure of the Lanelet2 class objects.", " Hi, I guess the idea of code duplication concern arose because of the suggestion that some nodes might not want to link to the lanelets shared library, and instead work directly on the message data. If a shared library is to be linked anyway, we may as well use lanelets, or might there be a need for a lighter encoder/decoder lib with less dependencies?", "Assumptions:", "If you do not know how the data will be used, you should not make this assumption.", "Message data must contain enough information to reconstruct LaneletMap data structure", "This places no requirement on the message data itself being in the Lanelets structure.", "Message should be flexible enough to be used in different use cases. e.g. message should be able to express custom tags without changing message definition", "Message should be scalable. e.g. It should be able to represent subset of a map.", "Detailed Use Case:", "These are not detailed. I recommend you follow a use case template. It will help you fill out the necessary detail. A good example is the ", ". Martin Fowler has also produced one, and I\u2019m sure there are thousands of other templates.", "Keep in mind that this is just the start. From your use cases, you will need to derive requirements.", "Thank you for your reply and sorry for my late response.", "You are right that we should not design message without defining use cases first. I have started writin use cases of map information with Cockburn\u2019s template(with some modifications).", "\n", "TEMPLATE Use case name: <the name should be the goal as a short active verb phrase> Context of use: <a longer statement of the goal, if needed, its normal occurence conditions> Level: <one of: summary, user-goal, subfunction> Primary Actor: <a role...", "\n", "\nI wrote them based on functions of current Autoware and functions that are likely implemented in the future, although it is not finished yet. We are planning to discuss about use cases and requirements in detail in the ", " so hopefully we can provide more sophisticated use cases in next few weeks. Then, we can continue our discussion on requirements and designs.", "Having said that, I would like to start Lanelet2 integration with unstructured(binary) message approach for v1.13, as it seems to be most supported message type so far. I believe fundamental purpose of replacing Aisan vector map implementation is to make Autoware\u2019s HD Map format public (both physical storage format and internal data), and having best message design is not crucial to the purpose as long as it transfers sufficient information. Current closed format is likely to discourage developers to create new features or even enter into discussions like this, and dropping Lanelet2 features from v1.13 could be a huge loss to the community. Therefore, I suggest to start implementation with binary message in order to hedge the risk. I am aware that the ", " is longer than last release, but I would like to submit merge requests in advance because there are many nodes that uses vector_map_info/* topics that needs to be replaced.", "I understand the importance of following engineering process, and we should continue designing best map message for Autoware in this thread as well as in Map WG. We will do our best to keep implementation of Lanelet2 nodes for v.1.13 to be independent from message structure so that we can replace it easily once we have the new message design.", "\nHow does this sound to you?", "Hi ", ",", "so, if we go for option (2) or option (3) we have something like that right ?", "So, because we are using the same library we don\u2019t need a big structure of ROS msg (it would duplicate the work), and just use lanelet to manage errors in read/write a msg.", ". should we have a way to send a ", " to the server in case the map delivered (XML or BIN) is good or bad ?", "Also \u2026 should the msg be a ", " or ", " ? in our case, for parking garage maps, request response is good enough (you ask for the map of a given area when you are getting closer to it). Is it similar to what you are doing ?", "So, because we are using the same library we don\u2019t need a big structure of ROS msg (it would duplicate the work), and just use lanelet to manage errors in read/write a msg.", "That was my idea.", " . should we have a way to send a ", " to the server in case the map delivered (XML or BIN) is good or bad ?", "I am not sure if we should have this feedback. If we are going to have one, then the server should do something when they get the error message, but I don\u2019t know if there is anything server can do about it. Maybe sending the topic again if it is pub/sub? Also, what do you mean by good or bad? Are you talking about corruption of data?", "Also \u2026 should the msg be a ", " or ", " ? in our case, for parking garage maps, request response is good enough (you ask for the map of a given area when you are getting closer to it). Is it similar to what you are doing?", "Our work on replacing current Aisan vector map implementation is done by pub/sub. This is just because current implementation is done that way. We should probably think about it again once we have all requirements listed up.", "I understand the importance of following engineering process, and we should continue designing best map message for Autoware in this thread as well as in Map WG. We will do our best to keep implementation of Lanelet2 nodes for v.1.13 to be independent from message structure so that we can replace it easily once we have the new message design.", "\nHow does this sound to you?", "If you want to do it that way for Autoware.AI, then I don\u2019t really mind. But remember two important things:", "ok, thanks for the explanation. It\u2019s clear.", "For the ", ", yes I mean the client could notify the server if it gets corrupted data (basically a map it cannot read).", "In that case, the server could", "\n(1) loop and resent the map until the client has a good map, or until it made N-failed-attempts,", "\n(2) notify another component, say ErrorHandler, or a human to log the error so that developers can investigate why in that area / to that client the map breaks.", "Maybe it\u2019s not necessary in this phase, but when the product is fully developed and used, it\u2019s a good tool for crisis management.", "Hello, excuse me for cutting in. Please let me write my idea.", "As for data corruption and feedback, what is the reason of data corruption?", "In my understanding, generally, if the sender sent correct data, the receiver will:  1. receive correct data,  or 2. not receive any data.", "\nIt won\u2019t receive corrupted data because it is detected in the transport layer.", "Or, if the map file itself is corrupted, it should be detected and fixed in the verification step, not in the runtime, and the sender should not send the data.", "So my opinion is,", "As for pub/sub or server, it depends on the target, Autoware.ai or Autoware.Auto.", "For Autoware.ai, since the map node sends whole map data, I think using Service is more natural than pub/sub.", "\nHowever, since the current implementation uses pub/sub, to keep using pub/sub might be better, considering the re-implementation cost.", "For Autoware.Auto, it should be drastically redesigned for more safety, including Lanelet2 library.", "\nI think sending structured data using pub/sub is best for now, but as we discussed at the last meeting, let\u2019s consider this using the AVP use case.", "By the way, I feel we need to have the same understanding of this WG\u2019s goals.", "\nSince map-related things have many aspects and the best solution depends on the target, we have to clarify what target we refer to.", "Therefore, please let me summarize the goal(for .AI/.Auto), current status, and milestones of this WG.", "\nI will write them maybe tomorrow in this thread: ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\nThis is similar approach with current vector_map_msgs. Map data is divided into multiple topics based on object types(point, line, lane, signal, traffic sign, etc.), and other nodes only subscribe to interested object. This keeps away from different nodes having all copy of map data which increases memory efficiency.", "\nIf we are going to use similar design for Lanelet2, we probably would divide topics into premitives: point, linestring, lanelet, and regulatory elements. (See here for lanelet2 primitives ", ") One of the concerns is that if we are going to use Lanelet2 library, then a node would need to subscribe all the topics anyway and have no merits of having separate topics.", "\n", "Merits:\n", "Memory efficient when there are many nodes subscribing to map data (but not likely with Lanelet2)", "Easy to look for specific object from ", "\n", "\n", "Demerits:\n", "Needs to synchronize the topics if there are any updates at run time", "Needs to subscribe many topics to obtain map data as a whole", "\n", "\n", "\nMap will be stored as xml data as string member of the message. Lanelet2 has functions to write maps into OSM(XML) format file, and we can write into ROS string message instead of writing into filestream. This way, we can convert the message back to LaneletMap(C++ class used in Lanelet2 library) just like loading map from a file. Developers can also check data with  ", "  and read the message data in XML format. I do have concern that output  ", "  do not process newline, and it just outputs the code itself(\u2019\\n\u2019). Therefore, although output of the command is in XML format, it is very hard to comprehend.  ", "\n", "merit:\n", "no need for synchronization (single topic)", "can reuse code from Lanelet2 writer/parser", "message data will be exactly same as loaded osm file", "\n", "demerit:\n", "data size would be relatively large since they are in text format", "\n", "\n", "\nThis approach would be same as the second approach, except that the data will be stored as binary instead of XML. Lanelet2 also has function to write maps in binary format(BinWriter class), so we can also reuse the code for this approach as well. Since the data will be binary, data size should be smaller, but output from ", " would be totally incomprehensible. However, I think this wouldn\u2019t be a problem if we have good visualization in RVIZ. (In fact, pointcloud data is using binary format for ROS Message and have been working so far).", "merit:\n", "no need for synchronization (single topic)", "can reuse code from Lanelet2 writer/parser", "data size will be smaller than XML data", "\n", "demerit:\n", "\n", " output would be incomprehensible", "\n", "Find road boundary around the vehicle.", "Find next traffic light position.", "Find next sign , stop line , \u2026", "Extract current lane information (center line waypoints,  direction, overtaking, left and right lanes, \u2026)", "Use of a map server with services to provide a subset of the map based on certain criteria (map in a specific area, lanes along a route, etc.), as well as a service that can provide the whole map.", "Whether you can send structured map data rather than wrapping an XML or binary chunk in a ROS message, because this would give more flexibility for other nodes to use the data even without the lanelets2 library, and would ease debugging for those who wish to debug at the topic level. This may require modifying the lanelets2 library but you should give this consideration.", "Whether using structured map data in ROS messages with that structure oriented towards use rather than source format would give more flexibility to support different map formats in the future. In other words, make your ideal map message(s) rather than lanelet2 messages.", "All nodes that access to map data should use Lanelet2 library. Although you mentioned about flexibility of using data without lanelet2 library, I think we should access to the map through the library if you don\u2019t have any critical reasons. This would keep us from having similar codes everywhere in Autoware.", "Message data must contain enough information to reconstruct LaneletMap data structure", "Message should be flexible enough to be used in different use cases. e.g. message should be able to express custom tags without changing message definition", "Message should be scalable. e.g. It should be able to represent subset of a map.", "retrieving traffic light related to current lane", "calculating route to goal from current position", "retrieving crosswalk area", "retrieving intersection information (yield, right of way)", "finding stop lines", "retrieve geometrical subset of map", "if we need to access the map via lanelet2 library, we need to keep the file into ", " instead of plain vanilla OSM", "retrive the nearest parking space available from the map, or a section of it", "The structure does not delineate any useful subsets of the data. The hierarchical nature of the format means that it hard to imagine requesting subsets of the map information that does not contain most information (i.e most requests will be at lanelet level, which requires points, linestrings and related reg.elems anyway). A lanelet message by itself does not have any spatial information, just attribute data. Conversely just sending points has little meaning either. I would argue using structured messages to send subsets of map (based on structure of format as opposed to geometry deffined subsets) would not be particularly useful. In a similar manner, nodes using the map data without lanelet2 library would have to reconstruct the lanelet data for it to be useful, leading to replication of code as Mitsudome-san mentioned. Perhaps excluding the lanelet2 library would be desirable for very lightweight nodes?", "The other consideration is that structured messages could be useful for topic level debugging. My initial reaction is that map data is only coherent viewed at higher levels, and that viewing the contents of individual primitives may not be so helpful for debugging. I it might be useful to verify that well formed data is being sent, or that a particular type of lane is being sent. Perhaps visualization tools would be more helpful in this regard? However, I certainly can appreciate that topic based debugging could be useful, and am open to including structured messages if others think in necessary", "We could support both? For certain cases it might make sense to debug using the structured messages, or that structured messages might encapsulation useful subsets of map data. In other cases binary data message is fast and easy.", "All nodes that access to map data should use Lanelet2 library. Although you mentioned about flexibility of using data without lanelet2 library, I think we should access to the map through the library if you don\u2019t have any critical reasons. This would keep us from having similar codes everywhere in Autoware.", "Define those use cases.", "Be very careful about allowing custom data in messages. You need to have a strategy in place for ensuring correct behaviour in the face of incorrect data.", "Although I agree with this, you need to define why it is necessary so you know exactly what a \u201csubset of a map\u201d means.", "retrieving traffic light related to current lane", "calculating route to goal from current position", "retrieving crosswalk area", "retrieving intersection information (yield, right of way)", "finding stop lines", "retrieve geometrical subset of map", "The Maps Working Group is for the Autoware Specification and its reference implementation, Autoware.Auto. You cannot ask the Maps WG to make decisions based on a need from Autoware.AI.", "If you make a hasty design decision now to get something out soon, you are encruing technical debt. This is an accepted software engineering practice, but in this case you will be paying back that debt by re-implementing all your Lanelet2-related stuff again later on.", "A simple resending feature is required because sometimes sending data might fail, but probably it can be done by ROS/ROS2 features.", "No excessive feature is not necessary."], "url": "https://discourse.ros.org/t/lanelet2-ros-message-design/9932"}
,{"title": "IPC in ros2", "thread_contents": ["Does ros2 use inter-process communication when we use it in  linux OS ? What about in case of bare metal micro-controllers where there is no OS?", "In the future, please set the category to \u201cNext Generation ROS\u201d when discussing ROS 2.", "Does ros2 use inter-process communication when we use it in  linux OS ?", "First, I\u2019ll assume you mean \u201cdoes ROS 2 use any operating system IPC mechanisms between nodes on the same machine, rather than sending data over UDP?\u201d, because the term \u201cinter-process communication\u201d only means communication between two processes, but does not imply what the mechanism is (i.e. UDP or TCP can be IPC).", "It depends on the middleware implementation you are using. Currently the default of Fast-RTPS does not use anything other than UDP single cast or multicast. It could, RTPS allows for this in the protocol. I believe some of the proprietary implementations provide a shared-memory based IPC mechanism when on the same machine. Fast-RTPS will hopefully support this in the future.", "What about in case of bare metal micro-controllers where there is no OS?", "There have been some experiments which use RTPS on a OS-less microcontroller to communicate with ROS 2 on a desktop and some other experiments to get our ROS 2 API compiling for microcontrollers, but there is no out of the box support for OS-less ROS 2 at this time. We did enough work to convince ourselves that is possible, but haven\u2019t followed through with sustained support because we didn\u2019t have time, though we would love to do that in the future.", "In that case though, with no OS, you probably would not be using processes so the idea of \u201cInter-", " communication\u201d doesn\u2019t really make sense. You would be using something more like our \u201c", "-process communication\u201d, which can avoid sending the message data over the network (UDP in most cases).", "RTI\u2019s DDS does use shared memory internally when possible, I think.", "Hi sagniknitr,", "Fast RTPS will support shared memory in future releases, is in our roadmap. Regarding microcontrollers we are developing a lightweight version of Fast RTPS based on an OMG future standard for constrained resources devices. Of course it is not a full DDS/RTPS implementation, but a reduced API and different protocol.", "You can see the first prototypes in the ", " use case, and we are working in an release now. This will let you to use ROS2 in micro-controlllers.", "If you want more details, please contact me.", "Jamie,", "Thanks for the update about how Fast RTPS will be evolving.  A few questions, is the prototype with Dronecode you\u2019re referring to the work you did on the ", "?  Additionally, can you point to any public links regarding this new OMG standard for constrained resources? I know that the ", " is publicly available, is a draft of the document you\u2019re referring to available?", "Thanks", "\n- Eddy", "The DDS XRCE (\u201ceXtremely Resource Constrained Environments\u201d because all OMG specifications need a cool acronym) specification isn\u2019t even close to finished yet. The ", " but you have to be an OMG member to get the working documents, unless someone involved is willing to provide them. When the specification is adopted, that becomes publicly available.", "As seems to happen a lot with the DDS specs these days, the RTI/eProsima/Twin Oaks camp and the PrismTech camp have produced their own versions of the XRCE specification and are struggling to reconcile them into a single specification that the OMG can adopt. I won\u2019t wade into the debate over which is better, but I\u2019ll let you draw your own conclusions from the fact that one of the two camps only has one company it it. ", " I hope it\u2019s not going to be the RPC for DDS spec all over again; the arguments over that could be heard in other meeting rooms. But it seems likely that it will be a year and a half or more before the final specification is released.", "Thanks for the informative reply ", ", I had no idea the work was under way.", "I attended the OMG meeting last week, so I took the opportunity to hear what PrismTech and RTI/TwinOaks/eProsima had to say about their competing submissions. The following are the notes I took during their presentations. Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "Prismtech presentation:", "RTI/eProsima/TwinOaks presentation:", "My summary:", "Thanks for the summary ", "!  I\u2019m very interested in how the work progresses", "Thanks Geoff, great stuff!", "Hello ", ",", "First of all I\u2019d like to point out that I am one of the author of PrismTech\u2019s/ADLINK XRCE proposal, thus you may think I am giving you my perspective, in that case I urge you to read original documents to see how what I am providing here are facts. That said, let me make a few rectifications to some of the points above.", "You say:", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure ", " You can find the result of our analysts here ", " but you are welcome to derive them by reading both specs.", "This is also partially correct. We had asked to present what would have been our final proposal in Bruxelles to give a chance to the task-force to digest and one more opportunity to the competing team to join. This has nothing to do with the Vote-to-Vote and the fact that the task-force did not decide to allow the Vote-to-Vote procedure. The OMG has very complex rules\u2026 I know that can seam strange, and they still surprise me after having spent more than 10 years dealing with it.", "The RFP (which I wrote) asks for a protocol not for an API.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "\nAdditionally, you may argue that this may add some complexity in the message parsing \u2013 but again \u2013 I think that checking a flag and deciding wether some field is going to be present or not does not belong to the realm of hard. It is also worth  pointing out that some flags are just informative and don\u2019t require any kind of branching in the decoding. Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes ", " And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple ", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples. Our proposal currently has 4 compared to that of RTI/TwinOaks/eProsima which has 16 bytes. We have had our implementation run on a Makeblock robot using BLE with an MTU of 20 bytes.  You can check the comparison on this deck ", " and should wonder why the competing proposal did not provide a proper analysis of the wire overhead. Additionally when leveraging batching we have a wire overhead of (3+n)/n where n is the number of samples being batched.", "Some other thing worth point out is that the protocol allows for data to be pushed, pulled or periodically pushed or pulled. The write protocol also allow to pace the streaming of data that results from a remote query. This is extremely important when dealing with resource constrained nodes that need to consume data little by little.", "I did not hear RTI saying:", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those. The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP. The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "I\u2019d like to understand why you say this:", "What do you think is our submission cutting out? We are wire efficient yes, extremely wire efficient but at the same tine we support:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions. I\u2019ll also be more than happy to answer any question you may have about our protocol.", "A+", "Dear ", ",", "You are being to nice to PrismTech as everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises. I think it is best for people to look with their eyes at what the two submission can do and make their own decisions.", "On my side I like debates, I like inquisitive minds and I like hard questions. Thus I\u2019ll be more than happy to answer any question on the XRCE protocol proposed by the PrismTech/ADLINK team explain why it is better than RTI proposal\u2026 And actually it is better than RTPS.", "Thus, please let\u2019s start the open debate to dissect the reasons why our proposal is the one which should be voted and by far the better one.", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being ", "A+", "Thanks ", " for jumping into the community in such an energetic manner. I think we all appreciate having one of the authors of the proposals providing feedback. That said:", "Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "which answers:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I think I\u2019ll stop here\u2026 for the time being ", " .", "Hi Angelo (", "),", "Coordinate different companies to get a common view on a complex matter is always hard, and in this case, there are multiple design options leading to different tradeoffs.", "Our submission (RTI, Twin Oaks & eProsima) already aligns the views of three different DDS vendors, and sure we will try to incorporate ideas of your submission.", "Our submission tries to accomplish the following:", "We coded a PoC of our submission, and during the presentation, you asked about numbers. At that point, with no optimizations at all, and in debug mode we answered 43Kb. I asked my team to optimize a little bit the code, and here are the numbers we have now for the client:", "Total Memory Use: 8 Kb", "But we could squeeze that even a little more. We are releasing this as Open Source (Apache 2) so anyone can review the results.", "But again, we are not aiming to be as small as possible. We are covering all the requisites of the DDS-XRCE RFP, and testing our solution in what we consider typical microcontrollers today.", "Regarding the process at the OMG meeting, we (RTI, Twin Oaks & eProsima) didn\u2019t want to confront both specifications and choose one of them, but have the time to incorporate ideas from your submission, and that is why now we have an extended deadline.", "Let me join in the fray. I\u2019m the other author of the PrismTech submission and the one who built our tiny prototype. I wasn\u2019t present at the OMG meetings, and I won\u2019t waste any words on what may or may not have happened there.", "Firstly, I don\u2019t think a contest of bytes of RAM adds real value to the discussion, although of course it is an honourable contest in itself ", " I am surprised that you, ", ", had never even had a proper look at the memory use of your implementation given the purpose of the exercise in the first place, but if it is 8kB now then it is much better already \u2014 if still 7kB overweight ", " In any case, memory use is determined more by the implementation than by the protocol messages.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already. Yet even that is not of such great interest to me in this discussion.", "What really matters in my opinion is a difference in philosophy. The two proposals suggest very different views of what one would ideally want to accomplish.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that. In a sense, it is just a hand-crafted alternative to CORBA with a lower overhead. (Simply using CORBA actually \u201cjust works\u201d if the DDS implementation is faithful to the IDL interface mappings, even if it is ugly.) To me, this route is a pragmatic way of going about satisfying the RFP, but at the same time, an uninteresting one. (Sorry ", " and others \u2026)", "We chose to design a compact protocol that can support what amounts to performing DDS operations remotely, but doesn\u2019t limit itself to it. Instead, it also supports a DDS-like peer-to-peer network with vastly lower overhead, and, in many ways a level of flexibility in specifying what data is of interest (through URIs and selections) that DDS doesn\u2019t natively support.", "All of that would be of little value if it doesn\u2019t perform well or doesn\u2019t scale well. Just like the code size and memory use are mostly determined by the implementation, so is maximum sustainable performance more determined by implementation than by the details of the protocol headers. Size-wise, my prototype can run as a client on an Arduino Uno (8-bit CPU, 2kB RAM). A small test application using the same implementation configured as a peer easily sends ~700k 8-byte msgs/s from one RPi3 to 3 others (CPU is << 100%, network load ~75% of Fast Ethernet, so I really should investigate why it isn\u2019t faster), and goes another order of magnitude faster when run over local loopback on my MBP. That\u2019s better than your typically DDSI implementation. Is this relevant? That depends on whether you have high rate, tiny messages \u2026", "Now my test application doesn\u2019t implement all of DDS \u2014 not even close \u2014 and this is another significant reason why it can do this with only a few kB of code and RAM. At the same time, this is, I believe, where it gets interesting for ROS2.", "As ROS2 has its own middleware abstraction layer that uses only a fraction of the DDS feature set, putting ROS2 directly on our protocol would get you a smaller and a faster system. Smaller and faster usually allows doing more interesting things, even if I can\u2019t say what exactly those interesting things will be.", "Disclaimer: I can\u2019t do run ROS2 over it today, there\u2019s more work to be done on my prototype before it supports all that is required. And I wish none of you would have to take my word for the data I mentioned, but that is not something that is in my power to solve today.", "It looks like I started something of a minor storm moments before starting a holiday\u2026", "I\u2019m thankful that the DDS vendors, PrismTech, RTI, Twin Oaks and eProsima, are all engaged enough in the ROS community to be present on the Discourse board. It is encouraging to future adopters of ROS 2.", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure", "I wasn\u2019t implying religious believe. It\u2019s an simply expression to describe someone making an assertion. ", " I fully agree that PrismTech\u2019s submission has much smaller messages than the RTI/Twin Oaks/eProsima submission based on the two presentations alone.", "The RFP (which I wrote) asks for a protocol not for an API.", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "It is quite common to use flags. TCP is full of them. My concern is that the protocol is, in my opinion, undoubtedly complex and, based solely on the presentations, more complex than the other submission. Complexity and size are often a balance and in this situation we appear to have one submission at each end of the balance.", "Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes", "With the sorts of overhead you are achieving, I\u2019m not surprised performance is amazing. I\u2019d still like to see numbers, though. ", "And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple", "As was stated elsewhere in this thread, eProsima\u2019s implementation wasn\u2019t optimised. Since you said you are trying to bring yours to market already and eProsima claimed theirs was a tech demo, I\u2019m not surprised yours is more optimised and thus smaller. Of course, the numbers are definitely in your favour for RAM usage. But I\u2019m curious how much program memory each implementation requires, too. This is often the limiting factor on embedded microprocessors rather than the RAM usage.", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP.", "In that case I am very interested in seeing what these AB members wrote.", "The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "What do you think is our submission cutting out?", "\u201cCutting down\u201d, not \u201ccutting out\u201d. I meant that you are trying to reduce the size of the messages on the wire as much as possible, at the expense of possibly needing more complex parsing code. I didn\u2019t mean to say that you are cutting out features. It was clear from the presentation that PrismTech supports more features and has more flexibility than the other submission. But there are trade-offs involved.", "Dynamic Discovery (RTI does not, please read their spec!)", "\nPeer to Peer Communication (RTI does not)", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Non IP Transports (RTI does not)", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "Generalised Queries (RTI only supports DDS-like queries)", "Well, it is ", "-XRCE, is it not?", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions.", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises", "While this is true, we are operating at a standardisation organisation, not a rubber stamp provider. There are interested parties beyond just the implementers. We would prefer not to just hold a vote on which submission to go forward with. We would prefer the submitters to actually work together and produce a single submission that combines the best of both without any technological compromises (yes, I\u2019m aware how hard that is).", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being", "Or, you could provide the answers to those questions, along with the equivalent answers for your submission, so we can see and compare the data to back up your claims.", "Our submission tries to accomplish the following:", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "\nNeither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use.", "This is the strongest impression I got from the presentation, as I said in my own notes. The data model is similar to DDS, which makes adoption by existing DDS users easier, and the ability to implement the protocol in a relatively simple piece of code (which makes it easier to verify and certify) was considered as important as saving bytes on the wire. I\u2019m not sure where the correct balance is between these two requirements, but the PrismTech implementation really gave the impression of being at one extreme.", "testing our solution in what we consider typical microcontrollers today", "This is something that I think is really relevant but that PrismTech have not addressed at all, and RTI/Twin Oaks/eProsima have not addressed enough. What are the typical microcontrollers in use today? What are the target environments for this protocol to be used in?", "The RFP explicitly says this:", "From a high level perspective, the key requirements that a DDS-XRCE implementation has to satisfy are (1) extremely low footprint \u2013 addressing targets with less than 100 Kbytes of RAM, (2) extremely efficient wire protocol \u2013 inducing a protocol overhead of just a few tens of byte over the user data, and (3) support devices that undergo aggressive sleep cycles.", "Both submissions fit within both the RAM usage (with much room to spare) and the protocol overhead.", "More specifically, the actual mandatory requirement is:", "6.5.3. DDS-XRCE protocol overhead, when sending or receiving user data, shall not exceed 24 bytes per packet.", "Let M be the size of the DDS-XRCE message sent out, and U the size of the user data, then M-U <= 24 bytes.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already.", "Again, this should have been in the RFP if it is so important. That would have saved a lot of trouble. All we got was an evaluation criteria:", "Protocol overhead shall be considered when evaluating submissions. Submissions with smaller overhead are preferable.", "This is a miserably small set of criteria for a complex design space. Even you, ", ", say that protocol overhead is not as important as the design philosophy.", "Which I agree is fundamentally different between the two proposals, and that this is where the root cause lies in the failure to reconcile them.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that.", "The RFP not-so-subtly pushes submitters in this direction. You cannot fault them for taking it at face value.", "I will read both submissions and when I do, I will report back with more technically-informed comments.", "Hello ", ",", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "And you see this as a positive aspect? Our model is simpler and more user friendly. For instance, how many people can digest DDS partitions? That said, we have a well defined mapping between XRCE resources and DDS topics.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "Are you an OMG member? If so I\u2019ll forward you the reviews. Both submissions went to the AB and the reviews were posted both on ", " and ", ". If you have access to those mailing list you\u2019ll be able to see them. I also recommend you take a look at those.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "It was impossible as the other vendors did not want to agree on such a low bound. The 24 bytes was the least we could agree on. This is why there is an evaluation on wire-efficiency. This matters were discussed at length, but again I don\u2019t think you attended those meetings, thus you are missing part of the history and the context. In any case, all of those documents are on the OMG archives, thus if of interest you to reconstruct it. Just search for presentation I did on XRCE for almost a year. starting from 2015!", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "Yes, that is correct as it is since the very beginning that we are trying to do a joint submission. They\u2019ve refused with futile arguments \u2013 if you ask me. We have put lots of effort to trying to join but that has not been corresponded. A pity that you were not in the Bruxelles meeting, otherwise you would have had a taste of it.", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Again, you did not attend the end-less arguments we had during the RFP drafting. RTI does not want peer-to-peer in XRCE because they fear it could become as substitute for DDSI-RTPS. Again, this is not something I am inferring, but something that was openly debated during the RFP drafting. We don\u2019t have any issue with that as we think that having a more efficient protocol than DDSI-RTPS for some use cases would be extremely useful.", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "For me that disqualifies completely the submission as in LowPAN environments nobody can afford to use TCP/IP\u2026", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "I am glad that this helped clarifying the situation, please don\u2019t hesitate to ask any other question. Concerning the code availability we are working on that. I\u2019ll keep you posted.", "A+", "Dear All,", "I wanted to let you know that we have just released under Apache 2 a peer-to-peer implementation of our zenoh protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating zenoh as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", "N.B. For those of you that are familiar with XRCE, zenoh is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as zenoh.", "A+", "Kydos,", "Thank you for publishing the Zeno-He library so we can all begin to interact with it.  It is especially useful for the ROS 2 user community to be aware of the effort since it implements the ATLab XRCE proposal.", "I know I would be extremely interested in someone benchmarking Zeno and the proposed epromisa XRCE implementation, and perhaps can find time to do that.", "This is really appreciated ", ", thanks for making this available.", "At present it\u2019s unlicensed code though (", ").", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They believe that their proposal is more efficient in the wire protocol (trying to save every single byte possible), and supports brokered as well as peer-to-peer communication.", "They have invited the competing submitters to join the PrismTech submission.", "PrismTech believes that they have presented their final submission in Brussels (June) and so were expecting to vote for acceptance in New Orleans (September), but because there are still two competing submissions, and no final submission document was received (only a presentation), this is not possible.", "PrismTech\u2019s reasons for going forward with their own submission:\n", "XRCE tries to target the most wire/power/memory efficient protocol, targeting not just IP infrastructures but a whole range of infrastructures.", "Their submission provides reliability and fragmentation.", "Their current prototype runs on an 8-bit microprocessor with 1 KB of RAM and has a wire overhead of 4 bytes for data samples.", "They got a review from the AB which was favourable (only editorial comments), and they believe that the AB review shows that they satisfy all the mandatory requirements.", "\n", "XRCE applications can be brokered into a DDS data space, or they can discover one another and communicate P2P.", "Their submission is only about the protocol, it does not say anything about the API.", "A DDS-XRCE Agent running on permanently connected hardware provides the access to the rest of the (non-XRCE) DDS domain.", "XRCE provides a data space abstraction in which applictions can read and write data autonomously and asynchronously.", "Data read and written by XRCE applications is associated with one or more resources identified by a URI.\n", "An XRCE resource is a closed description for a set of named values.", "Resource URIs allow for wildcards, which means that more than one resource can be targeted in one definition, which is useful for capturing collections of sub-namespaces.", "A resource with a cardinality of one is called a Trivial Resource.", "Resources also have properties, which are used to attach QoS settings to them, such as \u201cthis resource is transient\u201d or \u201cthis resource is reliable\u201d.", "An XRCE selection is the conjunction of a resource and a predicate over the resource content and properties, used to filter a selection. For example, finding all lights with a luminosity greater than zero. (It seems to provide functionality similar to a very basic subset of SQL.)", "\n", "There is a mapping from XRCE resources to DDS topics.", "Resource serialisation uses the same format as DDS.", "All XRCE messages are a single-byte header and a body.\n", "There are flags in the header for reliability and synchronicity.", "Messages can be \u201cdecorated\u201d by prefixing them with a one-byte pre-header, which determines some of the properties for the following header byte, such as fragmentation.", "The protocol is little-endian.", "Variable-length encoding is used to save space.", "\n", "For addressing, the source address for each message is assumed to be a unique address of the sender.", "The protocol is modular, with a core profile (required for any communication), an optional query profile, and an optional discovery profile.\n", "e.g. If using a communications system like Bluetooth, which already has discovery, the XRCE discovery profile can be removed, saving some space.", "Discovery happens by scouting (sending a scout message to ask for types of nodes to reply, i.e. broker nodes or durability services or peers or clients).", "Other nodes reply to a scout with a HELLO message.", "\n", "After discovery, two nodes need to open a session, which enables publishing and subscribing between those two nodes.\n", "Authentication data is included in the session open message.", "Locator information, telling the other node how it can be reached, is included in the open message.", "The receiving node replies with an accept message or a reject message.", "There is an FSM describing the states a session goes through.", "\n", "Resources and selections are uniquely identified by numerical IDs to save space on the wire.", "A declare message is sent to declare what resources, selections, etc. a node has or will publish or subscribe to.", "Subscriptions can be push, pull, periodic pull or periodic push.", "All data messages and declarations are transported over a conduit (for the session), which is a pair of a reliable and a best-effort channel. Multiple conduits may be used in parallel to avoid head-of-line blocking and allow multicasting.", "One decorator is used to select the conduit for the next message.\n", "It is possible to make this decorator one byte instead of two if the number of conduits is less than five.", "\n", "There is a sync message available to set the next sequence number to expect (e.g. for when not starting at zero).", "The ACKNACK message can acknowledge multiple messages at once, usually up to the given sequence number. It has the option to optionally request retransmission of one or more messages after that point.", "There is a one-shot write function in the protocol, which can do a write of a resource without needing to do any prior registration or discovery.", "This proposal appears to be a very complex protocol with a lot of options in message header structures. An implementation would have a lot of choice points during the decoding of a stream of messages. The \u201cdecorator\u201d idea especially may save bytes on the wire (and in message construction buffers) in some cases, but it complicates the protocol implementation.", "PrismTech are already trying to bring their submission to market as a product.", "This proposal also uses an XRCE agent to provide access from DDS-XRCE nodes and the DDS domain.", "An important use case for them is that devices will tend to mostly sleep, and only wake up occassionally to do some processing, and send and receive data. This means that two devices may never be awake at the same time. This is why they have the agent, which is permanently present and provides a way for XRCE nodes to communicate with each other.", "They say that their proposal focuses not just on the XRCE protocol, but also on the interaction between the XRCE protocol and the DDS domain.", "It is possible for an XRCE agent to behave as an XRCE client to another agent, allowing for a hierarchical structure of agents and clients.", "Their proposal is based on the web-enabled DDS specification, with a DDS-XRCE object model in the agent that has a one-to-one mapping to the DDS data model.", "eProsima have put up a demo on Youtube: ", "\n", "Their demo uses 43 KB of RAM (much bigger than the PrismTech proposal, which can fit in 1 KB).", "The XRCE Agent object model is very similar to the DDS object model, making the mapping very simple.", "XRCE objects are modeled as resources that are addressable by their name and have CRUD operations.\n", "Each resource has a name to address it within the agent and a context; a representation describing the resource; and an ID.", "Resources can be represented as a name, an XML description, or a binary representation.", "\n", "Authentication capability is built into the proposal.", "Types are typically pre-defined profiles in the XRCE Agent.\n", "It is possible to transmit types as binary or XML representations.", "\n", "Message structure:\n", "A message header is either 4 or 8 bytes, depending on if a clientKey is used.", "There is a sub-message header, which is another 8 bytes.", "It is possible to send data in a sequence, meaning the header only needs to be sent once for a bunch of samples.", "\n", "The transport can be message-oriented or packet-oriented.", "CDR and DDS-XML representations are reused.", "Message overhead is typically 12 bytes.\n", "They consider that further reduction in overhead would increase complexity and reduce robustness. e.g. They do not need different code paths for multiple sessions, re-connections, handling variable-length encoding.", "They say it compares favourably to TCP/IP overhead (40 bytes).", "They think they could save 6 bytes from their message header, but the reduction is only 6% in the context of total overhead (when considering the use case of using TCP/IP as the underlying transport protocol) and so is not worth it in the face of increased complexity.", "\n", "They also received 3 AB reviews, which they claim were supportive and did not find any non-editorial problems.", "The submissions are very different. PrismTech is aiming for cutting down the bytes used by the protocol to the absolute minimum at the expense of all else. RTI/eProsima/TwinOaks are favouring some overhead in order to achieve a simpler and more robust protocol and implementation.", "PrismTech\u2019s protocol is overly complex with too many choices during the decoding of a message.", "The extra overhead of the RTI/eProsima/TwinOaks submission is not likely to be a problem in the majority of embedded micro-processors used these days (although I\u2019m not sure how many 8-byte, 1KB-of-RAM micros there are in use in new products). However, their consideration that TCP/IP is going to be the most common transport may not be accurate.", "Dynamic Discovery (RTI does not, please read their spec!)", "Peer to Peer Communication (RTI does not)", "Client to Broker Communication", "Non IP Transports (RTI does not)", "Generalised Queries (RTI only supports DDS-like queries)", "Push/Pull/Periodic-Pull and Periodic-Pull Readers", "Unicast and Multicast communication \u2013 for both client to broker and peer-to-peer", "Durability", "The view ", " provides is a) unbiased, b) the view of a roboticist (that\u2019s what this community is about ", " ! )  and c) based on the information someone got from hearing \u201cyour presentation\u201d. Note the following:", "I believe we all appreciate technical argumentation and slides. I love slides. But what I love even more is code and things that I can reproduce. How can I verify your arguments through experimental results? Is there any open code that supports your arguments? More than bashing around, i think it will do a lot of good to facilitate implementations that others can reproduce in common platforms. Even early stages will do. You might find that you could even get some support (and feedback!) before launching it officially and furthermore, that\u2019ll definitely convince a lot of people on why your approach is better.", "Last but not least, I really hope that the passion you\u2019ve shown answering this thread is shown by supporting and making OpenSplice better.", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "Neither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use."], "url": "https://discourse.ros.org/t/ipc-in-ros2/2619"}
,{"title": "Keys in msg", "thread_contents": ["Most of the DDS vendors support the idea of a primary key in their message structures. Have we discussed enabling that from the ROS msg format? What are the issues here? A \u201ckey\u201d keyword seems fairly straightforward.", "We\u2019ve discussed it before, but it\u2019s largely seen as overlapping in functionality with topic namespaces.", "If you have a use case that is made possible with keys, then that would be interesting to hear about it.", "However, there are quite a few little details for implementing the key field which are not straightforward to me, like:", "Probably other things I cannot even think of now. So unless there is a big use case which is not possible with namespaced topics, I think we\u2019d probably not expose it.", "I just found this discussion when looking for keyed topic support.  At Houston Mechatronics we have been using a long ago branched version of ROS2.  We are now moving to Ardent and saw that keys still aren\u2019t there.  We use keys for several different things.  One of the things we have implemented is a parameter server purely in DDS which uses keyed topics so there is only one Parameter topic.  The key is the name of the parameter and the value is in the other part of the topic data.  This gives us great flexibility with the underlying DDS software because it handles the history.  If I put my history at keeping only 1 the DDS vendor software we are using (RTI) will intelligently give me the last of each instance of the key.  I only need to worry about one topic, but I can get all of the latest values and nothing else.", "RTI at least has suggested using keyed topics as one of the primary ways to efficiently use DDS in numerous parts of our system.  We\u2019ve had to fork rosidl and rosidl_dds to have this DDS feature on our end.  It supports the RTI version of keys in IDLs, but something similar could likely be extended for the other vendor solutions if they differ.", "Using keys for this use case has one drawback. Keys are not \u201cintrospectable\u201d without receiving the actual data. You can\u2019t answer the question what parameters exists - without actually receiving all the parameter values first.", "So if you would like to implement a user interface which should show the name of all parameters you can\u2019t do that efficiently. Imagine some parameters contain large data but the user only wants to see the names of all parameters and then decides to get only the \u201csmall\u201d value of one of the parameters.", "We\u2019ve been talking at length to RTI about this topic as well as some of the other vendors, and they suggested two concrete ways of using keys in ROS 2:", "For the first point, I think we decided that this would be very difficult to do correctly and efficiently. There are a lot of technical points as to why, but ", " alluded to one which is that keys are not part of discovery, so something like ", " becomes a lot less useful.", "In the end, we imagined a new feature that DDS could have which provides the performance benefits of ", " which you get when using keyed fields on a topic, but without having to have the key itself in the message definition. This new kind of key is something between keys and partitions, or another imperfect analogy is partitions which create instances. This is by no means a sure thing to happen, and even if it did we\u2019d have to wait on the DDS standard to add it and adopt it. But what I took away from it is that: keys are not ideal in all cases and we would love to use instances to reduce overhead and discovery traffic but not at the expense of requiring key fields in our messages.", "For the second point, there\u2019s no reason (as long as we\u2019re not using keys under the hood) that we could not allow users to use keys in their own way, perhaps in the way you described. The only hesitation we had was that this is a sophisticated feature, which really ties our API to DDS much more strongly. To be clear, we don\u2019t intend to support anything other than DDS for the foreseeable future, but we did take a lot of care to insulate our robot specific code from the communication system underneath. Perhaps this is not a realistic abstraction, but in our reference implementation of ROS 2 we\u2019re trying to provide it. Therefore, we\u2019re (or at least I am) wary of taking more and more complicated features into our middleware abstraction layer (rmw, see: ", "). It\u2019s not a deal breaker for key fields as a feature of ROS 2, but a consideration we have.", "To disagree or correct my self, I previously said:", "So unless there is a big use case which is not possible with namespaced topics, I think we\u2019d probably not expose it.", "I can now express a use case where keys would be much more efficient, which is the case of the parameter events topic.", "I\u2019m speaking here about parameters as described for ROS 2, see: ", ", where they are decentralized and owned by nodes, and there is no \u201cspecial\u201d centralized parameter server. Global parameters are realized with a node that has a conventional name (like ", ") and accepts all changes.", "So, every node publishes to the \u201cparameter event topic\u201d anytime a parameter it owns is changed (", "), then a tool can subscribe to the topic and monitor all changes to the parameter system.", "Now, this could either be \u201cevery node publishes to the same ", " topic\u201d or \u201cevery node publishes to its own ", " topic\u201d. The benefit of the first is that a tool that wants to monitor all changes only needs to subscribe to one topic (this is the same use case as ", ", many producers, one consumer of all producers). However, if you want to monitor one node, or only a few nodes, then you need to receive all the data and discard what you don\u2019t want, which is inefficient. For that use case, the second option is better, since you can subscribe to only the nodes you want to monitor, but is very inefficient if you want to monitor them all, because you need to subscribe to N topics, where N is the number of nodes with parameters.", "This is a perfect case where keys are optimal, because it gives granularity but also efficiency when doing a mass subscription. However, I think simply exposing the keyed fields feature would allow us to do this, we don\u2019t need to put it under the hood of our namespaced topics. Also, after thinking about it a lot, and looking for common patterns in ROS 1 which would benefit from keys, I come up with very few. Some of them are important, but I don\u2019t get the feeling that it would be a feature that could be applied all over the place to strict benefit. But that last part is just my opinion.", "I agree changing other ROS features to use keyed topics is a much larger undertaking.  The only thing we are specifically looking for is the ability to create a msg file which will cause them to be generated at this point.  RTI\u2019s implementation looks for the (@)key at the end of the line, so realistically just passing along comments from a msg file to the IDL would make this work.  For example, our msg file is below:", "string<=512 key #//(@)key", "\nstring<=512 value", "We made an update to basically pass this along so the IDL file now includes the following:", "struct Parameter_", "\n{", "};  // struct Parameter_", "This obviously doesn\u2019t solve the problems with discovery that ", " mentioned, but it allows the global parameter setup ", " mentioned, which is how we are using it.", "Dragging up an old thread, but I was curious about this part of your reply ", ":", "Now, this could either be \u201cevery node publishes to the same ", " topic\u201d or \u201cevery node publishes to its own ", " topic\u201d. The benefit of the first is that a tool that wants to monitor all changes only needs to subscribe to one topic (this is the same use case as ", ", many producers, one consumer of all producers). However, if you want to monitor one node, or only a few nodes, then you need to receive all the data and discard what you don\u2019t want, which is inefficient. For that use case, the second option is better, since you can subscribe to only the nodes you want to monitor, but is very inefficient if you want to monitor them all, because you need to subscribe to N topics, where N is the number of nodes with parameters.", "Without using keys, would this (ie: individual topics vs single topic with client-side filtering) not be something that could be worked-around (almost) by using content-based subscriptions?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["each vendor has its own way of defining them", "dealing with key fields in nested messages", "use of them has to be made visible from all of the ROS tools", "using keys implicitly under the hood to make our topics which have namespaces more efficient (sharing data readers and writers between otherwise unrelated subscriptions and publishers)", "exposing the feature to users like yourself so you can choose to use keys in specific situations"], "url": "https://discourse.ros.org/t/keys-in-msg/1942"}
,{"title": "Cameras with 360\u00ba FoV in ROS", "thread_contents": ["Hi,", "I noticed that more and more consmer-grade cameras with 360 FOV are available (Samnsung Gear 360, Nikon KeyMission, kodak pixpro 360, etc.)", "Wonder if anyone successfully managed to do live streaming from these cameras in Ubuntu and to use them in robotic application.", "Cheers", "Davide", "Hi,", "Disclaimer : self-promotion ^^.", "A few years back I was playing with a 360\u00ba imaging using two uEye camera UI-3240CP \u2013 IDS-Imaging each mounted with 185\u00ba FoV fish-eye lens.", "\nYou can find short videos ", ".", "\nSince then plenty of nice camera got to the market as you mentioned, unfortunately I didn\u2019t get to play with any of them.", "Cheers.", "I\u2019m not sure about those cameras, but Econ Systems makes a rig for the TX1 that supports 6 cameras to provide 360FOV.", "I can only talk about the Kodak PIXPRO SP360 (and 4K). They work as normal USB cameras. But connecting them via USB doesn\u2019t give them enough current to keep on forever (they drain more than they charge). Maybe fixable with some kind of powered USB cable?", "My little work can be found here: ", "I have a short experience with SP360 (firmware version 1.0.5). I think it is important to highlight the differences of SP360 and SP360 4K since AFAIK only the SP360 4K version appears as V4L2 device.", "The USB interface in SP360 serves only for storage access purposes. The only way I have found to stream the live image is setting the cam in WiFi mode and sniffing the MJPEG stream. Some instructions on that can be found ", ". Since this stream is actually meant for live preview on a mobile app, the quality and resolution is far from the cam is actually capable of. Although the cam has an HDMI output, it is only enabled in playback mode and does not provide live feed.", "The SP360 4K has a webcam functionality and enables live stream to HDMI (now the external capture cards are also an option).", "Hi,", "does anyone know if the ", " is compatible with the ROS this node or at least with Linux?", "I think ", " was being too modest. Not only did he get the Kodak 360 camera streaming, he wrote RViz plugins to blend and display a spherical image (requires 2 cameras, ~1 hemisphere each) and  use a VR headset.", "Spherical image:", "\n", "VR:", "\n", "Hi, we are using ", ".", "\nWe can use ", " package to obtain image via USB streaming.", "Hi!", "Is the RICOH Theta S working well with the UVC driver on Linux?", "\nWe bought a RICOH Theta V only to realize that it is using the UVC1.5 driver which is not supported. I can\u2019t seem to find any other confirmation besides your post about the compatibility of the \u201cS\u201d model, using UVC1.1, and it would be great if you can confirm that it is working fine before we buy another camera ", "\nThanks!!!", "Hi,", "We are using RICOH Theta S with libuvc_camera, using UVC1.1 (Motion Jpeg mode).  The frame is a  combined 1280x720 image at 14fps. I think you can use other package like gscam or usb_cam.", "I didn\u2019t know about RICOH Theta V, but do you mean you cannot make Theta V work with current driver and packages? I expected V model is upper compatible to Theta S\u2026 but checking the spec sheet now, it seems support only H264 streaming\u2026 (it means we need UVC1.5 or higher, which is not supported by libuvc ", ")", "Thank you for the confirmation! We will go ahead with the S model then.", "I will check periodically what the status of support for uvc1.5 is and post it here when I can make the V model work. I assumed compatibility as well, but they dropped support for 1.1, which is my bad, I should have checked before.", "I seriously doubt my boss will allow me to allocate time to develop the driver, but if it happens I will post the news here.", "It\u2019s wonderful if we can use UVC1.5 and H.264 encoding. I\u2019m waiting for good news!", "what do you mean with combined 1280x720 image, the equirectangular 360\u00ba panorama?", "Also, how does the integration work? Can I plug it to a computer and have the computer handle everything ( turning it On/Off, modes, data handling, etc)", "Has anyone had any success running the Theta S or V with UVC1.5?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/cameras-with-360-fov-in-ros/1833"}
,{"title": "ROS - C++ Development IDE Choice", "thread_contents": ["What is the best development IDE choice for ROS C++ development? I see here Eclipse IDE being recommended - ", ". However, some of the developers in our organization want to use QTCreator for ROS C++ development.", "How they both compared for primarily ROS centric development works?", "Please share your experience and thoughts.", "\nThanks.", "I\u2019ve used both Eclipse CDT and QtCreator for ROS C++ development. Both work without problems. I personally prefer QtCreator now because it feels a bit snappier.", "Note that the ", " you\u2019ve linked to also has instructions on how to use QtCreator with ROS. It also doesn\u2019t specifically recommend Eclipse over QtCreator.", "At our lab most people tend to use the two mentioned by Martin (QtCreator and Eclipse CDT), but recently I\u2019ve tried and also heard good things about CLion:  ", "The thing that I like about CLion is the clean and clever integration with CMake, and that it \u2018just works\u2019 with ROS packages without having to do special configuration (as long as you are running it from a console that has the ROS environmental variables).", "I\u2019ve been using Eclipse for C++ and not through CMake. Going to look at CLion mentioned by Alexis in another message since it integrates with CMake. I don\u2019t like having to add files to the CMake system that Eclipse already knows about. The trade off hassle is finding the libraries needed for linking. But so far it\u2019s worked well for me. I\u2019ve avoided having to go back and retrofit my builds into CMake.", "One semi snide comment is that the best IDE is the one you know.", "The thing that I like about CLion is the clean and clever integration with CMake, and that it \u2018just works\u2019 with ROS packages without having to do special configuration (as long as you are running it from a console that has the ROS environmental variables).", "Yes, exactly the same is true for QtCreator. Since all Catkin packages are just plain CMake projects (catkin is just a set of CMake macros), it all just works without any extra steps required when the ROS environment variables are set.", "+1 for Clion. For me much better than eclipse/qtcreator in terms of debugging and responsivness. Downside: license fee.", "QtCreator is a good choice. But I don\u2019t think that is the best one.", "\nI kind of like Clion\u2026but it takes ages to open. I tend to close the IDE by mistake once in a while and is fully annoying to wait until it opens again. Also I have the feeling that drains your computer resources\u2026Sometimes is only open it and start hearing the fun of the computer spinning. And of course, you have to pay for it.", "\nRoboWare had a nice idea behind, but I cannot edit .launch or .xml as freely as it should. Feels like that it requires time to grow up.", "\nRight now I am playing with Visual Studio\u2026I am happier. Free, fast to open, smooth\u2026I am using it since only 3 weeks but I feel that it deserves at least a try.", "I started using KDevelop recently and I am surprised how good it is. You should give it a try, supports CMake projects as well.", "\u201cbest IDE is the one you know\u201d - Good point. Thanks.", "Martin - thanks for pointing that out.", "My 2 cents:", "I used to use Eclipse, which works fine, but in recent years I\u2019ve started to like QtCreator better. In particular the CMake integration is getting some love with recent releases (see ", " and ", "). Make sure you get a recent version from the qt website, instead of the often very old version that comes packaged with your OS.", "I found that QtCreator\u2019s build-in cmake integration works great with ROS projects (both catkin_make and catkin tools), but make sure you check out the ROS IDS\u2019s wiki linked above for how to set things up.", "Also, there is a plugin for QtCreator that gives you even better ROS workspace integration. It has been available for catkin_make-based development for quite a while, but it is under active development and catkin_tools support has recently been added (see ", " and ", ").", "Thansk NikolausDemmel - very good information.", "I haven\u2019t had a great solution for a while, but I do use Eclipse w/ \u2018eclipsify\u2019 to help get a lot of the references working (still build with catkin tools though).", "For anyone using Eclipse (who hasn\u2019t already battled to set it up), I highly recommend \u2018eclipsify\u2019 from ethz-asl, to give you a jump-start in creating configured Eclipse projects for your catkin pkgs:", "\n", "eclipsify - Generate eclipse projects for catkin packages.", "\n", "I just checkout eclipsify (and their other dependency catkin_simple) into my ws, build, and run the tool over a catkin list of the projects I have checked out, and it will generate eclipse projects for you to Import, which start with some good settings they have discovered. You probably want to make a separate \u2018/projects/\u2019 dir to output the projects into, rather than the default \u2018/devel/\u2019, so a clean doesn\u2019t blow away your projects.", "I still tweak it a bit, but it resolves a lot of the dependencies, which I find one of the most useful IDE features for me personally: I make sure to setup \u2018Share settings/resources between projects\u2019, make sure the C++11 flag is set, and include the /opt/ros/ include+lib dirs for my main projects, to help the resolution along.", "Are you using Visual Studio on Windows or Visual Studio Code (", ") on a Linux machine? I tried Visual Studio Code a while back (6 months ago?) and was unimpressed, but maybe it\u2019s improved since then.", "On a Linux machine. Works perfectly with my ros workspaces, and you can activate a console where you can perform your catkin_make and compile directly. As I said, still trying, but I am happy right now.", "If anyone is interested, I built a tuto on how to use Kdevelop + catkin : ", "This IDE is the best I\u2019ve experience so far, alongside with atom for the crazy multi cursor support.", "I also use Eclipse in combination with catkin tools (", "), the config is set to create eclipse projects:", "\ncatkin config --cmake-args  -G\"Eclipse CDT4 - Unix Makefiles\" -D_ECLIPSE_VERSION=4.6 -DCMAKE_BUILD_TYPE=Debug", "This then creates a single project from each ROS package in the build folder of the workspace. In Eclipse you just need to import those you want to have in your workspace.", "For me this is the best and easiest way to use Eclipse with ROS I discovered so far.", "Using the normal catkin_make with the arguments above, you will always import all the packages in the workspace as one huge Eclipse project, which is not so nice in my opinion.", "Nice, you might want to link the tutorial from the ROS wiki IDE\u2019s page.", "Just a small nitpick:", "Cliking on build is equivalent to calling :", "I would assume that this is not correct, since ", " also builds dependent projects. Maybe it is similar to running ", ".", "Wasn\u2019t aware of eclipsify yet, nice! Could you maybe add this reference to the Eclipse section of the IDEs page in the ROS wiki.", "If you try CLion as mentioned above (at least with a good workstation) it\u2019s hard to go back to your previous IDE. It worths the money if you have enough RAM on your PC.", "If you are skeptical, just try it\u2019s debugging and refactoring features.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-c-development-ide-choice/1907"}
,{"title": "Building ROS on Mac in CI", "thread_contents": ["I am considering building some ROS packages on Mac regularly for catching up broken build/tests on this platform.", "The first step would be to do a prototype (", " looks promising). If it goes OK, this needs to be integrated with ROS Buildfarm.", "What do you think - useful, doable?", "The main issue is the need for a reproducible build, which is hard to do because macOS doesn\u2019t have docker and no one (outside of Travis-CI that I\u2019m aware of) has automated use of VM\u2019s with checkpoints to emulate a \u201cclean starting state\u201d for each build. This is a crucial step for building packages in my opinion, but AFAIK (it could have changed) ", " only provides hosting on mac minis, not the automation for repeatable jobs. Travis-CI does provide this as continuous integration, and I believe some people are using (abusing?) this to build Homebrew bottles ", " might know more about that then me at this point.", "If you\u2019re interested in trying anyways, you\u2019ll want to catch up on the existing efforts:", "Those two links are the best summaries I can find, but efforts so far have stalled, as just keeping it building from source on macOS with Homebrew has become of a burden than the community appears to be willing to bear.", "In order to be integrated into the \u201cROS buildfarm\u201d (by this I assume you mean ", "), we\u2019ll need a way to provision machines, generate artifacts on a per package basis (bloom does this for debian and fedora packaging), and extend the build farm itself to create the necessary jenkins jobs to actually execute the packaging on the build machines and upload the result somewhere. Once you get to this point, ", " and/or ", " might have more to add.", "Just glancing at ", " again, it looks like they now support provisioning VM\u2019s on demand and use VMware under the hood:", "Also, I\u2019ll mention that Homebrew does it\u2019s packaging (last time I checked, again could have changed) not by having VM\u2019s, but by deleting the homebrew directory each time before a new build starts. So this is more like ensuring you completely cleanup the system before finishing a job, rather than discarding the changes and going back to a previous docker image or VM snapshot. This might also be possible, but in my experience it\u2019s harder for ROS since we not only install things from Homebrew, but also things from pip for Python dependencies (also possibly Ruby\u2019s gem, etc\u2026).", "I maintain the osrf/simulation tap where I build bottles for gazebo and its dependencies. We use a few mac minis with jenkins for the bottle builds.", "I have seen folks attempt to build bottles with travis-ci, though I\u2019m not sure what the current status is:", "\n", "\n", "The biggest challenge with maintaining homebrew bottles on a non-core tap is that your bottles can break at any time if a new bottle is built for one of your dependencies (such as boost or protobuf). I have a daily job that tests one of our gazebo bottles, and then we manually rebuild them if we notice that job failing due to an outdated dependency. We try to stay on top of it and don\u2019t see too much downtime, but this is not scalable. I wouldn\u2019t want to be in charge of keep homebrew bottles for all of ROS working.", "\n", "\n", "My initial focus is just building and testing.  Packaging will come later if building and testing is successful.  I believe both things is going to be useful - knowing that a particular change causes a build break or a test failure on Mac is a useful information for package maintainers.", "Yes, doing a clean (or more generally determistic) build is a concern for any CI process, and this work should get some data on the topic.", "If you want to avoid building \u201ceverything\u201d at once (including all recursive dependencies) you will need packaging to provide the binary packages for the dependencies.", "If you want to avoid building \u201ceverything\u201d at once (including all recursive dependencies) you will need packaging to provide the binary packages for the dependencies.", "Dirk is exactly right. Even if you only want to use travis for CI (which is easy enough to set up), once you have enough dependencies, your build will timeout if you don\u2019t install them from pre-compiled binaries (bottles in the homebrew case).", "According to ", "  the", "\njob\u2019s timeout is 50 minutes. When I was following ", "  on my computer", "\nfor ", " I think the whole process was done in less than 50 minutes and it includes quite a few packages being built.", "I plan to start with something small, see how it works, and then extend if successful.", " and/or ", " might have more to add.", "I think most of the key points have been covered. Any automated build and testing will benefit the project but in order to be widely useful, it needs to be reproducible and make efficient use of resources. You might find the ", " repository interesting. It shows how we build and test ROS 2 from source on ", " which has several windows and mac machines attached to it. As the project grows we\u2019re starting to feel the pain of this approach as CI times climb.", "If it goes OK, this needs to be integrated with ROS Buildfarm.", "We would definitely encourage contributions for MacOS support into the ros_buildfarm repository; giving community members the opportunity to deploy it. However since MacOS is not an officially supported platform, it is a small fraction of the user base, and costs for Apple hardware or compute time are significantly higher than other platforms, deploying MacOS to the official infrastructure is not a decision that can be made based solely on the technical capability to do so. Once we have the capability, if an organization would like to sponsor efforts to deploy and support the ongoing maintenance of MacOS infrastructure it could be reconsidered.", "I started with Travis, picked up ", ", and added the building and testing on Mac.  ", ". Then I opened ", " to discover that the last build in Travis happened 5 years ago.  That\u2019s many changes ago.", "Is Travis an option? Having VMs for free for OS projects sounds like a good deal to me.", "For smaller packages like rosdep Travis is definitely an option that\u2019s already in use. You\u2019ve found the Travis CI history for the old repository location. It was moved to", "rosdep multi-package manager system dependency tool - ros-infrastructure/rosdep", "So you can see the current status is here: ", "It looks like you\u2019ve already found it, but for others reference there\u2019s now a [PR under review}", ")  for rosdep testing on macOS.", "As the project grows we\u2019re starting to feel the pain of this approach as CI times climb.", "Do you have any information on how you plan to address this pain?", "Do you have any information on how you plan to address this pain?", "I think what ", " said: provide binary packages for the dependencies. You can see my other comments in this thread about that current status for macOS.", "I am focusing right now on building python projects on macOs.  Example: ", ".", " folder contains few things, and it will be pretty much the same for every project.  I am thinking about moving the content of .travis into a separate repo and adding it to existing repos as a submodule.", "What do you think about using submodules for this case?", "Normally I dislike using submodules, but given that it\u2019s only required for travis, I think it would be ok. At least I can speak for the repositories I maintain, other maintainers might not feel that same way.", "I too dislike submodules, and although they would only be necessary for travis if they are there by default it can be confusing/disruptive for developers. If it\u2019s only necessary for travis it would be simple to package that functionality into a resource that travis would fetch/install during the setup phase, like it does like any other testing/tooling dependencies which are installed via the .travis ", " segment.", "Thanks for the ideas.", "MacOS CI for ROS packages would be a great addition, even for core repositories like ros_comm (see e.g. ", ").", "You might want to have a look at ", ", which can be easily added to other repositories without submodules, just loke ", " mentioned. Maybe it even makes sense to integrate mac support there, since this is already in use by many packages it seems.", "You might want to have a look at ", ", which can be easily added to other repositories without submodules", "The same goes for ", ". Afaik neither of the two offers support for macOS atm. For ", " it is certainly planned to also test on macOS and Windows in order to satisfy the need in ROS 2. Once that works on Jenkins it should be feasible to run the same on Travis (with macOS) and AppVeyor.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/building-ros-on-mac-in-ci/4071"}
,{"title": "Regional Robotics Sailing Winter Academy at Zhejiang University", "thread_contents": ["Sailing robots are autonomous surface vehicles (ASV) that use wind power as the only propulsion source. The robot can sailing on the sea on a much longer duration compared to the battery-powered counterpart and potentially could serve as a platform for the long-term ocean observation. In order to help teams get start with robotics sailing, we are organising a two days winter academy for students and professionals.", "\nThe lecture will be given by Yu Cao (Southampton Sailing Robot Team) and Prof. Chao Xu (Zhejiang University).", "\nDec.15 and 16, 2018 @ Zhejiang University, China // Free of charge", "\nRegistration can be made online via ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Introduction on robotics sailing \u2013 history and the challenges", "The physics on sailing craft", "Hardware and software structure of an autonomous sailing robot", "Software tutorials on the basics of ROS, python and git version control", "Sailing robot path planning and lower level control", "Sailing demonstration given by ZMART team (subject to weather condition)"], "url": "https://discourse.ros.org/t/regional-robotics-sailing-winter-academy-at-zhejiang-university/6980"}
,{"title": "Low light depth camera in high dust environment", "thread_contents": ["Hello all!", "I am researching what kinda of depth camera to use for my project but I am having a hard time finding some answers. I am building something that will need depth information in low light (and sunlight) but also exists in a high dust environment (this will be in a desert with fine particulate dust). I realize that every once in a while I would need to clean off the sensors, so I thought about putting the depth camera (whichever I end up with) behind some sort of acrylic or glass that would allow for easy wipe down periodically.", "Does anyone have any ideas or experience here? I am new to the various technologies that are available so perhaps this topic exists with answers, but not sure what do search for.", "Thanks for all your help!", "\nRyan", "I have no idea at all, but note that the Mars rovers with solar panels, chose to not automatically wipe the solar panels clean from the dust, because such a wiping action will statically charge the glass/plastic, and attract even more dust.  Plus, trying to wipe sand off, is basically the same thing as rubbing sandpaper on it.", "Underwater applications are pretty similar to this problem since water can have high particle count (turbidity). For these kind of environments a single point light can generate a lot of backscatter from the outgoing light reflecting off suspended particles. Some ideas for how to reduce this backscatter are:", "For underwater applications you can use sonar, the terrestrial equivalents would be ultrasound or radar. I\u2019m not sure ultrasound would be a good choice in the desert since wind can make the microphone signal pretty noisy but maybe radar maybe is an option. There are some commercial off the shelf radar units that wouldn\u2019t require wiping.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Place active illumination far away from the camera", "Use a laser fan for active illumination instead of a single point light", "Use a modulated light source / receiver, backscatter show up as white noise instead of a signal"], "url": "https://discourse.ros.org/t/low-light-depth-camera-in-high-dust-environment/12140"}
,{"title": "Cameras with 360\u00ba FoV in ROS", "thread_contents": ["Hi,", "I noticed that more and more consmer-grade cameras with 360 FOV are available (Samnsung Gear 360, Nikon KeyMission, kodak pixpro 360, etc.)", "Wonder if anyone successfully managed to do live streaming from these cameras in Ubuntu and to use them in robotic application.", "Cheers", "Davide", "Hi,", "Disclaimer : self-promotion ^^.", "A few years back I was playing with a 360\u00ba imaging using two uEye camera UI-3240CP \u2013 IDS-Imaging each mounted with 185\u00ba FoV fish-eye lens.", "\nYou can find short videos ", ".", "\nSince then plenty of nice camera got to the market as you mentioned, unfortunately I didn\u2019t get to play with any of them.", "Cheers.", "I\u2019m not sure about those cameras, but Econ Systems makes a rig for the TX1 that supports 6 cameras to provide 360FOV.", "I can only talk about the Kodak PIXPRO SP360 (and 4K). They work as normal USB cameras. But connecting them via USB doesn\u2019t give them enough current to keep on forever (they drain more than they charge). Maybe fixable with some kind of powered USB cable?", "My little work can be found here: ", "I have a short experience with SP360 (firmware version 1.0.5). I think it is important to highlight the differences of SP360 and SP360 4K since AFAIK only the SP360 4K version appears as V4L2 device.", "The USB interface in SP360 serves only for storage access purposes. The only way I have found to stream the live image is setting the cam in WiFi mode and sniffing the MJPEG stream. Some instructions on that can be found ", ". Since this stream is actually meant for live preview on a mobile app, the quality and resolution is far from the cam is actually capable of. Although the cam has an HDMI output, it is only enabled in playback mode and does not provide live feed.", "The SP360 4K has a webcam functionality and enables live stream to HDMI (now the external capture cards are also an option).", "Hi,", "does anyone know if the ", " is compatible with the ROS this node or at least with Linux?", "I think ", " was being too modest. Not only did he get the Kodak 360 camera streaming, he wrote RViz plugins to blend and display a spherical image (requires 2 cameras, ~1 hemisphere each) and  use a VR headset.", "Spherical image:", "\n", "VR:", "\n", "Hi, we are using ", ".", "\nWe can use ", " package to obtain image via USB streaming.", "Hi!", "Is the RICOH Theta S working well with the UVC driver on Linux?", "\nWe bought a RICOH Theta V only to realize that it is using the UVC1.5 driver which is not supported. I can\u2019t seem to find any other confirmation besides your post about the compatibility of the \u201cS\u201d model, using UVC1.1, and it would be great if you can confirm that it is working fine before we buy another camera ", "\nThanks!!!", "Hi,", "We are using RICOH Theta S with libuvc_camera, using UVC1.1 (Motion Jpeg mode).  The frame is a  combined 1280x720 image at 14fps. I think you can use other package like gscam or usb_cam.", "I didn\u2019t know about RICOH Theta V, but do you mean you cannot make Theta V work with current driver and packages? I expected V model is upper compatible to Theta S\u2026 but checking the spec sheet now, it seems support only H264 streaming\u2026 (it means we need UVC1.5 or higher, which is not supported by libuvc ", ")", "Thank you for the confirmation! We will go ahead with the S model then.", "I will check periodically what the status of support for uvc1.5 is and post it here when I can make the V model work. I assumed compatibility as well, but they dropped support for 1.1, which is my bad, I should have checked before.", "I seriously doubt my boss will allow me to allocate time to develop the driver, but if it happens I will post the news here.", "It\u2019s wonderful if we can use UVC1.5 and H.264 encoding. I\u2019m waiting for good news!", "what do you mean with combined 1280x720 image, the equirectangular 360\u00ba panorama?", "Also, how does the integration work? Can I plug it to a computer and have the computer handle everything ( turning it On/Off, modes, data handling, etc)", "Has anyone had any success running the Theta S or V with UVC1.5?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/cameras-with-360-fov-in-ros/1833"}
,{"title": "A do-it-yourself Turtlebot", "thread_contents": ["I am teaching a class to Computer Science students (clever software but not hardware types). I thought it would be a good experience to actually build or own turtlebot-like robot, not literally from a kit, but from a pre-acquired set of parts that I knew would go together. It would give them a more visceral experience of building. Our goal is to do mobile navigation with ROS, so we are also using off-the-shelf Robotis Turtlebot3 but they are very small and a \u201creal\u201d Turtlebot2 costs like $2K. Any links or suggestions from those of you who have traveled the same road?", "(Not sure if this question should\u2019ve been on ", ".)", "Hi pitosalas, while not exactly the same specs as Turtlebot, I am working on a ROS + OpenCV Raspberry Pi based robot kit called ROSbots - ", ".", "It is a differential drive robot that has a camera, wheel encoders, and a motor driver, and off course runs ROS and OpenCV pre-installed.", "Would that fit your needs?", "Jack", "Hi Jack", "Yes that\u2019s along the lines but I am looking for something a bit bigger and with a more powerful main computer. Do you have any other leads? Thanks!", "Pito Salas", "\nBrandeis Computer Science", "\nVolen 134", "If you don\u2019t want to build the TurtleBot2 from a kit, the full designs are available to build it from scratch at: ", " Many people have done that. And while you\u2019re at it you can modify the design for your usecase or application.", "Hi Pito,", "I apologize if you had already thought of this but if you can tolerate the network latency and your robot isn\u2019t too sensitive to slightly \u201clate\u201d sensor data (but still accurately timestamped), you can always have a more powerful computer (ie laptop or desktop) run the more computationally heavy ROS nodes as secondary processors - no need to have these processors actually on the robot (which just sucks up more power and requires heavier duty motors). Just another architectural thought to help your students keep cost down while still being able to build their own full fledged ROS robot.", "Happy to continue to hear your thoughts, and \u201cdiscourse\u201d (harhar!).", "Cheers", "\nJack", "The TB2 \u201cfull designs\u201d look quite intimidating. Did you say that the components are available to buy or is the idea that I literally 3d print the parts as specced out there?", "Hi Jack. Yes I was aware of that option but I also am aiming (maybe mistakenly) for building up the experience and skills to allow us to make a robot that will be able to carry a small load (1kg say) and eventually go outdoors. I know that\u2019s a lot. But that\u2019s why I am looking for something with a base the size of an iRobot create or something else\u2026 Thoughts?", "Hi Pito,", "You can check out ", " . It\u2019s not exactly a Turtlebot but it offers a \u201cblueprint\u201d for building a variety of DIY ROS compatible robots (2WD, 4WD, Mecanum Drive, and Car-Like steering) . The platforms are computer agnostic so you can choose a much capable board.", "Hope this helps. Cheers!", "You can buy individual parts for the Turtlebot 2 from Dabit Industries for around $800/kit plus the cost of a computer", "For a cheaper cost, you can grab a iRobot Create2 base ($200), iRobot Create2 Mounting Kit ($125), Orbbec Astra ($150), and some computer.", "\n", "**The Kinect and iRobot Create2 are NOT included in this kit \u00a0 This is a kit meant to screw directly into the iRobot Create2 Boss locations, and provide an adaptable and scalable base for all sorts of fun projects! \u00a0 Included in this kit: 1 x Middle...", "\n    ", "\n", "As for computers:", "For SBCs, you can take a look at:", "\nRaspberry Pi3: ", "\nASUS Tinker Board: ", "\nODROID-C2: ", "As for laptops, you can check out the following recommended Lenovo laptops:", "\nrefurbished x240 at $419:", "\n", "\n  \n  \n  \n  \n  ", "\n", "\n", "\nLenovo 11e at $369: ", "Hi Pito, you got some great suggestions from the community. Last but not least, while not exactly a kit but definitely a platform you can build on (capable of huge payload) - check out the Magni platform from the awesome folks at Ubiquity Robotics -", "\n", "Jack", "Besides the Ubiquity platform, you can download both their RPi sd card or Virtual Box images. The small Loki platform may become available this Spring if crowd funding is successful.  Also check out the SV-ROS github for how to convert a Neato Botvac to a Turtlebot like platform. I published two articles in Servo Magazine entitled Roll your own Turtlebot. So I think it is easiest to go with an existing platform. Putting together a platform from scratch, with motors, encoders and controllers from scratch just takes too long.", "Some of the autonomous racing folks are building around a rc car platform, but I don\u2019t think that\u2019s good for education. I believe Cousera used the Neato in their course on intro to autonomous robots.", "Disclaimer, I coordinate SV-ROS, and am a member of Ubiquity Robotics", "Good luck. The only way to get better at robotics is to build better robots.", " - do you happen to have pdfs of your two articles? Reading them online in servo is hard ", " Thanks!", "I can send you the botvac article plus youtube videos of  interest.  I have to look for the original Roll Your Own Turtlebot article. (about 3 laptops ago!)   Sending raw odt:", "Roll Your Own Turtlebot Part II", "By Alan N. Federman (Dr. Bot)", "Several years ago I wrote a \u201cServo\u201d article showing how you could convert an old Roomba vacuum cleaner and a Microsoft Kinect into a robot training platform capable of teaching yourself  ROS, the Robot Operating System from Willow Garage.  Willow Garage has now closed, and Clearpath Robotics can sell you a complete brand new Turtlebot for about $2100. Recently OSRF and Robotis has announced a less expensive Turtlebot, but this is still a bit out of the range for most serious amateurs  What if you could easily build the equivalent of a Turtlebot for under $300?  I am going to show you how easy it is to convert a Neato robotic vacuum cleaner into a fully functional training platform in less than a day.  Very little hardware skill or special tools are needed. Everything is available COTS, and the software is all Open Source.", "What you need to get, if you don\u2019t already have them:", "A laptop or wifi connected desktop running Unbuntu. This should be at least 14.04 and running ROS Indigo, but it would be better to upgrade to the same versions if running cross platform. ROS versions are usually matched to Ubuntu releases.", "\nA Neato Botvac or equivalent (I have seen used XV-12s for under $200, and new basic models for under $300)", "\nA Raspberry Pi 3 (camera is optional but highly recommended) ~$50", "\n16 Gig Sd Card for Pi ~$10", "\nRechargeable 5v power pack (Those for recharging cell phone are fine) ~$20", "\nUSB cables for battery pack to Pi (micro) and Pi to Botvac (can be mini or micro depending)", "\nSmall scraps of aluminum or a tin from can.", "\nSmall scraps of flat plywood or acrylic", "\nVelcro, double sided tape or other easy to remove adhesives", "Step 1 Modifying the Botvac", "Depending on your model, you may chose to ignore any hardware modifications entirely. Then if you mess up, you can just use it to cleanup your house!  I removed the brushes, the dust bin and used a stip of metal to disable the bin detector switch (See Photo 1)", "STEP 2 Preparing the Pi and attaching to Botvac", "Artfully arrange the Pi, battery pack and optional camera on a 6\u201d by 6\u201d flat piece of wood or plastic. Attach with double sided tape. On the bottom of the assembly, attach a piece of Velcro or similar quick release fastener. Attach the matching Velcro to the top of the Botvacs Lidar unit. Lastly plug in the USB cables.  You might want to charge your batteries.  It would be a shame to have all the software loaded and than have to wait to test it.", "STEP 3 Loading the software onto the PI.", "At the time of this writing, an official version of Ubuntu 16.04 was not available fro the Pi 3. I used the Ubuntu Mate (pronounced \u201cma tay\u201d) version. Instructions for loading Mate are found here:", "Download a copy of Ubuntu MATE", "And follow the instructions for 16.04 \u2013 Raspberry PI 2/3.", "You can use an HDMI TV and attached keyboard to initially set up the Pi, using the Graphical Environment.  It also helps to have a direct Ethernet connection when doing the initial set up, because you need to load a lot of software initially. Using the Desktop, it is pretty easy to get WiFi working.", "I suggest creating an 8 gig image on a 16gig SD card. After the initial software is on and you can bring up a graphical desktop, follow the intro screen and click on Raspberry PI info \u2013 it will enable you to expand the image to 16gig. It also will allow you to configure your WiFi.  I suggest you still use the Ethernet Connection, but you can at this point open a terminal, type sudo graphical disable, and then use ssh over WiFi to complete the installation.", "Once Ubuntu is working, continue loading ROS onto the PI,", "(", "\nYou may have to maintain your Ubuntu distributions; the following commands are useful:", "sudo apt-get update", "\nsudo apt-get upgrade  (must run both in sequence)", "and sometimes to clear dpkg errors:", "sudo dpkg \u2013configure -a", ")", "Summary:", "sudo sh -c \u2018echo \u201cdeb ", " $(lsb_release -sc) main\u201d > /etc/apt/sources.list.d/ros-latest.list\u2019", "sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116", "sudo apt-get update", "\nsudo apt-get install ros-kinetic-desktop-full", "(if -full is not available, just get ros-kinetic-desktop)", "sudo rosdep init", "\nrosdep update", "\necho \u201csource /opt/ros/kinetic/setup.bash\u201d >> ~/.bashrc", "\nsource ~/.bashrc", "\nsudo apt-get install python-rosinstall", "ROS Catkin Workspace installation", "mkdir -p ~/catkin_ws/src", "\ncd ~/catkin_ws/src", "\ncatkin_init_workspace", "\ncd \u2026", "\ncatkin_make", "And then edit .bashrc to change the source /opt/ros/kinetic/setup.bash to ~/catkin_ws/devel/setup.bash", "\nalso it helps to add the following line if the Pi is hosting the robot:", "export ROS_MASTER_URI=http://$HOSTNAME.local:11311", "Where \u201c$HOSTNAME\u201d is the name you have in the /etc/hostname file", "Your /etc/hosts file should look like this:", "127.0.0.1\tlocalhost", "\n127.0.1.1\t\u201cyour hostname\u201d", "::1     ip6-localhost ip6-loopback", "\nfe00::0 ip6-localnet", "\nff00::0 ip6-mcastprefix", "\nff02::1 ip6-allnodes", "\nff02::2 ip6-allrouters", "This will support ROS networking.", "You also may wish to install \u201cchrony\u201d to synchronize the time different \u2018nodes\u2019 are running at, this is because the Pi doesn\u2019t have a real time clock, and if your Wifi is not connected to the Internet, the Pi will have the wrong time.", "Next I suggest you load the following ROS packages into your catkin_ws/src workspace:", "ROS by Example part one (RBX1) from Patrick Goebel", "     This should go on both your laptop and the PI", "and", "the SV-ROS Botvac nodes courtesy of mostly Mr.  Ralph Gnauck", "Repository of packages and info for the SV-ROS Intro To ROS training series - SV-ROS/intro_to_ros", "follow the instructions in the README files to install and test.", "Example", "cd ~/catkin_ws/src", "git clone ", "  (this also should go on both)", "cd \u2026", "catkin_make", "TESTING", "On the laptop:", "roscd teleop", "If nothing is found,", "sudo apt-get install ros-kinetic-teleop-twist-keyboard", "On the Botvac, turn on the pi, and sign on via a terminal window from the laptop.", "I like to launch a custom base only node on the Pi", "roslaunch bv80bot_node bv80bot_njoy.launch (code included at the end of this article)", "Then you should hear the Neato Lidar unit start to spin.", "On the Laptop open up another terminal window and", "set up the ROS_IP  and ROS_MASTER_URI environment variables via the \u2018export\u2019 command.", "Test to see if you are getting topics:", "rostopic list", "and scans", "rostopic echo /scan", "Finally launch teleop", "rosrun teleop_twist_keyboard teleop_twist_keyboard.py", "You should be able to drive your robot.", "If you open RVIZ in another window, you should see the LIDAR returns.", "What Next?", "With just this simple robot, you can begin to learn how to accomplish advanced robotics tasks and begin to learn the subtleties of autonomous navigation. Because of the Neato\u2019s XV-11 LIDAR unit, you can simultaneously accomplish localization and obstacle avoidance. Support for webcams and the Raspberry Pi Camera are available through ROS nodes. I have gotten teleop via a blue tooth joystick to work through the laptop, but not directly on the Rpi.  Please note that though the Rpi has 4 USB slots, there is seldom enough power to run more than the Botvac interface and a WiFi dongle. A typical USB webcam will draw too much current and crash the Rpi.", "With a WiFi connected phone and some ingenuity, you would be able to issue voice commands. So you can call up your home robot from the office and ask it to find the cat. The Botvac is a little underpowered for bringing you a snack from the kitchen, but when the next more powerful platform is available, you\u2019ll know just how to program it.", "Figures and Code", "LISTING 1  /catkin_ws/src/intro_to_ros/bv80bot/bv80bot_node/launch/include/bv80bot_njoy.launch", "LISTING 2  Terminal output from launching startup nodes:", "roslaunch bv80bot_node bv80bot_njoy.launch &", "rostopic list", "/button", "\n/cmd_vel", "\n/cmd_vel_mux/active", "\n/cmd_vel_mux/parameter_descriptions", "\n/cmd_vel_mux/parameter_updates", "\n/joint_states", "\n/mobile_base_nodelet_manager/bond", "\n/odom", "\n/raw_cmd_vel", "\n/robot_cmd_vel", "\n/rosout", "\n/rosout_agg", "\n/scan", "\n/sensor", "\n/smoothed_cmd_vel", "\n/teleop_velocity_smoother/parameter_descriptions", "\n/teleop_velocity_smoother/parameter_updates", "\n/tf", "\n/tf_static", "Figures:", "   Fig 1  XV-12 dustbin removed", "  Fig 2  XV-12 Name Plate", "    Fig 3  XV-12  Brush Removed", "   Fig 4 RPi 3 mounted", "Youtubes of Turtlebot I from create base and voice control ~2013-2014? :", "Other Youtubes of interest:", "   Botvac 2015", "  Magni 2016", "  Loki 2015", "Very interesting and thank for your information. I am interested in building a robot that can carry a larger weight (>100kg). But I am lost when Raspberry Pi control the motor because, in comparison with Turtlebot, this robot will have a more powerfull motor. In your article it does not appear any other MCU, so my question is how is the motor controled?", "\nThank you.", "The Botvac has its own microprocessor, motor drivers, etc. The Pi connects via a USB cable. There is no interface required. This was not the case on the original Turtlebot 1 which required a USB to serial TTL converter.", "I am going to make your project and i am a beginner in ROS can you provide more details to build this project", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-do-it-yourself-turtlebot/3978"}
,{"title": "Using NUC with Kobuki", "thread_contents": ["Hi,", "It seems to becoming harder to find netbook size laptops to use with the Kobuki base and so I\u2019m looking into using the Intel NUC (e.g. NUC6i5SYH). I want fully autonomous operation where the Kobuki can dock and charge its batteries and the PC, so I wondered if anyone was aware of a power solution e.g. external battery that can connect to the Kobuki 19v output AND will allow the NUC to get a reading of the battery level e.g. via USB. Alternatively, is it possible to use a dc-dc converter with one of the other Kobuki power outputs (e.g. 12v 5A) to power the NUC?", "Or if anyone knows of a good choice of small laptop to use then that might be preferable.", "Thanks,", "\nLee.", "Hi,", "Have a look at the ", ", it\u2019s a DC-DC converter as well as UPS (6-30V input, 6-24V output). It supports multiple battery chemistries (e.g. Li-Po, SLA) and has an adjustable DC output.", "It has a USB interface, and Linux drivers, supporting ", ".", "I\u2019ve been using this device on my robot, I wrote a very simple Python ROS interface for this, which just publishes the charging state and individual Li-Po cell voltages. I\u2019ll upload this script and link it on this thread when I get the chance (I\u2019m currently away from my robot).", "Alex", "Thanks for the response. After posting I did end up looking at that and it looks like a good solution. Good to know that you have it up and running and that the Linux drivers work etc.", "I\u2019m thinking of using 6 of these batteries:", "\n", "7926", "\n", "\nwhich based on my very quick calculations should drive a NUC for about 6 hours (assuming NUC consumes 15W)?", "I asked the OpenUPS company if they could recommend any enclosures - did you find one that works well?", "Hi,", "we have a NUCi7 on our Turtlebot. We ordered an i5 but got an i7 due to", "\nsome issue while ordering. The i5 had a notebook powerbank with it which", "\nis not powerful enough for an i7 (it lasts about 30 minutes). Thus we", "\nare now testing a custom LiIon battery. It seems to be able to run the", "\ni7 for about 4-5 hours now but we are still working on a battery status", "\nintegration. Additionally, we need about 4-6A for charging the robot", "\nnow. From the internal wiring, this should not be a problem, but the", "\nwall charger only gives us 3.2A. So we also still need to get a powerful", "\nwall charger (right now using adjustable lab power supplies).", "Once the battery runs well, I can send you our setup. The OpenUPS looks", "\nvery nice as well, especially the already integrated battery status and", "\ncharging state. If you deploy it to your Turtlebot, could you give", "\nfeedback about it?", "Best,", "\nLasse", "Hi!", "We are also using Turtlebot (version 1) with an i7 NUC which actually requires 19V / 65 W. So far, we didn\u2019t use any DC/DC converters or something similar, but instead, we powered the NUC directly from the 12V / 5A output port on the Kobuki.", "This is certainly not ideal, and we experienced the NUC shutting down because of a \u201cprocessor thermal trip\u201d after some time (sometimes only 30 mins when the Kobuki battery would still last for long). First we thought it\u2019s a problem with the NUC (old thermal paste or something), but later we realized that the output voltage from the Kobuki is monotonically decreasing and will be too low at some point. This seems to have caused the overheating problems then.", "Either ", " or ", " seem like good solutions for this problem, but I\u2019m not very experienced with DC-DC converters or UPS and I\u2019m glad for any advice.", "Can anyone tell me if this would be a good buy for more reliable mobile NUC operation?", "We had a similar problem and solved it by purchasing a power bank. It charges from the kobuki base, provides 19V to the NUC and does not kill the Kobuki battery. We use one of the 19V connectors on the back of the Kobuki and make an adapter to charge the powerbank when the robot is on the charger.", "This is the one we used: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-nuc-with-kobuki/489"}
,{"title": "Turtlebot3 - Impressions so far", "thread_contents": ["It\u2019s taken longer than expected, but our Turtlebot3 is running.  Here are some brief impressions and lessons learned - contrasting thoughts and experiences are welcome!", "The build instructions are good, but not great.  Adding some emphasis to the orientation of components would be helpful.  I had the motors swapped initially, and it was maddening to get no response from the board, particularly when there is no real display of an error anywhere.  Is there a legend somewhere on the Internet stating what the OpenCR user LEDs mean with the base software?  Maybe it was signaling the problem, but I couldn\u2019t tell.  Also, the build instructions could use a plan view of the OpenCR board showing where things are plugged in.  I know, some of that stuff is available on line, but hunting for it is more difficult than it should be.", "The Intel Joule board is a significant weakness, for several reasons.  First, having the new user flash the board is understandable, but should be highlighted in the instructions well ", " the board is mounted in the bot.  The requirement to reflash the BIOS is also understandable, but maddening - I run Linux, not Windows 10, so finding a machine to flash the BIOS is a pain.  It also seems that more than one person has had to flash the board multiple times before it boots.  Installing Ubuntu - that was worse than it should have been.  How many of us have micro HDMI adapters handy, so we can actually do the install?  Also, how about pointing out in the instructions that charger power supply will also serve as a Joule power supply for the flashing and install?", "More Joule issues - the BIOS checks cause an amazingly slow boot up, particularly for an embedded board.  It also runs pretty darn hot for an embedded board.  Worst of all, it seems quite unstable.  I don\u2019t know if it\u2019s the fault of the Ubuntu distribution or the hardware, but it crashes far too often.", "The OpenCR board seems quite nice, and I like having all of the sensors integrated.  I have had one lock up where communication between the Joule and the OpenCR ceased - no idea where the fault lies.  In that particular case, the bot just kept chugging along until it ran into something.  It would be beneficial to add a power or reset button on the top platform.", "The overall kit quality and part quality are outstanding.  The Dynamixels are quite nice drive units, all of the cabling is well thought out, and the possibilities for expansion are great.", "The Turtlebot wiki page (", ") is very well done.  It lays out the required steps to get the machine running in a clear and logical manner (minus things like provisioning the Joule before build).  I do think it would benefit from a troubleshooting section, including screenshots of what the various steps should look like.", "So, summing up the lessons learned:", "Flash the Joule BIOS before anything else.  You will need a Windows 10 computer, the downloaded BIOS file, and the USB-C to A cable that comes with the Joule.  Be prepared to do this more than once, and to try different BIOS files - the first one didn\u2019t work for me.", "Install Ubuntu on the Joule next.  You will need the power supply, a micro-HDMI cable or adapter, a powered USB hub, a USB memory stick with a bootable Ubuntu image loaded, and a keyboard, display, and mouse.", "Pay obsessive attention to the build instructions, making sure that every part is oriented correctly.  When in doubt, check several pictures in the manual to be sure.", "The ROS learning curve is steep, and a lot of things are taken for granted.  Rviz looks friendly, but it\u2019s definitely not simple.  Reviewing tutorials on the ROS web site is very helpful, but plan to spend a significant amount of time figuring things out.", "Thank you so much for the detailed feedback ", "\nI agree with you for several points in sequence of assembly and software installation.", "\nI hope your feedback can help a lot of users for joining ROS community with TurtleBot3.", "\nWe\u2019ll see what we can improve on our manual.", "\nThanks a lot!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/turtlebot3-impressions-so-far/2907"}
,{"title": "[TB3] TurtleBot3 Automatic Parking under AR detection", "thread_contents": ["Hi,", "previously, we had presented ", ". Now we are going to present new autonomous parking method which uses vision processing to detect the AR Marker.", "These methods are choosable for specific purposes or environments, such as no-light. Make your own recharge stations or like that by using these packages.", "Source code:", "Applications for TurtleBot3. Contribute to ROBOTIS-GIT/turtlebot3_applications development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-turtlebot3-automatic-parking-under-ar-detection/4476"}
,{"title": "SBS (Smart Battery System) SMBus over USB?", "thread_contents": ["Hi all, this isn\u2019t ", " ROS-related, but I\u2019m trying to integrate a SBS laptop battery into a ROS-based robot I\u2019m building, and it\u2019s been a difficult road. Wondering if anybody here has had any experience with this and could share their perspective.", "I\u2019ve had minor amounts of luck with a few approaches", "I\u2019m hoping to discover a fairly \u201cout of the box\u201d solution to reliably retrieve data from the SMBus. The built-in state of charge measurement, and other metrics, is really attractive from these smart batteries. However, I don\u2019t have a lot of experience implementing these protocols on microcontrollers, so it\u2019s fairly daunting to try and get it working reliably.", "Once the communication is working, I\u2019m looking forward to making a node to talk to the SMBus communication layer and publish BatteryState messages", "For the curious I am currently using the following battery and charger from RRC - they\u2019re a bit pricey but seem really nice", "\n", "4S1P Standard-Batteriepack RRC2054 mit 14.40V/3.45Ah/49.70Wh. H\u00f6chste Performance, weltweit zugelassen, direkt verf\u00fcgbar. Entwicklungskosten & Zeit sparen!", "\n", "\n", "RRC-PMM240 Lademanagement-Modul als integrierte L\u00f6sung zum Laden unserer Standard-Batteriepacks. Mit 240W max. Ausgangsleistung & 82W max. Ladeleistung.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Arduino using ", ", which let me speak SMBus (Wire library doesn\u2019t format communication correctly for smbus) - but the library seems to not handle the multi-master situation well and freezes up the communications quickly", "I\u2019ve also tried this little project ", " which also seems to freeze up on me"], "url": "https://discourse.ros.org/t/sbs-smart-battery-system-smbus-over-usb/12836"}
,{"title": "Status and maturity of OE and ROS", "thread_contents": ["Hi there. We\u2019re looking at implementing ROS from the ground up to replace the charging, control, and motion capabilities of our robotic camera. While I have a few years of experience supporting creating BSPs and recipes for yocto, I have yet to play with ROS. I wanted to gauge the current functionality of ROS in yocto/OE. Seems like for meta-ros the Igloo release is fully in master, there are experimental branches to support Kenetic, and then there is another parallel effort using superflore to automatically generate a ROS distribution for use in OE.", "We\u2019re hoping to get up an running for our commercial product and shipping within a month. While we have tools and automation in place for yocto, are we better off reinventing our tools and automation for Ubuntu instead of sticking with yocto giving our timeline.", "Happy to help test or get these working under yocto, just curious which branch or release I should focus on first.", "You say you have no experience with ROS and you want to use it for a commercial product that ships in under four weeks?     That is a rather impressive goal.", "If you are pressed for time you\u2019d be best to take the road well traveled, the one where the potholes are mostly smoothed over and where there are others who you can ask for help.  ROS on Ubuntu on PC hardware is the easy path.", "With a very tight schedule I think \u201crisk reduction\u201d is the way to think.", "Using Yocto is risk reduction to me. We already have plenty of tools that leverage yocto and openembedded. Going to Ubuntu involves reinventing the wheel from a package selection, base os, monitoring, and automation standpoint. Not to mention the package size, loss of package control, tight integration, and overinflated size of the base OS that we lose just by using it. Should be only a few weeks at most to integrate ROS layer in OE, whereas redoing everything in ubuntu, puts this out to months for implementation.", "Anyway, anybody have any thoughts of maturity and functionality of the openembedded layers available? Or do we reinvent or wheels in place for Ubuntu as Chris has suggested.", "I think Chris was mostly commenting on:", "We\u2019re looking at implementing ROS from the ground up [\u2026] I have yet to play with ROS, [\u2026] We\u2019re hoping to get up an running for our commercial product and shipping within a month.", "Irrespective of the build/deployment infrastructure that you end up going with, that seems like a short time.", "As meta-ros maintainer for five years by now (time flies like an arrow), I obviously have a strong bias for meta-ros. So, I probably cannot give you an unbiased advice on using OpenEmbedded or Ubuntu. Furthermore, I have not maintained an Ubuntu ROS system for five years.", "I assume you are well aware of the strong and weak points of OpenEmbedded and The Yocto Project. So, I will just point out the current status of meta-ros:", "We offer recipes for about 200 ROS packages (all basic packages are covered). If that is enough for you, you might quickly get your full product ready. If you need OpenBLAS or OpenNI2, you will face some challenges. Some users are working on that for some time as it seems (visible on the github forks), but they did not provide pull requests yet.", "We have a CI system running that checks each pull request against the current master of openembedded-core and meta-openembedded, but we currently do not check on regular basis on a public system if the latest commit always works with the latest master commit of its dependent layers. We also try to keep changes backwards-compatible, but we do not test and maintain specific branches for Yocto releases. All other testing is really just other users (and me) test-building the recipes on a regular basis. If you want to improve the situation concerning CI testing, ", " and I can provide all information of our testing infrastructure.", "SDK support is still not merged yet, but some users have experience with it; but I could not find time to test it. If that is important to you, let me know and we can move this topic forward together  in the main-line repository.", "I know that ", " and his company explored the use of meta-ros a few years ago in 2013/2014, and he decided to build their embedded systems with Ubuntu. He might provide you reasons why he ultimately decided for Ubuntu.", "Also, here is an interesting comparison from ", " at ROSCon 2014 (again, from the developer around ROS ARM Ubuntu, so possibly biased in the other way):", "\n", "3.87 MB", "\n", "I hope that helps.", "Which ROS packages do you need? What features do you need? Which qualities do you expect?", "Help is certainly appreciated; I have a long list of topics, but probably it is best to know what would be important to you and move those things forward collaboratively.", "Lukas", "Hi!", "if you already have experience in yocto, stick with it. It is the ", " way to make a product with ros. Especially, because it is much more easy to get a realtime kernel, which you will need at some point, if you aim to place your product in the automation industry (and you don\u2019t want it to suck).", "I am working with meta-ros (kinetic) and I find it quite useful!", "Regards,", "Matthias", "Thanks for the votes of support. I would agree with using HorstLocal, and appreciate the feedback and support bulwahn in providing an updated layer to utilize kenetic. I have imported in the latest experimental branch, and things work well thanks to the large community effort around it. Now to clean up the disk space to get it all to compile.", "Using Yocto is risk reduction to me. We already have plenty of tools that leverage yocto and openembedded. Going to Ubuntu involves reinventing the wheel from a package selection, base os, monitoring, and automation standpoint. Not to mention the package size, loss of package control, tight integration, and overinflated size of the base OS that we lose just by using it.", "So, I probably cannot give you an unbiased advice on using OpenEmbedded or Ubuntu.", "Could one not use OpenEmbedded and ROS with kind of ", " (", ")\u2026", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "The indigo branch is maintained and kept in sync with the ROS distribution release updates.", "The kinetic branch is experimental (well, just a few packages maintained in indigo don\u2019t run yet on kinetic, but I did not move that to 100% completion yet) and it is not kept in sync with the release updates at the moment, again due to limited time for maintenance and as there isn\u2019t any user asking loud enough for proper maintenance and support of kinetic.", "The lunar branch is an experimental branch from ", "; I do not know its current state, but I think it is probably also close to 100% completion of all 200 packages cross-compiling, but not completely there yet.", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/status-and-maturity-of-oe-and-ros/4404"}
,{"title": "Featured ROSCon 2018 Talks", "thread_contents": ["As we start to get ready for ", " we will be featuring talks from the ", " to help inspire people to submit their own talks as well as give people a sense of what they can look forward to at ROSCon 2019!", "\n", " ", " ", "          ", "\n", "Astrobee is a free-flying robot designed by NASA to operate alongside astronauts inside the International Space Station, where it will carry out scientific and surveying tasks in microgravity. The robot can autonomously mate with a docking station to recharge, as well as perch to existing ISS handrails using a three degrees of freedom arm. Its open source flight software stack is built on ROS Kinetic, uses a delay-tolerant DDS bridge for space-to-ground communication, and is accompanied by a Gazebo simulator that enables researchers to develop and test behavioral algorithms. This presentation covers the the software architecture, challenges faced during the development process, facilities for testing prototype hardware, and broad lessons we have learned over the last three years.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "          ", "\n", "Over the past 10 years, ROS 1 has proven itself to be the framework of choice for prototyping and developing large robotic applications. Many limitations preventing ROS 1 from being used in production applications have been discovered over the years, and after significant prototyping, ROS 2 makes great headway in making ROS 2 suitable for production. Autonomous driving is the next great technology waiting to be realized and transform our society. A full autonomous driving stack is inarguably a large robotic system, and consequently ROS 2 is the right framework upon which to develop a full autonomous driving stack. To prove this, we have developed a part of the autonomous driving stack based on ROS 2.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "          ", "\n", "Over the past half year we have been working on the new launch system for ROS 2 based on roslaunch from ROS 1. The goal of this presentation is to summarize the state of the design document for launch, describe the current state of the reference implementation, and dive into the rationale behind some of the design decisions. The attendee should come away with a deeper understanding of the differences between roslaunch from ROS 1 and the new ros2 launch tool. We\u2019ll also have some short demonstrations to show how these differences might be useful to everyday users.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "Over the past couple of years, Open Robotics collaborated with NASA Ames Intelligent Robotics Group to develop the Resource Prospector lunar driving simulator. In particular, a strong focus was placed on generating high quality visual images from the cameras on the simulated rover. This resulted in in several improvements made to Gazebo, including support of high resolution digital elevation maps, improved shadows, integration of custom material shaders, and a new lens flare plugin. This presentation will show the resulting lunar terrain environment created in Gazebo and discuss some of the challenges in modeling the environment.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "In this talk we show the current state of ROS 2 features through hands-on demonstrations. We guide the developer through a set of features and functionalities of ROS 2 beginning with creating a simple \u201chello world\u201d package and consecutively increasing functionality. We show how to launch multiple nodes, how lifecycle nodes can be used to bootstrap a complete system, through to how the ROS 2 security features can be utilized to secure applications. This talk is meant to highlight the newly available features and tools of ROS 2 and aims to incline developers to start working with it.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "FIRST, or For The Inspiration and Recognition of Science and Technology, is an international organization focused on engaging students through STEM. This year, our team - The Zebracorns - was the first in the high school FIRST Robotics Competition (FRC) to control our robot entirely using ROS. In our presentation, we\u2019ll introduce the unique challenges presented by FRC with restricted hardware options, time, and resources. We\u2019ll talk about our motivation for implementing ROS, the specific application within FRC, and our ambitions for the future of ROS within the FRC community.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "Cruise Automation\u2019s self driving car runs on top on ROS. This talk will share some of the lessons we learned while scaling up the ROS stack to a very complex Robotics problem and 500+ engineers. We will talk about performance, reliability, code organization and health, and the ways we have found ROS to excel or fall short.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", "\n", "Reliable, life-long mapping and localization is an essential component for mobile robotics in continuously changing warehouse environments. We present a system based on Cartographer in which robots run finite-history SLAM for low-latency localization and continuously stream local map updates to a cloud service. The cloud component assembles and optimizes a globally consistent pose graph out of the streaming data of the agents. Magazino piloted cloud-based Cartographer in a customer warehouse using its fleet of mobile picking robots. By sharing the local map changes among each other, the robots were able to maintain their localization accuracy while dealing with dynamic environments.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "On January 11, 2018, Sony Corporation released aibo (", "). aibo that is back on market beyond the time of 12 years constructed via robotics framework named ROS. In this presentation, we introduce examples of development in aibo from the point of view of ROS, starting with introduction of aibo, architecture, embedded technology, real-time optimization, robot development environment, simulation etc.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "RViz is now available for ROS 2, including most of its features. This talk will look back at the migration of RViz from ROS 1 to ROS 2, and present the main challenges and changes performed during the migration. It will focus on a new package, rviz_visual_testing_framework, that makes writing automated UI tests - including the 3D rendering part of RViz \u2013 possible.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "\n", "The  ", "  stack is one of the core components of the ROS Ecosystem. This talk will present lessons learned from maintaining the navigation stack over the past five years, and discuss a new generation of interfaces for increased functionality. This talk will cover: the new  ", "  interfaces, with a new  ", " interface; the locomotor package which replaces move_base and demonstrates new functionality; and ROS 2.0 + Navigation.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "This presentation discusses the building of a task-driven, message-synchronized execution abstraction layer on top of ROS\u2019s message passing, aiming to mimic a deterministic system while continuing to use a non real time operating system. We discuss the overall structure of the framework, as well as the details of various capture and synchronization policies, and the profiling and diagnostics of task executions. We further discuss methods for using simulators and bagfiles in conjunction with the synchronization framework for repeatable testing and issue reproduction.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "In this talk we give a short introduction to Mozilla rr, a powerful C/C++ debugging tool for Linux, and show how to more effectively debug ROS nodes by republishing messages from rr recordings. Mozilla rr serves as a gdb (GNU debugger) replacement which efficiently records the execution of a process and then provides repeatable deterministic debugging of the recording, enabling a very powerful debugging experience with reverse execution.", "Thank you again to our Platinum Sponsor and Gold Sponsors for supporting ROSCon.", "\n", "\n", "\n", "This talk gives an introduction on how to contribute to ROS 2. Specifically this talk shows you how to take a ROS2 feature from a design discussion, to a concept document , to a series of pull requests, through to a released feature. The talk also discusses the process of patching bugs: moving from an ", " bug, through trouble shooting, bug report generation, and finally submitting a patch. If you\u2019ve never contributed to open source patching your own bug is a great place to start and this talk will guide you through the process.", "ROSCon registration is ", "! The deadline for regular price admission is just two weeks away. Get your tickets now before the prices go up.", "                    ", "\n\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops closes today. Submit ", ".", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Early registration is ", "! The deadline for early registration is August 24th, 2019 (or until the limited supply of early registrations sells out, whichever comes first).", "Early registration is ", "! The deadline for early registration is August 24th, 2019 (or until the limited supply of early registrations sells out, whichever comes first)."], "url": "https://discourse.ros.org/t/featured-roscon-2018-talks/9071"}
,{"title": "ROS Metrics", "thread_contents": ["As a programmer, I think there\u2019s something inherently pleasing about collecting and organizing data. Even more so when you can do it automatically. In that vein, let me introduce my latest nights-and-weekends side project: ", "The idea partially stemmed from ", " and my previous project ", ". The core is to supercharge ", " and graph trends over time.", "Some observations:", "Anyway, thanks to Tully for granting me access to the Discourse and Google Analytics APIs. I still want to get my hands on the underlying wiki data and the raw logs from ", " (hit me up OSU OSL). There\u2019s lots more that can be gleaned from this data. The graphs I\u2019ve made just scratch the surface.  I look forward to using the data to help us understand and improve the ROS community/ecosystem.", "\n", "Send me a message if you want access to the aggregated database files.", "  WoW! Awesome! I think this is really COOL and very useful information. I\u2019ve seen and used metrics information published by ROSCon once a year. But now, thanks to you, we can see the information we want anytime, anywhere and compare it with previous records. It will also be a big help in choosing a development direction. I\u2019m so happy to have a lot of information on one web page. Really, thank you so much. ", "This is super cool.", "Just curious. How is rank_product calculated? What does it represent?", "Every Github repo is ranked based on number of forks*, stars, and subscriptions. ", " for instance has 402 stars, which is the 19th most overall (which is what the ", " means). Its ranked 12th for forks and 12th for subs too. Multiply 19 * 12 * 12 and you get 2736, its rank product.", "In other words, it doesn\u2019t represent much and exact relative rankings should be taken with a ", ". I\u2019m open to alternative ranking systems.", "*Technically, its not forks but network-size, meaning it counts forks of forks, which is why ", " and ", " both have the same number of forks.", "Thanks for this work ", ", truly. Beyond the community I  believe this will benefit many of us trying to convince ros-outsiders/clients/users/investors/governments/etc of the relevance of ROS.", "Thanks for putting this together.", "Awesome work thanks a lot!", "Any plan to add non-package repo to the list (Github repos not listed in ros/rosdistro) ?", "\nSome are great tool like rosautodoc, marv, etc.", "Very nice indeed ", ".", "Looking at the ", " I really don\u2019t understand how ", " downloads can still count up to almost 50% of total downloads.", "Is that actually true?", " Is there a canonical place those are all listed?", " It\u2019s what the public logs say, but I\u2019ve been informed that the raw logs that Tully used to generate  ", " are more accurate. I\u2019m hoping to get access to the raw logs soon to make things more accurate.", "Any plan to add non-package repo to the list (Github repos not listed in ros/rosdistro) ?", "\nSome are great tool like rosautodoc, marv, etc.", "If these things are valuable for the ROS community I\u2019d encourage the developers to submit them to be indexed in the rosdistros. This will make them searchable on ", " as well as having documentation generated.", "There\u2019s a tutorial ", " for doing that.", "Looking at the ", " I really don\u2019t understand how ", " downloads can still count up to almost 50% of total downloads.", "Is that actually true?", " It\u2019s what the public logs say, but I\u2019ve been informed that the raw logs that Tully used to generate ", " are more accurate. I\u2019m hoping to get access to the raw logs soon to make things more accurate.", "Yeah this doesn\u2019t agree with the logs that I\u2019ve been analyzing. Looking at your script it appears to be using the urldetails page from awstats not the downloads page. I\u2019ve opened a ticket pointing to where that could be improved.", "Looking at your script it appears to be using the urldetails page from awstats not the downloads page. And it appears...", "I\u2019ll also see if that page can have the listing not truncated.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The number of registered users on ", "g dwarfs ", "\n", "You can measure attention to the different ROS distros by ", ", or the ", " or ", ". Over half of the commits in May 2019 were working on ROS2 Dashing", "Likewise, you can watch the ", "\n", "One of the ", " (made this year) is ", "\n", "The ", " to the ros_users mailing list dropped dramatically when ", " was introduced.", "Navigation-related repositories are among ", " (Also you can get a LOT done with the 5000 requests per hour the GitHub API gives you)", "You can plot the size of the backlog (issues/PRs) for any repo in ", ". I\u2019ve been tracking the size of the ", " for awhile now, and its sad that you can\u2019t see in this chart the period when I\u2019ve actively been trying to reduce it."], "url": "https://discourse.ros.org/t/ros-metrics/10861"}
,{"title": "Announcement: A ROS package for google's speech-to-text API and NLP API - Dialogflow", "thread_contents": ["Hi everyone,", "This is my first time contributing to open source in general so bear with me.", "\nI have developed a ROS package that uses google\u2019s speech to text API to publish text onto a topic. That is then parsed by their NLP platform, Dialogflow, to extract a user\u2019s intent and whether a developer wishes to run actions associated with that intent. It\u2019s much better explained when you see their console ", ".", "My package can be found here: ", "I\u2019d love to get some feedback on how to enhance this and what features to add. Also, would like to see if this is of interest to people.", "Best,", "\nAnas", "You have PR on a few little things. Thanks for your contribution.", "Anas,", "I have been working on adding Dialogflow to my Robot Commander app, which runs on Android.  So far it\u2019s looking good, but not ready for release. One problem:  I am using Dialogflow api v.1, because v,2 speech handling is subject to charges, and I don\u2019t see a way to pass them on.", "But if I understand Diagflow correctly, the use of v.1 does not allow sending audio to Google for complete processing including speech recognition, NLP, and voice interaction. Thus I am using speech recognition to get the user\u2019s utterance as text, then sending the text to Dialogflow as I think you are.  The drawback of this is 2 trips over the net instead of one, with the consequent performance penalty.", "I\u2019ll be interested to follow your progress.", "Joe", "Thanks Sam! Saw the PR and appreciate setting me on track. I did not know Python2 classes should explicitly inherit ", " so thats a +1.", "\nI did notice that there were some requirements already listed within rosdep so that will simplify things. Will update ", " in the meantime.", "\nMerged and will follow up accordingly.", "Hey Joe,", "So I am using a similar approach to yours and that\u2019s cause even in v2 the streaming API is not stable (not even implemented if I read the code correctly\u2026). So what I do is use the asynchronous continuous speech streaming function, get the text, send it to NLP and get the fulfillment text.", "Feel free to take a look at my implementation. You\u2019ll notice I have 2 nodes, one for TTS and the other for NLP.", "Once I get word that Dialogflow has audio streaming ready, I\u2019ll add that functionality.", "Anas", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcement-a-ros-package-for-googles-speech-to-text-api-and-nlp-api-dialogflow/4337"}
,{"title": "ROS Programming: Building Powerful robots", "thread_contents": ["Here comes the Learning Path book for ROS: ", " from Packt", "This Learning path is a combination of three published books in ROS to learn the technology from a single source.", "This book was released in last month.", "\n", "Herer are the links to buy it", "Packt: ", ": ", "Orielly: ", "Number of Pages: 1396", "Packt is having a sale.  The book is $10.   That is an insanely low price and no DRM so it works on any e-reader.   For $10 you get all three formats ePub, Kindel and PDF.  (a paper version costs more, if you want that)", "I have a copy.  This is one of the best books out there.", "(no connection with publisher or author, just wanted people-to know about the reduced price.)", "thanks Chris I will go for it.", "This sounds amazing and being cheap as I am, I\u2019ve got excited about this offer and decided to order. It all worked fine, but for some very bizarre reason I was also charged shipping of 11.90 USD which is very confusing in case of ebooks, which I have obviously just downloaded.", "\nI have contacted Packt and I\u2019m sure we will sort this out. And I\u2019m sure there is no bad intent. But maybe just double check before you pay.", "Please ping me if you are not getting any reply from PACKT. I think I can help you regarding the same.", "That\u2019s very kind of you. I am sure it will be alright though. It\u2019s probably just a misunderstanding.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-programming-building-powerful-robots/5059"}
,{"title": "ROS Bouncy Bolson Tshirts Available!", "thread_contents": ["Ordering will be open until July 9th. Order now!", "With each release of ROS we have a tradition of having a logo and making t-shirts. ROS Bouncy Bolson is coming out soon! To let you show your ROS colors and support this tradition, we have setup a Teespring Campaign in both the US and the EU. Note that both these campaigns can ship worldwide.", "Since this is the first public announcement, here\u2019s the full graphic for Bouncy Bolson.", "Thanks to everyone who\u2019s been helping prepare the Bouncy release. We\u2019re looking forward to the release this month. The shirts are expected to arrive in July.", "Slightly off-topic, but do you have any plans to get ROS stickers made? The link for \u201cgeneric ROS sticker\u201d ", " doesn\u2019t work anymore. I\u2019d love to get a new t-shirt ", " a sticker!", "We would like to provide stickers for order. Unfortunately with Sticker Mule discontinuing their marketplace services we no longer have a fulfillment service that can provide both high quality at a good price.", "If anyone knows of another service we could test out, I\u2019d be happy to investigate.", "Would it be possible to provide shirt options for those of us that are tall?  Specifically, in tall sizes. Nobody wants to see my tummy.", "Would it be possible to provide shirt options for those of us that are tall? Specifically, in tall sizes. Nobody wants to see my tummy.", "Unfortunately I don\u2019t think that there are any additional sizing options I can turn on through the interface. We did add a kids style (Bouncy seems apt for them ", " ) but I don\u2019t find a tall style we can add.", "I\u2019ve recently run across ", " in the last year. They have a variety of merch application options and I think the sky is the limit for ideas! ROS Bouncy throw pillows anyone? ", " ", "Anyway, just an idea to add to the \u201cwhere to make stickers\u201d pile!", "Thanks for all you\u2019re doing to make ROS awesome and fun!", "Here\u2019s a link\u2026", "Nobody wants to see my bouncy bolson ", "Any chance I can get the source file and get my own made?", "I am a Chinese and I can\u2019t find any way to get this T-shirt.help me please ,thx", "I am a Chinese and I can\u2019t find any way to get this T-shirt.help me please ,thx", "+1. I believe there are much more same needs from China.", "There is a typo on the anouncement, stating that the sale ends in ", " 9th instead of July.", "There is a typo on the anouncement, stating that the sale ends in ", " 9th instead of July.", "Thanks for the catch. Fixed above.", "I am a Chinese and I can\u2019t find any way to get this T-shirt.help me please ,thx", "I believe that both fulfillment options from Teespring will ship globally. Both options list international shipping rates. ", " and ", "It\u2019s not useful right now but Teespring ", ". Hopefully we\u2019ll have more options in the future.", "As much as I\u2019d like the shirt, TeeSpring punishes you for being big.", "\nI\u2019m a 2XL by build, muscle and fat.", "\nIt does not take $10 more in work or material to make a larger shirt.", "\n$3 or $5 more, I\u2019d be okay with that. $10. Silly.", "Hopefully next release you can find a vendor who doesn\u2019t pull these shenanigans.", "Thanks,", "\nD", "And what about releasing a DIY version of the stickers? I think many copy shops around can print to the sticker sheets, and cutting the stickers can even be done manually ", "As much as I\u2019d like the shirt, TeeSpring punishes you for being big.", "\nI\u2019m a 2XL by build, muscle and fat.", "\nIt does not take $10 more in work or material to make a larger shirt.", "\n$3 or $5 more, I\u2019d be okay with that. $10. Silly.", "I only see a $2 difference for the 2XL shirt in the US:", "Hoodies too:", "\n", "And no upcharge in the EU:", "\n", "Please ping me directly if you\u2019re seeing different prices and I can iterate with Teespring.", "And what about releasing a DIY version of the stickers? I think many copy shops around can print to the sticker sheets, and cutting the stickers can even be done manually ", "If you\u2019d like to make your own that\u2019s fine too. We have a repository with most of the artwork available here for non-commercial use:", "Artwork and Logos for ROS. Posters and stretched canvas prints are available at: https://www.zazzle.com/OpenRobotics - ros-infrastructure/artwork", "We generally want to make sure to be producing high quality stickers that will be crisply printed, stand up well on surfaces in daily use, and not fade over time. A lot of the vendors we\u2019ve tried so far have not been able to deliver those requirements so we\u2019ve held back from distributing them.", "Well, that\u2019s interesting. Seems like it was corrected.", "Thanks,", "\nD", "As a brief reminder there\u2019s just under one week to order your Bouncy Tshirts. Ordering will be open until July 9th.", "Today is the last day to get your Bouncy Bolson shirt, there\u2019s just 8 hours left to order!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-bouncy-bolson-tshirts-available/5139"}
,{"title": "Link to PCL seems to be hacked?", "thread_contents": ["Dear All,", "I don\u2019t know if any of you observed this or if this is a ", ". I just googled point cloud library and on clicking the link that used to be for PCL before leads me to an alert about a virus. This happened on a Mac and then also checked on a Linux distribution on a colleague\u2019s laptop.", "Just thought of sharing here so that PCL maintainers/developers can have a look.", "Thanks!", "A few days ago I did not load the page. but today he already charged me. And I realized that they were the page a little main page. Maybe that\u2019s why.", "Thanks for the ping, this should be fixed already, see: ", ". If not, please open an new issue over there.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/link-to-pcl-seems-to-be-hacked/5876"}
,{"title": "Looking for Kobuki power supply / battery charger", "thread_contents": ["I have a Turtlebot2 / Kobuki base but no charger for it. I\u2019ve searched online and can\u2019t seem to find any for sale anywhere. If anyone has a link to where I might purchase one, I\u2019d appreciate it.", "Thanks.", "Hi Matt,", "I looked at the spec sheet and you need an Input: 100-240V AC, 50/60Hz, 1.5A max; Output: 19V DC, 3.16A power supply.", "Here\u2019s an example on Amazon: ", "As for the barrel connector, I measured mine at 5mm. The one linked above is 5.5mm, which I assume would also probably work with a little tough love.", "S", "Thanks ", ", I\u2019ll give that one a try.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looking-for-kobuki-power-supply-battery-charger/8754"}
,{"title": "Step for large file upload request", "thread_contents": [" encourages using ", " as a public storage for larger files (1+MB), and request for upload is supposed to be the email sent to ", ".", "\nI feel like not spamming this mailinglist with a simple uplaod request. So communication on a github ticket somewhere might be better.", "\nOn the other hand, readers may appreciate being notified for new files (e.g. ", " files).", "Thoughts?", "We can\u2019t give you permissions to upload yourself to ", " since we don\u2019t have any fine grain permission system. I am happy to upload the data for you if you can provide either some short term links where I can download them from or send them to me by email.", "Is there some way we could exploit some cloud storage solution for this? Perhaps Github LFS?", "The data hosted on ", " is already \u201ccloud storage\u201d, it\u2019s a replicated geographically mirrored ftp site. It\u2019s paid for by OSRF, and in general we\u2019re committing to anything that we post there will be available as an archive for the life of the codebase. Since the older revisions of the code will stop working if we take down older versions of packages.", "I\u2019ve updated the wiki page to point to this category instead of the old ros-release mailing list. But I\u2019d suggest that we not setup new infrastructure. The volume is low and it is good to have full visibility of the changes. Unless we start getting overwhelmed with requests I\u2019d suggest we just keep the current system.", "See ", " for the continued conversation.", "Reason I suggested using some \u201ccloud storage\u201d file sharing solution is that they typically do have the \u201cfine grained permission\u201d systems in place that ", " mentioned.", "I\u2019m all for re-using whatever is already there, but I\u2019m also a fan of delegating management of such systems, as that would reduce the workload of whoever is in charge of it now. I\u2019m sure you have better things to do than to manage a bunch of bag files.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/step-for-large-file-upload-request/1250"}
,{"title": "Autoware.Auto Autonomous Valet Parking hackathon and demonstration - April, 2020", "thread_contents": ["The Autoware Foundation is planning a significant hackathon for Autoware.Auto in April, 2020. This post contains the details.", "For up-to-date information about the hackathon and demonstration, ", ".", "The goal of the hackathon is to ", " the Autonomous Valet Parking use case functionality and finalise getting it working on a real vehicle.", "Because it is not feasible to be implementing self-driving functionality within four days, the intention is to have the necessary functionality in Autoware.Auto completed before the hackathon starts. A simulation of the demonstration location in the LGSVL simulator will be used for testing and development by most developers prior to the hackathon.", "The hackathon will take place over five days in April, 2020. The first four days will be dedicated to finalising the demonstration and fixing any remaining issues in Autoware.Auto. The final day will be the demonstration itself.", "The hackathon will be held at Apex.AI\u2019s office in Palo Alto, California. The demonstration itself being held at the AutonomouStuff parking lot, which is about ten minutes\u2019 (manual) drive away.", "The hackathon and demonstration will be performed using Apex.AI\u2019s Lexus. This SUV has two Velodyne 16s mounted front and back for localisation and obstacle detection, an advanced GPS/IMU sensor, and a Nuvo computer.", "Please vote for the week of April, 2020 you prefer the hackathon to happen in. The dates for the hackathon will be chosen based on which week is most popular, weighted by attendence of key people.", "This poll will close on the 20th of November. Please vote before then.", "\n", "\n", "\n", "Just a clarification from AutonomouStuff: we have just completed the ROS2 port of our PACMod ", " but SSC (the Speed and Steering Control) software stack has yet to be ported. We are currently discussing this effort internally.", "Thanks for the clarification. I\u2019ve updated the post.", "Just to clarify a few things:", "Is there a registration link?", "\nAre there ", "?", "\nAre there any needs for coordination ahead of time?", "Is there a registration link?", "There is no registration link (yet?) because we\u2019ve only just started organising it.", "Are there ", " ?", "There might be beer if we actually get the demo to work? Perhaps Dejan\u2019s people will give a BBQ again! But in general we haven\u2019t thought about prizes and I\u2019m not sure what we could offer them for. Open to ideas! ", "Are there any needs for coordination ahead of time?", "I\u2019m not sure what kind of coordination you are talking about, but in terms of organising things, ", " and myself are on that. If the OSRF wants to contribute, we are very happy to have the help!", "I\u2019m not sure what deployment tooling there already is but I might be up for building an USB ISO ", " for the hackathon.", " thx. However we are talking here about the ROS 2 project. I see that ", " is about ROS 1.", "For the Autoware.Auto we currently use:", "If you think that you can simplify above and are willing to be in charge of the whole computer installation during the hackathon - you are more than welcome to join us.", " et al, couple of updates on above:", "LMK if there are more questions.", "I would like to follow our original 5-step plan up to the point where it stops making sense for the demo. This is:", "Obviously ", " doesn\u2019t make much sense for the hackathon\u2019s goals because we don\u2019t have access to a multi-level carpark. However I think trying for 1-4 in order, planning to achieve at an absolute minimum ", " and aiming to achieve ", ", makes the most sense.", "I think we should try to knock each of these off in the simulator and then bring the one with the highest chance of success to the hackathon.", "Personally I won\u2019t be satisfied unless we can achieve ", ", and I think it is possible.", "Chris and I provided a high level architecture diagram. We plan to refine that in the next week or two and present it in either Autoware or Maps WG meeting: ", "That\u2019s a good high-level block diagram, but it needs a lot of work before we can use it as an architecture diagram. We need to identify specific interactions (such that we can design the interfaces: data flows, service calls and actions); we need to understand data rates; we need to identify execution timings; we need to identify the safety architecture; we need to understand which nodes we can colocate and which should be in separate processes; etc.", " I agree with above. Sorry that I forgot that we defined this already before.", "Further information about the hackathon and demonstration can be found at ", ".", "The AWF is also going to hold a planning workshop on the 10th of December. ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The Velodyne driver will be the one developed originally for Autoware.Auto by Apex.AI and being pushed to the ", " repository for ROS 2.", "Localisation will be performed using the deterministic NDT implementation currently being proposed for Autoware.Auto by Apex.AI.", "Obstacle detection will be based on the euclidean clustering implementation present in Autoware.Auto.", "Planning will use a new behaviour-tree-based planning architecture provided by Tier IV.", "Motion control will use a deterministic implementation of MPC provided by Christopher Ho and Apex.AI.", "The vehicle interface will be provided by AutonomouStuff, who have just recently completed a ROS 2 version of their PACMod driver and considering a port of their Speed and Steering Control software stack.", "The map format used will be OpenDrive.", "Safety will be achieved via an emergency stop, due to the low speed.", "April 6 to April 10", "April 13 to April 17", "April 20 to April 24", "April 27 to May 1", "April 6 to April 10", "April 13 to April 17", "April 20 to April 24", "April 27 to May 1", "ubuntu/debian installer with the preseed file", "ansible role to install additional packages and scripts", "\n", " with the associated volume versioning ", " to install the right version of Autoware.Auto", "We talked to the main players for the hackathon and got commitment for the above \u201cPlanned software\u201d section.\n", "The change is that the Global Motion planner can also be done by Embotech instead of Tier IV. In the next TSC meeting we should get a confirmation if Tier IV will contribute something to this hackathon or not.", "LG is committed to provide the LGSVL simulator support", "AutonomouStuff full commitment to  ROS 2 version of Speed and Steering Control software stack is still pending but it is also likely yes. They already have ROS 2 version of the ", ",  ", " and ", "\n", "Map WG is also super active ", ", they have AutonomouStuff ", " but we were not yet able to talk to them yet", "We still need more people to provide\n", "tooling (rosbag record scripts, rosbag hosting, rviz config files, debugging, GUI)", "integrations (launch files, automated and manual testing, \u2026)", "NDT Pose and Odom fusion to obtain a vehicle state", "\n", "\n", "Chris and I provided a high level architecture diagram. We plan to refine that in the next week or two and present it in either Autoware or Maps WG meeting: ", "\n", "\n", "Several people approached me with the question \u201cWhat exactly do we plan to do/show?\u201d. I would like to see the following in order of preference:\n", "Lexus starts on point A (in the lane somewhere in a car park). Operator (back seat) gives it a target point B (in a parking stall somewhere in a car park):\n", "Lexus drives the whole path with dynamic obstacles (other cars, pedestrians, cyclists)", "Lexus drives the whole path with other static cars (parked on the lanes and in the parking stalls)", "Lexus drives the whole path on an empty parking lot", "\n", "\n"], "url": "https://discourse.ros.org/t/autoware-auto-autonomous-valet-parking-hackathon-and-demonstration-april-2020/11399"}
,{"title": "ROS2 Composition", "thread_contents": ["I\u2019m new to ROS2. How Can I spin the node using composition techniques. In the case of publishers, examples shows use of timer to bind the callback function. Is there any other way of continuously sniping the node without binding it to timer.", "In composition example.:", "\n// Use a timer to schedule periodic message publishing.", "\ntimer_ = create_wall_timer(1s, std::bind(&Talker::on_timer, this));", "My use case application is creating a video/image (OpenCV) publisher.", "Regards,", "\nMJay", "In the composition case none of the nodes is in charge of the main function / main thread. Therefore you either have to use a timer to get notified in regular intervals or start your own thread (with all the implications for locking / thread safety).", " Just a query regarding future composition related work\u2026Will there be anything like the onInit() function (like that of nodelets) in future ros 2 releases\u2026Or will all composed nodes in ros 2  be initialised at the constructor of the class ??", "You might want to consider to use a managed node. The current implement matches the design document which describes how the life cycle of managed nodes works: ", "There are currently no plans for changing anything in the life cycle itself. The configure step might be the right callback to perform your initialization.", "Thanks ", ". I was wondering how composition would support runtime polymorphism (like those supported by pcl_nodelets in pcl_ros) if all initialization were done at the constructor , instead of the the virtual onInit() function given in the nodelets.", "I was wondering how composition would support runtime polymorphism (like those supported by pcl_nodelets in pcl_ros)", "I am sorry - I don\u2019t understand the question. Maybe you can elaborate on it and provide more context.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-composition/2469"}
,{"title": "ROS only simpler", "thread_contents": ["Hello All,", "I apologize if this is not the right forum for this, but I recently started on a project that will use ROS2 for IPC and inter-container communication.", "For a little background, I have been a C++ developer for 20 years, and a C developer for 10 years before that. I have used MQTT on other projects, and fully appreciate what ROS2 offers.", "My first impressions of ROS2 have not been good. Don\u2019t get me wrong, I understand what ROS does, how it does it, and believe it is the right tool for the job, and in spite of the on-going difficulty in using ROS2, we are committed to using it.", "Our trouble stems form the build environment that ROS2 dashing has. I have built the basic tutorials at ", ", and after some trouble, they compile and all is well. The next step in my journey is to incorporate custom messages, and here is where the story has become unhappy. I started following the tutorial at: ", ", and the rails came undone. This tutorial fails to build with cryptic errors. One of our partners put me in touch with their ROS/ROS2 expert, and he is helping me work through the build environment issues. We will solve these issues, but this is pain I can do without. My first MQTT project worked in 30 minutes from start to hello world (with my own custom payload). I have so far invested 2 days of my time and at least a few hours of others time, and still do not have a working basic example of a custom payload with ROS2, in fact, we have so-far been unable to get the above tutorial to run on my development machine.", "From a new dev perspective, I have a few simple suggestions to hopefully help others who may follow in my steps:", "First, The existing build environment is far over complicated for a simple tutorial. As noted, I can\u2019t get the example programs to run because the build environment is broken. This is almost certainly not related to anything that is core to ROS2, and could easily have been avoided. The tutorials should include only a few files, and perform the command line compile by hand (avoid automated build tools: that\u2019s like taking a bazooka to a fly). For a good example of how a tutorial should be: see ", ". Notice that this tutorial requires only gcc, and nothing else, it distills the example down to its most basic form. A similar tutorial for ROS2 would require only the gcc compiler and the translation script that generates the C and C++ header files for the custom messages.", "\nSecond, document the translation scripts as standalone programs so that people who wish to use their own build environment do not have to reverse engineer yours just to extract the custom message translation mechanism. You have a good build system for complex projects (at least I assume it is good as I don\u2019t have it working yet), but it is grossly overkill for most projects, especially learning projects for people who are new to ROS, but have lots of experience as developers. I am in no way suggesting that your build system should be changed, just that the option to skip the build system be supported for those that don\u2019t need/want it, and for the sake of simple tutorials.", "This tutorial fails to build with cryptic errors.", "I know its not the point you\u2019re trying to make but: can you tell us the \u201cwhat was wrong\u201d and \u201cwhat was the error you got\u201d with this one? Documentation can always be easily updated with your experiences ", ". If you find something confusing, others would too. Be the change you want to see, and all that.", "Anyhow, to try to repeat back to you my understanding of your core issue:", "Those are actually very different points than I would have expected someone new to ROS2 to make about the tutorials. That perspective is interesting. This is the right place to share it, thanks for getting  a conversation started.", "I would not consider the custom message tutorial to be on the basic-first-look for some of the issues you bring up. It is a little involved, but if you were to be going through the tutorials in order, the only new macro here is ", ". The other ", " things you would have run into already to get a feeling for in several tutorials before this point.", "A similar tutorial for ROS2 would require only the gcc compiler and the translation script that generates the C and C++ header files for the custom messages", "Those do a bit more than just that, but I understand the sentiment. I have a feeling that someone more knowledgeable than me will come along to explain why it would likely be more challenging to compile from a commandline for new users.", "My higher-level opinion is that we functionally never do that.  Reinforcing the tools and teaching habits from the get go is important. If you tell someone how to compile it with a commandline, you\u2019re going to need to then make another tutorial about how to do it \u201cfor real\u201d. Most users are going to be using ament/colcon to compile their systems, something I\u2019d estimate in excess of 95%. Each additional reference of these tools someone can learn from has substantial value for reinforcing ideas. However, what doesn\u2019t have value, is if they are poorly explained, presented, or ordered in tutorials causing the experience that you had.", "I think that experience has alot of value and we\u2019d all learn alot from a proposal from you about how to better explain these concepts and support new users (ie to move around the tutorials, explain some concepts better, or give more examples presented in another way).", "In addition, adding an advanced tutorial explaining how to compile code without these tools could have value to an experienced developer like yourself that would rather not use the provided tooling.", "Those 2 action items don\u2019t resolve the specifics of your comments, but I think they resolve the spirit of where we can do better so that your comments won\u2019t happen to new users because the build environment will be better presented and working out of the box.", "I recently started on a project that will use ROS2 for IPC and inter-container communication", "The catch here, especially when comparing with MQTT, is that ROS is so much more than just IPC.", "There was quite a bit of discussion about the build tool recently, which might help explain why things are the way they are. There is room for improvement, especially in the documentation, but we do have reasons for why things are like this.", "If you are having problems, then I second the comments by ", ": Seeing those errors and hearing about the detail of the problems will go a long way to helping us find out specifically where we can improve.", "While I agree largely with what ", " and ", " have written in their replies, I just wanted to make sure that ", " doesn\u2019t get the impression we\u2019re trying to ignore his comments or dismiss them with an appeal to tradition (ie: \u201cthis is just how we do things here\u201d).", "The thread ", " linked to was the first thing that came to my mind as well, and I believe there are some good posts in it that ", " may want to read.", "I recently started on a project that will use ROS2 for IPC and inter-container communication", "The catch here, especially when comparing with MQTT, is that ROS is so much more than just IPC.", "This is true, and may not be apparent when starting to work with this infrastructure.", "I just wanted to make sure that ", " doesn\u2019t get the impression we\u2019re trying to ignore his comments or dismiss them with an appeal to tradition (ie: \u201cthis is just how we do things here\u201d).", "Absolutely. We know the tutorials are lacking, but there are so many tutorials and so much stuff we need to cover so hearing from ", " what specifically are the errors and problems will help us improve things.", "Yeah, I agree with the sentiments above.  I don\u2019t think it is a good idea to introduce something that is simpler, only to then backtrack to using the \u201creal\u201d tools.", "But I do agree whole-heartedly that the tutorials are lacking, and making things \u201cjust work\u201d is an important goal for us.  So I would also like to know the particulars of your trouble, and how we can improve the build tools/code/documentation so that the next newcomer doesn\u2019t run into them.", "Follow up", "Good news! I was able (with the help of one of our local CMake experts) to dig back to the root cause of the problems I was having with the build environment.", "At one point, I had been working on just one of the included packages, and saw the CMakeLists.txt, and naturally thought to run cmake . followed by make. This worked fine for that 1 specific compile (except for build files being put in funny places), but after that, cmake had cached the build settings which conflicted with those that colcon was trying to use, and cmake just ignored the colcon setting to use the cache. To fix the problem, all that needed to be done was to clear the cmake cache (the cmake equivalent of make clean).", " shared a link to a general discussion topic about moving ROS2 over to a pure CMake environment.I still contend that for a tutorial, the build environment is a distraction, however pure CMake would help to improve the situation a lot. CMake is quickly becoming the defacto standard for many platforms, and as such, would be easier for some of us to transition to. (Disclaimer: I work for Kitware).", "I appreciate all of the responses, and to be clear, I am not necessarily expecting any particular action, and I am absolutely not suggesting that the default for ROS2 be anything other than the build environment as you have built it. For most (all?) new projects, I think that the current default build environment makes reasonable sense. That having been said, it is always a good idea to give users more flexibility when you can, especially developers, and I can think of several cases where the single build environment would be more hindrance than good. The first and most obvious is for a legacy project where they want to add ROS2. That project already has its own build environment, and porting that entire project to use colcon would be prohibitive at best, and would most likely make the change a non-starter. A second example would be conflicting with other projects that have incompatible build requirements. For example, incorporating anything involving GPGPU processing or multi-language environments generally already have their own build systems in place that, in some cases, do not play nice with CMake. Merging ROS into such a project would be a royal PITA. Granted, these other projects are as much or more at fault for picking a non-standard build system, but I think you see where I am going with this. Another use case is for embedded systems, where one of the packages in a system belongs on an embedded device. Due to the nature of cross compilation, it is almost always easier to have a standalone build system for the cross-compiled parts. As you can see, it is easy enough to keep separate workspaces for each of these examples, but under these examples, the colcon build system provides no real value added.", "One thing I do want to be sure to convey is my appreciation for the documentation.  In my experience good quality documentation is hard to write and hard to find, but the plethora of ROS/ROS2 documentation has been a happy exception to that norm. Being able to find most of the information I need without resorting to google-fu has been very refreshing.", "Thank you again for the wonderful replies. You have built a really powerful tool in ROS/ROS2, and it is clear to me that no matter what decisions are made, there are dedicated and capable people working to make ROS2 better, and that is what really matters.", "That having been said, it is always a good idea to give users more flexibility when you can, especially developers, and I can think of several cases where the single build environment would be more hindrance than good.", "This is an interesting statement, because it is actually the opposite of how I think about the ROS build tools.", "One point that I keep coming back to (and maybe not explaining well) is that tools like Make, CMake, SCons, etc are inherently single-project tools.  That\u2019s fine, that is their goal.  But ROS is set up in a much more federated fashion, where you have many projects that you combine in different ways to achieve your goals.  So while I wouldn\u2019t stand in the way of anybody trying to improve the documentation and tools for the single CMake case, in my mind it just doesn\u2019t fit in well with the federated nature of the ROS ecosystem.", "That being said, I\u2019m glad that you solved your problem.  One thing we could add to the documentation is to make sure that your CMake environment is cleaned up before trying to build.  For the record, what were the commands/actions that you had to take to clear our your environment?", "For the record, what were the commands/actions that you had to take to clear our your environment?", "I restored the package directory to the pre-cmake state. In my case, I accomplished this by deleting the package directory under src, and replaced it with the original from the tutorial. When I ran cmake from the package directory, it built all of the files and directories that colcon would have placed under /build, and put them in the local package directory instead. I had to remove those files (which included the cmake cache information). I probably only needed to remove the CMakeCache.txt, Makefile and the CMakeFiles directory, but it was easier to restart fresh in my case.", "My higher-level opinion is that we functionally never do that. Reinforcing the tools and teaching habits from the get go is important. If you tell someone how to compile it with a commandline, you\u2019re going to need to then make another tutorial about how to do it \u201cfor real\u201d.", "Python is a good example where they do this poorly. The usual intro tutorial is just how to print hello world to the terminal, it explains nothing about importing packages or pip or setting up requirements.txt and setup.py, which is of course how you do it \u201cfor real\u201d", "I hope you\u2019ll forgive the sarcasm but I thought it was the most effective way to get my point across.", "Most of arguments against making ROS simple enough to be able to have simple tutorials center around \u201cbut ROS is federated\u201d, which is actually a sort of catch-22 because the build system enforces this federated model and when someone like OP tries to work outside of it, they get bit.", "I\u2019d argue that Python is also \u201cfederated\u201d, in the sense that there\u2019s a vibrant package ecosystem (which is one of the greatest things about Python and about ROS as well). I think Python/pip could provide a good model for how to do ROS packages and one of these days, when I find the time, I plan to work on a proposal for how ROS packages could work in a more pip-like way that is both friendly to new users but also decipherable for those who want to dive in \u201cunder the hood\u201d.", "That project already has its own build environment, and porting that entire project to use colcon would be prohibitive at best, and would most likely make the change a non-starter.", "You mixing two completely separate parts of the process: the build system (CMake, Python setuptools, etc.) and the build tool (colcon) [see ", "]:", "And as usually you can always mix-and-match: build some packages with colcon, then some manually (however you like), and then another set using colcon.", "Due to the nature of cross compilation, it is almost always easier to have a standalone build system for the cross-compiled parts.", "I don\u2019t think this statement is true. When you want to cross compile any project which consists of more than a single monolithic piece you are facing the exact same challenges as for a native build. Therefore the benefits of a build tool apply just the same. Coincidentally just yesterday a tool to make cross compiling was announced ", " which internally uses ", " for that very reason.", "because the build system enforces this federated model", "See my comment above which clarifies that the build system only has knowledge about a ", " package. So by definition can\u2019t have anything to do with the federated model which means dealing with ", " packages.", "And by design the build tool is optional to use. You are able to build all the packages of a ROS distribution just fine without a build tool. You \u201cjust\u201d have to manually figure out in which order packages need to be build and how to build each individual package. So I hardly see where we enforce the build ", " anywhere (except that doing that kind of work manually sounds like a huge amount of wasted time to me since it can be easily automated).", "I think Python/pip could provide a good model for how to do ROS packages", "I think this comparison isn\u2019t actually applicable. Afaik there is no way in Python to build and install a set of packages where you have the source code in a local directory - and especially do that by honoring their inter-dependencies.", "Great discussion!  I just started ROS2 in November, I didnt have any ROS1 experience. I did find the build system complex even though I am an expert in cmake, but colcon is now working for me too and I like it\u2026just takes time. I struggled like OP\u2026let\u2019s face it ROS is a big ecosystem and requires perseverance to master. I took a udemy ROS2 course, and just kept at it. I feel at ease with the build system now but had the exact same issues as OP on my first custom msgs build. I love cmake and now after only 2 months I really love ROS2\u2026such an improvement over ROS1. I would say keep colcon in the mix, but second improving documentation and tutorials especially given in progressions. Should be spoon fed a bit in the beginning. API-like tech docs can get overwehelming for newbies (like I was).", "I am not necessarily expecting any particular action", "See, but I ", " want actions. I think you bring up a good point in things not being as clear as they should be. Most (if not all) of those tutorials didn\u2019t exist with I learned ROS2, so you have a unique perspective as not only a new user, but a competent software engineer. Most new users don\u2019t come in with 10+ years of experience to draw from ", " . I\u2019d think we would all very seriously learn alot from your stream of consciousness on how to improve the first experience, if we take colcon/ament as a given.", "But it sounds like you got yourself in this situation by going off the beaten path of the tutorials.", "This is an interesting statement, because it is actually the opposite of how I think about the ROS build tools.", "I agree with ", ". If you come from anywhere in C++ land prior to ROS, this stuff is a little weird to deal with. Once you own the fact that you\u2019re just going to use ROS for the long-haul ", ", its a worthwhile investment. But if you\u2019re just wanting to dip your toes in or you\u2019re really only trying to support 1 package, that perspective is very different. I don\u2019t want to argue on the technology components here, mostly because I think my opinion is under-educated, and partially because I don\u2019t really think its the point.", "Howdy, I am also a newer ROS2 software engineer, I started picking it up about 6 months ago. I just recently had to implement custom messages and I found the tutorial lacking in one specific area that I am suggesting gets added.", "The tutorial offers insight in creating a custom message within the same package, and also includes the option for interfacing with a message in an external package, however, the latter is coupled with the former. That means the cmakelists had both internal and external references for custom messages and it took a bit of extra work to determine what was needed for only referencing external custom messages. It would be useful to add an extra step that shows just that. I hope this suggestion was clear!", "Also, while I am here, I was wondering if anyone had a simple answer to whether or not it is possible to compose node components that are in different packages, or even further, different ros workspaces. I dug around and could not find an answer, which leads me to believe that it is not possible, especially with the c++ build and compiling requirements. I could be missing something though, so if anyone has an answer, I would appreciate it!", "Thanks,", "\nRobert", "I was wondering if anyone had a simple answer to whether or not it is possible to compose node components that are in different packages, or even further, different ros workspaces.", "That should work just fine. The only constraint I would see is that both shared libraries need to be compatible in terms of being loadable into the same process (e.g. not linking against the same dependency but with a different version which have conflicting symbols).", "I\u2019d argue that Python is also \u201cfederated\u201d, in the sense that there\u2019s a vibrant package ecosystem (which is one of the greatest things about Python and about ROS as well). I think Python/pip could provide a good model for how to do ROS packages and one of these days, when I find the time, I plan to work on a proposal for how ROS packages could work in a more pip-like way that is both friendly to new users but also decipherable for those who want to dive in \u201cunder the hood\u201d.", "While I agree with the sentiment of \u201cinstall the packages in your system and just use them\u201d, I think your comparison between pip and colcon has flaws:", "Best practice in Python these days (and in Ruby and a bunch of other languages) is to create a virtual environment for every project and every test execution and so on. This allows you to be certain about dependencies, not have to worry about what the system has installed, and keep things in general isolated. A virtual environment is effectively what a Colcon workspace is, in most respects, and this is typically how I use them - I have a bunch of ", " (and now ", " functions and aliases that I use to activate workspaces, including the ROS distribution in use, depending on what I\u2019m working on. It makes me just as happy to do this with C++ code as Python developers are with ", ".", "Python packages usually do not need to be built before you use them, they just need to be present. This means that you do not need to worry about ordering of builds beyond a simple depth-first approach used to grab depedencies from PyPI.", "pip only has to deal with Python, not C++ and Java and Lisp and protentially half a dozen other languages being treated as first-class citizens in a single virtual environment (or system).", "pip only works on a single package and its dependencies, not, as ", " said, a bunch of packages at once.", "Can you either submit a PR to the message tutorial or DM me a summary of what you think needs to be added so I can add it.", "I am also in the process of writing a \u2018hello world\u2019 tutorial, any feedback will be appreciated.", "Build Robot using Robot Operating System (ROS 2) and Gazebo - bunchofcoders/basic_bocbot", "You mixing two completely separate parts of the process: the build system (CMake, Python setuptools, etc.) and the build tool (colcon)", "This distinction feels academic. If there were tutorials or use cases where ament_cmake was used by itself, then it might make sense, but from an outside perspective it appears to be designed to work hand-in-glove with colcon as a single unit.", "And as usually you can always mix-and-match: build some packages with colcon, then some manually (however you like), and then another set using colcon.", "Is this your idea of a reasonable workflow???", "And by design the build tool is optional to use.", "Another academic point that\u2019s missing the spirit of the discussion. Saying the build tool is optional to use is like saying that using either Linux or Windows is optional. It\u2019s also not true, the build system doesn\u2019t generate the setup.bash file.", "I think this comparison isn\u2019t actually applicable.", "I meant it in the sense that if I wanted to learn how to write some Python code, I can go learn how to write Python code. If I later want to publish that code, I can learn how the packaging system works. With ROS, you can either learn how to package the code first and then how to write it, or learn them simultaneously. This is the main criticism, that the package and build system/tool get in the way of writing code, and that perhaps there\u2019s another approach where you can write the code and package it too without having to mix these two things, like the way they do it in Python. Obviously I\u2019m not literally suggesting that ROS move to the exact Python/pip model, just pointing it out as a potential inspiration for a potential rethink to the package system. While ROS has a system that works and is very popular, I think it could do more to get out of the user\u2019s way and I think this would be beneficial to ROS in the long term.", "This distinction feels academic. If there were tutorials or use cases where ament_cmake was used by itself, then it might make sense, but from an outside perspective it appears to be designed to work hand-in-glove with colcon as a single unit.", "It\u2019s just following the ", " - one tool is doing thing - and only one thing: the build system processes one package, the build tool invokes the build system for each package in the needed order. Also since ", " is only providing helper functions to make writing CMake project more convenient it can be used like any other CMake project - which is a very well established and documented process.", "It\u2019s also not true, the build system doesn\u2019t generate the setup.bash file.", "You are wrong about this. ", " packages actually do generate setup files for the package they build. You can find them in the install prefix under ", ". Obviously they only setup the environment necessary for that package. The rational is also straight forward: multiple packages shouldn\u2019t try to write the same setup files in the root of a workspace - that was e.g. a major issue for ", " in ROS 1. Instead a single package - namely ", " - is responsible to produce these prefix-level setup files for e.g. the Debian packages (in which process ", " is not involved at all).", "Obviously I\u2019m not literally suggesting that ROS move to the exact Python/pip model, just pointing it out as a potential inspiration for a potential rethink to the package system. While ROS has a system that works and is very popular, I think it could do more to get out of the user\u2019s way and I think this would be beneficial to ROS in the long term.", "Please consider proposing specific improvements rather than talking in generalities. Otherwise it is just not very likely that something can / will be done about what you would like to see improved.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You believe that the build environment is complex", "Because its complex, some of  the tutorials are unclear", "The  build system is in charge of processing a single package. In the case of CMake that usually boils down to using it as ", " / ", " / ", ". The fact that ROS packages use CMake helper functions provided by ", " are actually not relevant for this since they are an implementation detail when you look at how to invoke the build system. You can achieve the same with plain CMake - it will just take you more lines of CMake code.", "The build tool ", " only determines in which order a set of packages needs to be built and what build system a package uses in order to decide how to build that package. ", " is very modular, if you want to use a build system which isn\u2019t supported atm you can easily write an extension to teach colcon how to invoke that build system. E.g. CMake support is just an extension for colcon which implements the invocation logic mentioned above.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-only-simpler/12619"}
,{"title": "ROS 2 TSC Meeting Minutes: 2019-09-19", "thread_contents": ["Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees\n", "Brian Gerkey, Dirk Thomas, Louise, Tully - Open Robotics", "Jaime Martin Losa - eProsima", "Joe Speed - ADLINK", "Toffee Albina - TRI", "Kyle Fazzari - Canonical", "Lyle Johnson - Apex.AI", "Matt Hansen - Intel", "Pyo - ROBOTIS", "Steve Macenski - Samsung Research America", "Adam - AWS", "Karsten - Bosch", "Brian Shin - LG", "Geoff - Tier IV", "Sean Yen - Microsoft", "Jerry Towler - SwRI / GVSC", "\n", "Old business\n", "[Gerkey] ROS Trademark update\n", "Moving forward slowly.", "\n", "\n", "New business\n", "[Gerkey] ARM stepping back from TSC\n", "Thanks for their contributions to date!", "\n", "[2 mins] [Gerkey] Intro of new TSC members\n", "ADLINK: Joe Speed", "Canonical: Kyle Fazzari", "\n", "[2 min] [Joe Speed] interested in Edge AI working group to enable \u201cbetter user experience for HW accelerated ML\u201d? Will draft proposal with other TSC members.\n", "Interest from Tier IV, AWS, and Samsung, follow up afterward with Joe", "\n", "[M. Hansen] Manipulation WG ownership and MoveIt 2\n", "Plenty of interest; need someone to lead the group. No volunteers at this point; tabled until next TSC meeting.", "Potential goal: get MoveIt 2 ready for ROS 2 F-Turtle", "\n", "[A. Duncan] New Working Group \u201cROS 2 Tools\u201d\n", "We\u2019ll start broad and adjust/refine as we go", "Announcement on Discourse 9/20", "First meeting planned for first week in October", "\n", "[A. Duncan] Forthcoming proposal for ROS 2 Development/Release framework\n", "Currently pre-circulating in small cycles. Planning an out of band design review for the TSC.", "\n", "\n", "\n", "\n", "Apex.AI updates for ROS 2 E release\n", "\n", " => Update 2019-Sep-19", "Question: Apex.AI seems to be the only one actively updating the comments in ", "\n", "\n", "Bosch Updates\n", "Diagnostics released for Dashing\n", "contains self-test, diagnostics_updater", "diagnostic-aggregator waiting on cross-platform release for bond-core", "\n", "zero-copy-api\n", "first draft of a design doc in collaboration with Apex ", "\n", "development branch for reference implementation (zero_copy_api) PRs being opened ASAP.", "\n", "executor development\n", "joint collaboration with Nobleo ", "\n", "Nobleo has first draft of static executor (which allows to measure in return the computational cost for considering dynamic nodes by iterating over the corresponding weak-pointers over and over again in the rclcpp single threaded executor) ", "\n", "rcl LET Executor (developed in context of micro-ROS)\n", "\n", " ", "\n", "\n", "\n", "timing analysis\n", "tracetools could be used to analyze timing behavior: ", "\n", "\n", "\n", "Samsung Updates: Theme: Configurability\n", "Navigation2\n", "Created nav2_core plugin definitions for planner, recovery, controller. Still thinking about the best options for configurability in the behavior tree", "Created nav2 planner server / recover server to act as the action server for N plugins", "Factored out algorithms of the existing recoveries and planner to use new new2_core plugin definitions", "Other: Began porting Slam Toolbox to ROS2. Will be available shortly.", "Ported ROS2 Dashing for TEB local planner", "\n", "Robot localization\n", "Not yet released in Dashing, not really stable in ROS2 yet", "Working from the ground up to get all its upstream dependencies ported to ROS2 and released.", "\n", "\n", "ADLINK Updates: Theme: faster, smaller, more reliable ROS 2\n", "ROS 2 E contribs mostly ", "which is in ", "\n", "community suggested enhancements, e.g. ", "\n", "working on ROS 2 test coverage", "Linux, OSX, Windows, ", ". QNX late 2019 for RT WG", "built into ", " for Real-Time WG build farm", "DDS Security v1.1 in 4Q 2019", "Modern C++ DDS API for ROS 2 E release", "one user\u2019s experience ", " & ", ", there are ", "\n", "\n", " - ROS 2 E & Nav2 fixes ", ". DDS Security is WIP for community edition, is in commercial edition", "\n", "eProsima\n", "Fast RTPS 1.9.1 Release (end of this week)\n", "Bug fixing release", "Support for Windows, Linux, OSX, QNX, VxWorks, Android, iOS", "DDS Security", "X-Types", "Static Memory Allocation", "Non-Blocking Calls", "Discovery Server 1.0", "\n", "Fast RTPS Patch for WIFI (from Fast RTPS 1.8.1)\n", "More reliable discovery on multicast lossy networks", "\n", "Shared Mem - WIP - Open Source", "Modern C++ DDS API - WIP - Open Source", "Micro-ROS\n", "Hired key engineer to work on hardware layer", "\n", "\n", "Tier IV updates\n", "Still resource-constrained\n", "Hoping to have good luck at ROSCon JP next week\u2026", "\n", "Potentially will hire a company that specialises in hard real-time and deterministic embedded software to work on making the ROS 2 stack real-time-safe (especially memory-safe) and also look into adding executors/APIs for deterministic behaviour of systems)", "Considering hiring a new engineer as a full-time contributor to ROS 2", "\n", "Need to know who to talk to about getting Debian supported as Tier 2 in either Eloquent or F\n", "Point relevant people at Dirk (for Eloquent might relay to the ROS Boss in charge of that ROS distro which is Michael Carroll\u2026)", "\n", "TRI Updates\n", "Continuing to work with OSRC on the Eloquent items that we are addressing jointly", "Continuing to test updates/fixes of ROS 2 on robots", "\n", "ROBOTIS Updates\n", "TurtleBot3 for ROS 2 Dashing\n", "\n", "OpenMANIPULATOR for ROS 2 Dashing\n", "\n", "\n", "Canonical Updates\n", "Writing design documents for node IDL and its ramifications (including security)", "Completed ROS 2 port of teleop_tools", "\n", "Microsoft Updates\n", "Enabled VSCode ROS Extension for ROS2 on existing features and working on the debugging capability for ROS2\n", "\n", "\n", "LG Electronics Updates\n", "Updates to OpenEmbedded repo of ROS/ROS2 packages meta-ros\n", "Updated to match updated Dashing index", "Preparing to change structure or repo to split layers based on ROS distro", "Working on Turtlebot3 physical robot demo with OpenEmbedded OS (", ").", "\n", "Designing CI system for keeping meta-ros up-to-date with rosdistro index", "\n", "\n", "\n", "\n", "[K. Knese] Embedded\n", "Every 4th Monday of the month.\n", "Next meeting is Monday, 23rd September at 7am UTC (-> 9am MEST, 4pm JST)", "Minutes of last meeting: ", "\n", "Akihiko (eSOL) can help organizing meetings if local meetings are needed in Asia/Pacific. However, no participant was interested in supporting Borja (eProsima) and Ingo (Bosch) in organizing the WG itself", "\n", "\n", "[M. Hansen] Navigation\n", "Meets every Thursday 3pm Pacific\n", "Need to add to Google Groups / calendar\n", "Forthcoming integrated calendar / group system (from Tully)", "\n", "\n", "Integrated system test into upstream ROS build farm\n", "Working through issues found in low % of test runs (1-5%)", "Test now passing >95% rate", "\n", "Verified Nav2 using cycloneDDS", "Updated Nav2 to work with Cartographer", "map_server was re-written (Rover Robotics contribution)", "Adding plugin layer for local & global planners (nav2_core)", "Dashing update - 0.2.4 release", "Overall, 53 PRs were merged, 42 issues were closed in last 30 days\n", "\n", "\n", "[L. Johnson] Real time\n", "The WG met on August 21: ", ". Attendance around 15 people. Main items discussed were:\n", "Create an rmw implementation which works for a single process => ADLink volunteered to do this", "Tobias Blass presented his paper ", "\n", "William Woodal presented findings of memory audit in rmw, rcl and rclcpp", "Meeting recordings available here: ", "\n", "Andrei Kholodnyi volunteered to co-chair the group", "\n", "Next Meeting: Week of Sep 30", "\n", "[G. Biggs] Safety\n", "Still struggling to find a feasible direction", "There is growing interest in creating a catalogue of safety patterns and how to do them in ROS, so we may try and push in that direction", "There is also interest in creating some kind of safety architecture, but this is difficult. However, will not rule it out.", "Next meeting is on September 26th. Meetings are registered in the ROS WGs calendar.", "\n", "[K. Fazzari / A. Duncan] Security\n", "The WG met on September 17th: ", ". Attendance around 10 people. Main items discussed:\n", "Canonical will start chairing the Security WG", "roslaunch2 sandboxing\n", "\n", "Ramifications of breaking the 1-1 mapping between nodes and DDS participants", "\n", "\n", "\n", "Other:\n", "Make meeting 90 minutes?\n", "No objections. Brian to update calendar invite.", "\n", "Current alternating schedule working OK?\n", "No reasonable alternatives proposed", "\n", "Reminder: put your contributions and updates for Eloquent in the github ticket.", "Have an in-person TSC meeting at ROSCon: Brian to schedule", "\n"], "url": "https://discourse.ros.org/t/ros-2-tsc-meeting-minutes-2019-09-19/10817"}
,{"title": "New PCA9685 driver for ROS", "thread_contents": ["I wrote a driver for the PCA9685 I2C PWM driver chip. This chip is intended to be used with LED applications but is also used in a number of servo and motor controller boards, including a few Adafruit and Waveshare boards among others.", "A couple of notable features I added that I haven\u2019t seen done in other similar packages:", "Contribute to dheera/ros-pwm-pca9685 development by creating an account on GitHub.", "Hi dheera,", "\nThanks for sharing your package. I am new to ROS so I\u2019m still confusing after reading your readme file. You\u2019re using Int32MultiArray as the message type, but in your description, the subscriber take 16 values for each channel. How are those two related. If I want to write send command to this node, how do I define the values of label, size, stride, data_offset, and data respectively to specify the channel?", "\nThanks much,", "\nZhouqiao", "Hi Zhouqiao,", "\nThe code currently only reads the data field and expects 16 values. It doesn\u2019t actually read label/stride/data_offset, so it\u2019ll work without setting those, but it\u2019s probably most \u201ccorrect\u201d to just define it as a one-dimensional array of 16 values. i.e.", "dim[0].label = \u201ccommand\u201d", "\ndim[0].size = 16", "\ndim[0].stride = 16", "Thanks, works perfect.", "Hi Dheera,", "\nI have not been able to get this to work.", "\nI have the Adafruit servo board working outside of ROS but I need to make it work within ROS as this project does.", "\nI am fairly new to ROS but I do have several nodes working.", "\nI work in python.", "\nIs there something particular to get a cpp script working?", "The approach I took was to download your project in a catkin workplace and compile.", "If you could guide me on how to install your files in an existing project, that would be awsome.", "\nThanks.", "Marc Boudreau via ROS ", " \u65bc 2019\u5e7412\u670819\u65e5 \u9031\u56db \u4e0a\u53488:42\u5beb\u9053\uff1a", "Hi,", "Thanks for replying, I appreciate it", "Like I said, I am new to cpp nodes.  I did not realize that you do not place the file extension \u201c.cpp\u201d as you need to place the \u201c.py\u201d when using python.", "I now have the node running!", "However, I have yet to figure out the ", " nomenclature.", "I have tried variations of:", "rostopic pub Int32MultiArray/command  {data:[32767,32767,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]}", "but always get errors:", "Usage: rostopic pub /topic type [args\u2026]", "rostopic: error: no such option: -]", "Le jeu. 19 d\u00e9c. 2019 \u00e0 17:08, Dheera Venkatraman via ROS ", " a \u00e9crit :", "it works!", "\nThis makes me happy!", "Thanks for sharing your work!", "Le jeu. 19 d\u00e9c. 2019 \u00e0 17:08, Dheera Venkatraman via ROS ", " a \u00e9crit :", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The ability to update only certain channels in a ROS command, and not update all of them. This allows multiple ROS nodes to publish to the same command topic but take charge of different subsets of the 16 channels.", "A timeout. If a channel isn\u2019t updated within that time, it gets set to 0. This is useful to avoid servos burning out if your control logic crashes, and also as a heartbeat timeout in the case of motor controllers.", "Did you get a compile or run error, and what is the error?", "What hardware do you have the servo board connected to?", "Are you able to see the servo board using i2cdetect?"], "url": "https://discourse.ros.org/t/new-pca9685-driver-for-ros/8299"}
,{"title": "Autoware meeting at RosCon 19?", "thread_contents": ["Hi Autoware users, who is going to be at RosCon in Macau?", "It would be great to meet for 1h and start planning for the hackathon in Palo Alto in April 2020.", "Please reply to this message and we will post the date/place accordingly.", "D.", " from Arm will be there.", "I propose to hold a hackathon in Moscow. I will be at ROSCON.", "I\u2019ll be there and also join the real-time workshop tomorrow.", "Yo.", "(This post was less than 20 characters.)", "Yo , there, camping near the coffee machine.", " let\u2019s meet on Friday at 8 am at the ROSCon registration desk. We will discuss the work packages needed to make an autonomous valet parking demo possible in April 2020: ", " ", " ", " ", " ", " ", " ", " ", " would anyone from osrf want to participate too?", "Friday at 8 am is OK for me.", "Sorry just saw this. I can try to answer any follow questions if you guys point them out to me.", "I wil try to join this meeting maybe with some other guys from StreetScooter", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Hackathon in April, 2020 in Palo Alto at the Apex.AI office.", "Goal is to achieve full autonomous valet parking in the simulator before the hackathon, with the hackathon purpose being vehicle integration and polishing", "Hackathon will be 5 days\n", "4 days work on preparing the demo", "1 day for the demo", "\n", "We will use the existing Apex.AI Lexus with its current hardware and sensor configuration\n", "Nouvo board", "Two velodynes", "\n", "Will build on Eloquent, which in turn means Ubuntu for the OS.", "Tasks\n", "Someone needs to take charge of the simulation and recreate the Apex.Autonomy demo in Autoware.auto", "Someone needs to to take charge of the map", "ROS 2 interface to the vehicle", "Someone needs to be in charge of the architecture", "\n", "Fail-safe operation will be to just perform emergency stop", "Apex.AI can put out an engineer to lead the architecture work for two weeks full time. This person will design the architecture; Tier IV will help to write it up.", "Scenario-based design", "Map:\n", "Want to go to OpenDrive so we can use the same map in the simulator", "Run-time access API is lanelet2 or something customised from it", "We need a ros2 launch command that will just work", "Brian/Parkopedia can probably push this because they have an immediate need; the Autoware WG will provide need to the Maps WG, which will coordinate the work", "\n", "Simulator:\n", "LGE needs to make sure the simulator runs and provides the correct APIs", "\n", " , ", " and ", "  to talk about what is.", "\n", "Localisation:\n", "NDT is just about in Autoware.Auto", "\n", "Perception:\n", "Autoware.Auto has object detection sufficient for the AVP demo", "\n", "Planner:\n", "Tier IV will provide a new planning architecture", "Fallback solution is just choose and follow way points", "Will make a decision on whether to go with the fallback on 1 December.", "Embotech has a planning architecture for parking which may be useful. Apex.AI is talking to them.", "\n", "Controller:\n", "Chris/Apex.AI has an MPC controller for the Lexus", "\n", "Vehicle interface and low level controller\n", "AutonomouStuff? Dejan to talk to them.", "\n", "Other\n", "Logger functionality is very important, but secondary to the demo", "We need something comprehensive and tamper-proof", "\n", "Set meetings with: (", " ", ")\n", "AutonomouStuff (include ", ")", "Embotech (include ", ")", "LGE (include ", " and ", ")", "\n", "Run a poll to find the most-preferred week in April for the hackathon and demo ", "\n", "Set some intermediate deadlines ", " ", "\n"], "url": "https://discourse.ros.org/t/autoware-meeting-at-roscon-19/11263"}
,{"title": "Autoware v1.12-alpha.1", "thread_contents": ["The first alpha for the next version of Autoware, ", ", is now available for testing. We encourage all Autoware users to try it out and wring out any remaining bugs before the code-freeze date of June 24th.", "Get the source from ", " and install it using the ", ".", "Docker images are available at dockerhub:", "\n", "The following changes have been made since 1.11.", "The following changes planned for 1.12 have not yet been made (we expect to do a second alpha for these in the next couple of weeks). You can follow progress on the ", ".", "Following on from the first alpha, we now have alpha.2.", "GitLab.com", "The major change in this alpha is the move to GitLab and the split repositories. The new install instructions have not yet been written, but in a nutshell here is how to get and build Autoware 1.12.0-alpha.2 from source:", "\nIt seems like Google Chrome changes the file name to autoware.ai.txt when downloading ", " from above link. Content does not change so just change the filename after download. (Firefox worked fine)", "I created the feedback issue about 1.12.0-alpha.2 release based on our experiment on the real vehicle. Please check and let us know what do you think.", "\n", "We tested 1.12.0-alpha.2 release on the real car, Autonomous Lexus with 4 Velodyne LiDARs. Based on the results of this experiment, the issues to be solved as the fix merge...", "\n", "I\u2019ve updated the instructions to fix the issue that ", " mentions above. There are also instructions there for the upcoming 1.12.0-beta.1.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["MPC waypoints follower (", ")", "Fix setting files for rosbag demo (", ")", "Modify stop states (", ")", "Fix autoware launcher readme. (", ")", "Split drive state (", ")", "melodic support (", ")", "add battery charging state (", ")", "modify mssion state for fms (", ")", "add install to the resources and plugin.xml (", ") (Fixes ", ")", "Add tests around Openplanner utilities (", ")", "Support full functions of g30esli_interface (", ")", "Update Docker cross-build Image (", ")", "add predicting convex hull (", ")", "Fix Segmentation fault at velocity_set by asynchronous base_waypoints and closest_waypoint (", ") (", ")", "Fix gazebo not found in colcon_release_cross synquacer (", ")", "fix behavior_state in imm_ukf_pda_track (", ")", "runtime_manager: change params definition from int to float (", ")", "Fix don\u2019t show run time in runtimemanager ", " (", ")", "ekf_localizer (", ")", "[fix]: make sure cost is set with expanding_polygon_size (", ")", "Remove points2costmap (", ")", "fix install directive for waypoint extractor (", ")", "Adding install directive for node (", ")", "Fix bug that limits the data rate of DataRateCheckerPlugin (", ")", "Updated paths in quick_start launch files and parameters in default.rviz (", ")", "Remove enablePlannerDynamicSwitch (", ")", "Add .repos file for setting up an Autoware workspace using vcs (", ")", "Split repositories and shift to using ", " for install", "Clean up the quick start documentation", "Download the ", " file ", " and remember where you saved it.", "Run the following commands to download and build Autoware", "\n"], "url": "https://discourse.ros.org/t/autoware-v1-12-alpha-1/9396"}
,{"title": "Autoware 1.12 released", "thread_contents": ["Autoware 1.12 is now available for general use.", "\nTo install it from source, follow the ", ".", "\nPlease note that with this release, Autoware has changed its repository structure and its install method.", "\n", "Docker images are available at Dockerhub:", "\n", "Once you have Autoware installed, you can try it out using some recorded data by ", ".", "This is a list of the major changes in Autoware 1.12:", "There are some known issues in 1.12.", "As always, the current reported issues can be ", " and the current", "\nproposed bug fixes and new features can be ", ".", "Going forward, will the \u201cReleases\u201d section of gitlab be used?", "\n", "GitLab.com", "\n", "Or will release summaries be primarily done here on discourse?", "We haven\u2019t had any discussion about that, but I can say that since Discourse is our primary communications channel we will need to do something here.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["New MPC waypoints follower for more accurate path following", "New Enhanced Kalman Filter (EKF)-based localiser", "Improved decision maker state machine, including support for better stop states and battery charging", "Convex hull prediction", "Support for ROS Melodic Morenia (but see the known issues, below)", "Shift to GitLab as the host of the code", "Split the repositories along functionality lines to improve CI times", "Shift to using ", " to install Autoware", "Numerous bug fixes and smaller changes; see the full list of changes below", "MPC waypoints follower (", ")", "Fix setting files for rosbag demo (", ")", "Modify stop states (", ")", "Fix autoware launcher readme. (", ")", "Split drive state (", ")", "melodic support (", ")", "add battery charging state (", ")", "modify mssion state for fms (", ")", "add install to the resources and plugin.xml (", ") (Fixes ", ")", "Add tests around Openplanner utilities (", ")", "Support full functions of g30esli_interface (", ")", "Update Docker cross-build Image (", ")", "add predicting convex hull (", ")", "Fix Segmentation fault at velocity_set by asynchronous base_waypoints and closest_waypoint (", ") (", ")", "Fix gazebo not found in colcon_release_cross synquacer (", ")", "fix behavior_state in imm_ukf_pda_track (", ")", "runtime_manager: change params definition from int to float (", ")", "Fix don\u2019t show run time in runtimemanager ", " (", ")", "ekf_localizer (", ")", "[fix]: make sure cost is set with expanding_polygon_size (", ")", "Remove points2costmap (", ")", "fix install directive for waypoint extractor (", ")", "Adding install directive for node (", ")", "Fix bug that limits the data rate of DataRateCheckerPlugin (", ")", "Updated paths in quick_start launch files and parameters in default.rviz (", ")", "Remove enablePlannerDynamicSwitch (", ")", "Add .repos file for setting up an Autoware workspace using vcs (", ")"], "url": "https://discourse.ros.org/t/autoware-1-12-released/9817"}
,{"title": "Generalized Traffic Light Classification Architecture", "thread_contents": ["Currently, Autoware.ai contains 3 types of traffic light classification systems:", "Each of these assumes that it will be handed two pieces of information:", "These traffic light ROIs (regions of interest) are provided by the ", " (feature projection) node which uses information from the vector map and camera intrinsic and extrinsic parameters to project the bulb locations from the map in 3D space into the 2D plane of the camera\u2019s field of view.", "In developing our own traffic light classification node, we have run into the following issues:", "We think it is possible to provide a general-purpose architecture for traffic light recognition (and other constrained-ROI-type image classification problems) but some pros/cons must be considered for each approach. I\u2019m not going to speak too much to the types or capabilities of neural networks which would fit the bill since that isn\u2019t my area of expertise - I\u2019ll invite my colleague Joe Driscoll to speak on this topic - but more on the overall architecture. I think our intended goals for the new architecture look something like this:", "For implementing an architecture which does the above, these possible options come to mind:", "Well, now that I\u2019ve written a novel on the subject, please provide feedback and suggestions on how we can make an awesome traffic light detector that works well enough to be used in real traffic!", "\nThank you for starting the discussion. I believe a lot of people are interested in the topic.", "About approach 1&3, don\u2019t you still need to calculate ROI from vector_map information even with DNN? Determining the presence of relevant traffic light does not ensure that irrelevant traffic lights are not in the image. If DNN detects multiple traffic lights in the image, you need information to choose which one to look at.", "You are correct. That is a fact that I missed. However, we could use the centroid of the initial ROI estimate from the feature projection node to find the closest detected light from those that the DNN node found.", "It\u2019s really a trade-off between attempting to make the feature-projection node more robust to differences between the idealized world (vector map) and the real world and providing an alternative to feature projection that is less prone to the same real-world pitfalls as feature projection but needs feature projection as an anchor to the idealized world.", " thank you for your comprehensive analysis of the current status and for prividing possible solutions related to the classification of traffic lights.", "Here are some of my comments related to your post:", "If you agree with me, (A) even with current ADAS map format,  the definition of which traffic light applies to which lane is not well defined. Current format defines \u201cclosest lane ID\u201d, but not exactly which one it is, it also only allows the definition of a ", " lane. (not sure about other ADAS formats)", "(B) just like you mentioned previously, current definition of a traffic light is ambiguous. It requires the search of individual \u201clamps\u201d to define a traffic light object. This is not as straightforward as it should be. In my opinion a new layer defining the traffic lights objects need to be added to the ADAS format.", "Now, regarding the 3 of the proposed solutions. I still think that approach (2) of your list is the most \u201cdynamic\u201d, since different nodes can be used to classify regardless the DL framework. However, I also agree that it needs to be improved.", "The recognizer must publish three pieces of information:", "In my opinion, the ", " nodes should only publish an array of results, for each image in a similar fashion to ", ", but containing the Traffic Light classification results.", "Visualization (super_impose, and markers) should be handled in an independent ", " node.", "The recognizer must classify a found traffic light from the image as having one of four states:", "I mentioned above an array of classification results, instead of a single result, so the classifier can handle more complex combinations.", "\nCase in point, only in central Japan these three types of traffic can be found:", "\n", "Example:", "would match to:", "\n", "I believe this should be able to handle different combination in different parts of the world.", "What do you think?", "Is it possible to only emit a single ROI from feature-projection node by a simple filtering rule such as FOV cropping or largest ROI.", " I agree with your assessment of the shortcomings of the mapping format with regard to traffic lights. However, we are unable to modify the existing ADASMap format (it is Aisan\u2019s proprietary format) which is why Autoware is moving toward a new \u201cAutoware Map\u201d format. This format supports many-to-one relationships between lights and lanes (see page 17 of ", "). I believe the new format also supports what you are suggesting in regard to multiple lights on a signal. ", " may be able to comment more on this issue.", "I like the idea for ", " but it will run into the following problem:", "In this scenario, because of the latency introduced by the classifier, the ", " cannot correctly identify which raw image is tied to the classification results. If it publishes the most recently-received version of both, it will be superimposing old classifications on new images. The time delay won\u2019t be much but I just wanted to mention this issue.", "Publishing probability with the state would just push the logic of decision to the consumer module. Could cause duplication in the case of multiple consumer modules. IMHO tl modules are the best place to have definitive decision and output a definitive result. Please give a possible scenario where this probabilistic information is useful for a consumer module.", "Approach 1 I really like for the following reasons:", "PS: Crazy idea, draw ROI using perception node, run small NN on cropped ROI. eliminating feat_proj.", " - I agree with your assessment about passing the probability on to another node for the decision. However, these would be very useful for training/tweaking so I don\u2019t think publishing it is a bad thing. We could publish it and just not use it in any downstream nodes.", "Regarding approach 1, I would tend to agree with most of your points. However, as far as using the", "\nperception node for the light ROI, as ", " mentioned, we need to have some measure of certainty about which light applies to the lane we are currently in and we can\u2019t determine this without data from the vector map, thus necessitating ", ".", "which is why Autoware is moving toward a new \u201cAutoware Map\u201d format. This format supports many-to-one relationships between lights and lanes (see page 17 of ", ").", "Actually, we have decided to use Lanelet2 format as IO format and also as internal format in TSC meeting. (AutowareMapFormat will disappear.)", "\nHowever, we would need to add some extension to Lanelet2 format so that it has enough information to minimize the degradation of current Autoware functionalities. I will post the idea about extended format on discourse soon.", "I believe the new format also supports what you are suggesting in regard to multiple lights on a signal.", "Lanelet2 supports a single lane linked to multiple traffic lights and also multiple lanes linked to a single traffic light.", " I also agree that it would be simpler to have tl modules to have definitive decision. However, one of the concerns that ", " is mentioning is that having only four states (Stop, Yield, Go,  Unknown) is not enough in some situations (at least in Japan).", "Even the lighting pattern doesn\u2019t change, whether you are allowed to \u201cGo\u201d or not depends on which way the vehicle is going. In the following example from ", "\u2019s post, the vehicle is allowed to \u201cGo\u201d only straight or to the left, but not to right.", "\n                    ", "\n\n", "Therefore, you would need more than the four states.", "Yes more states should be helpful. Attaching probability to each state, not. I wonder if the following in enough:", "So, I think there are multiple ways to handle the light layout that ", " describes above. Here are a couple:", "Thoughts?", "I am having a hard time imagining how would approach 2 work. What is the image that the classifier will classify? It will have to be an image of a whole light cluster ie: ", "  I don\u2019t think there is a way to subdivide this image any further.", "Also what do you mean by \u201cdirection-based\u201d signals? Who create this signals? Do you mean 3 separate ros topics (left, right, ahead) to publish separately a [r, g, y] state for each?", "Please clarify.", "Maybe I misunderstood the meaning of the lights in your example cluster. I\u2019m not very familiar with traffic lights in Japan. Are each of the arrow lights on the bottom row associated with a single light on the top row or does the entire top row indicate something totally separate from the bottom row?", " will have to correct me. But If I was driving in Japan, I would interpret this as:", "In the above picture:", "Hence the full state of the signal for all 3 directions can only be obtained after recognizing the whole cluster. Hence we wouldn\u2019t be able to subdivide the cluster.", "It looks like the TLD need to be modified for geographical regions anyways. e.g.", "Hence I think it would be more important to design the topics to be flexible enough to cover most traffic rules. Then the implementation can be swapped in and out for different regions.", "WRT the discussion of detection + classification 2 stage pipeline V.S. a single stage pipeline, here are some research on both approaches:", "2 also references a large number of research papers using 2 stage solution, which uses either image processing technique as a first stage, or YOLO-like object detection CNN as a first stage.", "To offer maximum flexibility, I propose the following:", "In a graph", "\n", "To improve ease of internationalisation it would be helpful to split TLD black box into 2 parts,", " - In your revised graph above, wouldn\u2019t the purpose of the \u201cTLD_Vision_Detector\u201d only be to verify that the light exists inside the ROI? Doesn\u2019t this then become a two-stage detection system as was described above? I feel like we might be better off offering alternative implementations of the \u201cTLD_BlackBox\u201d described in your first graph for different countries or just flags which set the country to enable/disable functionality within the node. Thoughts?", "I think Feature_Projection and ROI can be deprecated. Neural networks are capable of detecting and classify traffic lights from the raw image. I left it in the graph to be a \u201coptional\u201d input.", "For example, there could be multiple traffic light in view of the camera. the Vision Detector would  detect all of them. TLD_internationalisation would then take the one \u201cclosest\u201d to the vehicle and output its state.", "This way if special sauce is needed for a particular country, for example right on red in US (default green for turning right), then one need only to modify TLD_internationalisation, and not have to touch any neural network bits in TLD_Vision_Detector.", "That is my intention, but I would have no problem if all this is done as a blackbox inside a single node.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A heuristic approach (", ")", "A DNN approach which utilizes the MXNet framework (", ") and CUDA", "A DNN approach which utilizes the SSD framework (", ") and CUDA", "A list of regions of interest which contain the traffic lights", "A raw image", "The results from the heuristic approach were terrible and nearly unusable in real-world scenarios.", "Both the MXNet and SSD/Caffe detectors contain backbones which were trained using the COCO dataset, which is not available for commercial use. This severely limits the applicability of these nodes since they were designed specifically for these frameworks.", "Much of the functionality is duplicated between the DNN-based nodes.", "Attempting to separate the cropping/ROI extraction functionality of these nodes from the classification functionality leads to either timing issues or chicken-and-egg problems (described in more detail in a bit).", "The ", " node is somewhat naive in it\u2019s cropping mechanisms. Variance in pitch/roll/yaw of the vehicle, multiple bulbs in certain vector map versions, and the field-of-view of the detection camera lead to ROI images which are either too large, too small, only covering a portion of the light, or not pointing at the light whatsoever.", "The recognizer must take in raw images from one or more cameras and detect if a traffic light which exists in the vector map also exists within those images.", "If a traffic light exists in the vector map and is applicable to the currently-occupied lane and direction of travel but is outside the field of view of the camera(s), the recognizer should provide this feedback.", "The recognizer must classify a found traffic light from the image as having one of four states:\n", "Stop (usually red)", "Yield (usually yellow)", "Go (usually green or blue)", "Unknown", "\n", "The recognizer must publish three pieces of information:\n", "An overall recommendation to the vehicle control system about how to proceed given the detected and classified traffic light(s).", "An image which contains the contents of the raw image plus superimposed squares around the detected traffic lights and their associated classifications (this is currently provided by all classifiers but is only useful for human interaction and not necessary for system functionality).", "A ", " which contains markers for the detected lights and their classification results (again, this is not necessary for system functionality but is provided by all current classifiers).", "\n", "Combine the problems of detection and classification into a single, neural-network-based approach.", "Pros:\n", "Simplifies the architecture immensely", "Reduces compute resources (no image re-publishing, cropping, etc.)", "Gets rid of latency and chicken-and-egg problems", "\n", "Cons:\n", "Difficult to make general-purpose given the feature, language, and library support of the many neural network frameworks that are available.", "Difficult to troubleshoot since the task domain has increased and you can\u2019t obtain partial results.", "People who are using Autoware without a GPU are out of luck.", "\n", "Keep the detection task separate from the classification task but improve ", "'s ROI selection and, in the classification task,separate the actual classifier from the ROS node by either creating the classifier as a library or a separate node with a ROS service call in-between.", "Pros:\n", "Reuses existing code.", "The classification task can be heuristic or DNN-based and the classifier-interface node doesn\u2019t have to care - it just makes a service call or overloaded function call.", "\n", "Cons:\n", "The classifier-interface node can run into latency issues. Whether using a library or separate-node approach for the classifier, the latency between receiving the raw image and ROIs for that image and publishing the superimposed image with classification results is non-trivial. To keep consistency in the published superimposed image, the classifier-interface node has to receive the raw image and ROIs, make a call to the classifier, wait for the classifier to return the result, annotate the image and marker arrays, and then publish them. This can cause incoming images and ROIs to not be processed while the classifier-interface node is waiting for the current classification result. Making this call in any sort of asynchronous way means that you have to manage a list of received raw images and ROIs and coordinate them with the returned classification results.", "If using a library-based approach, only one language can be used and the choice of the language for the classifier-interface node would determine which language would be supported for the classification libraries (e.g., if using C++, can\u2019t use Python for the classifier).", "The detection task essentially remains the same but contains band-aids and hacks to make it better at feature projection in a real-world environment.", "\n", "Use feature projection to either a) determine if a relevant traffic light is in the image(s) AND find the ROI for that light or b) if a GPU is available, just determine if a relevant traffic light is in the image(s) while handing the detection task to a DNN-based node if the traffic light is in the image(s). In addition, separate the classifier as described in 2.", "Pros:\n", "Makes all involved nodes \u201cgeneral-purpose\u201d and usable with or without a GPU.", "Improves the ROI generation with a common approach that is known to produce high-reliability results (the DNN-based detector).", "The placement of traffic lights in the vector map and the yaw/pitch/roll of the vehicle are much less relevant to the detection task because the DNN-based detector only uses the raw image as input.", "The neural network for traffic light detection could be very shallow if only looking for one type of object in the image.", "Each of the stages of detection and classification can be more easily troubleshot and fine-tuned due to increased visibility into the pipeline.", "Same pros as 2.", "\n", "Cons:\n", "More computationally expensive than other approaches because of multiple neural networks running simultaneously and more nodes.", "Same cons as 2.", "\n", "Stop (usually red)", "Yield (usually yellow)", "Go (usually green or blue)", "Unknown", "Camera publishes raw image", "Feature Projection publishes ROIs (or light positions, as suggested)", "Classifier consumes raw image and ROIs (or light positions, as suggested) and produces classification (takes unknown amount of time)", "\n", " consumes raw image and classification", "Simplicity, simplify code and compute graph, reduce large message passing", "More robust, given all problem with current feature extraction node, a DNN approach with some filtering from vector map information should give a lot more robust result. Especially in areas that vector map is inaccurate or lacking.", "Achievability, seems to be a well-explored approach, a lot of networks and training data that can be used. ", ", ", ", ", "\n", "Speed, people running autoware without a GPU should already be in a lot of pain given that perception and tl detection module already reply on NNs. A CPU only simple heuristic node could still be provided by combining current feat_proj and region_tlr nodes, but I suspect the resultant quality would fit nobody\u2019s needs.", "tfd node publish a light color for going forward, turn left, turn right. Default to red. Hence the tfd would be in charge of condensing the different shapes of lights into these 3 channels.", "DecisionMakerNode currently have logic to do straight/left/right recognition by using angle. This node can then make a go/hold decision based on tfd output.", "Have the entire light cluster classified by the tlr in one go. This is difficult to implement because of the required number of statuses for different light combinations and the complete re-training required for the existing neural networks, not to mention the number of images required for each state for each combination of lights for training.", "Create individual, \u201cdirection-based\u201d signals. Using the example image from above, create 3 signals with seperate classification states for each. We would need to add metadata to the mapping format to indicate the direction of travel that each signal controls and then either have the feat_proj choose the correct one based on a \u201cdirection-of-travel\u201d input or have a decision node after the classification decide which to use. The upside of this approach is that the existing tlr NNs would only require a bit of transfer learning to train rather than compete re-training.", "Create \u201cdirection-based\u201d signals as described in 2 but have the classifier learn to distinguish between the different signal types and produce both a signal type and a state classification.", "The top row is the normal traffic light scenario, I.E. 3 lights with R/Y/G. Meaning Stop/Slow/Go for all directions. (I think each position only shows one colour, otherwise, it would be difficult for colour blind people.)", "The bottom row contains modifiers that overwrite the top row instruction. (they are only ever green, they have 2 states, off/green)", "Stop for all directions", "For a left turn, override to Go. For going ahead override to forward.", "In Japan, the green light is blue, the pre-processing of the raw image need to account for that.", "In the US there is right on red, and TLD or other node needs to be able to recognize the (No turn on red) sign.", "In Europe, there is ever only one directional modifier in addition to the main light.", "Join traffic sign and traffic light detection and classification using a single DNN. ", "\n", "2 stage pipeline - ", "\n", "specify a single stage TLD. If multiple stages are required, they can be implemented as internal stages of the TLD.", "feat_proj will continue to exist but improved to take into account the pose of the car", "TLD takes the output of feat_proj as a suggestion but doesn\u2019t wholly rely on the map to provide accurate information", "TLD outputs a go/slow/stop states for each of right/left/ahead directions.", "A decision node will take the output of TLD and path planner to issue STOP/GO signals to the vehicle.", "To deal with geographical differences, TLD node will have different implementation depending on the application.", "Autoware can provide a generic TLD node trained using a publically available database. As there are established research projects in this area, a reference implementation can be done following one of the papers.", "TLD_Vision_Detector to perform purely the task of detecting the lights from an image.", "TLD_internationalisation to integrate information from other sources to condense vision detection into light/right/ahead light signals.", "TLD_Vision_Detector would only deal with running a neural network and is hence very simple.", "TLD_internationalisation would then take multiple signal and discern a left/right/ahead signal."], "url": "https://discourse.ros.org/t/generalized-traffic-light-classification-architecture/9354"}
,{"title": "Simulation software requirements", "thread_contents": ["Hello,", "We would like to kickstart a discussion around simulation requirements for users of Autoware. While my team specifically is working on ", ", a more general discussion on simulation software needs and requirements for Autoware users can hopefully begin a useful conversation around how developers are currently using Autoware and what they are lacking. What do you need in a simulator for Autoware? What do you feel is missing with current solutions? What are some specific requirements in a simulator for your use case?", "A few things off the top of my head that I think we need from one or more simulators. I say \u201cone or more\u201d because it\u2019s likely that different simulators will be better at different things.", " we looked at this extensively couple of months back and we also talked to lots of simulation companies and none could really full-fill what we were asking for.", "In short these would be our prioritized requests:", "The entire, non-prioritized list would be as follows:", "Headless mode, logger (if the accident happens in the simulator, the logger logs this situation) and scenario mode is necessary for running simulation in CI.", "\nI want to use simulator for CI.", "CARLA hits several of the requirements that we need.", "Here\u2019s another one for when simulating light-based sensors: simulating things like sun glare behind a traffic light or a bright reflection from a glass pane into a camera for an instant.", "These are all really good points. What are the specific use cases of simulators as it pertains to people working on Autoware right now? As with ", "\u2019s  post, I think it\u2019s helpful from our perspective (the LGSVL Simulator team) at least to think about prioritization of features from users\u2019 point of view. We know about some of the use cases of the Autoware Foundation, but it would be interesting to hear from any additional active Autoware developers and what their needs are for their specific use case. As ", " entioned, maybe it will help more clearly define different simulators\u2019 roles and their differences as well.", "As for LGSVL Simulator specifically, I\u2019ve addressed some of the feature requests above as they relate to our roadmap.", "Currently, our immediate priorities are:", "Some requests (hardware specification description, which exact sensor models are supported) are a documentation need, and we will continuously add to the documentation (", ").", "Control/scripting capability of simulator:", "\nWe are currently working on implementing a Python API to run the simulator - this includes start/stop, run-time configuration, stepping through/running faster than real-time, controlling the environment, controlling ego/non-ego agents, state/sensor data retrieval.", "\nThis will let us more easily define or use standardized scenario descriptions.", "Determinism:", "\nLGSVL Simulator is based on Unity, and to my knowledge the PhysX Engine in Unity does guarantee the same simulation result when all inputs are the same. This is certainly a whole topic itself as to the degree of determinism that is required for use cases - the act of running on different machines (if in cloud, for example) can change behavior, and the extent to which it does and whether it\u2019s acceptable is probably an important question.", "Headless mode for CI:", "\nWe do plan to enable running the simulator in non-rendering mode.", "Parallelizing simulation runs:", "\nThis is on our roadmap (we need to upgrade Unity version and implement multi-threading)", "Behavior models of non-ego agents:", "\n", "  Do you mean dynamics models of agents as well or higher-level behavior?", "\nThis can be enabled by the API (agents can have specified behavior such as following waypoints or randomness).", "\nCurrently by default, traffic vehicles follow the vector map, and pedestrians walk random paths.", "Scenario database:", "\nWe are currently working on being able to support scenarios, and after this definitely plan to look at generating test suites of scenarios, integrating with existing scenario databases, etc.", " I\u2019d like a plan for simulation support and usage to be put together. Every simulator has its strengths and weaknesses, and that is not going to change no matter how much one simulator tries. It\u2019s not just a question of features but also of underlying technologies used, how features are implemented, etc. So we need to understand what each simulator is best at and where it can be best applied. For example, if I were making a demonstration video, I would probably use LG\u2019s simulator. If I were doing simulated cameras, Cognata\u2019s simulator produces images that are more photorealistic. If I were doing statistical data gathering of planning performance across a range of scenarios, I would also probably use Cognata\u2019s because they have a good system for  generating scenarios procedurally and deterministically, and executing them deterministically. LG\u2019s simulator and Carla appear to have better physics simulation and so are good for control testing. Things like this need to be considered and each simulator developer needs to decide on what sort of usage they wants to focus on. Then we can decide how to apply each one.", "Do you mean dynamics models of agents as well or higher-level behavior?", "\nThis can be enabled by the API (agents can have specified behavior such as following waypoints or randomness).", "\nCurrently by default, traffic vehicles follow the vector map, and pedestrians walk random paths.", " thanks for the reply first. To reply to above question first, it would make sense to have non-ego agents behave lawfully as well unlawfully (e.f. jay walking, crossing on red, etc.). So a higher level behavior indeed.", "Further, do you for any of above features also have a timeline?", "Hi,", "I would like to propose CARLA to be the official AutoWare simulation platform. Let me explain why i think CARLA is the best option right now.", "Let me start from the non-technical aspect:", "Totally open source: code and assets are free to use for both non-commercial and commercial purposes", "CARLA is sponsored by Intel, Toyota Research Institute, General Motors and KPIT, large corporations that help to keep CARLA aligned with the needs of the driving industry", "In addition the CARLA challenge, is backed up by Waymo, Uber, AWS, Audi and AlphaDrive", "CARLA has a strong community of users around the world within academia and industry", "Multiple startups are building services around CARLA to provide advance behaviour models, simulation in the cloud, procedural generation of maps, etc. Our ecosystem is large and it keeps growing.", "CARLA has a strong team: more than 20 engineers working full time in the platform", "\nWe provide monthly releases with new features and bug fixes", "From the perspective of technical features:", "A modular server - multi-client architecture to distribute simulation load. Multiple clients, running in different nodes can connect to the same server. Feature releases will include a multi-server - multi-client architecture.", "Flexible and extensible python API. CARLA itself is designed to be an API.", "Everything can be controlled in CARLA using the API: maps, actors (vehicles, pedestrians), weather, traffic lights.", "Full-stack simulation, from sensor simulation, planning simulation to control. Now we also expose an interface to the low-level physic parameter of vehicles, so that users can define frictions, mass, etc.", "Standard mechanism to define maps and navigability based on OpenDrive", "We now have a high-performance mode (sensor simulation off) that enables simulation at 800-1000 Hz. Hours of driving acquired in minutes", "Sensor simulation: multiple cameras, LIDARs, GPS, IMU and others. Moreover, we have plans to extend these sensors with RADAR and more accurate sensors models", "New logging and playback mode: It is possible now to log the entire simulation and resume it back at from any timestamp. This enables to try different stacks or variations of an algorithm (the simulation can be resumed and diverge from the log).", "CARLA provides now a query engine to automatically find interesting events that happened during the simulation round. For instance, find collisions and then resume the simulation just at the interesting part.", "Simulation can run synchronously or asynchronously. This simplifies prototyping and enables the use of a large number of sensors if the user is willing to give up real-time. Total control of the simulation step (tick).", "We have created workflows and tools to simplify the generation of new maps. Now users can make use of RoadRunner to semi-automatically (and most recently totally automatically) create new maps. Those maps are now ready to ingest in CARLA.", "The new 0.9.4 release can ingest maps on the fly without recompilation", "We also have an engine for traffic simulation to generate interesting traffic scenarios based on templates and decision trees. This engine is being used for the CARLA challenge, creating pseudo-random traffic situations on the fly inspired by the NHTSA pre-crash typology.", "CARLA supports ROS via the CARLA-ROS bridge", "CARLA is integrated with AutoWare", "CARLA is being integrated with Apollo", "Headless mode. It works in docker too.", "I hope this information helps.", "Best regards,", "\nGerman", "I would like to propose CARLA to be the official AutoWare simulation platform.", "Just to clarify, we won\u2019t have ", " \u201cofficial\u201d simulation platform. We intend to support as many as we can based on technical capability to use a simulator and resources to maintain a connection.", "However, we will have a ", " simulator that will be used in tutorials and quick start guides, etc. My opinion is that licensing reasons make CARLA the best option here. ", " has also made good arguments for why CARLA is technically the best option. As always, we are open to other opinions. In the long-run, what we want most is a healthy ecosystem of simulators that cover each other\u2019s weaknesses.", "Hi ", ", we are all in the same page. That is what I meant.", "Let us know how we can help.", "Best regards,", "\nGerman", "We\u2019ve been running CARLA to do comparison with LGSVL Simulator and there are few points worth mentioning:", "I\u2019m not sure having tutorials and quick-start guides with the simulator which is unable to run Autoware in real-time, difficult to setup and requires to pay 5% if you provide it to customers is a good idea.", "BR", "\nDmitry Z.", " I also agree with ", "\nWe will not have \u201csingle\u201d official simulator platform.", "\nBut, we want to integrate and add Documentation for some simulators in terms of usability.", " I have some opinion to CARLA.", "\nI try to integrate carla_bridge into Autoware and it was very very hard.", "In those reasons, I had to integrate carla bridge as Docker image, and I think it damages usability of carla and Autoware.", "\nThis is the feature branch for Carla Integration.", "Open-source software for self-driving vehicles. Contribute to CPFL/Autoware development by creating an account on GitHub.", "Hi,", "Are you referring to the ROS-bridge or to our AutoWare integration?", "What do you mean by CARLA being very \u201cheavy\u201d?", "The simulator is hard to build given the current instructions or just takes many steps? I mean, are there points where you got blocked and therefore need attention or is it just that since it is a UE4 project it deviates from a common ROS-based compilation recipe?", "Vehicle dynamics is strange sounds like a very vague description. What problems are you having with vehicle dynamics? In fact the dynamics of the vehicles are configurable by the user through the API.", "Right, the bridge is not a ROS package yet. We can work on making this happen", "The PythinAPI is a super-set of the egg. There are some additional functionalities. But we remove this dependency.", "I would like to propose CARLA to be the official AutoWare simulation platform. Let me explain why i think CARLA is the best option right now.", "Thanks for chiming in Germ\u00e1n. As ", " said, we are not going to have an official simulator, but a set of recommended ones. It may happen that a given simulator is better suited for a specific scenario than others and it\u2019ll be labeled as the default, very much like what ROS2 did with the DDS vendors. For example, ROS2 has no official DDS vendor, but there\u2019s a default one that users can just assume will work well in most cases. Users are still free to use another DDS vendor if they want (e.g. they already have a license, their usecase is very specific, they want to run ROS2 on an unsupported OS/platform, etc.)", "We just want to make it easier for our users to  decide which simulator to use. In order to do that, we want to ensure that all the recommended simulators are well integrated with Autoware and with good performance, they all use open standards (e.g. Lanelet2, OpenDrive), have clear licensing that doesn\u2019t restrict its use, etc.", "On the more technical side, both ", " and ", " have listed many of the features we\u2019d like simulators to have. I\u2019d say the highlights are being able to run faster than realtime, integration with CI (including scripting and headless) and a fairly realistic physics engine, but many others are also very important.", "I\u2019m not sure having tutorials and quick-start guides with the simulator which is unable to run Autoware in real-time, difficult to setup and requires to pay 5% if you provide it to customers is a good idea.", "As I said, this is not a competition, so the more simulators that are well integrated with Autoware that our users can pick from, the better.", "Having a clear, well-understood license that does not restrict the use is essential for all the simulators that we intend to collaborate with. I\u2019m not a lawyer, but it seems that the Unreal license wouldn\u2019t be a problem here, royalties are collected by Epic only if you sell the product that uses the Unreal engine, which doesn\u2019t seem to be the case with Carla since it\u2019s entirely opensource. Carla has a very common license for the code (MIT) and the assets (Creative Commons-BY). That doesn\u2019t preclude anyone (including third parties) from taking Carla, repackaging it as a proprietary product and sell it, as long as they pay royalties to Epic, but that\u2019s not a goal the Autoware Foundation has.", "If we are going down the path discussing licensing issue, let\u2019s at least get on the same page with definitions: Open-Source != Free (free as a freedom, not as a free beer). Here is an article about that matter ", "The Free Software Definition written by Richard Stallman and published by Free Software Foundation (FSF), defines free software as being software that ensures that the end users have freedom in using, studying, sharing and modifying that software. The term \"free\" is used in the sense of \"free speech,\" not of \"free of charge.\" The earliest-known publication of the definition was in the February 1986 edition of the now-discontinued GNU's Bulletin publication of FSF. The canonical source for t The d...", " Again Epic charges 5% from ALL gross revenues produced by products built with Unreal:", "Example:", "Please read all the details below. Even if CARLA source code is MIT, CARLA can\u2019t control usage of the simulator, it is very much restricted by Unreal Terms.", "Generally, you are obligated to pay to Epic 5% of all gross revenue after the first $3,000 per game or  application per calendar quarter, regardless of what company collects the revenue. For example, if your product earns $10 from sales on the App Store, the royalty due is $0.50 (5% of $10), even though you would receive roughly $7 from Apple after they deduct their distribution fee of roughly $3 (30% of $10).", "\nRoyalty payments are due 45 days after the close of each calendar quarter. Along with the payment, you must send a royalty report on a per-product basis. For more information, ", ".", "If we are going down the path discussing licensing issue, let\u2019s at least get on the same page with definitions: Open-Source != Free (free as a freedom, not as a free beer). Here is an article about that matter ", "I didn\u2019t mean to get into a debate about what Open Source means (I pasted a link to the definition of the Open Source Initiative ", "), but was just trying to reframe the discussion to give it a more positive and constructive spin to foster collaboration from Autoware with as many simulator teams as we can.", "By the way, from the link you pasted:", "Despite the philosophical differences between the ", " and the ", ", the official definitions of ", " by the ", " and of ", " by the ", " basically refer to the same software licences, with a few minor exceptions", "The Open Source Definition was copied verbatim from the Debian Free Software Guidelines, the only difference between Free Software and Open Source, is that the former is politically and philosophically motivated, whereas the latter is more about pragmatism.", "As for the Unreal license, it seems fairly clear (at least to me) that only when you\u2019re selling your game or application (i.e. you\u2019re getting revenue from selling licenses) the royalties clause applies.", "But in any case, neither you or I are lawyers, so let\u2019s better leave it to the experts and focus on how Autoware can collaborate with the many simulation teams out there.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Deterministic execution. When executing deterministically, the same starting conditions should give exactly the same results.", "Ability to control the simulation step (pause, step forwards a specific time, step forwards one loop of the control algorithms, run faster than 1:1 time, etc.).", "Headless mode for running as part of a CI pipeline.", "Modular framework (links to the above requirement, but broader). We need to be able to do things like plug in different pedestrian simulation algorithms, different sensor simulation models (even for the same sensor types), and so on.", "Fault simulation in all parts of the car, from wheels to engine to sensors.", "Sensor noise and interference simulation.", "Simulation of different times of day and different weather conditions, including their impact on road conditions and thus the response of the car to control.", "Simulation runs can be scripted", "Simulation runs can be parallelized", "Lidar + Camera Sensor models out of the box (including interference as Geoff points out)", "Simulation can happen at faster than real time (and in headless mode as Geoff points out)", "Traffic participants have behavior models", "(Pseudo-)random traffic scene generation. Possibly there is a connection to some database with the pre-configured traffic scenario, e.g. ", ".", "Computer hardware requirements for simulation should be listed", "Interfaces (mocks) supported by simulation (CAN, ethernet, \u2026)", "Crisply defined software APIs", "Support/integration for ROS. ", " ", " that we already had with LG guys and I felt that our opinion was not really taken seriously.", "Fidelity of sensor models\n", "E.g. do you accurately model different surface reflectivity/absorptivity, incidence angle, etc\u2026", "\n", "How to create our own sensor models?", "Map formats supported", "Maps that come with it?", "How to include our own maps?", "Precision/ real-time capabilities in general", "Use for stress testing, e.g. an extremely high sensor load (>10 cameras, >10 lidars, etc) in realtime, precision/ real-time capabilities/guarantees during stress testing", "Can simulation run in faster than real time?", "How is the behavior of other cars governed in simulation?\n", "Are there API\u2019s/plugins that we can define to modify this behavior", "\n", "How are traffic scenarios generated?", "Is there any randomness in the simulations?\n", "If yes, can we specify sources of randomness?", "\n", "Is there collision detection?", "Can simulation run in a headless state?\n", "E.g. Can it run without a GUI?", "\n", "Can we programmatically access the simulation state?\n", "Or do we have to interact via filesystem (e.g. xml)", "\n", "Are car dynamics simulated?\n", "E.g. suspension, slip etc\u2026", "\n", "Can the simulator be integrated with the CI runner and does it output simulation/test results in e.g. junit format for test result validation?", "API implementation", "New map generation/import", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "CARLA performance was very poor, it was impossible to run complete Autoware or Apollo setup with CARLA in real-time", "CARLA building and setup process was fairly complicated", "CARLA uses Unreal engine which prohibits any commercial usage without registering all the revenues and paying 5% ", " Unreal keeps the right to perform audits to make sure agreement terms are followed. So it does not really matter if CARLA code is under MIT license.", "Simulator is very heavy.", "Simulator is very very hard to build.", "Vehicle dynamics is strange.", "bridge software was not released as ROS package.", "Why bridge software depends PythonAPI source codes in CARLA simulator?", "\nWhy do not you to distribute PythonAPI .egg file as binary?", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "You sell Autoware to customer and include CARLA Simulation DEMO to prove it works.", "\nYou need to pay 5% from the whole amount of the deal", "You run Autoware and CARLA Simulator demo in the cloud and charge customers for getting access to Simulator.", "\nYou need to pay 5% from the whole amount you are charging"], "url": "https://discourse.ros.org/t/simulation-software-requirements/7735"}
,{"title": "Technical Steering Committee (TSC) Meeting #8 Minutes", "thread_contents": [" Geoffrey Biggs (Tier IV)", "Hey Guys, as Arm as struggling to attend the Asia/USA timezone friendly TSC meetings, would it be Ok for me to nominate a colleague of mine from our San-Jose California office to attend as a proxy?  If so, what is the process for doing so?", "Also, I was not aware that voting for the Face-2-Face was open (and now closed!), Arm\u2019s preference would be to hold the summit on the 30th October as we are keen to push a healthy work/life balance, and asking people to be away from their family over the weekend does not sit well with this policy.", "Cheers", "/Matt", "Hey Guys, as Arm as struggling to attend the Asia/USA timezone friendly TSC meetings, would it be Ok for me to nominate a colleague of mine from our San-Jose California office to attend as a proxy? If so, what is the process for doing so?", "You can nominate an alternative representative. Currently Filipe is your alternative.", "I was not aware that voting for the Face-2-Face was open (and now closed!)", "You probably aren\u2019t watching the Autoware category. You should watch it so that you get notified of new posts. Discourse is our main communications medium so you need to treat both the Autoware and the TSC categories like mailing lists.", "Ok, will it be possible to keep Filipe as a reserve for the UK friendly TSC meetings, but add Kasper (kasperomeck) for the US friendly slots?", "I will check with the people who decide these things and get back to you.", "Looks like we can do that. Please send me their name, email address and Discourse user name by private message.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Lee Baldwin (AutonomousStuff)", "Alfredo Bencomo (Open Robotics)", "Geoffrey Biggs (Tier IV)", "Cheng Chen (AutoCore)", "Kenji Funaoka (Tier IV)", "Brian Holt (Parkopedia)", "Shinpei Kato (AWF)", "Seonman Kim (LGE)", "Dejan Pangercic (Apex.AI)", "Paul Sastrasinh (TRI-AD)", "Antonis Skadasis (StreetDrone)", "Stephane Strahm (Kalray)", "Akihiko Tsukuda (eSOL)", "Opening remarks and new member introductions (Board)", "\n", " Confirmation of previous minutes (All)", "\n", " Action items from previous meeting (All)", "\n", " Names and photos of TSC members for the AWF website", "\n", " Autoware annual summit", "\n", " Autoware (", ", ", ")", "\n", " Map formats (", ")", "\n", " Vehicle interfaces (", ", ", ")", "\n", " ECU/Platform (", ")", "\n", " Simulation", "\n", " Hiring a system architect for Autoware.Auto (Apex.AI/Tier IV)", "Release the safety case example\n", "\n", "Start a Discourse poll on when to hold the Autoware Summit\n", "\n", "Begin scoping the work and skills for an autonomous driving system architect\n", "\n", ", ", "\n", "\n", "New observer: Jit Chowdhury from Auro/Ridecell", "Minutes approved", "Close remaining Slack channels except general\n", "Geoff", "\n", " Channels closed", "\n", "Follow up about free AWS minutes\n", "Geoff & Esteve & Jan", "\n", " Jan is setting up a meeting to talk to AWS", "\n", "Provide a set of launch files for starting just waypoint following-related functionality in Autoware.AI\n", "Parkopedia", "\n", " A repository has been created (please send link). This work is being done closely with StreetDrone.", "\n", "Start OpenDrive discussion in the Autoware Discourse category\n", "Parkopedia", "\n", " Not started; will be done as part of the maps working group.", "\n", "List planned contributions that go towards Foundation work and can be used to meet the milestone demos. ", ".\n", "Kalray", "\n", " Planned contributions listed", "\n", "Begin coordinating working groups\n", "Geoff", "\n", " Four working groups have been formed and starting working", "\n", "Safety case for autonomous driving in constrained evironments (i.e. autonomous valet parking) sample\n", "Parkopedia", "\n", " There are some legal problems with releasing the safety case documentation; the group who made the documents are worried about liability so Brian is working on persuading them.", "\n", "The Autoware working group held its first meeting on 16th of July\n", "Working group announcement: ", "\n", "Working group meeting minutes: ", "\n", "\n", "The Autoware working group is proposing that its main output be a specification that defines what Autoware is\n", "See presentation: ", "\n", "\n", "Autoware.AI 1.12 was released.\n", "There are known bugs so a patch release is planned.", "\n", "The first release of Autoware.Auto (named Axle) has been made.", "\n", " The Autoware working group proposes that the Autoware Foundation produces a specification defining Autoware as one of its primary products, with Autoware.Auto being the reference implementation of this specification. All working groups will work towards producing a part of this specification, with the Autoware working group being responsible for cross-cutting aspects and the overall specification.", "AutonomousStuff: The specification idea is the right way to move forward, but what about Autoware.AI?\n", "It will not be forcefully aligned with the specification because it is due to be gradually phased out. We may need to alter Autoware.AI to match the specification in places to make the bridge work, but we want to keep these to a minimum because any resources put into Autoware.AI development are resources taken from Autoware.Auto development.", "\n", "Apex.AI: Will the specification have functional units or just high level stuff.\n", "We will be defining things at the ROS interfaces level and the architecture level, i.e. nodes.", "\n", "Apex.AI: Will this be a code-first or specification-first approach?\n", "A hybrid, iterative approach. We will design the specification to achieve some capability, then implement to prototype and check, then revise the specification.", "\n", "Proposal approved. Autoware working group will begin preparing a process for the specification work.", "First meeting was held just a few hours ago.", "There was initially quite a lot to discuss. (Minutes available \u2026)", "Next meeting will be held in a week.\n", "Will meet every week for the next few weeks until the direction is set, then will probably drop down to every two weeks.", "\n", "Before the next meeting, the group will analyse a set of map formats and understand how they are useful, what they offer, etc.", "Apex.AI: Will there be different formats for simulation and on-vehicle software? Will the working group answer this question?\n", "The working group will definitely answer this question, especially because the current expectation is that simulation will use OpenDrive extensively, while on-vehicle will use a more suitable format.", "The fact that most people do not have access to the Navigation Data Standards Association specifications makes it hard to use the data widely. Converters will be available.", "\n", "Apex.AI: Is Lanelet2 still in the race?\n", "We need to separate out the storage format from the libraries/abstractions/APIs that we use to interact with map data; Lanelet2 is the latter. However Lanelet2 has become very closely coupled to the underlying storage format they currently use (OpenStreetMap), so work needs to be done to reduce this coupling.", "\n", "Apex.AI: Will the evaluation will be done at a theoretical level or will some practical testing be done?\n", "There are some sample maps available that can be used to assist with the evaluation, but there are not enough. We will need to refer to other evaluations as well as do our own work.", "\n", "Second meeting was held last week.\n", "Attended by AS, StreetDrone, Apex.AI and Tier IV", "\n", "Use cases from each member have been collected.", "\n", "\n", "The next step is to start working on a common design to satisfy these use cases.", "Apex.AI: Comment on the use cases", "Working group was launched with a call for participants, but first meeting has not yet been held.", "Working group outline is being prepared", "\n", "\n", "Tier IV: There will be more ECU vendors and silicon vendors joining the foundation, so we need to clarify what they can contribute to the foundation in this working group. Their roles in the foundation (not the market) need to be clear.\n", "This will be clarified in the first call.", "\n", "Apex.AI: Please think about hardware support for real-time/time determinism/other safety related stuff.", "Tier IV proposes a new working group to deal with matters relating to simulation.\n", "\n", " proposed as one leader.", "\n", "Parkopedia: There will be some cross-over with the map WG, e.g. does the map provide enough information for the simulator? However in general having a WG to give direction to simulation will help with this.", "AutonomousStuff: Agree with having a simulation WG to help with the V&V aspects of Autoware. Metamoto is a company that could be interested in being involved.", "Tier IV: Having coordination of the various different simulators would help to understand the strengths and weaknesses of each.", "Tier IV: The WG could put some effort into producing scenarios (e.g. EURO NCAP using OpenScenario) that can be used widely in different simulators.", "Apex.AI/Parkopedia: We are working on carpark maps, so this could be something to provide simulation worlds for.", "Parkopedia: There seems to be a general movement towards Carla; the IV 2019 workshop had two presentations about Carla.", "LGE: We are interested in a simulation working group, and Dmitry can be the main contact for that, and possibly co-leader.", "Will start a simulation working group.", "Although we have a lot of engineers avialable and the two leaders of Autoware are skilled in software development for critical systems, we don\u2019t have a system architect who is experienced in autonomous driving. We need someone who can do things like choose the best algorithms and understand what the interfaces need to do.", "For example, the current default path following algorithm is pure pursuit, but it is definitely not the best. MPC is being introduced but it is still not optimal.", "Therefore we should find someone who has this experience and knowledge and employ them full-time to do this work for Autoware.", "StreetDrone: It would be useful to have someone to do the top-level design, but we need to carefully specify what this person would need to do.", "Parkopedia: Would this person be employed directly by the Autoware Foundation, or be employed by a member company to work 100% for the AWF?\n", "Apex.AI: No opinion, but not sure if the AWF can even employ someone. So probably best would be to start through one company.", "Tier IV: We don\u2019t even know if there is such a person available because autonomous driving technology is so closed. But we agree on the need for someone to be in charge of the architecture full time.", "\n", "AutonomousStuff: Agree completely because you need to have someone full-time on the system architecture who can make binding decisions.", "Begin by specifying what we need from this role some more so we can identify potential candidates.\n", "Dejan and Geoff to work on an initial list of skills.", "\n", "The board needs to discuss if there is money available.", "Summit proposed to happen in Macau on November 2nd (the day after ROSCon 2019).", "Alternative date: Morning of October 30th (before ROSCon workshops).", "A poll will be made on Discourse in the open Autoware category; all TSC members are ", " to answer."], "url": "https://discourse.ros.org/t/technical-steering-committee-tsc-meeting-8-minutes/10093"}
,{"title": "Using NUC with Kobuki", "thread_contents": ["Hi,", "It seems to becoming harder to find netbook size laptops to use with the Kobuki base and so I\u2019m looking into using the Intel NUC (e.g. NUC6i5SYH). I want fully autonomous operation where the Kobuki can dock and charge its batteries and the PC, so I wondered if anyone was aware of a power solution e.g. external battery that can connect to the Kobuki 19v output AND will allow the NUC to get a reading of the battery level e.g. via USB. Alternatively, is it possible to use a dc-dc converter with one of the other Kobuki power outputs (e.g. 12v 5A) to power the NUC?", "Or if anyone knows of a good choice of small laptop to use then that might be preferable.", "Thanks,", "\nLee.", "Hi,", "Have a look at the ", ", it\u2019s a DC-DC converter as well as UPS (6-30V input, 6-24V output). It supports multiple battery chemistries (e.g. Li-Po, SLA) and has an adjustable DC output.", "It has a USB interface, and Linux drivers, supporting ", ".", "I\u2019ve been using this device on my robot, I wrote a very simple Python ROS interface for this, which just publishes the charging state and individual Li-Po cell voltages. I\u2019ll upload this script and link it on this thread when I get the chance (I\u2019m currently away from my robot).", "Alex", "Thanks for the response. After posting I did end up looking at that and it looks like a good solution. Good to know that you have it up and running and that the Linux drivers work etc.", "I\u2019m thinking of using 6 of these batteries:", "\n", "7926", "\n", "\nwhich based on my very quick calculations should drive a NUC for about 6 hours (assuming NUC consumes 15W)?", "I asked the OpenUPS company if they could recommend any enclosures - did you find one that works well?", "Hi,", "we have a NUCi7 on our Turtlebot. We ordered an i5 but got an i7 due to", "\nsome issue while ordering. The i5 had a notebook powerbank with it which", "\nis not powerful enough for an i7 (it lasts about 30 minutes). Thus we", "\nare now testing a custom LiIon battery. It seems to be able to run the", "\ni7 for about 4-5 hours now but we are still working on a battery status", "\nintegration. Additionally, we need about 4-6A for charging the robot", "\nnow. From the internal wiring, this should not be a problem, but the", "\nwall charger only gives us 3.2A. So we also still need to get a powerful", "\nwall charger (right now using adjustable lab power supplies).", "Once the battery runs well, I can send you our setup. The OpenUPS looks", "\nvery nice as well, especially the already integrated battery status and", "\ncharging state. If you deploy it to your Turtlebot, could you give", "\nfeedback about it?", "Best,", "\nLasse", "Hi!", "We are also using Turtlebot (version 1) with an i7 NUC which actually requires 19V / 65 W. So far, we didn\u2019t use any DC/DC converters or something similar, but instead, we powered the NUC directly from the 12V / 5A output port on the Kobuki.", "This is certainly not ideal, and we experienced the NUC shutting down because of a \u201cprocessor thermal trip\u201d after some time (sometimes only 30 mins when the Kobuki battery would still last for long). First we thought it\u2019s a problem with the NUC (old thermal paste or something), but later we realized that the output voltage from the Kobuki is monotonically decreasing and will be too low at some point. This seems to have caused the overheating problems then.", "Either ", " or ", " seem like good solutions for this problem, but I\u2019m not very experienced with DC-DC converters or UPS and I\u2019m glad for any advice.", "Can anyone tell me if this would be a good buy for more reliable mobile NUC operation?", "We had a similar problem and solved it by purchasing a power bank. It charges from the kobuki base, provides 19V to the NUC and does not kill the Kobuki battery. We use one of the 19V connectors on the back of the Kobuki and make an adapter to charge the powerbank when the robot is on the charger.", "This is the one we used: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-nuc-with-kobuki/489"}
,{"title": "Invitation to the 3rd international workshop on Robotics Software Engineering (RoSE)", "thread_contents": ["Hi all, we are a network of European researchers organizing the workshop series on ", " (", "). Its previous two editions were held at the International Conference on Software Engineering (ICSE) and the are proposing to co-locate the next iteration of the workshop to IROS 2020 in October in Las Vegas, USA.", "We are strongly interested in engaging more the ROS community and are looking for:", "Would you be interested in some of the points above? If yes, directly contact me (i.malavolta@vu.nl) and we will discuss about it.", "Thanks in advance!", "Ivano", ":::", "Ivano Malavolta | Assistant professor | Vrije Universiteit Amsterdam, The Netherlands | ", "ROSCon was co-located to IROS in previous years \u2026 make sure that you do not try to organise a ROS workshop in parallel to ROSCon\u2026", "\nwhat dates have you thought of?", "\nMaybe a good idea to contact ", " or ", " of Open Robotics\u2026", "We should have a formal ROSCon announcement in the next week or so, stay tuned with the final location and dates.", "We should have a formal ROSCon announcement in the next week or so", "can\u2019t wait.", "Thanks Thilo for the advice. Indeed, the idea is to have our workshop \u201cin between\u201d ROSCon and IROS so to attract attendants of both conferences.", "Great! Thanks for letting us know.", "So, ROSCon takes place Nov 14 - 16 in New Orleans.", "If you want to make your workshop after IROS, make sure that your US participants can still go to the presidential elections on November 3rd\u2026", "So, ROSCon takes place Nov 14 - 16 in New Orleans.", "If you want to make your workshop after IROS, make sure that your US participants can still go to the presidential elections on November 3rd\u2026", "Ok, thanks for the information Thilo.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["speakers for invited talks", "candidates for the Program Committee of the workshop", "participants to the workshop"], "url": "https://discourse.ros.org/t/invitation-to-the-3rd-international-workshop-on-robotics-software-engineering-rose/12584"}
,{"title": "What do you want to see in an educational ROS platform?", "thread_contents": ["We\u2019ve started developing a table top / ground ROS robot for use in educational environments. We\u2019d like to hear from educators that really want to teach ROS based robotics, but don\u2019t feel they can because currently available robots are either too expensive, too complicated or too [insert your adjective here].", "We\u2019d also like to hear from educators that are teaching ROS robotics but want to broaden what they do, or find alternative platforms.", "If you are an educator who offers courses and would like to do so with ROS can you respond to this thread with the one thing that you\u2019d like to see in a educational platform for ROS. If enough people express interest we will do a couple of group calls so we can hear what people would like and make trade-off decisions. Then we will do our best to build what the group wants with open source repos to go along with it!", "Please respond with the one thing you\u2019d like to see in an ROS robot platform oriented for education.", "David", "P.S. Thanks to Tully and Kat for suggesting this!", "I\u2019m not using ROS in education yet but I\u2019ve been looking for a platform like this a while back. I was looking at this from a hobbyst perspective, so a low price was a must (Turtlebot looks good but it\u2019s a bit too expensive if someone wanted to buy it to get into ROS).", "Some of the platforms I looked at didn\u2019t have wheel encoders but I think it\u2019s a very good to have if you are teaching about odometry.", "Here are some things I was looking for in the robot platform:", "Functionality wise I wanted to be able to build a full ROS stack on the platform, starting with the drivers, through ros_control and ending with navigation and localization. I was hoping that with a platform like this I could guide students through all levels of software design for a robot.", "Sorry it\u2019s not a single thing but I thought some of these observations could be useful for you.", "Perfect educational resource to my mind is github repo with whole bunch of small working examples. Each of them compiles, runs, has reach comments and illustrates particular piece of ROS functionality.", "\nE.g.", "\nexample 1: how to use tf2", "\nexample 2: how to use can_bridge", "\nexample 3: how to use pointcloud and access individual points", "\nexample 4: you name it\u2026", "Awesome list! I agree price is the key thing that is missing here.", "How about if we designed it so that you\u2019d have ROS topics shared between a workstation computer and the robot itself and the heavy lifting was done on the workstation? Do you think that would be workable? This is one strategy to keep costs down for the robot - it also is a great example of the power of ROS.", "This reminds me a bit of the vector_ros: ", ". I can see a value in that for learning some high level concepts.", "The reason I was initially looking at low level concepts is because I think there is a niche in this area. At the time I was looking into it I couldn\u2019t find any tutorials on how to create a robot from scratch with ROS (I remember it took me quite a while to figure out how ros_control works).", "If you were to share the topics with the user\u2019s workstation would a user be able to add their own sensors on the robot? Because to me that is the single best thing about ROS and projects like vector_ros are very interesting but because you are not able to extend the robot the educational value might be a bit limited.", "That is definitely possible and workable, given decent WiFi. ", " is nontrivial I guess for complete noobs, but once that\u2019s taken care of, running code from a workstation removes the need to sync code from workstation to the robot.", "Running hardware interfaces of course needs to happen on the robot.", "Two places too look at are the SV-ROS github, which uses a Neato Botvac as a ROS platform, and the ROS by Example books by Patrick Goebel, available from Lulu. It is possible to build a ROS mock Turtlebot using a Raspberry Pi, and a Botvac that has a \u201cLidar\u201d for $300.", "In my experience with new comers (or in some cases even research labs that have been using ROS for years) is that network issues tend to cause them a lot of grief. I think the idea to have some of the logic run on the workstation is good but that would require the existence of very thorough noob friendly tutorials about how to get your network just right.", "Yes network provisioning is usually messy.", "We made it significantly less messy at Ubiquity Robotics by building PiFi. On bootup it scans all the available networks then boots in to AP mode with a unique network name (something like ubiquityrobotXXXX) where XXXX is the last 4 digits of the MAC address.", "You can connect to the robot via AP mode then when you do it presents a list of available networks and you can elect to connect to one of those or you can just stay in AP mode.", "It works well, although educational institutions some times have problems with new WiFi networks and also don\u2019t always make it easy to connect to the available infrastructure networks. Our solution is pretty slick and I can\u2019t think of a better one - but I am all ears for suggestions.", "I have used turtlebot3 for a while and for what it\u2019s intended for (as an introduction to ROS), it doesn\u2019t really justify the price of it.", "In the case of turtlebot3, the heavy lifting such as SLAM, map navigation is done on a workstation. But, I found this rather limiting.", "Ideally the onboard computer can function as a WiFi hotspot, possibly with a 4G/LTE dongle, so it can connect to internet if it needs to. In this case any computer could connect to such network. This will minimize network infrastructure.", "With waffle-pi, beginners could run the examples, launch the ros nodes and configure it with the available setup. But, that\u2019s pretty much the extent of what they can do. If they want to level up eventually, they will seek for better sensors and onboard computer.", "It\u2019s hard to scale the turtlebot or to upgrade it without replacing the core components (raspberry pi 3 model B, the dynamixel motors, etc.). When I plug the OpenCR to an Intel NUC, it doesn\u2019t immediately work out of the box, without some configuration and re-testing the Arduino code. This is what most beginners are not aware of.", "I also noticed people have used more powerful computers such as the Jetson TX2 to run processing onboard because tracking or depth cameras such as realsense / zed are too heavy for a raspberry pi 3, and yet they are quite popular among robotics researcher.", "Perhaps something like the PULP platform, OpenMV camera is a good alternative considering the costs.", "On the software side, it\u2019s not clear how to migrate to ROS 2 while keeping the software stack to be the same. This is if we still want to run the same SLAM / navigation packages. Though I understand it really depends on whether there\u2019s an upgrade on dependencies.", "In short, I would suggest that there should be a guidance on such \u201cupgrade\u201d path and scalability issues. As probably those who started to learn robotics are here to invest their time and skills in the long run.", "My apologies I\u2019m not too familiar with the tb3 but trying to understand why SLAM (I assume against an rplidar) qualifies as a heavyweight algorithm. What\u2019s being used as a default SLAM algorithm for tb3?", "The default setup (as per documentation) is to run the SLAM algorithm and navigation on a remote computer. Running gmapping (the default) or hector with rpildar A3 on the raspberrypi 3 model B is fine and I have tested that. But, I think cartographer is heavier. The other default option is to run frontier_exploration.", "To build the DWA local planner and map_server on the rpi3 itself requires pcl_ros to be installed as a dependency, which is unnecessary in most cases. So, up until now I never run the DWA local planner + map_server onboard.", "Then, once I setup a realsense T265 as a tracking camera to improve odometry, the realsense nodelets used up about 50% of memory on average. Occasionally the realsense camera manager crashes. If I run SLAM onboard.", "If I just use a 3S LiPo 5000mah battery (it\u2019s already larger than the default). The realsense fisheye camera nodes are not streaming their images. I assume there\u2019s not enough power from OpenCR. Although these camera images are fine if I use a larger capacity battery and higher voltage, such as 6S.", "For sure, I would need a better computer and more battery cells + capacity to run something like RTAB-Map with D435 + T265 for example.", "The impression is the rpi3 seem to only be utilised to run the turtlebot bringup or at most gmapping. I am in the process to at least migrate to rpi4 for its USB3 as realsense cameras worked best with USB3.", "At Stanford, one researcher ran a compute intensive Turtlebot II with extra  18v battery packs for a gaming Nvidia equiped laptop.  Runs up to 1 hour autonomous were obtained.", "But, I think cartographer is heavier.", "For what it is worth, I successfully ran cartographer on a ", " attached to a Turtlebot 2.  The caveat was that I had to run it in 2D mode; in 3D mode, it was too heavyweight.  You can see my short presentation from ROSCon 2017 about it ", ".", "Interesting, thanks for sharing your talk!", " does seem to have a better setup from what I\u2019ve seen. It doesn\u2019t have waffle plates. But, they are not really necessary.", "It comes with ", " camera or ASUS Xtion pro live if it\u2019s bought from clearpath robotics. Also dimension is better, while turtlebot3 require additional plates and plate support.", "I\u2019ve been teaching with Turtlebot 2 for several years now. In terms of hardware, it is hard to beat:", "There are a few downsides:", "I\u2019m at the point where I need to re-equip our robotics classroom, and I\u2019m at a loss. It looks like Turtlebot 3 is the default choice at this point, but it doesn\u2019t have any of the pros I list above.", "So\u2026 to answer your question. The platform I\u2019m looking for is something that looks a lot like TB2, with nice clean ROS and ROS2 packages.", "At Ubiquity Robotics we have been running platforms off RPi for a couple of years. We have a simplified navigation node called move-basic that is suitable for student learning and runs on low powered CPUs.", "I going to suggest a few things to make this discussion more productive.", "Please make it clear if you are an educator or not and if so what your student body looks like. Not all students are the same and it helps us if we understand the different constituencies of users.", "If you have a request or an idea please frame it in terms of the subject matter you want to teach not the hardware. Hardware changes from quarter to quarter; fundamentals more slowly.", "I\u2019m not an educator.", "But however we\u2019d like to consider robotics as an abstraction like how software is. It\u2019s not the same, hardware is also an important subject.", "A junior engineer could quickly drop a robot platform that doesn\u2019t do all the things he/she could see on videos of latest research.", "What they couldn\u2019t see is the efforts, workarounds, to make an algorithm work in a certain environment with certain setup of hardware.", "If I would to educate someone, be it a student or an engineer. I would emphasize:", "The JPL Mars rover is probably also a good reference as an educational platform.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Aforementioned wheel encoders", "IMU", "Onboard computer running ROS (extra points for this to be an option, lot\u2019s of people already have embedded computers)", "Ease of integration of custom sensors (consideration for space, robot payload)", "Some built-in DC-DC converters would be great to power external sensors (3.3, 5 and 12V would be superb)", "The size is good.  It is small enough to be safe and portable, but big enough to operate on human-scale problems like office delivery, tour guide etc.", "RGBD sensors provide a nice bang for the buck.  We can do 2d-slam, 3d-slam, computer vision etc.", "Using a laptop for computation makes it much easier to use in the classroom.  Trying to get networking set up correctly and keep it working for multiple robots is a pain.  It is much easier to just write some code on a laptop and plug it in.", "It would be nice if it were cheaper (though honestly, I don\u2019t think the price is unreasonable).", "The ROS Turtlebot packages are not very easy for novices to make sense of.  TB2 is relatively easy for beginners to use, but it is hard for beginners to modify. It would be nice to see an educational platform that serves as a clear, well-documented, example of how to set up a robot with ROS support.", "As far as I can tell, TB2 is going away. It\u2019s not clear if the Kobuki base is even being manufactured anymore.", "\n", "\n", "\n", "\n", "Environments that a robot should operate. This will cover perception, map, navigation, kinematics (if we\u2019re considering a more complicated movement than 2-wheeled differential drive robot)", "Some mechanical / electronics foundation (enough to get something working)", "OS & networking fundamentals, why Python / C++ as a common programming language. Although anyone can use other languages, but these two are the most common.", "Upgrade / scalability problems", "Algorithms", "Software management", "User Interface", "Security (this would be the advanced subject)"], "url": "https://discourse.ros.org/t/what-do-you-want-to-see-in-an-educational-ros-platform/10958"}
,{"title": "ROSshow: View ROS topics in the terminal (updated!)", "thread_contents": ["Have you ever ssh\u2019d into a robot wanting to see if a topic is outputting what it should? Have you ever wished you could get something more meaningful that what you\u2019d get by doing ", " or ", "? Have you ever been annoyed about how much work it takes to fire up a visualization in rviz just to see if a sensor is working? Read on.", "I created this more than a year ago but there were some suggestions to make it a ROS node as well as Python 2.x compatibility. Got too busy and never got around to it back then. Well, I think I\u2019ve done that now. It is now a ROS package that you can install. I also added support for several more data types, including CompressedImage, PointCloud2, and time series plots of most std_msgs types.", "Please keep the suggestions coming! Contributions welcome too!", "For std_msgs types, you\u2019ll get a time series plot, which is great for topics like motor current or voltage.", "Thanks for making this. It looks really useful! I think it might be a good idea to include some gifs showing it in action (I wonder if ", " could capture it?).", "Couldn\u2019t help myself!", "\nHere is a gif of rosshow with laser scan data (took a bag file from hector_slam):", "\n", "Some feedback on the install process: instead of running", "I run catkin_make as I would normally do with all of my ROS packages and it worked perfectly.", "Another nice addition would be an up to date list of message type that you support (ideally in the readme?).", "Then it would be nice for you to have a wiki page at: ", ", start thinking of a release and maybe refactor the project structure a bit :).", "I will be looking forward to see this project grow! Thanks for all the hard work!", "I will also feature your project at ", " either this week or a week after.", "Thanks! Much appreciated. I will add a list of supported types to the readme, and will create wiki page soon.", "Question on the install procedure \u2013 so yes, I intended for this to work via catkin_make, the normal way, and it should, but ", " to be installable to the system e.g. alongside \u201crostopic\u201d, \u201crosrun\u201d, and other commands, since I use this to visualize things from multiple workspaces on the same machine. That also allows me to use it as \u201crosshow /topic\u201d instead of \u201crosrun rosshow rosshow /topic\u201d. Of course, installing to the system is intended for convenience only and completely optional.", "For installing to the system I\u2019ve always found I needed to do", "which is quite the mouthful to type every time, which is why I created this ros-install-this which I use for every such tool I make that I want to use across multiple independent workspaces. Is there any plan for simpler way to \u201cinstall\u201d something to the system? Even for a lot of hardware driver ROS nodes that seldom change, I typically install those to the system as well instead of having a copy in every workspace.", "This is awesome! Thanks!", "Hi,", "There is another way to connect to a ROS Master using ", " and accessing all topics visualized in Rviz itself.", "You can control the bot remotely too. Hope this helps too.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosshow-view-ros-topics-in-the-terminal-updated/8422"}
,{"title": "ROS2 and DDS Security enhancement on arm platforms", "thread_contents": ["Hi,", "I\u2019m from arm. I\u2019d like to share our enhancement of ROS2 and DDS security based on arm platforms.", "In arm V7/V8 core architecture, we have ", " support (please take a look at this ", ") which can enhance the DDS Security Plugins currently implemented based on OpenSSL.", "Through the use of arm TrustZone feature, we can switch the system execution states into:", "\n\u2022\ta ", " (rich OS environment is executing here) and", "\n\u2022\ta physically isolated ", " (here a trusted OS is running which protects many ROS2 security assets, like root keys through hardware protection).", "\nAs shown in below figure, the ROS2 runs in Normal World (Non Trusted) and the security assets are protected in Secure World (Trusted). Since Secure World is physically isolated from Normal World, the Secure World can protect the ROS2/DDS sensitive security assets from leakage to Normal World even if Normal World is hacked.", "\n", "\nIn contrast, since OpenSSL runs in Normal World which is not considered as trusted, the security assets in OpenSSL might be vulnerable if rich OS or applications are hacked.", "With arm TrustZone, ROS2 with DDS security can run on billions of arm devices in an enhanced security environment.", "\nWe are very glad to discuss with you in details. Looking forward to hearing from you.", "Thank you.", " ", "Hi ", "! This is certainly interesting for those of us trying to make use of ROS 2 in real applications.", "Are you guys currently working into integrating these enhancements in any specific DDS implementation? Is there any working example available that we can review? Will you open source these \u201cimprovements\u201d in an open source DDS implementation together with the security plugins as a reference guide?. This will definitely help the community (and vendors) evaluate it and eventually adopt it.", "I\u2019m particularly interested in examples working with lead DDS vendors such as RTI. Any plans on this regard?", "Slightly off-topic, but ", "\u2019s post reminded me of it: ", " (", ").", "Hi ", "!", "We are currently developing our secure libraries based on arm TrustZone to support some popular DDS, together with the lead DDS vendors.", "\nWe will definitely release the implementations after it is verified, thus billions of arm platforms with ROS2/DDS can benefit from it.", "Thank you.", "Honestly, there is no \u201cabsolute\u201d secure. It\u2019s just about the cost(complexity) to crack.", "\nFor the CLKSCREW issue:", "\nYou can see the first step is to crack kernel in which case no secret can be kept for openssl solution.", "And this attack is HW design specific.", "\ne.g. The fundamental is that non-secure SW can control the regulators of clock and voltage.", "\nThis might not be true for all the platforms. As I know, some designs are using an MCU running in the secure world to access the regulators which are also in the secure world. The Kernel in the non-secure world can\u2019t access these regulators directly. It can just send the high-level requests to this secure MCU through Arm-TrustedFirmware. So this MCU is a safeguard to restrict the range of frequencies and voltages that can be configured.", "\nAnother example is to use the hardware crypto engine to generate/store the keys and decryption/encryption also happen in HW. In this case, CLKSCREW can\u2019t attack it anyway.", "Thanks ", " for posting this.", "\nIs it possible to share the more details about the design and the API that the DDS Security plugins should implement to be able to leverage TrustZone?", "Thanks!", "/cc ", "Hi ", ", we are very glad to share our design. Any suggestion is welcomed.", "Please help check the two documents in below.", "\n", " (1.3 MB) introduces the general idea of our design. A group of generic internal security APIs is inserted to connect vendor DDS Security Plugins and diverse security implementations, such as OpenSSL or OP-TEE based on arm TrustZone.", "\n", " (2.1 MB) introduces the generic internal security APIs in details.", "Looking forward to your feedback. Thanks a lot.", "thanks ", "!", "\nI think several members of the community will be interested in having a closer look at these documents.", "\nGetting feedback from DDS experts such as ", ", ", " and ", " would be extremely valuable. Moving towards adoption and support for Trustzone in all supported DDS implementations would be really awesome!", "Just started reviewing the material.  Looks promising so far.  Just want to get some clarification on overall design.", "ARM DDS SecureLib defines an interface library but does not appear to be responsible for context / resource management.  For a prospective deployment context, it is possible there could exist different levels of applications attempting to leverage TrustZone, thus a form of resource management that can properly handle the context switching securely would need to be supported.", "I am assuming that this form of feature would likely be presented in the TEECLibrary existing in layer ", " in the ", " due to context awareness or is this something that is being attempted to be pushed into the ", " ", "?", ", thanks for the comment.", "TEE Client library, TEE drivers and TEE-OS protect the TEE Client Application context in Non-Secure World. The context, including the resource, will be isolated and managed by TEE Client library, TEE drivers and TEE-OS.", "If there is any further question, please feel free to let me know.", "To give a small update on this topic: we implemented a small application in ROS2 that can be found in ", ". This application is using TrustZone support with Optee-OS to encrypt and decrypt ROS2 messages from the application level.", "\nAs of now, this is not using the ARM DDS SecureLib (as this is still under development) but it gives a small example on how to use TrustZone+Optee-OS to secure messages by having security assets in a secure memory.", "is there any update on this thread?", "\ncould share with us the development schedule for libddssec? approximately is just fine if possible.", "thanks in advance.", "Hi ", ",", "Since David\u2019s initial post on this initiative, we have been through some different proof-of-concepts. are now finishing the design for the shape we want it to be. In parallel we have been developing the project\u2019s skeleton with proper testing infrastructure and a Fast Model\u2019s based development environment we will release together with the library.", "We cannot give you a proper release date at the moment. The intention was to already have something released but some aspects of the investigation (finding the correct balance on what assets to move into the secure-world took longer than expected).", "We are getting closer, watch this space and we promise to give an update in a month or so.", "In the meantime, you haven\u2019t seen, here is our slides from the ROSCon2018 presentation of the project:", "\n", "3.14 MB", "\n", "Cheers,", "\nFilipe", "Hi ", "thanks for the quick response and sharing information.", "alright, i  was just curious and checking status for that.", "actually i was there to make presentation about \u201caibo\u201d at ROSCon2018@Madrid and had a little chat with David. i think that this security extension based on platform specific backend is really good, that brings me on this thread.", "thanks,", "\nTomoya", "Hello all,", "We released libddssec v0.1 on Github: ", ".", "As described in previous posts, the main goal of libddssec is to improve the security implementation of the DDS plugins on Arm based systems by leveraging the use of the TrustZone IP. It is based on Arm Trusted Firmware A (", ") and OP-TEE (", "). Secure assets (private keys,\u2026) are stored in a secure memory that can only be accessed by the Trusted Application. The Trusted Application is encrypted and stored into the Linux file system (under ", " ). A run-time daemon (tee-supplicant) must be launch in order to load the TA into OP-TEE OS. More information can be found under the readme or doc/ folder of the libddssec project.", "This initial release implements only the Authentication support.", "There is a PR (", ") which integrates libddssec into Fast-RTPS. ", "great work!!!", "tomoya", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-and-dds-security-enhancement-on-arm-platforms/3677"}
,{"title": "TurtleBot3 software and firmware update and 'waffle_pi'", "thread_contents": ["Hello everyone ", "I announce that TurtleBot3 is huge updated!!!", "\nThis update considered many issues and requests from users. We are sincerely thankful to them.", "\nMore interest makes more progress. If you have any issues or suggestions, please feel free to get ", "Existing users need to download new version in master branch of turtlebot3 and turtlebot3_msgs", "Direct download in repository ", ", ", "or using command line", " sudo rm -rf turtlebot3/", "\n$ git clone ", " sudo rm -rf turtlebot3_msgs/", "\n$ git clone ", "Open Arduino -> Toos -> Board: -> Board Manager\u2026 -> Update (v 1.0.15)", "Best regards!", "\nDarby", "I assume the Waffle Pi is a lower-cost waffle with a Raspberry Pi instead of the Joule. This is great news!", "Is there any progress on a Burger or Waffle version that comes with no sensors or compute boards, for those of us who have all that stuff already?", "Hi ", " thanks alot for the update.", "\ni have a question\u2026 Do we need to update the OpenCr board? and How? i get \u201cChecksum does not match\u201d error . thanks alot\u2026 ", "Hello ", "You can follow below instruction", "or you can find more detail in ", "Thanks", "\nDarby", "I am interested in using the new software update on my TB3 and thank you for these instructions. Are there also updates planned or available to the documentation to describe the ROS commands necessary to show the new Publisher /diagnostic, /battery_state information and to connect a speaker to the RasPi3 or OpenCR to hear the generated sounds? Ross", "Hello ", " ", "We have a plan to be compatible with ROS2 though we don\u2019t have any details.", "\nWe provide a ", " and it will be added more information soon for these software updates.", "\nAdditionally, you don\u2019t need to connect separate speaker due to OpenCR already has it ", "Thanks", "\nDarby", "Darby, Following your good instructions and compiling the revised catkin_ws/src, I easily updated the ROS SBC TB3 and OpenCR software and very pleased that it all works well, including \u201crostopic echo /diagnostics\u201d node that is very informative. I look forward to the new wiki to learn how to connect the new /sound Subscription that roswtf reports as unconnected. I note there is a reference to a Adafruit display driver in the TB3 github-Is that a future additional feature? A small request is make available the pdf version of the previous wiki.Thank you and your colleagues for their hard work and producing an excellent, fun and educational robot :).", "I\u2019m still getting a checksum mismatch. I upgraded to OpenCR 1.0.15 and also tried 1.0.16. I upgraded with turtlebot3.git and on turtlebot3_msg.git on both my RemotePC and TurtleBot.", "This is the execution", "\n<", "\n$ roslaunch turtlebot3_bringup turtlebot3_core.launch", "\n\u2026 logging to /home/eepp/.ros/log/5a7d18a4-0545-11e8-9b6b-080027c0cb1e/roslaun", "\nch-orras-3438.log", "\nChecking log directory for disk usage. This may take awhile.", "\nPress Ctrl-C to interrupt", "\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS", "NODES", "ROS_MASTER_URI=http://10.0.0.159:11311", "process[turtlebot3_core-1]: started with pid [3447]", "\n[INFO] [1517266129.977476]: ROS Serial Python Node", "\n[INFO] [1517266130.051262]: Connecting to /dev/ttyACM0 at 115200 baud", "\n[ERROR] [1517266132.304481]: Creation of publisher failed: Checksum does not ma", "\ntch: 427f77f85da38bc1aa3f65ffb673c94c,d537ed7b8d95065b6c83830430b93911", "\n[INFO] [1517266132.362502]: Note: publish buffer size is 1024 bytes", "\n\u2026", "\n/>", "Nice! we are preparing updated wiki including your requests.", "\nThanks you for your interest ", "Hello ", " ", "Have you set network config??", "\nYou can check how to config network btw RemotePC and TB3 on ", "Best regards", "\nDarby", "ROS guys ", "Thank you for your interest on TB3 ! (I am happy as if i get a sweet coffee)", "\nBut ", " is not proper page to create issue ", "If you have any question for TB3, please use Github ", " or ", ".", "\nYou can meet me in there ", "Thanks", "\nDarby", "Darby,", "Thanks for your quick response. This is how I set it up", "Remote PC", "$ ifconfig", "\nenp0s3    Link encap:Ethernet  HWaddr 08:00:27:c0:cb:1e", "\ninet addr:10.0.0.159  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.159", "\nexport PATH=$PATH:$HOME/tools/arduino-1.8.5", "turtlebot3", "$ ifconfig", "\nwlp1s0    Link encap:Ethernet  HWaddr a0:c5:89:4b:3a:e3", "\ninet addr:10.0.0.158  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.158", "\nexport TURTLEBOT3_MODEL=waffle", "Ed", "I followed the instructions in 7.1.5  for Porting OpenCR1.0 to Arduino IDE. I want to reason through my issue. I\u2019m not familiar with Arduino checksum generation. I connected my OpenCR to my Remote PC. I ran the Arduino IDE from there and loaded the bootloader. So I assume I can load the OpenCR from any machine as long as I have the right bootloader version. Where and how are the checksums being generated and compared? Will I have the same bootloader checksum as everyone else in the world? If that is correct, someone should be able to tell me if I have the correct one and tell me which side is incorrect. If not correct, does the checksum depend on my hardware and/or software.", "ed", "Hi Darby", "Just wanted to confirm when you pointed to this article for updating, you meant: ", "Is it possible to update the board- manager as defined in the wiki using dfu-util?", "Or you meant some other steps such as: ", "Can you please confirm?", "Thanks", "\nSandip", "Hi ", " ", "You don\u2019t need to enter dfu mode.", "\nIf you have used OpenCR, you just update it using ", ".", "Please follow below link, it will help you", "Thanks", "\nDarby", "Thanks Darby for the reply,", "\nI updated the Board Manager on Arduino IDE to 1.0.15 (also tried 1.0.16) --> the tried updating the Bootloader using Arduino IDE (but failed) with the following error message:", "I made sure the", "lsusb", "is showing the STMicroelectronics (NOTE: There is no DFU mode written as you mention in the emanual - but this entry in lsusb starts showing up only after DFU mode triggered on using Boot + Reset button)", "Can you let me know some tips to go ahead with debugging?", "Thanks", "\nSandip", "Hi ", ",", "This is a space for discussion, so the question is not appropriate.", "\nI will continue on the issue page of the link below.", "\n", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "\n", "Thanks!", "Exactly same Checksum error here - before I start digging\u2026was it solved? any ideas? Tnx Michael", "BIG SORRY - my fault\u2026didn\u00b4t read the error messages after uploading correctly\u2026no issue anymore!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["turtlebot3_controller - include RC100(for remote control) library", "turtlebot3_diagnosis - include diagnostic functions", "turtlebot3_motor_driver - include DYNAMIXEL SDK", "turtlebot3_sensor - include functions for IMU, battery, magnetic filed and analog Input", "/version_info - Contains the hardware, firmware and software information", "/battery_state - Contains battery voltage and status", "/magnetic_field - Contains magnetic field information", "/diagnostic - Contains self diagnostic information", "/sound - Output beep sound", "/motor_power - Dynamixel torque on/off", "/reset - Reset odometry and re-calibration IMU", "add Sound.msg", "Simple command makes USB setup", "It shows state of IMU, motor, lidar, battery, button and version information", "Add Waffle PI", "Now, we are preparing new version of TurtleBot3 called ", ".  Meet Waffle PI in ", " before you get an it.", "Software (v 1.0.0)", "Firmware", "I did a roscore on my RemotePC", "a ssh into the TurtleBot", "did a \u201croslaunch turtlebot3_bringup turtlebot3_core.launch\u201d on the TurtleBot", "/rosdistro: kinetic", "/rosversion: 1.12.12", "/turtlebot3_core/baud: 115200", "/turtlebot3_core/port: /dev/ttyACM0"], "url": "https://discourse.ros.org/t/turtlebot3-software-and-firmware-update-and-waffle-pi/3729"}
,{"title": "New Packages for Indigo Jade and Kinetic 2016-10-24", "thread_contents": ["We have a simultaneous sync of Indigo Jade and Kinetic packages. This includes close to 100 new packages as well as several hundred updated packages. Please see the full details below. And as always thank you to everyone who has contributed to the many updates included!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "#", " to kinetic", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-aubo-driver: 0.2.1-0", "ros-indigo-aubo-trajectory: 0.2.1-0", "ros-indigo-cob-docker-control: 0.6.5-0", "ros-indigo-cob-hand: 0.6.1-0", "ros-indigo-cob-hand-bridge: 0.6.1-0", "ros-indigo-cob-phidget-em-state: 0.6.8-0", "ros-indigo-cob-phidget-power-state: 0.6.8-0", "ros-indigo-cob-reflector-referencing: 0.6.5-0", "ros-indigo-cv-detection: 0.0.2-0", "ros-indigo-grid-map-pcl: 1.4.1-0", "ros-indigo-ihmc-msgs: 0.8.0-6", "ros-indigo-ihmc-ros-common: 0.8.0-6", "ros-indigo-ihmc-ros-control: 0.5.0-1", "ros-indigo-ihmc-ros-core: 0.8.0-6", "ros-indigo-ihmc-ros-diagnostics: 0.8.0-1", "ros-indigo-ihmc-ros-java-adapter: 0.8.0-6", "ros-indigo-imagezero: 0.2.3-0", "ros-indigo-imagezero-image-transport: 0.2.3-0", "ros-indigo-imagezero-ros: 0.2.3-0", "ros-indigo-libconcorde-tsp-solver: 0.6.7-0", "ros-indigo-libdlib: 0.6.9-0", "ros-indigo-libopengm: 0.6.9-0", "ros-indigo-lpg-planner: 2.0.17-0", "ros-indigo-mrpt-rbpf-slam: 0.1.3-0", "ros-indigo-nao-dcm-bringup: 0.0.2-0", "ros-indigo-reapp-description: 0.1.1-0", "ros-indigo-reapp-msgs: 0.1.1-0", "ros-indigo-ros-type-introspection: 0.3.1-0", "ros-indigo-rwt-image-view: 0.0.3-1", "ros-indigo-rwt-moveit: 0.0.3-1", "ros-indigo-rwt-plot: 0.0.3-1", "ros-indigo-rwt-speech-recognition: 0.0.3-1", "ros-indigo-rwt-utils-3rdparty: 0.0.3-1", "ros-indigo-sick-visionary-t-driver: 0.0.3-1", "ros-indigo-slic: 2.0.17-0", "ros-indigo-visualization-rwt: 0.0.3-1", "ros-indigo-warthog-control: 0.0.1-0", "ros-indigo-warthog-description: 0.0.1-0", "ros-indigo-warthog-desktop: 0.0.1-0", "ros-indigo-warthog-msgs: 0.0.1-0", "ros-indigo-warthog-viz: 0.0.1-0", "ros-indigo-ar-track-alvar: 0.5.3-0 -> 0.5.4-0", "ros-indigo-aruco: 0.1.0-0 -> 0.2.0-0", "ros-indigo-aruco-msgs: 0.1.0-0 -> 0.2.0-0", "ros-indigo-aruco-ros: 0.1.0-0 -> 0.2.0-0", "ros-indigo-assimp-devel: 2.0.14-0 -> 2.0.17-0", "ros-indigo-aubo-description: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-gazebo: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-i5-moveit-config: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-kinematics: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-msgs: 0.1.1-3 -> 0.2.1-0", "ros-indigo-bayesian-belief-networks: 2.0.14-0 -> 2.0.17-0", "ros-indigo-camera-calibration-parsers: 1.11.10-0 -> 1.11.11-0", "ros-indigo-camera-info-manager: 1.11.10-0 -> 1.11.11-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-base-drive-chain: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-base-velocity-smoother: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-bms-driver: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-bringup: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-bringup-sim: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-calibration-data: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-cam3d-throttle: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-camera-sensors: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-canopen-motor: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-cartesian-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-collision-velocity-filter: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-command-gui: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-command-tools: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-common: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-control: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-control-mode-adapter: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-control-msgs: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-controller-configuration-gazebo: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-dashboard: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-default-env-config: 0.6.3-0 -> 0.6.4-0", "ros-indigo-cob-default-robot-behavior: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-robot-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-description: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-driver: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-elmo-homing: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-environments: 0.6.3-0 -> 0.6.4-0", "ros-indigo-cob-extern: 0.6.4-0 -> 0.6.9-0", "ros-indigo-cob-footprint-observer: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-frame-tracker: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-gazebo: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-objects: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-worlds: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-generic-can: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-hardware-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-head-axis: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-image-flip: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-interactive-teleop: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-lbr: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-light: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-mimic: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-model-identifier: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-monitoring: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-msgs: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-object-detection-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-object-detection-visualizer: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-obstacle-distance: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-omni-drive-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-perception-common: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-perception-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-phidgets: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-relayboard: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-robots: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-safety-controller: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-scan-unifier: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-script-server: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-sick-lms1xx: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-sick-s300: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-simulation: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-sound: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-srvs: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-substitute: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-teleop: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-trajectory-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-twist-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-undercarriage-ctrl-node: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-utilities: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-vision-utils: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-voltage-control: 0.6.7-0 -> 0.6.8-0", "ros-indigo-collada-urdf-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-indigo-compressed-depth-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-compressed-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-concert-conductor: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-master: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-concert-schedulers: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-link-graph: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-manager: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-concert-service-utilities: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-software-farmer: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-utilities: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-workflow-engine-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-cv-bridge: 1.11.13-0 -> 1.11.14-0", "ros-indigo-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-downward: 2.0.14-0 -> 2.0.17-0", "ros-indigo-eus-assimp: 0.2.4-0 -> 0.2.5-0", "ros-indigo-euscollada: 0.2.4-0 -> 0.2.5-0", "ros-indigo-eusurdf: 0.2.4-0 -> 0.2.5-0", "ros-indigo-ff: 2.0.14-0 -> 2.0.17-0", "ros-indigo-ffha: 2.0.14-0 -> 2.0.17-0", "ros-indigo-frida-driver: 0.6.4-0 -> 0.6.5-0", "ros-indigo-gateway-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-grid-map-core: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-cv: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-filters: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-loader: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-msgs: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-ros: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-visualization: 1.4.0-0 -> 1.4.1-0", "ros-indigo-hironx-calibration: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hironx-moveit-config: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hironx-ros-bridge: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-indigo-husky-base: 0.2.5-0 -> 0.2.6-0", "ros-indigo-husky-bringup: 0.2.5-0 -> 0.2.6-0", "ros-indigo-husky-robot: 0.2.5-0 -> 0.2.6-0", "ros-indigo-image-common: 1.11.10-0 -> 1.11.11-0", "ros-indigo-image-exposure-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-image-geometry: 1.11.13-0 -> 1.11.14-0", "ros-indigo-image-transport: 1.11.10-0 -> 1.11.11-0", "ros-indigo-image-transport-plugins: 1.9.3-0 -> 1.9.5-0", "ros-indigo-imu-monitor: 1.6.16-2 -> 1.6.16-4", "ros-indigo-jsk-3rdparty: 2.0.14-0 -> 2.0.17-0", "ros-indigo-jsk-apc2015-common: 1.5.1-0 -> 2.0.0-0", "ros-indigo-jsk-apc2016-common: 1.5.1-0 -> 2.0.0-0", "ros-indigo-jsk-interactive: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-interactive-marker: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-interactive-test: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-model-tools: 0.2.4-0 -> 0.2.5-0", "ros-indigo-jsk-roseus: 1.5.3-0 -> 1.6.0-0", "ros-indigo-jsk-rqt-plugins: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-rviz-plugins: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-visualization: 1.0.33-0 -> 1.0.34-0", "ros-indigo-julius: 2.0.14-0 -> 2.0.17-0", "ros-indigo-laser-filters-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-indigo-libcmt: 2.0.14-0 -> 2.0.17-0", "ros-indigo-libntcan: 0.6.4-0 -> 0.6.9-0", "ros-indigo-libpcan: 0.6.4-0 -> 0.6.9-0", "ros-indigo-libphidgets: 0.6.4-0 -> 0.6.9-0", "ros-indigo-librealsense: 0.9.2-3 -> 1.11.0-1", "ros-indigo-libsiftfast: 2.0.14-0 -> 2.0.17-0", "ros-indigo-mapviz: 0.0.6-0 -> 0.0.7-0", "ros-indigo-marti-can-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-common-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-data-structures: 0.0.12-0 -> 0.0.13-0", "ros-indigo-marti-nav-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-perception-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-sensor-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-visualization-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-mini-maxwell: 2.0.14-0 -> 2.0.17-0", "ros-indigo-mrpt-ekf-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-indigo-mrpt-icp-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-indigo-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-nextage-description: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-gazebo: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-ik-plugin: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-moveit-config: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-ros-bridge: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nlopt: 2.0.14-0 -> 2.0.17-0", "ros-indigo-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-opt-camera: 2.0.14-0 -> 2.0.17-0", "ros-indigo-parrot-arsdk: 3.9.1-6 -> 3.10.1-0", "ros-indigo-pgm-learner: 2.0.14-0 -> 2.0.17-0", "ros-indigo-pid: 0.0.17-0 -> 0.0.18-0", "ros-indigo-pointgrey-camera-description: 0.12.1-1 -> 0.12.2-0", "ros-indigo-pointgrey-camera-driver: 0.12.1-1 -> 0.12.2-0", "ros-indigo-polled-camera: 1.11.10-0 -> 1.11.11-0", "ros-indigo-pr2-bringup: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-camera-synchronizer: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-computer-monitor: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-controller-configuration: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-ethercat: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-robot: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-run-stop-auto-restart: 1.6.16-2 -> 1.6.16-4", "ros-indigo-prace-common: 0.6.4-0 -> 0.6.5-0", "ros-indigo-prace-gripper-driver: 0.6.4-0 -> 0.6.5-0", "ros-indigo-raw-description: 0.6.5-0 -> 0.6.6-0", "ros-indigo-realsense-camera: 1.4.0-0 -> 1.5.0-0", "ros-indigo-ridgeback-control: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-description: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-msgs: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-navigation: 0.1.7-0 -> 0.1.8-0", "ros-indigo-rocon-app-manager: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-app-manager-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-app-platform: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-app-utilities: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-apps: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-bubble-icons: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-concert: 0.6.11-0 -> 0.6.11-1", "ros-indigo-rocon-console: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-device-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-ebnf: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-gateway: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-gateway-tests: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-gateway-utils: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-hub: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-hub-client: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-icons: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-interaction-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-interactions: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-launch: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-master-info: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-multimaster: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-python-comms: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-redis: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-utils: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-wifi: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-semantic-version: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-service-pair-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-std-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-test: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-tf-reconstructor: 0.6.11-0 -> 0.6.11-1", "ros-indigo-rocon-tools: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-tutorial-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-unreliable-experiments: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-uri: 0.1.23-0 -> 0.1.23-1", "ros-indigo-roseus: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-mongo: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-smach: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-tutorials: 1.5.3-0 -> 1.6.0-0", "ros-indigo-rospatlite: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rosping: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rostwitter: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rtmros-hironx: 1.1.16-0 -> 1.1.17-0", "ros-indigo-rtmros-nextage: 0.7.8-0 -> 0.7.10-0", "ros-indigo-rviz: 1.11.14-0 -> 1.11.15-0", "ros-indigo-scheduler-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-schunk-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-libm5api: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-modular-robotics: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-powercube-chain: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-sdh: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-sdhx: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.7-0 -> 0.6.8-0", "ros-indigo-statistics-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-swri-console-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-geometry-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-image-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-math-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-nodelet: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-opencv-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-prefix-tools: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-roscpp: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-rospy: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-route-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-serial-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-string-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-system-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-transform-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-yaml-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-theora-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-vision-opencv: 1.11.13-0 -> 1.11.14-0", "ros-indigo-voice-text: 2.0.14-0 -> 2.0.17-0", "ros-indigo-web-video-server: 0.0.4-0 -> 0.0.5-0", "ros-indigo-wfov-camera-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-grid-map", "ros-indigo-grid-map-demos", "ros-indigo-tiago-bringup", "ros-indigo-tiago-gazebo", "ros-indigo-tiago-moveit-config", "ros-indigo-tiago-robot", "ros-indigo-tiago-simulation", "Alexander Bubeck", "Alexander Tiderko", "Andy Zelenak", "Bence Magyar", "Benjamin Maidel", "Daniel Stonier", "David Gossow", "Davide Faconti", "Devon Ash", "Dongwook Lee", "Doug Stephen", "Ed Venator", "Edmond DuPont", "Elliot Johnson", "Felix Messmer", "Florian Weisshardt", "Hitoshi Kamada", "IK Fast Plugin Creater", "Isaac I.Y. Saito", "Isaac IY Saito", "Isaac Isao Saito", "Jack O\u2019Quin", "Jan Fischer", "Jesper Smith", "Jihoon Lee", "Jose Luis", "Jose Luis Blanco Claraco", "Joshua Hampp", "Julius Kammerl", "Kei Okada", "Kentaro Wada", "Kris Kozak", "Liuxin", "Mani Monajjemi", "Marc Alban", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Mikael Arguedas", "Mike Purvis", "MoveIt Setup Assistant", "Nadia Hammoudeh Garcia", "Noda Shintaro", "P. J. Reed", "Paul Bovbel", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Rajvi Jingar", "Richard Bormann", "Russell Toris", "Ryohei Ueda", "Scott Niekum", "Sergey Dorodnicov", "TORK", "Takuya Nakaoka", "Thiago de Freitas", "Tianjiang Hu", "Tony Baltovski", "Vincent Rabaud", "Vladislav Tananaev", "Yohei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "k-okada", "liuxin", "ros-jade-grid-map-pcl: 1.4.1-0", "ros-jade-imagezero: 0.2.3-0", "ros-jade-imagezero-image-transport: 0.2.3-0", "ros-jade-imagezero-ros: 0.2.3-0", "ros-jade-lpg-planner: 2.0.17-0", "ros-jade-mrpt-rbpf-slam: 0.1.3-0", "ros-jade-mrpt-slam: 0.1.3-0", "ros-jade-ros-type-introspection: 0.3.1-0", "ros-jade-slic: 2.0.17-0", "ros-jade-swri-console: 0.2.0-0", "ros-jade-ar-track-alvar: 0.5.3-0 -> 0.5.4-0", "ros-jade-assimp-devel: 2.0.14-0 -> 2.0.17-0", "ros-jade-bayesian-belief-networks: 2.0.14-0 -> 2.0.17-0", "ros-jade-camera-calibration-parsers: 1.11.10-0 -> 1.11.11-0", "ros-jade-camera-info-manager: 1.11.10-0 -> 1.11.11-0", "ros-jade-collada-urdf-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-jade-compressed-depth-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-compressed-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-cv-bridge: 1.11.13-0 -> 1.11.14-0", "ros-jade-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-downward: 2.0.14-0 -> 2.0.17-0", "ros-jade-eus-assimp: 0.2.4-0 -> 0.2.5-0", "ros-jade-euscollada: 0.2.4-0 -> 0.2.5-0", "ros-jade-eusurdf: 0.2.4-0 -> 0.2.5-0", "ros-jade-ff: 2.0.14-0 -> 2.0.17-0", "ros-jade-ffha: 2.0.14-0 -> 2.0.17-0", "ros-jade-grid-map: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-core: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-cv: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-demos: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-filters: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-loader: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-msgs: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-ros: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-visualization: 1.4.0-0 -> 1.4.1-0", "ros-jade-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-jade-image-common: 1.11.10-0 -> 1.11.11-0", "ros-jade-image-exposure-msgs: 0.12.1-0 -> 0.12.2-0", "ros-jade-image-geometry: 1.11.13-0 -> 1.11.14-0", "ros-jade-image-transport: 1.11.10-0 -> 1.11.11-0", "ros-jade-image-transport-plugins: 1.9.3-0 -> 1.9.5-0", "ros-jade-jsk-3rdparty: 2.0.14-0 -> 2.0.17-0", "ros-jade-jsk-interactive: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-interactive-marker: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-interactive-test: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-model-tools: 0.2.4-0 -> 0.2.5-0", "ros-jade-jsk-roseus: 1.5.3-0 -> 1.6.0-0", "ros-jade-jsk-rqt-plugins: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-rviz-plugins: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-visualization: 1.0.33-0 -> 1.0.34-0", "ros-jade-julius: 2.0.14-0 -> 2.0.17-0", "ros-jade-libcmt: 2.0.14-0 -> 2.0.17-0", "ros-jade-libsiftfast: 2.0.14-0 -> 2.0.17-0", "ros-jade-marti-can-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-common-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-data-structures: 0.1.5-0 -> 0.1.6-0", "ros-jade-marti-nav-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-perception-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-sensor-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-visualization-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-mini-maxwell: 2.0.14-0 -> 2.0.17-0", "ros-jade-mrpt-ekf-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-jade-mrpt-ekf-slam-3d: 0.1.1-0 -> 0.1.3-0", "ros-jade-mrpt-icp-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-jade-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-nerian-sp1: 1.3.3-0 -> 1.4.0-0", "ros-jade-nlopt: 2.0.14-0 -> 2.0.17-0", "ros-jade-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-opt-camera: 2.0.14-0 -> 2.0.17-0", "ros-jade-parrot-arsdk: 3.9.1-3 -> 3.10.1-0", "ros-jade-pgm-learner: 2.0.14-0 -> 2.0.17-0", "ros-jade-pid: 0.0.17-0 -> 0.0.18-0", "ros-jade-pointgrey-camera-description: 0.12.1-0 -> 0.12.2-0", "ros-jade-pointgrey-camera-driver: 0.12.1-0 -> 0.12.2-0", "ros-jade-polled-camera: 1.11.10-0 -> 1.11.11-0", "ros-jade-roseus: 1.5.3-0 -> 1.6.0-0", "ros-jade-roseus-mongo: 1.5.3-0 -> 1.6.0-0", "ros-jade-roseus-smach: 1.5.3-0 -> 1.6.0-0", "ros-jade-rospatlite: 2.0.14-0 -> 2.0.17-0", "ros-jade-rosping: 2.0.14-0 -> 2.0.17-0", "ros-jade-rostwitter: 2.0.14-0 -> 2.0.17-0", "ros-jade-rviz: 1.11.14-0 -> 1.11.15-0", "ros-jade-statistics-msgs: 0.12.1-0 -> 0.12.2-0", "ros-jade-swri-console-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-geometry-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-image-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-math-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-nodelet: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-opencv-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-prefix-tools: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-roscpp: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-route-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-serial-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-string-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-system-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-transform-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-yaml-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-teb-local-planner: 0.5.1-0 -> 0.5.2-0", "ros-jade-theora-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-vision-opencv: 1.11.13-0 -> 1.11.14-0", "ros-jade-voice-text: 2.0.14-0 -> 2.0.17-0", "ros-jade-wfov-camera-msgs: 0.12.1-0 -> 0.12.2-0", "Alexander Tiderko", "Andy Zelenak", "Christoph R\u00f6smann", "David Gossow", "Davide Faconti", "Edmond DuPont", "Elliot Johnson", "Hitoshi Kamada", "Jack O\u2019Quin", "Jose Luis", "Jose Luis Blanco Claraco", "Julius Kammerl", "Kei Okada", "Konstantin Schauwecker", "Kris Kozak", "Mani Monajjemi", "Marc Alban", "Mike Purvis", "Noda Shintaro", "P. J. Reed", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Ryohei Ueda", "Scott Niekum", "Takuya Nakaoka", "Tony Baltovski", "Vincent Rabaud", "Vladislav Tananaev", "Yohei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "k-okada", "ros-kinetic-dynamic-tf-publisher: 2.1.2-1", "ros-kinetic-grid-map-pcl: 1.4.1-1", "ros-kinetic-image-view2: 2.1.2-1", "ros-kinetic-imagezero: 0.2.3-0", "ros-kinetic-imagezero-image-transport: 0.2.3-0", "ros-kinetic-imagezero-ros: 0.2.3-0", "ros-kinetic-jsk-common: 2.1.2-1", "ros-kinetic-jsk-data: 2.1.2-1", "ros-kinetic-jsk-network-tools: 2.1.2-1", "ros-kinetic-jsk-roseus: 1.6.0-0", "ros-kinetic-jsk-tilt-laser: 2.1.2-1", "ros-kinetic-jsk-tools: 2.1.2-1", "ros-kinetic-jsk-topic-tools: 2.1.2-1", "ros-kinetic-laser-scan-publisher-tutorial: 0.2.3-0", "ros-kinetic-multi-map-server: 2.1.2-1", "ros-kinetic-navigation-stage: 0.2.3-0", "ros-kinetic-navigation-tutorials: 0.2.3-0", "ros-kinetic-octomap-mapping: 0.6.1-0", "ros-kinetic-octomap-server: 0.6.1-0", "ros-kinetic-odometry-publisher-tutorial: 0.2.3-0", "ros-kinetic-point-cloud-publisher-tutorial: 0.2.3-0", "ros-kinetic-robot-setup-tf-tutorial: 0.2.3-0", "ros-kinetic-roomba-stage: 0.2.3-0", "ros-kinetic-ros-type-introspection: 0.3.1-0", "ros-kinetic-roseus: 1.6.0-0", "ros-kinetic-roseus-smach: 1.6.0-0", "ros-kinetic-rqt-wrapper: 0.1.3-0", "ros-kinetic-simple-navigation-goals-tutorial: 0.2.3-0", "ros-kinetic-swri-console: 0.2.0-0", "ros-kinetic-virtual-force-publisher: 2.1.2-1", "ros-kinetic-wts-driver: 1.0.4-0", "ros-kinetic-concert-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-concert-service-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-concert-workflow-engine-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-gateway-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-geometric-shapes: 0.5.1-0 -> 0.5.2-0", "ros-kinetic-grid-map: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-core: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-cv: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-demos: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-filters: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-loader: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-msgs: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-ros: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-visualization: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-kinetic-librealsense: 1.11.0-0 -> 1.11.0-1", "ros-kinetic-marti-can-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-common-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-data-structures: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-marti-nav-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-perception-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-sensor-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-visualization-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-mavlink: 2016.9.9-0 -> 2016.10.10-0", "ros-kinetic-moveit-python: 0.2.17-0 -> 0.2.17-1", "ros-kinetic-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-nerian-sp1: 1.3.3-0 -> 1.4.0-0", "ros-kinetic-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-pid: 0.0.17-0 -> 0.0.18-0", "ros-kinetic-robot-state-publisher: 1.13.2-0 -> 1.13.3-0", "ros-kinetic-rocon-app-manager: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-app-manager-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-app-platform: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-app-utilities: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-apps: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-bubble-icons: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-console: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-device-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-ebnf: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-gateway: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-gateway-tests: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-gateway-utils: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-hub: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-hub-client: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-icons: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-interaction-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-interactions: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-launch: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-master-info: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-multimaster: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-python-comms: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-redis: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-utils: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-wifi: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-semantic-version: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-service-pair-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-std-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-test: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-tools: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-tutorial-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-unreliable-experiments: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-uri: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rtabmap-ros: 0.11.8-0 -> 0.11.8-1", "ros-kinetic-rviz: 1.12.1-0 -> 1.12.3-0", "ros-kinetic-scheduler-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-swri-console-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-geometry-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-image-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-math-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-nodelet: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-opencv-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-prefix-tools: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-roscpp: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-route-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-serial-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-string-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-system-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-transform-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-yaml-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-teb-local-planner: 0.6.3-0 -> 0.6.4-0", "AlexV", "Alexander Tiderko", "Andy Zelenak", "Armin Hornung", "Chittaranjan Srinivas Swaminathan", "Christoph R\u00f6smann", "Daniel Stonier", "David Gossow", "Davide Faconti", "Dongwook Lee", "Edmond DuPont", "Elliot Johnson", "Ioan Sucan", "Jihoon Lee", "Kei Okada", "Konstantin Schauwecker", "Kris Kozak", "Marc Alban", "Mathieu Labbe", "Michael Ferguson", "P. J. Reed", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Ryohei Ueda", "Sergey Dorodnicov", "Vladimir Ermakov", "William Woodall", "YoheiKakiuchi"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-jade-and-kinetic-2016-10-24/722"}
,{"title": "New Packages for Kinetic 2017-07-25", "thread_contents": ["We\u2019re happy to announce another set of packages for Kinetic as well. We have 86 new packages as well as 78 updated packages in this sync.", "Thank you to all the contributors and maintainers who make these packages available to the community.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-cob-3d-mapping-msgs: 0.6.10-0", "ros-kinetic-cob-base-drive-chain: 0.6.10-0", "ros-kinetic-cob-base-velocity-smoother: 0.7.0-0", "ros-kinetic-cob-bms-driver: 0.6.10-0", "ros-kinetic-cob-calibration-data: 0.6.7-0", "ros-kinetic-cob-cam3d-throttle: 0.6.10-0", "ros-kinetic-cob-camera-sensors: 0.6.10-0", "ros-kinetic-cob-canopen-motor: 0.6.10-0", "ros-kinetic-cob-cartesian-controller: 0.7.0-0", "ros-kinetic-cob-collision-velocity-filter: 0.7.0-0", "ros-kinetic-cob-common: 0.6.7-0", "ros-kinetic-cob-control: 0.7.0-0", "ros-kinetic-cob-control-mode-adapter: 0.7.0-0", "ros-kinetic-cob-control-msgs: 0.7.0-0", "ros-kinetic-cob-default-env-config: 0.6.5-0", "ros-kinetic-cob-description: 0.6.7-0", "ros-kinetic-cob-docker-control: 0.6.6-0", "ros-kinetic-cob-driver: 0.6.10-0", "ros-kinetic-cob-elmo-homing: 0.6.10-0", "ros-kinetic-cob-environments: 0.6.5-0", "ros-kinetic-cob-extern: 0.6.11-0", "ros-kinetic-cob-footprint-observer: 0.7.0-0", "ros-kinetic-cob-frame-tracker: 0.7.0-0", "ros-kinetic-cob-gazebo-plugins: 0.7.1-0", "ros-kinetic-cob-gazebo-ros-control: 0.7.1-0", "ros-kinetic-cob-generic-can: 0.6.10-0", "ros-kinetic-cob-hand: 0.6.2-0", "ros-kinetic-cob-hand-bridge: 0.6.2-0", "ros-kinetic-cob-head-axis: 0.6.10-0", "ros-kinetic-cob-image-flip: 0.6.10-0", "ros-kinetic-cob-light: 0.6.10-0", "ros-kinetic-cob-mimic: 0.6.10-0", "ros-kinetic-cob-model-identifier: 0.7.0-0", "ros-kinetic-cob-msgs: 0.6.7-0", "ros-kinetic-cob-object-detection-msgs: 0.6.10-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.10-0", "ros-kinetic-cob-obstacle-distance: 0.7.0-0", "ros-kinetic-cob-omni-drive-controller: 0.7.0-0", "ros-kinetic-cob-perception-common: 0.6.10-0", "ros-kinetic-cob-perception-msgs: 0.6.10-0", "ros-kinetic-cob-phidget-em-state: 0.6.10-0", "ros-kinetic-cob-phidget-power-state: 0.6.10-0", "ros-kinetic-cob-phidgets: 0.6.10-0", "ros-kinetic-cob-reflector-referencing: 0.6.6-0", "ros-kinetic-cob-relayboard: 0.6.10-0", "ros-kinetic-cob-safety-controller: 0.6.6-0", "ros-kinetic-cob-scan-unifier: 0.6.10-0", "ros-kinetic-cob-sick-lms1xx: 0.6.10-0", "ros-kinetic-cob-sick-s300: 0.6.10-0", "ros-kinetic-cob-sound: 0.6.10-0", "ros-kinetic-cob-srvs: 0.6.7-0", "ros-kinetic-cob-substitute: 0.6.6-0", "ros-kinetic-cob-supported-robots: 0.6.7-0", "ros-kinetic-cob-trajectory-controller: 0.7.0-0", "ros-kinetic-cob-twist-controller: 0.7.0-0", "ros-kinetic-cob-undercarriage-ctrl: 0.6.10-0", "ros-kinetic-cob-undercarriage-ctrl-node: 0.7.0-0", "ros-kinetic-cob-utilities: 0.6.10-0", "ros-kinetic-cob-vision-utils: 0.6.10-0", "ros-kinetic-cob-voltage-control: 0.6.10-0", "ros-kinetic-eband-local-planner: 0.3.1-0", "ros-kinetic-follow-waypoints: 0.3.0-2", "ros-kinetic-grid-map-costmap-2d: 1.5.2-0", "ros-kinetic-grid-map-octomap: 1.5.2-0", "ros-kinetic-image-overlay-scale-and-compass: 0.2.1-0", "ros-kinetic-julius-ros: 2.1.4-0", "ros-kinetic-libconcorde-tsp-solver: 0.6.11-0", "ros-kinetic-libdlib: 0.6.11-0", "ros-kinetic-libntcan: 0.6.11-0", "ros-kinetic-libpcan: 0.6.11-0", "ros-kinetic-libphidgets: 0.6.11-0", "ros-kinetic-libqsopt: 0.6.11-0", "ros-kinetic-opengm: 0.6.11-0", "ros-kinetic-raw-description: 0.6.7-0", "ros-kinetic-rostune: 1.0.5-1", "ros-kinetic-safe-teleop-base: 0.0.2-0", "ros-kinetic-safe-teleop-stage: 0.0.2-0", "ros-kinetic-schunk-description: 0.6.9-0", "ros-kinetic-schunk-libm5api: 0.6.9-0", "ros-kinetic-schunk-modular-robotics: 0.6.9-0", "ros-kinetic-schunk-powercube-chain: 0.6.9-0", "ros-kinetic-schunk-sdh: 0.6.9-0", "ros-kinetic-schunk-simulated-tactile-sensors: 0.6.9-0", "ros-kinetic-soem: 1.3.0-0", "ros-kinetic-urdf-geometry-parser: 0.0.2-0", "ros-kinetic-urdf-sim-tutorial: 0.3.0-1", "ros-kinetic-aruco-detect: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-assimp-devel: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-bayesian-belief-networks: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-bin-pose-emulator: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-bin-pose-msgs: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-binpicking-utils: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-checkerboard-detector: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-default-cfg-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-analysis: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-common-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-updater: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-downward: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-ff: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-ffha: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-fiducial-detect: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-lib: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-msgs: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-pose: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-slam: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducials: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-grid-map: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-core: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-cv: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-demos: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-filters: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-loader: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-msgs: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-pcl: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-ros: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-rviz-plugin: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-visualization: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-imagesift: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-3rdparty: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-jsk-pcl-ros: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-pcl-ros-utils: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-perception: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition-msgs: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition-utils: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-julius: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-libcmt: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-libsiftfast: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-lpg-planner: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-magni-bringup: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-demos: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-description: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-nav: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-robot: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-teleop: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-master-discovery-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-master-sync-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-mavlink: 2017.6.6-0 -> 2017.7.7-0", "ros-kinetic-mini-maxwell: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-move-basic: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-multimaster-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-multimaster-msgs-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-multiwii: 2.0.0-0 -> 2.0.1-0", "ros-kinetic-nlopt: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-node-manager-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-opencv-apps: 1.11.15-0 -> 1.12.0-0", "ros-kinetic-opt-camera: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-pgm-learner: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-plotjuggler: 1.1.2-0 -> 1.1.3-0", "ros-kinetic-resized-image-transport: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-rosdiagnostic: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-rosjava-core: 0.3.4-1 -> 0.3.5-0", "ros-kinetic-roslisp: 1.9.20-0 -> 1.9.21-0", "ros-kinetic-rospatlite: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-rosping: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-rqt-tf-tree: 0.5.7-0 -> 0.5.8-0", "ros-kinetic-self-test: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-slic: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-test-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-thormang3-tools: 0.1.0-0 -> 0.1.2-0", "ros-kinetic-urdf-tutorial: 0.2.5-0 -> 0.3.0-1", "ros-kinetic-voice-text: 2.0.20-0 -> 2.1.4-0", "Aaron Blasdel", "Alexander Bubeck", "Alexander Tiderko", "Austin Hendrix", "Benjamin Maidel", "Brice Rebsamen", "Charles DuHadway (maintained by Benjamin Pitzer)", "Christian Rauch", "Damon Kohler", "Daniel Snider", "David V. Lu!!", "Davide Faconti", "Felix Messmer", "Florian Weisshardt", "Frantisek Durovsky", "Georg Bartels", "George Stavrinos", "Guillaume Autran", "Hitoshi Kamada", "Jan Fischer", "Jim Vaughan", "Joshua Hampp", "Kei Okada", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Nadia Hammoudeh Garcia", "Noda Shintaro", "Piyush Khandelwal", "Pyo", "P\u00e9ter Fankhauser", "Richard Bormann", "Rohan Agrawal", "Ruben Smits", "Ryohei Ueda", "Takuya Nakaoka", "Vincent Rousseau", "Vladimir Ermakov", "Yohei Kakiuchi", "Youhei Kakiuchi", "Yuki Furuta", "Yuto Inagaki", "durovsky"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2017-07-25/2307"}
,{"title": "New Packages for Indigo 2017-07-25", "thread_contents": ["We\u2019re happy to announce another set of new packages for Indigo. We have 20 new packages as well as over 200 updates available.", "Full details are below. Thanks to all the contributors and maintainers who have made this possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-cob-helper-tools: 0.6.6-0", "ros-indigo-cob-map-accessibility-analysis: 0.6.5-0", "ros-indigo-cob-supported-robots: 0.6.7-0", "ros-indigo-generic-throttle: 0.6.6-0", "ros-indigo-grid-map-costmap-2d: 1.5.2-0", "ros-indigo-grid-map-octomap: 1.5.2-0", "ros-indigo-jsk-pr2-desktop: 1.1.0-1", "ros-indigo-julius-ros: 2.1.4-0", "ros-indigo-libconcorde-tsp-solver: 0.6.11-0", "ros-indigo-libqsopt: 0.6.11-0", "ros-indigo-moveit-goal-builder: 0.1.0-0", "ros-indigo-opengm: 0.6.11-0", "ros-indigo-parameter-pa: 1.0.0-0", "ros-indigo-rostune: 1.0.5-1", "ros-indigo-safe-teleop-base: 0.0.2-0", "ros-indigo-safe-teleop-pr2: 0.0.2-0", "ros-indigo-safe-teleop-stage: 0.0.2-0", "ros-indigo-service-tools: 0.6.6-0", "ros-indigo-spin-hokuyo: 1.0.0-0", "ros-indigo-urdf-sim-tutorial: 0.3.0-0", "ros-indigo-assimp-devel: 2.0.20-0 -> 2.1.4-0", "ros-indigo-baxtereus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-bayesian-belief-networks: 2.0.20-0 -> 2.1.4-0", "ros-indigo-can-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-402: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-chain-node: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-master: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-motor-node: 0.6.7-0 -> 0.6.8-0", "ros-indigo-checkerboard-detector: 1.1.2-0 -> 1.2.2-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-android: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-msgs: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-resource-server: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-script-server: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-settings: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-base-drive-chain: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-base-velocity-smoother: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-bms-driver: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-calibration-data: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-cam3d-throttle: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-camera-sensors: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-canopen-motor: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-cartesian-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-collision-velocity-filter: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-command-gui: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-command-tools: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-common: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-control: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-control-mode-adapter: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-control-msgs: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-dashboard: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-env-config: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-description: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-docker-control: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-driver: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-elmo-homing: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-environments: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-extern: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-footprint-observer: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-frame-tracker: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-gazebo-plugins: 0.6.4-0 -> 0.6.6-0", "ros-indigo-cob-gazebo-ros-control: 0.6.4-0 -> 0.6.6-0", "ros-indigo-cob-generic-can: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-hand: 0.6.1-0 -> 0.6.2-0", "ros-indigo-cob-hand-bridge: 0.6.1-0 -> 0.6.2-0", "ros-indigo-cob-head-axis: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-image-flip: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-interactive-teleop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-light: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-linear-nav: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-mapping-slam: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-mimic: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-model-identifier: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-monitoring: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-msgs: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-navigation: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-config: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-global: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-local: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-slam: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-object-detection-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-object-detection-visualizer: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-obstacle-distance: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-omni-drive-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-perception-common: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-perception-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidget-em-state: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidget-power-state: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidgets: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-reflector-referencing: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-relayboard: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-safety-controller: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-scan-unifier: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-script-server: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-sick-lms1xx: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-sick-s300: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-sound: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-srvs: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-substitute: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-teleop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-trajectory-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-twist-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-undercarriage-ctrl-node: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-utilities: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-vision-utils: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-voltage-control: 0.6.8-0 -> 0.6.10-0", "ros-indigo-collada-urdf-jsk-patch: 2.0.20-0 -> 2.1.4-0", "ros-indigo-default-cfg-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-analysis: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-common-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-updater: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diff-drive-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-downward: 2.0.20-0 -> 2.1.4-0", "ros-indigo-effort-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-fetcheus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-ff: 2.0.20-0 -> 2.1.4-0", "ros-indigo-ffha: 2.0.20-0 -> 2.1.4-0", "ros-indigo-force-torque-sensor-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-forward-command-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-grid-map: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-core: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-cv: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-demos: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-filters: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-loader: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-msgs: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-pcl: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-ros: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-rviz-plugin: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-visualization: 1.4.2-0 -> 1.5.2-0", "ros-indigo-gripper-action-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-imagesift: 1.1.2-0 -> 1.2.2-0", "ros-indigo-imu-sensor-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-joint-state-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-joint-trajectory-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-jsk-2015-05-baxter-apc: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-201504-miraikan: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-2016-01-baxter-apc: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-3rdparty: 2.0.20-0 -> 2.1.4-0", "ros-indigo-jsk-apc2015-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-apc2016-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-arc2017-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-baxter-desktop: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-baxter-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-baxter-web: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-common-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-fetch-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-footstep-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-gui-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-hark-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-interactive: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-interactive-marker: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-interactive-test: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-nao-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pcl-ros: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pcl-ros-utils: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pepper-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-perception: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pr2-calibration: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pr2-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pr2eus: 0.3.11-0 -> 0.3.13-0", "ros-indigo-jsk-recognition: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-recognition-msgs: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-recognition-utils: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-robot: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-robot-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-robot-utils: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-rqt-plugins: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-rviz-plugins: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-visualization: 2.1.0-0 -> 2.1.2-0", "ros-indigo-julius: 2.0.20-0 -> 2.1.4-0", "ros-indigo-laser-filters-jsk-patch: 2.0.20-0 -> 2.1.4-0", "ros-indigo-libcmt: 2.0.20-0 -> 2.1.4-0", "ros-indigo-libdlib: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libntcan: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libpcan: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libphidgets: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libsiftfast: 2.0.20-0 -> 2.1.4-0", "ros-indigo-lpg-planner: 2.0.20-0 -> 2.1.4-0", "ros-indigo-master-discovery-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-master-sync-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-mini-maxwell: 2.0.20-0 -> 2.1.4-0", "ros-indigo-multimaster-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-multimaster-msgs-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-naoeus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-naoqieus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-nlopt: 2.0.20-0 -> 2.1.4-0", "ros-indigo-node-manager-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-opencv-apps: 1.11.15-0 -> 1.12.0-0", "ros-indigo-opt-camera: 2.0.20-0 -> 2.1.4-0", "ros-indigo-parrot-arsdk: 3.11.0-0 -> 3.12.6-0", "ros-indigo-peppereus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-pgm-learner: 2.0.20-0 -> 2.1.4-0", "ros-indigo-plotjuggler: 1.1.2-0 -> 1.1.3-0", "ros-indigo-posedetection-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-position-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-pr2-base-trajectory-action: 1.0.6-2 -> 1.1.0-1", "ros-indigo-pr2eus: 0.3.11-0 -> 0.3.13-0", "ros-indigo-pr2eus-moveit: 0.3.11-0 -> 0.3.13-0", "ros-indigo-pr2eus-tutorials: 0.3.11-0 -> 0.3.13-0", "ros-indigo-rail-manipulation-msgs: 0.0.9-0 -> 0.0.10-0", "ros-indigo-raw-description: 0.6.6-0 -> 0.6.7-0", "ros-indigo-resized-image-transport: 1.1.2-0 -> 1.2.2-0", "ros-indigo-ros-canopen: 0.6.7-0 -> 0.6.8-0", "ros-indigo-ros-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-rosauth: 0.1.7-0 -> 0.1.7-1", "ros-indigo-rosdiagnostic: 1.9.0-0 -> 1.9.2-0", "ros-indigo-roseus-remote: 1.0.6-2 -> 1.1.0-1", "ros-indigo-roslisp: 1.9.20-0 -> 1.9.21-0", "ros-indigo-rospatlite: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rosping: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rostwitter: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rqt-joint-trajectory-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-rqt-tf-tree: 0.5.7-0 -> 0.5.8-0", "ros-indigo-schunk-description: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-libm5api: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-modular-robotics: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-powercube-chain: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-sdh: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.8-0 -> 0.6.9-0", "ros-indigo-self-test: 1.9.0-0 -> 1.9.2-0", "ros-indigo-slic: 2.0.20-0 -> 2.1.4-0", "ros-indigo-socketcan-bridge: 0.6.7-0 -> 0.6.8-0", "ros-indigo-socketcan-interface: 0.6.7-0 -> 0.6.8-0", "ros-indigo-speech-recognition-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-surface-perception: 0.1.1-0 -> 0.1.3-0", "ros-indigo-test-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-indigo-transform-graph: 0.1.4-0 -> 0.2.1-0", "ros-indigo-urdf-tutorial: 0.2.5-0 -> 0.3.0-0", "ros-indigo-velocity-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-voice-text: 2.0.20-0 -> 2.1.4-0", "ros-indigo-jsk-apc", "ros-indigo-jsk-arc2017-baxter", "Aaron Blasdel", "Adolfo Rodriguez Tsouroukdissian", "Alexander Bubeck", "Alexander Tiderko", "Austin Hendrix", "Bence Magyar", "Benjamin Maidel", "Brice Rebsamen", "Charles DuHadway (maintained by Benjamin Pitzer)", "David Kent", "David V. Lu!!", "Davide Faconti", "Felix Messmer", "Florian Weisshardt", "Georg Bartels", "George Stavrinos", "Guillaume Autran", "Hasegawa Shun", "Hitoshi Kamada", "Jan Fischer", "Joshua Hampp", "Justin Huang", "KazutoMurase", "Kei Okada", "Kentaro Wada", "Mani Monajjemi", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Nadia Hammoudeh Garcia", "Noda Shintaro", "Peter Weissig", "P\u00e9ter Fankhauser", "Richard Bormann", "Russell Toris", "Ryohei Ueda", "Sachin Chitta", "Sarah Bertussi", "Shohei Fujii", "Takuya Nakaoka", "Yohei Kakiuchi", "YoheiKakiuchi", "Youhei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "inagaki", "k-okada"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2017-07-25/2306"}
,{"title": "New Packages for Indigo 2018-01-09", "thread_contents": ["We\u2019re happy to anounce 6 new packages and 168 updated packages for Indigo.", "Thanks as always to all the contributors and maintainers. The list of releases and maintainers is below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-cost-map: 0.3.3-0", "ros-indigo-cost-map-core: 0.3.3-0", "ros-indigo-cost-map-cv: 0.3.3-0", "ros-indigo-cost-map-demos: 0.3.3-0", "ros-indigo-cost-map-ros: 0.3.3-0", "ros-indigo-cost-map-visualisations: 0.3.3-0", "ros-indigo-baldor: 0.1.1-0 -> 0.1.2-0", "ros-indigo-care-o-bot: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-desktop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-robot: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-simulation: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-android: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-msgs: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-resource-server: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-script-server: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-settings: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-base-drive-chain: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-base-velocity-smoother: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-bms-driver: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-bringup: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-bringup-sim: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-calibration-data: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-cam3d-throttle: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-camera-sensors: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-canopen-motor: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-cartesian-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-collision-monitor: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-collision-velocity-filter: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-command-gui: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-command-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-common: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-control: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-control-mode-adapter: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-control-msgs: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-dashboard: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-default-env-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-robot-behavior: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-default-robot-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-docker-control: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-driver: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-elmo-homing: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-environments: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-extern: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-footprint-observer: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-frame-tracker: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-gazebo: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-gazebo-objects: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-gazebo-plugins: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-ros-control: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-worlds: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-generic-can: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-grasp-generation: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-hand: 0.6.2-0 -> 0.6.3-0", "ros-indigo-cob-hand-bridge: 0.6.2-0 -> 0.6.3-0", "ros-indigo-cob-hardware-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-helper-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-image-flip: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-interactive-teleop: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-kinematics: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-light: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-linear-nav: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-lookat-action: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-manipulation: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-map-accessibility-analysis: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-mapping-slam: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-mimic: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-model-identifier: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-monitoring: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-moveit-bringup: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-moveit-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-moveit-interface: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-navigation: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-global: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-local: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-slam: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-object-detection-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-object-detection-visualizer: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-obstacle-distance: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-obstacle-distance-moveit: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-omni-drive-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-perception-common: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-perception-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidget-em-state: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidget-power-state: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidgets: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-pick-place-action: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-reflector-referencing: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-relayboard: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-robots: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-safety-controller: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-scan-unifier: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-script-server: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-sick-lms1xx: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-sick-s300: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-simulation: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-sound: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-srvs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-substitute: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-supported-robots: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-teleop: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-trajectory-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-twist-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-utilities: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-vision-utils: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-voltage-control: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cost-map-msgs: 0.3.2-0 -> 0.3.3-0", "ros-indigo-dynamic-tf-publisher: 2.2.5-0 -> 2.2.6-0", "ros-indigo-gazebo-grasp-plugin: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-state-plugins: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-test-tools: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-world-plugin-loader: 1.0.1-0 -> 1.0.2-0", "ros-indigo-generic-throttle: 0.6.6-0 -> 0.6.7-0", "ros-indigo-grasp-planning-graspit: 1.1.2-0 -> 1.2.0-0", "ros-indigo-grasp-planning-graspit-msgs: 1.1.2-0 -> 1.2.0-0", "ros-indigo-grasp-planning-graspit-ros: 1.1.2-0 -> 1.2.0-0", "ros-indigo-graspit-tools: 1.1.2-0 -> 1.2.0-0", "ros-indigo-image-view2: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jaco-graspit-sample: 1.1.2-0 -> 1.2.0-0", "ros-indigo-jsk-common: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-data: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-network-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-tilt-laser: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-topic-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-libconcorde-tsp-solver: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libdlib: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libntcan: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libpcan: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libphidgets: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libqsopt: 0.6.11-0 -> 0.6.12-0", "ros-indigo-moveit-controller-multidof: 1.0.0-0 -> 1.0.1-0", "ros-indigo-moveit-object-handling: 1.0.0-0 -> 1.0.1-0", "ros-indigo-moveit-planning-helper: 1.0.0-0 -> 1.0.1-0", "ros-indigo-multi-map-server: 2.2.5-0 -> 2.2.6-0", "ros-indigo-opengm: 0.6.11-0 -> 0.6.12-0", "ros-indigo-pcdfilter-pa: 1.1.0-0 -> 1.2.0-0", "ros-indigo-raw-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-description: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-libm5api: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-modular-robotics: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-powercube-chain: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-sdh: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.9-0 -> 0.6.10-0", "ros-indigo-service-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-universal-robot: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-bringup: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-description: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-driver: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-gazebo: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-kinematics: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-msgs: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur10-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur3-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur5-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-urdf-processing-tools: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-transform: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-traverser: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-viewer: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf2graspit: 1.1.2-0 -> 1.2.0-0", "ros-indigo-urdf2inventor: 1.0.1-0 -> 1.0.2-0", "ros-indigo-virtual-force-publisher: 2.2.5-0 -> 2.2.6-0", "ros-indigo-visp: 3.0.1-1 -> 3.1.0-2", "ros-indigo-xpp: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-examples: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-hyq: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-msgs: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-quadrotor: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-states: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-vis: 1.0.3-0 -> 1.0.4-0", "ros-indigo-cob-head-axis", "ros-indigo-cob-undercarriage-ctrl-node", "Alexander Bubeck", "Alexander W. Winkler", "Benjamin Maidel", "Bruno Brito", "Daniel Stonier", "Fabien Spindler", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Felx Messmer", "Florian Weisshardt", "Francisco Suarez-Ruiz", "Jannik Abbenseth", "Jennifer Buehler", "Joshua Hampp", "Kei Okada", "Matthias Gruhler", "Peter Weissig", "Richard Bormann", "Ryohei Ueda", "YoheiKakiuchi"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2018-01-09/3639"}
,{"title": "New Packages for Kinetic 2018-01-09", "thread_contents": ["We\u2019re happy to announce 22 new packages and 175 updated packages. This includes the restoration of the regressed costmap packages from the last sync.", "Thank you to all the maintainers and contributors who have helped make this possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Added Packages", "Is there a way to add hyperlinks to package home pages that would provide some summary what that package is about?", "\nNow I type google search (example: bus-server), go through a few pages that are unrelated, like general ROS \u201cpackage\u201d installation, and finally hit something usable after 6 links that the search returned. It can be easier than that.", "If others already know a short way to find information, please share.", "Cheers", " was bus_server supposed to make it into this sync? I removed it from the kinetic rosdistro after the rename to ", ".", "EDIT: Looks like I forgot to rename the dependency in loki_bringup, ran a new release. Is there anything else needed to ensure proper deletion?", "Is there a way to add hyperlinks to package home pages that would provide some summary what that package is about?", "Now I type google search (example: bus-server), go through a few pages that are unrelated, like general ROS \u201cpackage\u201d installation, and finally hit something usable after 6 links that the search returned. It can be easier than that.", "I totally agree with ", ", I also searched on Google for several packages without direct success.", "The default location to find a package is on the ROS wiki with the package name. So for package ", " which is packaged into the debian package ", " see ", "I opened a ", " as a link in the summary last week. We are targeting to roll it out before the next sync.", "WRT to bus_server as ", " alluded, that was a package ", " but after review was renamed to ", " which is why you can\u2019t find any documentation. There\u2019s an outstanding ticket to ", " but we haven\u2019t had time to implement it.", "The default location to find a package is on the ROS wiki with the package name. So for package cost_map which is packaged into the debian package ros-kinetic-cost-map see ", "Yes, I know, but it is not always available. A fallback to the Github repo would be nice too.", "I opened a PR to embed the declared homepage as a link in the summary last week. We are targeting to roll it out before the next sync.", "Very good news, thanks!", "Yes, I know, but it is not always available. A fallback to the Github repo would be nice too.", "An alternative is to check the ", " automatically generated by the ROS buildfarm. They provide a complete list of all released packages including a link to the source repository as well as a link to the wiki page for each package if it exists.", "\nexample: ", " package", "\n", " provides a link to the ", " and the ", "In the case of packages that don\u2019t have a source of a doc entry (like ", ") the status page provides only a link to the ", ".", "On a related note, there\u2019s an unmerged PR that aims to encourage package releasers to create pages on ROS wiki ", " (I haven\u2019t had time to make an improvement).", "An alternative is to check the status pages automatically generated by the ROS buildfarm. They provide a complete list of all released packages including a link to the source repository as well as a link to the wiki page for each package if it exists.", "Oh, really interesting.", "\nI did not know that page.", "\nThanks!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-bus-server: 0.1.1-0", "ros-kinetic-care-o-bot: 0.6.6-0", "ros-kinetic-care-o-bot-desktop: 0.6.6-0", "ros-kinetic-care-o-bot-robot: 0.6.6-0", "ros-kinetic-care-o-bot-simulation: 0.6.6-0", "ros-kinetic-cob-manipulation: 0.7.1-0", "ros-kinetic-cob-pick-place-action: 0.7.1-0", "ros-kinetic-cost-map: 0.3.3-0", "ros-kinetic-cost-map-core: 0.3.3-0", "ros-kinetic-cost-map-cv: 0.3.3-0", "ros-kinetic-cost-map-demos: 0.3.3-0", "ros-kinetic-cost-map-ros: 0.3.3-0", "ros-kinetic-cost-map-visualisations: 0.3.3-0", "ros-kinetic-darknet-ros-msgs: 1.1.2-0", "ros-kinetic-loki-base-node: 0.2.1-0", "ros-kinetic-loki-bringup: 0.0.1-0", "ros-kinetic-loki-demos: 0.0.1-0", "ros-kinetic-loki-description: 0.0.1-0", "ros-kinetic-loki-nav: 0.0.1-0", "ros-kinetic-loki-robot: 0.0.1-0", "ros-kinetic-loki-teleop: 0.0.1-0", "ros-kinetic-pcdfilter-pa: 1.2.0-0", "ros-kinetic-baldor: 0.1.1-0 -> 0.1.2-0", "ros-kinetic-cob-3d-mapping-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-android: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-msgs: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-resource-server: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-script-server: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-settings: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-base-drive-chain: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-base-velocity-smoother: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-bms-driver: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-bringup: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-bringup-sim: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-calibration-data: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-cam3d-throttle: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-camera-sensors: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-canopen-motor: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-cartesian-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-collision-monitor: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-collision-velocity-filter: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-command-gui: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-command-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-common: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-control: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-control-mode-adapter: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-control-msgs: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-dashboard: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-default-env-config: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-default-robot-behavior: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-default-robot-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-description: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-docker-control: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-driver: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-elmo-homing: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-environments: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-extern: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-footprint-observer: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-frame-tracker: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-gazebo: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-gazebo-objects: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-gazebo-plugins: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-gazebo-ros-control: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-gazebo-worlds: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-generic-can: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-grasp-generation: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-hand: 0.6.2-0 -> 0.6.3-0", "ros-kinetic-cob-hand-bridge: 0.6.2-0 -> 0.6.3-0", "ros-kinetic-cob-hardware-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-helper-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-image-flip: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-interactive-teleop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-light: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-linear-nav: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-lookat-action: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-map-accessibility-analysis: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-mapping-slam: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-mimic: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-model-identifier: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-monitoring: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-moveit-bringup: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-moveit-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-moveit-interface: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-msgs: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-navigation: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-config: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-global: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-local: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-slam: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-object-detection-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-obstacle-distance: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-obstacle-distance-moveit: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-omni-drive-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-perception-common: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-perception-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidget-em-state: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidget-power-state: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidgets: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-reflector-referencing: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-relayboard: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-robots: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-safety-controller: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-scan-unifier: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-script-server: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-sick-lms1xx: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-sick-s300: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-simulation: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-sound: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-srvs: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-substitute: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-supported-robots: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-teleop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-trajectory-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-twist-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-undercarriage-ctrl: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-utilities: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-vision-utils: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-voltage-control: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cost-map-msgs: 0.3.2-0 -> 0.3.3-0", "ros-kinetic-dynamic-tf-publisher: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-generic-throttle: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-geometry2: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-image-view2: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-interactive-marker-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-jsk-common: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-data: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-network-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-tilt-laser: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-topic-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-kobuki: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-auto-docking: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-bumper2pc: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-capabilities: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-controller-tutorial: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-description: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-keyop: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-node: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-random-walker: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-rapps: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-safety-controller: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-testsuite: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-libconcorde-tsp-solver: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libdlib: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libntcan: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libpcan: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libphidgets: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libqsopt: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-librviz-tutorial: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-mavlink: 2017.12.12-0 -> 2018.1.1-0", "ros-kinetic-moveit-visual-tools: 3.3.0-0 -> 3.4.0-0", "ros-kinetic-multi-map-server: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-opengm: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-pose-cov-ops: 0.1.7-0 -> 0.2.0-0", "ros-kinetic-raw-description: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-robot-localization: 2.4.0-0 -> 2.4.2-0", "ros-kinetic-rviz: 1.12.14-0 -> 1.12.15-0", "ros-kinetic-rviz-plugin-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-rviz-python-tutorial: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-schunk-description: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-libm5api: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-modular-robotics: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-powercube-chain: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-sdh: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-simulated-tactile-sensors: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-service-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-tf2: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-bullet: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-eigen: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-geometry-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-kdl: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-py: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-ros: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-sensor-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-tools: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-universal-robot: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-bringup: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-description: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-driver: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-gazebo: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-kinematics: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-msgs: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur10-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur3-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur5-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-virtual-force-publisher: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-visualization-marker-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-visualization-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-xpp: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-examples: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-hyq: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-msgs: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-quadrotor: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-states: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-vis: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-cob-head-axis", "ros-kinetic-cob-undercarriage-ctrl-node", "Alexander Bubeck", "Alexander W. Winkler", "Benjamin Maidel", "Bruno Brito", "D. Hood", "Daniel Stonier", "Dave Coleman", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Felx Messmer", "Florian Weisshardt", "Francisco Suarez-Ruiz", "Jannik Abbenseth", "Jorge Santos Simon", "Jose-Luis Blanco-Claraco", "Joshua Hampp", "Kei Okada", "Koji Terada", "Marcus Liebhardt", "Marko Bjelonic", "Matthias Gruhler", "Peter Weissig", "Richard Bormann", "Rohan Agrawal", "Ryohei Ueda", "Tom Moore", "Tully Foote", "Vincent Rabaud", "Vladimir Ermakov", "Wayne Gramlich", "William Woodall", "YoheiKakiuchi", "Younghun Ju"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2018-01-09/3641"}
,{"title": "Cozmo with ROS2 without SDK", "thread_contents": ["Hi there,", "for all of you using Cozmo from Anki", "I implemented a simple wrapper for using it with ROS2 without depending on the Cozmo SDK. Please, check it out and let me know what you think:", "At the moment, it allows to move the robot (wheels, head and lift) using a modified version of the teleop_twist_keyboard and publishes the images from the camera to a topic.", "Feel free to contribute to the repo.", "That\u2019s super cool, and the price point is pretty good for the features (~$115US on Amazon). Looks like there is quite a bit more possible. Pycosmo supports:", "Sensors:", "Actuators:", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Hi,", "Good news! So far Cozmo publishes:", "Now that it is working, the rest is quite straightforward (and boring ", ") so some help with the implementation would be much appreciated.", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Not yet but I will try. In theory the pycozmo wrapper I am using for it supports it. I just have to convert the data to ROS2 msgs.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Camera", "Cliff sensor", "Accelerometers", "Gyro", "Battery voltage", "Cube battery voltage", "Cube accelerometers", "Wheel motors", "Head motor", "Lift motor", "Backpack LEDs", "IR LED", "OLED display", "Speaker - work progress", "Cube LEDs", "Platform LEDs", "Odometry", "Image", "Imu (orientation, gyro and accel)"], "url": "https://discourse.ros.org/t/cozmo-with-ros2-without-sdk/11689"}
,{"title": "Chat for ROS on robotics.stackexchange.com", "thread_contents": ["I created a ", " on ", " for ROS. The main purpose and rationale is to bring experts together to discuss interesting problems (related to ROS) and helping beginners (and non-) in solving their problems in real-time for a more active and hopefully healthy community.", "Isn\u2019t that why we have ", " and ", "?", "Yes, but that is a more real-time alternative which hopefully will attract users from Stack Exchange communities. As a matter of fact, you answered me after almost 1h, whereas in that chat, if someone is constantly there, you can tag that person and that person will be notified and eventually will help faster.", "If you\u2019re really bored, you can hang out in the barren wasteland that is ", "I came to make a comparison between the ", " and Mars (been there forever, may have had life once, now populated by bots). But I checked the channel first and discovered, much to my amazement, that it\u2019s actually been fairly active recently!", "I fully agree! The ", " IRC channel is a nice place to hangout, as is the ", " IRC channel for everything MoveIt!", "How can we get logins to the slack account ?", "In light of the Slack channel being even quieter than the IRC we\u2019d like to officially deprecate it. Setting up the Slack channel was an experiment to see if we could build more community through the new tool. However it seems that it\u2019s been more of a distraction and split peoples limited focus.", "I would like to encourage people interested in realtime chat about ROS to use the IRC channel.", "Bringing everyone together in one public place is the most likely to get critical mass.", "As linked above you can find the ", " channel on freenode. Details are at: ", "Logs are viewable at: ", "And there are multiple web clients if you don\u2019t have an IRC client installed locally.", "\nYou can follow either of these links. (Please change your username)", "How can we get logins to the slack account ?", "This is part of the challenge of slack. We had to setup a special heroku app to allow people to request invites to even be able to view the convesations. Maintaining this is part of the overhead we don\u2019t want to keep incurring for such a low volume.", "In the IRC chat room there\u2019s usually just one person that is capable of answering a few questions, and many times it is inactive. Furthermore, Stack Exchange communities seem to be more visible to a wider range of people with respect to IRC chats. If I had to bet on a growing and sustainable community I would not do it on the IRC chat room.", "The stack exchange chat doesn\u2019t seem to have a client. +1 for irc", "As far as I know, you\u2019re right, so far there isn\u2019t a client for the Stack Exchange\u2019s chats (except clearly the browser). A plus for the chat I created is that it\u2019s really a chat on a website regarding robotics, so, possibly, there we could group a wider range of experts in the field.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/chat-for-ros-on-robotics-stackexchange-com/1841"}
,{"title": "ROSIN: Call for Education Projects (EPs) - Deadline: September 14, 2018", "thread_contents": ["The EU H2020 ", " project has the goal to advance open-source robot software for industry and the robotics community as a whole. This includes also to improve educational activities about ", ".", "Similar to the already announced Focused Technical Projects (", "), the ROSIN project grants education activities, so-called Education Projects (EPs), conducted by third-parties (non-consortium members).", "You identify a particular project that you want to support the ROSIN educational activities with and outline a sound and sustainable plan to achieve it. If successfully evaluated, you receive 50% of the total cost to perform the work, provided that you commit to sustain the remaining 50%.", "You can apply at any time, there are 3 cut-off dates per year.", "To find out more about what is being funded and how to apply, check the Education Project section at the ROSIN website: ", "The next cut-off date:     September 14, 2018", "The next cut-off date: September 14, 2018", "That is in exactly 1 month. You may consider this as a friendly reminder to get ready to hand in your proposal of Education Projects in due time.", "Many thanks", "\nThilo", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosin-call-for-education-projects-eps-deadline-september-14-2018/5448"}
,{"title": "ROSCon 2018 Program Published", "thread_contents": ["The ROSCon Organizing Committee is happy to announce the ", " has been published on the main ROSCon website: ", "We\u2019re looking forward to seeing these talks in September.", "If you haven\u2019t registered already, there\u2019s just 2 weeks before the early registration deadline.", "Registration site can be found ", ". Get your tickets before the price goes up!", "See you in Madrid!", "Thank you again to our Platinum Sponsor, Erle, and to all of our Gold Sponsors: Amazon, Apple, Clearpath, Eprosima, Fetch Robotics, Google, Locus, Microsoft, ROBOTIS, SICK, Tier IV, Toyota Research Institute, Universal Robots.", "From the program:", "\n\u201cSupported by robotics development companies, academic researchers,", "\nthe U.S. Government, ", ", ROS-Military is a multi-faceted approach to effectively", "\nbringing the collaborative success of the ROS ecosystem to U.S.", "\nmilitary robotics \u2026\u201d", "Interesting to see this talk in ROSCon and advertise OSRF", "\ninvolvement, in particular the same year as Google makes news by", "\nterminating a DoD contract", "\n(", ").", "BTW, What the next killer-application build in ROS with the support of OSRF?", "I\u2019m very disappointed to see OSRF supporting the U.S. military (or any military for that matter) as well. I get that there\u2019s money to be made, but I believe a non-profit charity should not be in the business of war.", ", I appreciate your concern and understand your perspective. I\u2019d like to add some context to the discussion.", "Open Robotics would not exist today without the support of the US Department of Defense (DOD). Back at Willow Garage around 2010-2011 we often talked about creating a separate organization to specifically focus on open source development of ROS & Gazebo. What was missing was a way to fund such an organization. A contract from DARPA to develop simulation software for the forthcoming DARPA Robotics Challenge (DRC) allowed us to leave Willow and start OSRF in 2012. We were helped by a generous donation from Willow to cover our initial operating expenses, but it was the DRC contract that made it possible to build a team and create a sustainable organization around it. The DRC program alone supported ~10 people working on Gazebo and related projects for more than 3 years.", "When Willow was winding down in early 2013, we were able to hire the core ROS team and thereby take responsibility for ROS itself only because of a grant that we had through the National Robotics Initiative. The grant was 1/3 funded by the US Army, with the rest funded equally by NASA and the National Science Foundation (NSF). Taken together that funding supported ~4 people for 3 years to maintain ROS and to start work on ROS 2.", "It\u2019s impossible to know for sure, but it\u2019s reasonable to wonder whether ROS and/or Gazebo would have become orphaned projects had we not received that DOD funding at that time. Perhaps the community or another organization would have picked them up. But in my opinion, probably not.", "Since that time we\u2019ve worked (and continue to work) on a number of DOD-funded programs from various agencies. We\u2019ve never tried to hide that support; you can find 4 DOD agency logos on our ", ". We\u2019ve also never tried to hide the work that we do for those agencies. Quite the opposite: in all these programs the code and documentation that we develop is 100% open source. To my mind, that\u2019s the best possible use of public money that\u2019s paying for R&D: build technology that can be used by everyone, everywhere.", "In addition, the fact that our government funding, DOD and otherwise, is all channeled into open source products has an important consequence: we cannot work on anything that is classified or subject to export control. As you might imagine, this constraint rules out pretty much anything related to weapons or other offensive technology. We go further, judging each potential project based on the apparent intent of the people running the program and the likely application of the technology. Over the years we have declined many funding opportunities because we found them to disagree with our personal and organizational principles.", "Of course it would be naive in the extreme to imagine that technology that we develop and release won\u2019t ever be used in a manner that we disagree with. I\u2019m quite sure that somewhere out there are weaponized robots running ROS and being simulated with Gazebo. We can\u2019t stop that from happening and still make our software open source; clause 6 of the ", " says, \u201cThe license must not restrict anyone from making use of the program in a specific field of endeavor.\u201d But we can and do choose not to build those robots or simulations ourselves.", "Today, thanks in large part to the maturation of Gazebo and the growing promise of ROS 2, Open Robotics enjoys significant support from non-government sources. In 2013 government funding, primarily from DOD, made up more than 85% of our funding. In 2017 that figure was less than 25%. In the future could we move completely away from government funding? Perhaps, but it\u2019s worth considering all the implications: while we\u2019re very grateful for the support of our industry sponsors, they generally (and understandably) are asking us for near-term improvements and releases that will support their product or service offerings. Those priorities are vital to keep us grounded in the needs of today, but it\u2019s primarily government R&D programs that give us the time and flexibility to take on bigger, riskier, more speculative projects. And in the US, those programs are primarily funded by DOD. I\u2019d prefer to see agencies like NSF have a larger budget, but I\u2019m not in a position to make that happen.", "It\u2019s worth noting that this debate is an old one. For an eloquent argument on the other side, see ", " at Michigan. Ben doesn\u2019t just make the argument, he lives it, which I greatly respect.", " I would like to thank you for that great explanation. My first feeling was the same than ", ", but being able to see where OSRF came from and that you keep an effort on considering all implications helps on my peace of mind.", "Thanks ", " and ", " to put the issue on the table, and thanks ", " for your transparent answer (and for your final reference to Ben Kuipers).", "\nAs ", " says, I\u2019m now more peaceful with the subject.", "Yes, I\u2019m fine as well now. Thanks ", " for clarifying things. I knew some of the back story already, and I don\u2019t have a problem with OSRF taking money from DARPA. After all, DARPA funds a lot of US robotics projects without a direct military application. I am also aware that this funding is what kept the core ROS and Gazebo development team afloat, and that any open source technology can be appropriated for military purposes.", "I think where a line would be crossed (for me; everyone has to listen to their own conscience and draw their own line) would be if OSRF was directly contributing to weapons technology, or to a project with a thinly veiled pretense which would likely result in a weapon. That\u2019s the part of ", "\u2019s anwer that reassured me the most: that all of your technology (in the military context) remains open source, and that you\u2019re judging each potential project on a moral basis. As long as you stick to those principles, there\u2019s nothing to worry about. I think you really have to look at potential partner\u2019s background to distinguish between a project that\u2019s honestly trying to make a contribution to e.g. a Search and Rescue (SAR) robot, and one that\u2019s pretending to develop a SAR robot with the intent of repurposing the technology for Search and Destroy.", "The announcement of a dedicated \u201cROS Military\u201d initiative sounded a bit nefarious, but I guess I\u2019ll have to wait for ROSCon to find out.", "Anyway, let\u2019s not hijack this thread any further than we already have. It\u2019s really about the ROSCon program, and it looks awesome! ", "That\u2019s the part of ", "\u2019s anwer that reassured me the most: that all of your technology (in the military context) remains open source, and that you\u2019re judging each potential project on a moral basis.", "You\u2019ll notice that ", " did not specify what moral basis is being applied. ", " spend a lot of words to explain that taking funds from the DOD is morally ok (or not), which was however not the topic.", "The actual problem arises when OSRF actively supports ROS-Military. ROS MIlitary has the goal of improving technology that helps killing for political goals, and in particular do so for the US government. Due to that, it is political, it takes sides in global conflicts. It\u2019s not immoral for ROS-Military to exist, it is very transparent about it\u2019s goals, including the political aspects. Read the ROSCon program: \u201cThe United States military\u2026\u201d Could not be more transparent.", "But it is impossible to offer any support to ROS-Military without being political. OSRF can take all the money the DOD wishes to throw at it, OSRF can tolerate the existence of ROS-Military, but it should in no case directly support ROS.Military, and from the ROSCon program it seems that it crossed this line, and nothing in ", " s response even denied that.", "So any organisation in this world which thinks about sponsoring OSRF one way or the other now has to consider that OSRF helps ROS Military, and thus helps military operations inside and outside the US that protect the American petrol-fuelled way of life.", "This is different from taking DOD money or supporting civilian DOD funded projects (setting aside the arguments by Ben Kuipers for the sake of the argument).", "ROS-Military is not politically neutral, and it is not civilian. People will die thanks to ROS Military. People will die for political goals of the US. That\u2019s all fair enough to participate in for any individual at their discretion, but not for OSRF, IMHO.", "Hello everyone,", "It was quite hard for me to write a follow-up to ", "\u2019s last reply, so I exchanged some private messages with members of the program committee instead. Still, I fail to understand why ", ", ", " and ", " feel relieved by ", "\u2019s explanation and thus want to give my personal response here in public. I expect it represents the opinion of a significant fraction of the contributing ROS community.", "I agree with a lot of what ", " means to convey, but the way he puts it is neither productive nor easy to agree with. The discussion about whether or not OSRF should be funded by military organizations in the future (too) is a valid one, but it is per se unrelated to the program of ROSCon and thus does not belong here. I am sure we will see a lot of discussion on that at the conference and I believe constructive statements in a different thread are welcome.", "It seems there were a number of submissions for this ROSCon on military-related projects and the program committee had to decide whether or not they want to have these talks at the conference.", "\nLooking through the program, the outcome was to accept two talks with obvious military funding or background, the one by Alejandro R. Mosteo (Centro Universitario de la Defensa, Zaragoza) on Ada support for ROS2 and the one by Jerry Towler, apparently summarizing military projects for the US military.", "I object to the scheduled talk by Jerry Towler.", "The abstract does not describe any project with civil use cases (or any concrete project at all for that matter).", "\nAfter reading it many times, I would expect him", "Don\u2019t get me wrong, I believe this can be an interesting overview talk.", "\n``ROS-M\u2019\u2019 might even be an interesting movement, given they open-source all their code.", "\nI would like to see the presentation as a 20-minutes video on vimeo.", "\nBut I do not want it to be part of the ROSCon conference.", "ROSCon is an event to build and support lobbies and communities around ROS and software projects.", "\nThe committee chooses which lobbies are supported by deciding on talks, within the quality and number of submissions they receive.", "\nBut some communities and lobbies do not mix well.", "\nRyan Gariepy said at the end of last year\u2019s ROSCon something like", "\n\u201cWe would never have imagined to have a conference where we talk about robot coloring books and fully autonomous S-classes at the same event\u201d.", "\nImagining this year\u2019s statement additionally contains \u201cand autonomous convoy vehicles for border patrol\u201d, or anything comparable, makes me cringe.", "\nIn my opinion, military applications of ROS, though they obviously exist, have no place in the worldwide happy-go-lucky, lobby-supporting community that OSRF always advertises at their events.", "Discussing military applications in the field of autonomous robotics breaks a taboo for many academics - especially in places where funding does not in large part come from the domain. At least in Germany, there is a long-standing debate about the so-called ", ", which requires universities that accept it to reject all research and collaboration in projects with military objectives, sometimes even potential military applications.", "All in all, I do not oppose talks by military-related researchers or projects in general.", "\nThey might present novel concepts or software modules that are useful for many civil applications as well, and if they do not only focus on how military robotic systems can benefit from them, they are clearly interesting for every researcher.", "\nBut I believe the ROS community - to a significant proportion represented at (and by) ROSCon - should not feature military applications and a talk titled \u2018\u2018ROS-Military: Progress and Promise\u2019\u2019 crosses that line.", "\nThis has nothing to do with \u2018\u2018hiding objectionable projects\u2019\u2019, it is a matter of public support.", "After hearing the presentation at the conference, I fail to understand the decision by the program comitee altogether. Clearly, there have been alternative proposals with more content.", "There was not a single item of information in the presentation that would be relevant to anyone listening.", "\nThe organizers might want to reconsider how they advertise the event:", "ROSCon is a developers conference, in the model of PyCon and BoostCon. [\u2026] the two-day program will comprise technical talks and tutorials that will introduce you to new tools and libraries, as well as teach you more about the ones you already know.", "The only tangible outcome of the talk is that OSRF and SwRI now have a recorded presentation entry that demonstrates that ROS-Military is apparently a topic present in / and involving the community.", "\nThis might help in future management meetings that decide on project budgets.", "I appeal to the comitee to never accept non-concrete military-related talks in the name of the community again. The community, including almost every participant I talked to, disagrees.", "Hi ", " , everyone,", "Speaking frankly, the ROSCon Organizing Committee (OC) is disappointed in the misalignment between the proposal for the talk in question and the presented content, especially given the concerns which members of the community expressed publicly and privately in advance. We expect that all presenters keep in mind that the ROS community is a global and diverse community, and that certain topics, motivations, and even phrases are going to be received very differently when presenters leave their home countries and their local professional communities.", "We will be making some changes to our review process, but we will ", " be putting in place a blanket ban on any topics at this point in time. We believe that such a policy would be tantamount to ignoring the potential impact ROS may have on the world outside of our labs and workshops. Furthermore, it would be insufficient. Considering the rapidly changing pace of robotics development and deployment, it is all but certain that there will be equally controversial topics proposed in the near future which are unrelated to the military.", "The nuances present in these sensitive topics also mean that face-to-face discussions are more important, not less. Speaking for myself, I had several hours of discussions with community members afterwards, none of which would have happened if I was limited to Discourse. It should also be noted that many of the people whom I spoke with were not objecting to the ", " of the talk, but instead were more concerned with how it was presented. That also suggests to us that our focus should be on helping presenters improve how to express topics which are now far more controversial than the \"rosbuild vs. catkin\" debates found at the first ROSCons.", "The OC will reserve the right to request a substantially complete draft ", " the conference of any presentation which has at its foundation a controversial topic. This includes ", " talks which focus on military initiatives. If you would like an opinion in advance of submission, feel free to contact the ROSCon 2019 OC at the to-be-announced email address.", "We will be making some changes to our review process, but we will ", " be putting in place a blanket ban on any topics at this point in time. We believe that such a policy would be tantamount to ignoring the potential impact ROS may have on the world outside of our labs and workshops.", "+100 to this: personally I\u2019d rather know about something and not like it than ignore it and pretend it isn\u2019t happening.", "topics which are now far more controversial than the \u201crosbuild vs. catkin\u201d debates found at the first ROSCons.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["to introduce \u2018\u2018ROS-M\u2019\u2019, maybe comparable to ``ROS-I\u2019\u2019 but with more centralized funding,", "summarize some non-classified portions of US-military robotics projects in the past,", "show how they can benefit from using ROS,", "explain how they have interesting research problems also in uncontroversial scenarios,", "state that they will open-source everything", "and thus not touch weapon controls."], "url": "https://discourse.ros.org/t/roscon-2018-program-published/5537"}
,{"title": "IPC in ros2", "thread_contents": ["Does ros2 use inter-process communication when we use it in  linux OS ? What about in case of bare metal micro-controllers where there is no OS?", "In the future, please set the category to \u201cNext Generation ROS\u201d when discussing ROS 2.", "Does ros2 use inter-process communication when we use it in  linux OS ?", "First, I\u2019ll assume you mean \u201cdoes ROS 2 use any operating system IPC mechanisms between nodes on the same machine, rather than sending data over UDP?\u201d, because the term \u201cinter-process communication\u201d only means communication between two processes, but does not imply what the mechanism is (i.e. UDP or TCP can be IPC).", "It depends on the middleware implementation you are using. Currently the default of Fast-RTPS does not use anything other than UDP single cast or multicast. It could, RTPS allows for this in the protocol. I believe some of the proprietary implementations provide a shared-memory based IPC mechanism when on the same machine. Fast-RTPS will hopefully support this in the future.", "What about in case of bare metal micro-controllers where there is no OS?", "There have been some experiments which use RTPS on a OS-less microcontroller to communicate with ROS 2 on a desktop and some other experiments to get our ROS 2 API compiling for microcontrollers, but there is no out of the box support for OS-less ROS 2 at this time. We did enough work to convince ourselves that is possible, but haven\u2019t followed through with sustained support because we didn\u2019t have time, though we would love to do that in the future.", "In that case though, with no OS, you probably would not be using processes so the idea of \u201cInter-", " communication\u201d doesn\u2019t really make sense. You would be using something more like our \u201c", "-process communication\u201d, which can avoid sending the message data over the network (UDP in most cases).", "RTI\u2019s DDS does use shared memory internally when possible, I think.", "Hi sagniknitr,", "Fast RTPS will support shared memory in future releases, is in our roadmap. Regarding microcontrollers we are developing a lightweight version of Fast RTPS based on an OMG future standard for constrained resources devices. Of course it is not a full DDS/RTPS implementation, but a reduced API and different protocol.", "You can see the first prototypes in the ", " use case, and we are working in an release now. This will let you to use ROS2 in micro-controlllers.", "If you want more details, please contact me.", "Jamie,", "Thanks for the update about how Fast RTPS will be evolving.  A few questions, is the prototype with Dronecode you\u2019re referring to the work you did on the ", "?  Additionally, can you point to any public links regarding this new OMG standard for constrained resources? I know that the ", " is publicly available, is a draft of the document you\u2019re referring to available?", "Thanks", "\n- Eddy", "The DDS XRCE (\u201ceXtremely Resource Constrained Environments\u201d because all OMG specifications need a cool acronym) specification isn\u2019t even close to finished yet. The ", " but you have to be an OMG member to get the working documents, unless someone involved is willing to provide them. When the specification is adopted, that becomes publicly available.", "As seems to happen a lot with the DDS specs these days, the RTI/eProsima/Twin Oaks camp and the PrismTech camp have produced their own versions of the XRCE specification and are struggling to reconcile them into a single specification that the OMG can adopt. I won\u2019t wade into the debate over which is better, but I\u2019ll let you draw your own conclusions from the fact that one of the two camps only has one company it it. ", " I hope it\u2019s not going to be the RPC for DDS spec all over again; the arguments over that could be heard in other meeting rooms. But it seems likely that it will be a year and a half or more before the final specification is released.", "Thanks for the informative reply ", ", I had no idea the work was under way.", "I attended the OMG meeting last week, so I took the opportunity to hear what PrismTech and RTI/TwinOaks/eProsima had to say about their competing submissions. The following are the notes I took during their presentations. Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "Prismtech presentation:", "RTI/eProsima/TwinOaks presentation:", "My summary:", "Thanks for the summary ", "!  I\u2019m very interested in how the work progresses", "Thanks Geoff, great stuff!", "Hello ", ",", "First of all I\u2019d like to point out that I am one of the author of PrismTech\u2019s/ADLINK XRCE proposal, thus you may think I am giving you my perspective, in that case I urge you to read original documents to see how what I am providing here are facts. That said, let me make a few rectifications to some of the points above.", "You say:", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure ", " You can find the result of our analysts here ", " but you are welcome to derive them by reading both specs.", "This is also partially correct. We had asked to present what would have been our final proposal in Bruxelles to give a chance to the task-force to digest and one more opportunity to the competing team to join. This has nothing to do with the Vote-to-Vote and the fact that the task-force did not decide to allow the Vote-to-Vote procedure. The OMG has very complex rules\u2026 I know that can seam strange, and they still surprise me after having spent more than 10 years dealing with it.", "The RFP (which I wrote) asks for a protocol not for an API.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "\nAdditionally, you may argue that this may add some complexity in the message parsing \u2013 but again \u2013 I think that checking a flag and deciding wether some field is going to be present or not does not belong to the realm of hard. It is also worth  pointing out that some flags are just informative and don\u2019t require any kind of branching in the decoding. Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes ", " And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple ", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples. Our proposal currently has 4 compared to that of RTI/TwinOaks/eProsima which has 16 bytes. We have had our implementation run on a Makeblock robot using BLE with an MTU of 20 bytes.  You can check the comparison on this deck ", " and should wonder why the competing proposal did not provide a proper analysis of the wire overhead. Additionally when leveraging batching we have a wire overhead of (3+n)/n where n is the number of samples being batched.", "Some other thing worth point out is that the protocol allows for data to be pushed, pulled or periodically pushed or pulled. The write protocol also allow to pace the streaming of data that results from a remote query. This is extremely important when dealing with resource constrained nodes that need to consume data little by little.", "I did not hear RTI saying:", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those. The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP. The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "I\u2019d like to understand why you say this:", "What do you think is our submission cutting out? We are wire efficient yes, extremely wire efficient but at the same tine we support:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions. I\u2019ll also be more than happy to answer any question you may have about our protocol.", "A+", "Dear ", ",", "You are being to nice to PrismTech as everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises. I think it is best for people to look with their eyes at what the two submission can do and make their own decisions.", "On my side I like debates, I like inquisitive minds and I like hard questions. Thus I\u2019ll be more than happy to answer any question on the XRCE protocol proposed by the PrismTech/ADLINK team explain why it is better than RTI proposal\u2026 And actually it is better than RTPS.", "Thus, please let\u2019s start the open debate to dissect the reasons why our proposal is the one which should be voted and by far the better one.", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being ", "A+", "Thanks ", " for jumping into the community in such an energetic manner. I think we all appreciate having one of the authors of the proposals providing feedback. That said:", "Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "which answers:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I think I\u2019ll stop here\u2026 for the time being ", " .", "Hi Angelo (", "),", "Coordinate different companies to get a common view on a complex matter is always hard, and in this case, there are multiple design options leading to different tradeoffs.", "Our submission (RTI, Twin Oaks & eProsima) already aligns the views of three different DDS vendors, and sure we will try to incorporate ideas of your submission.", "Our submission tries to accomplish the following:", "We coded a PoC of our submission, and during the presentation, you asked about numbers. At that point, with no optimizations at all, and in debug mode we answered 43Kb. I asked my team to optimize a little bit the code, and here are the numbers we have now for the client:", "Total Memory Use: 8 Kb", "But we could squeeze that even a little more. We are releasing this as Open Source (Apache 2) so anyone can review the results.", "But again, we are not aiming to be as small as possible. We are covering all the requisites of the DDS-XRCE RFP, and testing our solution in what we consider typical microcontrollers today.", "Regarding the process at the OMG meeting, we (RTI, Twin Oaks & eProsima) didn\u2019t want to confront both specifications and choose one of them, but have the time to incorporate ideas from your submission, and that is why now we have an extended deadline.", "Let me join in the fray. I\u2019m the other author of the PrismTech submission and the one who built our tiny prototype. I wasn\u2019t present at the OMG meetings, and I won\u2019t waste any words on what may or may not have happened there.", "Firstly, I don\u2019t think a contest of bytes of RAM adds real value to the discussion, although of course it is an honourable contest in itself ", " I am surprised that you, ", ", had never even had a proper look at the memory use of your implementation given the purpose of the exercise in the first place, but if it is 8kB now then it is much better already \u2014 if still 7kB overweight ", " In any case, memory use is determined more by the implementation than by the protocol messages.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already. Yet even that is not of such great interest to me in this discussion.", "What really matters in my opinion is a difference in philosophy. The two proposals suggest very different views of what one would ideally want to accomplish.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that. In a sense, it is just a hand-crafted alternative to CORBA with a lower overhead. (Simply using CORBA actually \u201cjust works\u201d if the DDS implementation is faithful to the IDL interface mappings, even if it is ugly.) To me, this route is a pragmatic way of going about satisfying the RFP, but at the same time, an uninteresting one. (Sorry ", " and others \u2026)", "We chose to design a compact protocol that can support what amounts to performing DDS operations remotely, but doesn\u2019t limit itself to it. Instead, it also supports a DDS-like peer-to-peer network with vastly lower overhead, and, in many ways a level of flexibility in specifying what data is of interest (through URIs and selections) that DDS doesn\u2019t natively support.", "All of that would be of little value if it doesn\u2019t perform well or doesn\u2019t scale well. Just like the code size and memory use are mostly determined by the implementation, so is maximum sustainable performance more determined by implementation than by the details of the protocol headers. Size-wise, my prototype can run as a client on an Arduino Uno (8-bit CPU, 2kB RAM). A small test application using the same implementation configured as a peer easily sends ~700k 8-byte msgs/s from one RPi3 to 3 others (CPU is << 100%, network load ~75% of Fast Ethernet, so I really should investigate why it isn\u2019t faster), and goes another order of magnitude faster when run over local loopback on my MBP. That\u2019s better than your typically DDSI implementation. Is this relevant? That depends on whether you have high rate, tiny messages \u2026", "Now my test application doesn\u2019t implement all of DDS \u2014 not even close \u2014 and this is another significant reason why it can do this with only a few kB of code and RAM. At the same time, this is, I believe, where it gets interesting for ROS2.", "As ROS2 has its own middleware abstraction layer that uses only a fraction of the DDS feature set, putting ROS2 directly on our protocol would get you a smaller and a faster system. Smaller and faster usually allows doing more interesting things, even if I can\u2019t say what exactly those interesting things will be.", "Disclaimer: I can\u2019t do run ROS2 over it today, there\u2019s more work to be done on my prototype before it supports all that is required. And I wish none of you would have to take my word for the data I mentioned, but that is not something that is in my power to solve today.", "It looks like I started something of a minor storm moments before starting a holiday\u2026", "I\u2019m thankful that the DDS vendors, PrismTech, RTI, Twin Oaks and eProsima, are all engaged enough in the ROS community to be present on the Discourse board. It is encouraging to future adopters of ROS 2.", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure", "I wasn\u2019t implying religious believe. It\u2019s an simply expression to describe someone making an assertion. ", " I fully agree that PrismTech\u2019s submission has much smaller messages than the RTI/Twin Oaks/eProsima submission based on the two presentations alone.", "The RFP (which I wrote) asks for a protocol not for an API.", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "It is quite common to use flags. TCP is full of them. My concern is that the protocol is, in my opinion, undoubtedly complex and, based solely on the presentations, more complex than the other submission. Complexity and size are often a balance and in this situation we appear to have one submission at each end of the balance.", "Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes", "With the sorts of overhead you are achieving, I\u2019m not surprised performance is amazing. I\u2019d still like to see numbers, though. ", "And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple", "As was stated elsewhere in this thread, eProsima\u2019s implementation wasn\u2019t optimised. Since you said you are trying to bring yours to market already and eProsima claimed theirs was a tech demo, I\u2019m not surprised yours is more optimised and thus smaller. Of course, the numbers are definitely in your favour for RAM usage. But I\u2019m curious how much program memory each implementation requires, too. This is often the limiting factor on embedded microprocessors rather than the RAM usage.", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP.", "In that case I am very interested in seeing what these AB members wrote.", "The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "What do you think is our submission cutting out?", "\u201cCutting down\u201d, not \u201ccutting out\u201d. I meant that you are trying to reduce the size of the messages on the wire as much as possible, at the expense of possibly needing more complex parsing code. I didn\u2019t mean to say that you are cutting out features. It was clear from the presentation that PrismTech supports more features and has more flexibility than the other submission. But there are trade-offs involved.", "Dynamic Discovery (RTI does not, please read their spec!)", "\nPeer to Peer Communication (RTI does not)", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Non IP Transports (RTI does not)", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "Generalised Queries (RTI only supports DDS-like queries)", "Well, it is ", "-XRCE, is it not?", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions.", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises", "While this is true, we are operating at a standardisation organisation, not a rubber stamp provider. There are interested parties beyond just the implementers. We would prefer not to just hold a vote on which submission to go forward with. We would prefer the submitters to actually work together and produce a single submission that combines the best of both without any technological compromises (yes, I\u2019m aware how hard that is).", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being", "Or, you could provide the answers to those questions, along with the equivalent answers for your submission, so we can see and compare the data to back up your claims.", "Our submission tries to accomplish the following:", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "\nNeither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use.", "This is the strongest impression I got from the presentation, as I said in my own notes. The data model is similar to DDS, which makes adoption by existing DDS users easier, and the ability to implement the protocol in a relatively simple piece of code (which makes it easier to verify and certify) was considered as important as saving bytes on the wire. I\u2019m not sure where the correct balance is between these two requirements, but the PrismTech implementation really gave the impression of being at one extreme.", "testing our solution in what we consider typical microcontrollers today", "This is something that I think is really relevant but that PrismTech have not addressed at all, and RTI/Twin Oaks/eProsima have not addressed enough. What are the typical microcontrollers in use today? What are the target environments for this protocol to be used in?", "The RFP explicitly says this:", "From a high level perspective, the key requirements that a DDS-XRCE implementation has to satisfy are (1) extremely low footprint \u2013 addressing targets with less than 100 Kbytes of RAM, (2) extremely efficient wire protocol \u2013 inducing a protocol overhead of just a few tens of byte over the user data, and (3) support devices that undergo aggressive sleep cycles.", "Both submissions fit within both the RAM usage (with much room to spare) and the protocol overhead.", "More specifically, the actual mandatory requirement is:", "6.5.3. DDS-XRCE protocol overhead, when sending or receiving user data, shall not exceed 24 bytes per packet.", "Let M be the size of the DDS-XRCE message sent out, and U the size of the user data, then M-U <= 24 bytes.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already.", "Again, this should have been in the RFP if it is so important. That would have saved a lot of trouble. All we got was an evaluation criteria:", "Protocol overhead shall be considered when evaluating submissions. Submissions with smaller overhead are preferable.", "This is a miserably small set of criteria for a complex design space. Even you, ", ", say that protocol overhead is not as important as the design philosophy.", "Which I agree is fundamentally different between the two proposals, and that this is where the root cause lies in the failure to reconcile them.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that.", "The RFP not-so-subtly pushes submitters in this direction. You cannot fault them for taking it at face value.", "I will read both submissions and when I do, I will report back with more technically-informed comments.", "Hello ", ",", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "And you see this as a positive aspect? Our model is simpler and more user friendly. For instance, how many people can digest DDS partitions? That said, we have a well defined mapping between XRCE resources and DDS topics.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "Are you an OMG member? If so I\u2019ll forward you the reviews. Both submissions went to the AB and the reviews were posted both on ", " and ", ". If you have access to those mailing list you\u2019ll be able to see them. I also recommend you take a look at those.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "It was impossible as the other vendors did not want to agree on such a low bound. The 24 bytes was the least we could agree on. This is why there is an evaluation on wire-efficiency. This matters were discussed at length, but again I don\u2019t think you attended those meetings, thus you are missing part of the history and the context. In any case, all of those documents are on the OMG archives, thus if of interest you to reconstruct it. Just search for presentation I did on XRCE for almost a year. starting from 2015!", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "Yes, that is correct as it is since the very beginning that we are trying to do a joint submission. They\u2019ve refused with futile arguments \u2013 if you ask me. We have put lots of effort to trying to join but that has not been corresponded. A pity that you were not in the Bruxelles meeting, otherwise you would have had a taste of it.", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Again, you did not attend the end-less arguments we had during the RFP drafting. RTI does not want peer-to-peer in XRCE because they fear it could become as substitute for DDSI-RTPS. Again, this is not something I am inferring, but something that was openly debated during the RFP drafting. We don\u2019t have any issue with that as we think that having a more efficient protocol than DDSI-RTPS for some use cases would be extremely useful.", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "For me that disqualifies completely the submission as in LowPAN environments nobody can afford to use TCP/IP\u2026", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "I am glad that this helped clarifying the situation, please don\u2019t hesitate to ask any other question. Concerning the code availability we are working on that. I\u2019ll keep you posted.", "A+", "Dear All,", "I wanted to let you know that we have just released under Apache 2 a peer-to-peer implementation of our zenoh protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating zenoh as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", "N.B. For those of you that are familiar with XRCE, zenoh is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as zenoh.", "A+", "Kydos,", "Thank you for publishing the Zeno-He library so we can all begin to interact with it.  It is especially useful for the ROS 2 user community to be aware of the effort since it implements the ATLab XRCE proposal.", "I know I would be extremely interested in someone benchmarking Zeno and the proposed epromisa XRCE implementation, and perhaps can find time to do that.", "This is really appreciated ", ", thanks for making this available.", "At present it\u2019s unlicensed code though (", ").", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They believe that their proposal is more efficient in the wire protocol (trying to save every single byte possible), and supports brokered as well as peer-to-peer communication.", "They have invited the competing submitters to join the PrismTech submission.", "PrismTech believes that they have presented their final submission in Brussels (June) and so were expecting to vote for acceptance in New Orleans (September), but because there are still two competing submissions, and no final submission document was received (only a presentation), this is not possible.", "PrismTech\u2019s reasons for going forward with their own submission:\n", "XRCE tries to target the most wire/power/memory efficient protocol, targeting not just IP infrastructures but a whole range of infrastructures.", "Their submission provides reliability and fragmentation.", "Their current prototype runs on an 8-bit microprocessor with 1 KB of RAM and has a wire overhead of 4 bytes for data samples.", "They got a review from the AB which was favourable (only editorial comments), and they believe that the AB review shows that they satisfy all the mandatory requirements.", "\n", "XRCE applications can be brokered into a DDS data space, or they can discover one another and communicate P2P.", "Their submission is only about the protocol, it does not say anything about the API.", "A DDS-XRCE Agent running on permanently connected hardware provides the access to the rest of the (non-XRCE) DDS domain.", "XRCE provides a data space abstraction in which applictions can read and write data autonomously and asynchronously.", "Data read and written by XRCE applications is associated with one or more resources identified by a URI.\n", "An XRCE resource is a closed description for a set of named values.", "Resource URIs allow for wildcards, which means that more than one resource can be targeted in one definition, which is useful for capturing collections of sub-namespaces.", "A resource with a cardinality of one is called a Trivial Resource.", "Resources also have properties, which are used to attach QoS settings to them, such as \u201cthis resource is transient\u201d or \u201cthis resource is reliable\u201d.", "An XRCE selection is the conjunction of a resource and a predicate over the resource content and properties, used to filter a selection. For example, finding all lights with a luminosity greater than zero. (It seems to provide functionality similar to a very basic subset of SQL.)", "\n", "There is a mapping from XRCE resources to DDS topics.", "Resource serialisation uses the same format as DDS.", "All XRCE messages are a single-byte header and a body.\n", "There are flags in the header for reliability and synchronicity.", "Messages can be \u201cdecorated\u201d by prefixing them with a one-byte pre-header, which determines some of the properties for the following header byte, such as fragmentation.", "The protocol is little-endian.", "Variable-length encoding is used to save space.", "\n", "For addressing, the source address for each message is assumed to be a unique address of the sender.", "The protocol is modular, with a core profile (required for any communication), an optional query profile, and an optional discovery profile.\n", "e.g. If using a communications system like Bluetooth, which already has discovery, the XRCE discovery profile can be removed, saving some space.", "Discovery happens by scouting (sending a scout message to ask for types of nodes to reply, i.e. broker nodes or durability services or peers or clients).", "Other nodes reply to a scout with a HELLO message.", "\n", "After discovery, two nodes need to open a session, which enables publishing and subscribing between those two nodes.\n", "Authentication data is included in the session open message.", "Locator information, telling the other node how it can be reached, is included in the open message.", "The receiving node replies with an accept message or a reject message.", "There is an FSM describing the states a session goes through.", "\n", "Resources and selections are uniquely identified by numerical IDs to save space on the wire.", "A declare message is sent to declare what resources, selections, etc. a node has or will publish or subscribe to.", "Subscriptions can be push, pull, periodic pull or periodic push.", "All data messages and declarations are transported over a conduit (for the session), which is a pair of a reliable and a best-effort channel. Multiple conduits may be used in parallel to avoid head-of-line blocking and allow multicasting.", "One decorator is used to select the conduit for the next message.\n", "It is possible to make this decorator one byte instead of two if the number of conduits is less than five.", "\n", "There is a sync message available to set the next sequence number to expect (e.g. for when not starting at zero).", "The ACKNACK message can acknowledge multiple messages at once, usually up to the given sequence number. It has the option to optionally request retransmission of one or more messages after that point.", "There is a one-shot write function in the protocol, which can do a write of a resource without needing to do any prior registration or discovery.", "This proposal appears to be a very complex protocol with a lot of options in message header structures. An implementation would have a lot of choice points during the decoding of a stream of messages. The \u201cdecorator\u201d idea especially may save bytes on the wire (and in message construction buffers) in some cases, but it complicates the protocol implementation.", "PrismTech are already trying to bring their submission to market as a product.", "This proposal also uses an XRCE agent to provide access from DDS-XRCE nodes and the DDS domain.", "An important use case for them is that devices will tend to mostly sleep, and only wake up occassionally to do some processing, and send and receive data. This means that two devices may never be awake at the same time. This is why they have the agent, which is permanently present and provides a way for XRCE nodes to communicate with each other.", "They say that their proposal focuses not just on the XRCE protocol, but also on the interaction between the XRCE protocol and the DDS domain.", "It is possible for an XRCE agent to behave as an XRCE client to another agent, allowing for a hierarchical structure of agents and clients.", "Their proposal is based on the web-enabled DDS specification, with a DDS-XRCE object model in the agent that has a one-to-one mapping to the DDS data model.", "eProsima have put up a demo on Youtube: ", "\n", "Their demo uses 43 KB of RAM (much bigger than the PrismTech proposal, which can fit in 1 KB).", "The XRCE Agent object model is very similar to the DDS object model, making the mapping very simple.", "XRCE objects are modeled as resources that are addressable by their name and have CRUD operations.\n", "Each resource has a name to address it within the agent and a context; a representation describing the resource; and an ID.", "Resources can be represented as a name, an XML description, or a binary representation.", "\n", "Authentication capability is built into the proposal.", "Types are typically pre-defined profiles in the XRCE Agent.\n", "It is possible to transmit types as binary or XML representations.", "\n", "Message structure:\n", "A message header is either 4 or 8 bytes, depending on if a clientKey is used.", "There is a sub-message header, which is another 8 bytes.", "It is possible to send data in a sequence, meaning the header only needs to be sent once for a bunch of samples.", "\n", "The transport can be message-oriented or packet-oriented.", "CDR and DDS-XML representations are reused.", "Message overhead is typically 12 bytes.\n", "They consider that further reduction in overhead would increase complexity and reduce robustness. e.g. They do not need different code paths for multiple sessions, re-connections, handling variable-length encoding.", "They say it compares favourably to TCP/IP overhead (40 bytes).", "They think they could save 6 bytes from their message header, but the reduction is only 6% in the context of total overhead (when considering the use case of using TCP/IP as the underlying transport protocol) and so is not worth it in the face of increased complexity.", "\n", "They also received 3 AB reviews, which they claim were supportive and did not find any non-editorial problems.", "The submissions are very different. PrismTech is aiming for cutting down the bytes used by the protocol to the absolute minimum at the expense of all else. RTI/eProsima/TwinOaks are favouring some overhead in order to achieve a simpler and more robust protocol and implementation.", "PrismTech\u2019s protocol is overly complex with too many choices during the decoding of a message.", "The extra overhead of the RTI/eProsima/TwinOaks submission is not likely to be a problem in the majority of embedded micro-processors used these days (although I\u2019m not sure how many 8-byte, 1KB-of-RAM micros there are in use in new products). However, their consideration that TCP/IP is going to be the most common transport may not be accurate.", "Dynamic Discovery (RTI does not, please read their spec!)", "Peer to Peer Communication (RTI does not)", "Client to Broker Communication", "Non IP Transports (RTI does not)", "Generalised Queries (RTI only supports DDS-like queries)", "Push/Pull/Periodic-Pull and Periodic-Pull Readers", "Unicast and Multicast communication \u2013 for both client to broker and peer-to-peer", "Durability", "The view ", " provides is a) unbiased, b) the view of a roboticist (that\u2019s what this community is about ", " ! )  and c) based on the information someone got from hearing \u201cyour presentation\u201d. Note the following:", "I believe we all appreciate technical argumentation and slides. I love slides. But what I love even more is code and things that I can reproduce. How can I verify your arguments through experimental results? Is there any open code that supports your arguments? More than bashing around, i think it will do a lot of good to facilitate implementations that others can reproduce in common platforms. Even early stages will do. You might find that you could even get some support (and feedback!) before launching it officially and furthermore, that\u2019ll definitely convince a lot of people on why your approach is better.", "Last but not least, I really hope that the passion you\u2019ve shown answering this thread is shown by supporting and making OpenSplice better.", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "Neither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use."], "url": "https://discourse.ros.org/t/ipc-in-ros2/2619"}
,{"title": "How to support ROS2 on MacOS?", "thread_contents": ["We have at least one user of Navigation2 who is trying to run on MacOS. We haven\u2019t had the resources to set up any CI or do any testing using MacOS.", "So, I wanted to start the discussion of how we can support this in the most scalable way? I believe that the ROS build farm is building packages for Mac, but I\u2019m not sure what testing is being done? Ideally, I\u2019d like to leverage the build farm and not have to test every change on a Mac as well as Ubuntu.", "If anyone has a solution for this that doesn\u2019t involve a lot of manual setup and support, I\u2019d love to hear it.", "I believe that the ROS build farm is building packages for Mac, but I\u2019m not sure what testing is being done?", "macOS is a \u201ctier 1\u201d platform, see: ", " builds an archive every night and every release for macOS. Also, pull requests for any change to repositories in the ", " file must pass on macOS as well. We have nightly jobs for macOS also. We treat macOS the same as Linux and Windows in that respect.", "Currently we\u2019re using an old version of macOS, but we have plans to update our farm\u2019s machines in the near future.", "Ideally, I\u2019d like to leverage the build farm and not have to test every change on a Mac as well as Ubuntu.", "I\u2019m not sure what you\u2019re asking for here, you mean depend on the build farm testing all of your dependencies or\u2026?", "If you\u2019re going to support macOS you\u2019ll either have to build every change against macOS (to prevent regressions) or build periodically (nightlys) and tackle regressions as they come up.", "If anyone has a solution for this that doesn\u2019t involve a lot of manual setup and support, I\u2019d love to hear it.", "Travis offers builds for macOS. I haven\u2019t tried it, but it might be possible to just download our nightly archive (", ") and do the steps in our \u201cbinary install for macOS\u201d tutorial (", ").", "I think there are some other options as well, e.g. setting up an appveyor server on your own hardware: ", ".", "Thanks for the help William!", "So If we want our code to build automatically on macOS in the build farm, we need to add our project to the ros2.repos file, is that correct? If so, we can submit a PR for that.", "So If we want our code to build automatically on macOS in the build farm, we need to add our project to the ros2.repos file, is that correct? If so, we can submit a PR for that.", "I believe there was talk of adding a new nightly job on ", " that additionally tests the navigation stack, rather than adding it to the ", " file, simply because our CI is already over 2 hours, so we\u2019re trying to keep the list short until we have a more scalable solution (people are working on that for ", ").", " were we not talking about a navigation nightly at one point? Or am I imagining that?", "As for packaging, I suppose we could try to add it to packaging but not CI, which is the case for the ros1 bridge for example. We also have some \u201csupplemental\u201d repos files which can be used in various ways when running jobs on ", ", e.g.: ", "were we not talking about a navigation nightly at one point?", " and ", " were we not talking about a navigation nightly at one point?", "We\u2019ve been working on CI jobs for navigation2 but not via ", ".", "\nWe\u2019ve been using the CI jobs feature of ros_buildfarm to run builds here: ", "Our efforts have been on getting ros_buildfarm into a state where we can formalize that job and improve the CI experience on ", " rather than putting more onto ", ". ", " doesn\u2019t currently have support for non-Linux platforms.", "I don\u2019t think that we have the capacity to sustainably build community projects on ", " as part of the omnibus packages ether via a broader repos file for packaging jobs or via an additional \u201clayer\u201d of builds on top of the current packaging jobs. This is due in part because of raw compute resources available and in part because managing that in the medium term sounds like it would eventually turn into re-implementing a bunch of what we have for rosdistro distribution management and if we\u2019re going to do that I would rather focus on the long term goal of migrating all packaging from being .repos file based on ros2/ci to being rosdistro distribution.yaml-based via ros_buildfarm.", "I don\u2019t know your CI infrastructure very well (bare minimum TBH), so can someone tell me how I should proceed? If adding to the ros2.repos file is not the way to go, then what is?", "I don\u2019t know your CI infrastructure very well (bare minimum TBH), so can someone tell me how I should proceed?", "I think ", "\u2019s point is that we don\u2019t have the resources to provide macOS jobs for everyone in the community, and I agree. We\u2019re already running into cases where nightly jobs don\u2019t finish until well into the workday for us.", "I was thinking a single nightly job might be possible, but the reality is that we\u2019re pretty much at capacity and don\u2019t have plans to expand it.", "On the other hand, ", " is meant for the whole community (and is where the nav nightly\u2019s are run), but it currently doesn\u2019t support anything but Linux. I think the plan is to eventually have Windows and even macOS for ", ", and in a way that can scale properly, but not in the near future.", "If adding to the ros2.repos file is not the way to go, then what is?", "I don\u2019t have a solution for you other than to point you to resources that might help you setup your own macOS testing (travis, appveyor, or your own instance of our ", ").", "OK, I\u2019ll look into the Travis option for now.", "Then maybe we can discuss getting Windows and MacOS support into ", " soon. ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/how-to-support-ros2-on-macos/10451"}
,{"title": "Reliability, safety, security, maintenance and support in ROS", "thread_contents": ["In the last edition of ", " there is an ", " (1.3 MB) with the title ", " on ROS and especially on the differences between the \u201cROS approach\u201d and \u201cexisting robot manufacturers approach\u201d. The article is written by Henk Kiela, professor in Mechatronics and Robotics. He raises some interesting points and gives advice for both ROS developers as \u201cnon-ROS developers\u201d. I am very interested in your opinions on this article.", "It is an interesting article.", "The main takeaway for me is that ROS community prefer to go broad (the more, the better) whilst a typical robotic product need to do deeper.", "In the specific context of mobile robot navigation, it is not surprising that some commercially available solutions are better, since they could focus full time on a single use case.", "Additionally, I believe that ROS was designed from the ground up as a easy-to-learn framework for non experts; this is, at the same time, its biggest strength and weakness, since the openesses of the system also makes very hard to deal with safety.", "About the comparison table: I do not agree with a few points and it feels a little bit biased, even if I agree on most of the items (80% maube?)", "\nThere is one in particular that I consider very interesting:", "Open source is \u2018free\u2019; the effort to make it a reliable and safe system is unpredictable.", "This is unfortunately true; I have multiple examples of situations in which I could have been better served, in the long term, by an expensive COTS solution rather than the open-source \u201cfree\u201d one.", "I have been complaining a lot about the open source model and the issue of sustainability. I like open source software (hey, I have my own OSS projects!), but it is naive to expect that:", "Thanks for sharing the article.", "Davide", "Hi,", "In my humble opinion, I do not agree with the focus of the article. I think you can not compare a final product of two companies with ROS, even if we limit it to their navigation capabilities. ROS provides a base where to build concrete applications, and it is the responsibility of the person who develops a final application to complete the generalities of ROS to adapt it to a final solution.", "There are many robot companies that have robots navigating reliably and safely in real environments using ROS. We have a Pepper navigating 24x7 in a real office environment (", ") in which, of course, there was a lot of work of adjusting parameters.", "Of course, this requires a lot of work to adjust parameters, but no more than those that would have to be adjusted in any other navigation system, with the advantage that in ROS there is a community that develops and tests software that you do not have to implement from scratch .", "Anyway, thanks for sharing,", "\nFrancisco", "Thanks for sharing, it\u2019s an interesting article.", "I have to grudgingly agree with most of the points he makes. Right now it\u2019s very difficult to integrate ROS safely in an industrial grade robot, having either to encapsulate the cell in a parallel, isolated safety system or making ROS act as a kind of supervisor, encapsulating more low level and safety critical systems.", "I see the value of focusing ROS on scientific research and foregoing more mundane features such as these, but support from industrial manufacturers could really ensure the longevity of the project.", " great that you brought this up for discussion and thanks!", "A few thoughts from my side: Henk summarized pretty nicely the relevance of ROS and some of the issues related to building robots from ground up. One must note that Prof. Kiela has several decades of experience in this area and a deep industrial insight. However, to the best of my knowledge, he\u2019s not very actively involved in the ROS community (at least it didn\u2019t feel that way last time we spoke!).", "He presents ROS 2.0 as", "a project was started recently, incorporating the new", "\nrequirement for mission-critical functionality and indeed safety", "Arguably, some members of the community will object since ROS 2.0 started quite a few years ago (e.g. see ", "). Nevertheless, ROS 2.0 has indeed become official within the last year. Much has happened since though!", "In addition and as pointed by ", ", I disagree with several aspects exposed in the comparison made in table 1. E.g., ROS is indeed being used in safe environments and just recently, at ROS-Industrial Conference, a speaker shared quite a few details about their safe setup while using ROS (1)  and how they did it (they followed an isolated safety system-approach making the ROS setup as a behavior coordinator, as mentioned by ", " above).", "Of course, this doesn\u2019t mean they \u201ccertified ROS\u201d (completely) and one should questions whether this makes sense at all. Selecting, however, individual ROS packages (or stacks) and adapting them for complying with a particular set of guidelines is something that several companies are already doing.", "A store of ROS-certified modules could provide a similar", "\nfunction to the user community. The certification should", "\nprovide minimum qualifications for the performance,", "\nsafety, security and maintainability of a module. Such a", "\nscheme could be adopted for ROS 2.0 in the future, but this", "\nshould also be done right away for ROS Industrial.", "I think it\u2019s relevant to note here that by modules, Henk here refers to both software and/or hardware modules. Certification of such is an ongoing discussion and new standards like ISO 22166, hopefully, will provide some answers. A pre-requisite and one of the aspects required for modularity (and hardware reconfiguration) is interoperability. In the article, it\u2019s claimed that \u201c", "\u201d in ROS. I also disagree with this. Furthermore, there exists new projects like ", " that aim to facilitate interoperability by defining a common information model that generates artifacts in an MDE fashion (while offering a structure that allows for its extension in other robotic frameworks).", "I think you can not compare a final product of two companies with ROS, even if we limit it to their navigation capabilities. ROS provides a base where to build concrete applications, and it is the responsibility of the person who develops a final application to complete the generalities of ROS to adapt it to a final solution.", "I find this statement pretty interesting ", " and while I don\u2019t fully disagree when it comes to  \u201c", "\u201d , I actually have a pretty strong opinion about the usefulness of ROS (ROS 2 in particular) to compare different individual components. Our team, often, finds that comparing two different pieces of hardware of the same kind (e.g. two cameras or two actuators aimed for a similar task) is actually pretty hard. Specially because things like communication interfaces and APIs differ substantially (which makes system integration a complete hell most of the time).", "Our approach for the last years has been to \u201cfind common ground\u201d to attack these comparisons. Typically, assuming you have component A (", ") and component B (", ") with their respective interfaces, let\u2019s say ", " and ", ",  to compare them. Often, you\u2019d go ahead and create an abstraction layer on top of ", " and ", " called ", " that allows you to speak to both components and effectively make the comparisons. This way, you can inspect which component ", " or ", " actually performs better for your needs.", "To us, ", ". I think this is one of the core principles of ROS and also, the reason why we selected it when we started building ", " years ago covering not only logical and electrical interfaces but a wide variety of aspects required to go from a component to a module (note that modules imply certain characteristics including interoperability).", "Now, back to your statement, I believe it depends very much on the \u201cfinal product\u201d (assuming we\u2019re speaking of robots, final robots) itself. E.g., we started a while defining ", ". While this is still a work in progress, we managed to interface with several 6DoF arms (", ") and that provided quite a bit of insight that we later used in the development of our latest robots so I\u2019d argue that you can actually compare ", " final products with ROS.", " I\u2019m very interested to hear what\u2019s your reaction to this since as I said, we\u2019re still exploring this path and we certainly could use additional insight.", "ROS is indeed being used in safe environments and just recently, at ROS-Industrial Conference, a speaker shared quite a few details about their safe setup while using ROS (1)", "  Is there a recording of this talk?", "there is an inofficial ", " of the talk \u2026 and there will be a publication of the talk via the official ", " channel with dedicated playlists for each day of the conference \u2026 I will keep you posted\u2026", "there is an inofficial ", " of the talk \u2026", "I think ", " means the talk from Georg Heppner and Fabian Fuerst (Flexible Automotive Assembly with Industrial Co-workers).", "Georg Heppner and Fabian Fuerst (Flexible Automotive Assembly with Industrial Co-workers).", "that was on Day 3, the playlist for ", " is not yet finished. Sorry, it looks like you will have to wait until January 2019\u2026", "  That is great to hear.  Thanks for making it available.", "Hey there, happy new year.", "As ", " said, it\u2019s the talk about Flexible Assembly. AFAIK it\u2019s not ready just yet. Let\u2019s wait until ", " and his colleagues publish it.", "Would be happy to keep discussing it then.", "Cheers!", "Here is the ", " of ROS-Industrial Conference 2018 in Stuttgart, Germany.", "The talk on Flexible Automotive Assembly with Industrial Co-workers", "\nby Fabian Fuerst, Opel, and Georg Heppner, FZI", "\ncan be ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The quality of an horizontal OSS software match the quality of a vertical and commercial one.", "That it is possible to create high quality software without a sustainable way to pay full-time, highly-skilled developers."], "url": "https://discourse.ros.org/t/reliability-safety-security-maintenance-and-support-in-ros/7146"}
,{"title": "ROS Quality Assurance Working Group meeting minutes Kick Off Meeting - 10/01/2018", "thread_contents": ["ROS Quality Assurance Working Group meeting minutes Kick Off Meeting", "\nTime: 9 a.m. UTC and 5 p.m. UTC", "Participants:", "\n9 a.m. UTC Group", "5 p.m. UTC Group", "Notes:", "\nROSIN quality assurance (QA) initiatives were discussed. Below is a summary of the discussion. The following problems and solutions were discussed:", " (31.0 KB)", " (15.3 KB)", " (70.1 KB)", "Problem: There is a lack of a centralized source for community quality assurance practices, knowledge, and collaboration.", "Problem: The quality of packages is not visible.", "Problem: Inconsistent practice of code review", "Problem: Recruiting maintainers is a \u201creal problem\u201d for ROS and ROS-I. This has led to an increasing number of orphan packages. This is a capacity issue within the core team. The team is struggling to attract new maintainers. The team capacity does not reflect the maintenance effort required. This is also applicable to non-core packages. There is a lack of willingness to contribute to packages\u2019 maintenance. It is a challenge to attract and retain new maintainers.", "On maintaining orphaned packages", "Recruiting more maintainers", "I recommend using the github tools for code reviews.", "GitHub is where people build software. More than 28 million people use GitHub to discover, fork, and contribute to over 85 million projects.", "Where can I get informed about future meetings of this initiative?", "I\u2019ll send you the Doole link for next meeting to your email. I have your email. I believe I sent you for the first meeting.", " ah, I see. Yes, I got a doodle inquiry, but did not respond. So probably that\u2019s why I didn\u2019t get the result\u2026", "In the future, I\u2019ll know to respond. For me personally, I would welcome getting the meeting date in any case, regardless of whether I responded or not, but I can see how other people might feel differently.", " I don\u2019t know if I\u2019m still on time to be included in the next meetings, but I\u2019m also interested and glad to help if possible.", ", you more than welcome. Just send me your email and I\u2019ll include you in the next invite.", ", I\u2019ll make sure you get all the invites.", "Can I get invite too please: ", ".", "thx, D.", "Im interested in the meeting too. ", "Cheers,", "\nJihoon", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Adam Alami", "Akshay\tJain", "Andrzej Wasowski", "Geoffrey Biggs", "Kei Okada", "Adam\tAlami", "Aaditya Saraiya", "David Bensoussan", "Dirk Thomas", "Gijs van der Hoorn", "Ian McMahon", "Luca Marchionni", "Matt Droter", "Shaun Edwards", "Victor Lopez", "\n", "\n", "Solutions:\n", "Quality Hub: Would inform about existing practices and would be a central \u201cgo-to\u201d place for QA knowledge sharing (documentation of QA practices)\n", "Discussion:\n", "Make the content of the website educational and easy to digest.", "The content should capture the knowledge most engineers do not already have.", "The website should be incorporated into the existing infrastructure (i.e., Wiki, ROS Answers).", "\n", "\n", "Quality Discourse: A dedicated QA forum\n", "Discussion:\n", "A chapter was created for Quality Assurance.", "\n", "\n", "\n", "\n", "\n", "\n", "Solution:\n", "Make ROS packages\u2019 quality visible.", "\n1. Discussion:\n", "A \u201cQuality Stamp\u201d was suggested. We can use a script (leverage existing Github feature) to generate the stamp.", "Enforce the stamp creation in the distribution process.", "\n", "\n", "\n", "\n", "\n", "Solution:\n", "Energize the code review process.\n", "Discussion:\n", "It was recommended to use the combination of a tool and peer review.", "It was suggested to create a website (i.e., similar to ", ") dedicated to code review.", "Motivation was discussed. What would motivate community members to do code review? A reward system similar to the \u201cKarma\u201d system was discussed.", "Review and update the current standards.", "Possibly provide tutorials on how to review a pull request.", "\n", "\n", "\n", "\n", "\n", "\n", "Solutions:\n", "Propose and implement a funding model for the maintenance activities.", "Organize periodic campaigns to recruit new maintainers for both core and non-core packages.", "Define an onboarding process for both core and non-core community members.", "Document the onboarding process, including online educational materials (i.e., tutorials).", "Implement the onboarding process.", "Formalize the code ownership process.", "\n1. Discussion:\n", "Reward maintainers with Github Bounty.", "Identify a sustainability strategy.", "The possibility of using ROSIN FTPs to finance maintenance was discussed.", "\nLinks:", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-quality-assurance-working-group-meeting-minutes-kick-off-meeting-10-01-2018/3664"}
,{"title": "[CFP] ACM/IEEE IPSN 2019 in CPSWeek", "thread_contents": ["The 18th ACM/IEEE The International Conference on Information Processing in Sensor Networks (IPSN)", "===================================================================", "\nApril 16-18, 2019, Montreal, Canada", "IPSN\u201919 is part of CPS-IoT Week 2019, co-located with HSCC, ICCPS, CPS-IoT, and RTAS.", "===================================================================", "The International Conference on Information Processing in Sensor Networks (IPSN) is a leading annual forum on research in networked sensing and control, broadly defined. IPSN brings together researchers from academia, industry, and government to present and discuss recent advances in both theoretical and experimental research. Its scope includes signal and image processing, information and coding theory, databases and information management, distributed algorithms, networks and protocols, wireless communications, collaborative objects and the Internet of Things, machine learning, mobile and social sensing, and embedded systems design. Of special interest are contributions at the confluence of multiple of these areas.", "Like prior years, IPSN 2019 will continue its co-location with CPS-IoT WEEK, the premier venue for research and development of Cyberphysical Systems, and its strong focus on algorithms, theory, and systems for information processing using networks of embedded, human-in-the-loop, or social sensors, as well as new hardware and software platforms, design methods, architectures, modelling, implementation, evaluation, deployment experiences, and tools for networked embedded sensor systems and the Internet of Things. Topics of interest include, but are not limited to:", "In addition to IPSN\u2019s traditional focus, IPSN 2019 will place a particular emphasis on embedded machine learning and computer vision, with a special track centered on these topics. In this case, topics of interest include but are not limited to:", "\nComing soon.", "\nAs in previous years, IPSN will also elicit a review and response process. After the initial review, the program committee may request a short response from selected papers for additional clarifications. The authors will get a short time window of roughly 24 hours to respond. Participation in the response process is not mandatory.", "\nThe two conferences are collaborating to create new synergies, also exploiting their CPSWEEK bi-annual co-location. Prospective authors are thus strongly encouraged to consider these guidelines:", "The TPC chairs of both conferences may agree to move a submitted paper from one to the other if they see a better fit, subject to the authors\u2019 consent. The final decision about where to submit, and therefore what program committee will review the work, ultimately rests with the authors.", "\nPaper Abstract Registration: October 10th, 2018 11:59pm AoE(UTC-12)", "\nPaper Submission Deadline: (Firm*): October, 17th 2018 11:59pm AoE(UTC-12)", "\nAuthors\u2019 rebuttal period December 17th - 19th, 2018", "\nAcceptance Notification: January 16th, 2019", "\nCamera-Ready Deadline: TBD", "\nGeneral Chair:", "\nRasit Eskicioglu (University of Manitoba)", "Technical Committee Chairs:", "\nLuca Mottola (Politecnico di Milan and RISE SICS)", "\nBodhi Priyantha (Microsoft Research)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Sensor data storage, retrieval, processing", "Streaming sensor system tasking and operation", "Coding, compression, and information theory", "Theoretical foundation and fundamental bounds", "Network and system architectures and protocols", "IoT gateway platform architecture and services", "Outdoor, wide-area sensing systems", "Location, time, and other network services", "Programming models, languages, and systems", "Programming models for IoT ensembles", "Modeling, simulation, and measurement tools", "Operating systems and runtime environments", "Applications in health, wellness & sustainability", "Applications in smart cities and urban health", "Experiences, challenges, comparisons of platforms", "Discovery, coordination, and use of IoT services", "Security and privacy in heterogeneous systems", "IoT reliability, adaptability, and dependability", "Technical assessment of emerging IoT standards", "Wearable systems and data processing algorithms", "Sensor-enabled drone platforms and algorithms", "Machine learning and deep learning on sensor data", "New hardware and system design to enable machine learning on sensor data", "Novel embedded machine learning algorithms", "Data related issues, such as methods, tools, and analysis", "Computer vision for resource-constrained and mobile platforms", "Contributions of embedded nature and applying to network segments from the IoT gateway to field devices should be submitted to IPSN. Examples include localization, low-power wireless networking, and embedded data processing.", "Contributions with an end-to-end perspective or applying to network segments from the IoT gateway to the cloud should be submitted to IoTDI. Examples include cloud data processing, edge computing, and systems covering multiple network segments."], "url": "https://discourse.ros.org/t/cfp-acm-ieee-ipsn-2019-in-cpsweek/6240"}
,{"title": "[meetings] ROSDevCon: Call for Participation - ROS Developers Conference 2019", "thread_contents": ["(*This conference has no relation with the official ROSCon.)", "Dear colleagues,", "The ROSDevCon 2019 is a hands-on online conference for ROS developers. The conference aims to connect ROS developers around the world without geographical restrictions and to advance ROS levels through real-time practice.", "During the conference, all participants will practice in real-time on any type of computer while the speakers are presenting. With a ready-made ROSject, participants will be able to launch the robot simulation, access the project\u2019s code, start developing control algorithms\u2026without any previous setup. All the participants will have", "Access to the conference LIVE streaming", "Full access to the ROS Development Studio (ROSDS) for programming", "Full access to the online chat tool with all participants and ROS experts", "Access to ROSjects (containing simulations, packages with pre-defined code)", "An e-book of ROS2 Basics", "Access to the video recording", "[ KEYNOTE SPEAKERS ]", "Carlos Rosales (CTO at Beta Robots): CHESS LAB", "Davide Faconti (Senior Robotic Architect at Blue Ocean Robotics): Finite State Machines are dead. Long life Behavior Trees", "Dominik Nowak (CEO at Husarion): Object search in ROS", "James Carroll (Associate Professor at Clarkson University): Use an open Manipulator to play tic-tac-toe", "Miguel Angel Rodriguez (CTO at The Construct): ROS connection to a RaspberryPi PanAndTilt through ROSDS", "Ludovic Delval (Software Engineer at Fraunhofer IPA): Porting a node from ROS1 to ROS2", "Luca Marchionni (CTO at PAL Robotics): Table segmentation with PCL ROS", "Tomoya Fujita (Software Developer at Sony Corporation): Unix Domain Socket communication in ROS", "[ ORGANIZER ]", "The Construct", "You can contact us with questions and doubts here: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/meetings-rosdevcon-call-for-participation-ros-developers-conference-2019/9481"}
,{"title": "Navigation2 WG changes and HELP WANTED", "thread_contents": ["All,", "\nI\u2019m saddened to say that the Nav2 team at Intel that has been on the project for the last 18+ months is being disbanded.", "What this means is that ", " to continue moving Navigation2 forward to make it even better.", "I\u2019m passing the torch of leadership of the Navigation2 WG to Steve Macenski at Samsung, who has been a major contributor to the project. My hope is to continue contributing as much as I can, time allowing, and to stay active in the ROS2 community.", "Matt", "Discourse needs a \u201cdon\u2019t like this post\u201d button next to the \u201clike this post\u201d button.", "It\u2019s sad to hear that Intel\u2019s nav2 team is going away. You\u2019ve all done a huge amount of work over the last 18 months to bring navigation to ROS 2.", "Is Intel cutting back its ROS 2 contributions in general or is this just a refocusing?", "Is Intel cutting back its ROS 2 contributions in general or is this just a refocusing?", "It\u2019s part of an overall restructuring and re-organization that is happening. I can\u2019t answer with more than that right now.", "I was looking for the sad button on Discourse to react on this post,", "\nalthough I have to say, you did a great job until now, and passing the leadership to Steve is the best for that project to keep going on", " you forget what gifs are for:", "I definitely appreciate any help I can get. Both from junior software developers wanting to get into ROS to experts in algorithms to help modernize our technologies and anywhere in between. Matt and his team has been crucial to getting the stack to where it is today and the skeleton crew of ", " isn\u2019t enough waking hours to do navigation and my other research oriented goals.", "If any company wants to donate a robot to me to develop on and have first class support in ROS2 Navigation, let me know ", "We\u2019ll miss you and your great open source leadership Matt!", "Thanks for all the blood, sweat, and tears you\u2019ve given us ", ". It won\u2019t be forgotten.", "Just a random user(developer?) passing by. This is my first thought.", "Still, thanks for your awesome contribution to ROS2 and Nav2.", "Thanks for the contributions ", " and everyone else in your team!", "This is indeed sad news ", ", but like ", " ", " and others said, many thanks for the awesome contributions that you have made up to this point! ", " ", " ", "Thank you, ", " and team for your work. Was a real pleasure to talk with you at the ROS-I conference. I wish we can still see you around.", "at the ROS-I conference", "in case you missed it\u2026 here\u2019s the video of Matt giving a ROS2 Robot Dev Kit feat. Navigation2 Overview at ", "\u2026 more videos to be released soon\u2026", "\n", "Sorry yo hear that ", ". Thanks ", " for your work", "Thanks everyone for the nice comments.", "I want to make sure that everyone in the team is acknowledged:", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Thanks to all of them for the hard work they\u2019ve done to this point and hopefully to more opportunities in ROS2 in the future!", "Yeah, big thanks to you and all of the nav2 team. I didn\u2019t have the opportunity to work with you all often, but I really appreciate the value you guys helped bring to the community.", "Thanks to the entire Nav2 team at Intel for their contributions, and for spearheading much of the ros2 migration process. The navigation2 project wouldn\u2019t be anywhere as far along as it is today without the time and effort they\u2019ve put in. It was a pleasure to collaborate with you all, and I hope to see you continually around the community.", "I wanted to acknowledge all the efforts put down by you guys. And a great thank you for building such a good piece of software.", "Feeling sad that our team is getting disbanded ", "We were just getting started! With plenty of ideas on the ", ".", "Anyhow, thanks ", ", for your great leadership. ", " and Intel-team, it\u2019s been a pleasure working with all of you.", "Its been a pleasure working with you too!", "I\u2019m sure we\u2019ll cross paths again. It\u2019s a small community.", "Thanks Matt and the team\u2019s excellent work on leading Navigation2 WG and its development. It is also my pleasure and honor to work with you as part of Intel\u2019s contribution to ROS2 community.", "I wish you all the best in the future and keep touch.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/navigation2-wg-changes-and-help-wanted/12348"}
,{"title": "Ada client library for ROS2", "thread_contents": ["Hello,", "I\u2019m just starting to write an Ada client library for ROS2. The Ada language is a strong fit for the aims of the ROS2 project: it was designed with safety, real time, embedded, high integrity, long maintenance concerns in mind and is today one of the languages of choice in industries like aerospace, air control, and the like (see e.g. ", " for a nice overview).", "I\u2019m going to work mostly full time on this for the next three months, and assuming it reaches an acceptable level of maturity I intend to continue maintaining it for the foreseeable future, since it will be a tool used internally in our laboratory.", "At the moment there is nothing available (I\u2019m still in the documentation phase) but any progress will be published here: ", "Besides announcing this here for anyone interested, I welcome any pointers, suggestions or experiences on writing a new client library for ROS2 that you may have. I\u2019m reading all I can find out there but if you have some direct advice it will be much appreciated.", "I\u2019m just starting to write an Ada client library for ROS2. The Ada language is a strong fit for the aims of the ROS2 project: it was designed with safety, real time, embedded, high integrity, long maintenance concerns in mind and is today one of the languages of choice in industries like aerospace, air control, and the like (see e.g. here  for a nice overview).", "Wow, that\u2019s really cool. A lot of my former colleagues favored Ada (for avionics project) exactly due to the points you mentioned. They would love to play around with ", " I guess.", "I\u2019m going to work mostly full time on this for the next three months, and assuming it reaches an acceptable level of maturity I intend to continue maintaining it for the foreseeable future, since it will be a tool used internally in our laboratory.", "I would love to have that much free time allowing me to contribute more than I actually do ", "Besides announcing this here for anyone interested, I welcome any pointers, suggestions or experiences on writing a new client library for ROS2 that you may have. I\u2019m reading all I can find out there but if you have some direct advice it will be much appreciated.", "Some time ago I collected some ", ". It was meant for people which never worked with ROS2 before. Anyway probably helpful\u2026", "Some time ago I collected some high level \u201cfinger pointers\u201d in a short list for some probable implementation of a ROS2 client library in D . It was meant for people which never worked with ROS2 before. Anyway probably helpful\u2026", "Thanks a lot, Florian. That sure helps in getting my bearings around.", "As a followup to my original post, and in the wake of the ROS2 Bouncy release, it\u2019s my pleasure to announce that RCLAda has reached its first milestone and is ready for public use.", "RCLAda comes with several examples mimicking the ones in the C++/Python client libs for topics and services. There are also examples of concurrent event processing and custom memory allocators. At this time the API might still change a bit but all critical functionality should be in place for initial testing.", "Currently missing, and the target for the next release, are options for QoS (which for now is always at the default setting) and static type generation of messages (which is currently sidestepped by using the introspection features of ROS2 and Ada typed reference views).", "Check it out at ", "Cheers,", "\n\u00c1lex", "That sounds great! You might want to consider ", " the individual packages into Bouncy to make them available as Debian packages.", "Yup, that\u2019s among the list of pending issues I have ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ada-client-library-for-ros2/4630"}
,{"title": "New Packages for Indigo", "thread_contents": ["HI Everyone,", "We have another batch of packages available for Indigo. We continue to add many new packages as well as having a lot of maintenance releases.", "Thanks to everyone who contributed. The list of packages and maintainers are below.", "Your ROS Release Team", "Updates to indigo", "\nAdded Packages [58]:", "Updated Packages [228]:", "Removed Packages [2]:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-alfred-bot: 0.1.121-0", "ros-indigo-alfred-sr-linux: 0.1.20-0", "ros-indigo-arm-components-name-manager: 1.0.0-0", "ros-indigo-astra-camera: 0.1.5-0", "ros-indigo-astra-launch: 0.1.0-0", "ros-indigo-axcli: 0.1.0-0", "ros-indigo-baselib-binding: 1.0.0-0", "ros-indigo-bwi-joystick-teleop: 0.3.8-0", "ros-indigo-catkin-pip: 0.1.8-0", "ros-indigo-convenience-math-functions: 1.0.0-0", "ros-indigo-convenience-ros-functions: 1.0.0-0", "ros-indigo-gazebo-grasp-plugin: 1.0.1-0", "ros-indigo-gazebo-state-plugins: 1.0.1-0", "ros-indigo-gazebo-test-tools: 1.0.1-0", "ros-indigo-gazebo-world-plugin-loader: 1.0.1-0", "ros-indigo-jsk-fetch-startup: 1.0.6-2", "ros-indigo-jsk-pr2-startup: 1.0.6-2", "ros-indigo-jsk-robot: 1.0.6-2", "ros-indigo-logger-binding: 1.0.0-0", "ros-indigo-media-msgs: 0.1.58-0", "ros-indigo-micros-mars-task-alloc: 0.0.5-1", "ros-indigo-moveit-controller-multidof: 1.0.0-0", "ros-indigo-moveit-object-handling: 1.0.0-0", "ros-indigo-moveit-planning-helper: 1.0.0-0", "ros-indigo-network-wakeonlan: 0.1.65-0", "ros-indigo-object-msgs: 1.0.0-0", "ros-indigo-object-msgs-tools: 1.0.0-0", "ros-indigo-parrot-arsdk: 3.9.1-3", "ros-indigo-path-navigation-msgs: 1.0.0-0", "ros-indigo-pyros-config: 0.1.2-0", "ros-indigo-pyros-utils: 0.1.2-0", "ros-indigo-rosjava-dynamic-reconfigure: 0.2.4-0", "ros-indigo-smarthome-comm-msgs: 0.1.19-0", "ros-indigo-smarthome-comm-msgs-java: 0.1.15-0", "ros-indigo-smarthome-common-driver: 0.1.61-0", "ros-indigo-smarthome-heater-msgs: 0.1.21-0", "ros-indigo-smarthome-heater-msgs-java: 0.1.21-0", "ros-indigo-smarthome-light-msgs: 0.1.2-0", "ros-indigo-smarthome-media-kodi-driver: 0.1.57-0", "ros-indigo-smarthome-media-model: 0.1.60-0", "ros-indigo-smarthome-media-msgs: 0.1.59-0", "ros-indigo-smarthome-media-msgs-java: 0.1.82-0", "ros-indigo-smarthome-media-onkyo-driver: 0.1.64-0", "ros-indigo-smarthome-media-samsungtv-driver: 0.1.58-0", "ros-indigo-smarthome-msgs: 0.1.3-0", "ros-indigo-smarthome-msgs-java: 0.1.1-0", "ros-indigo-smarthome-network-wakeonlan: 0.1.66-0", "ros-indigo-smarthome-network-zeroconf: 0.1.66-0", "ros-indigo-swri-console: 0.1.0-0", "ros-indigo-trac-ik: 1.4.3-0", "ros-indigo-trac-ik-examples: 1.4.3-0", "ros-indigo-trac-ik-kinematics-plugin: 1.4.3-0", "ros-indigo-trac-ik-lib: 1.4.3-0", "ros-indigo-urdf-tools: 1.0.0-0", "ros-indigo-urdf-transform: 1.0.0-0", "ros-indigo-urdf-traverser: 1.0.0-0", "ros-indigo-urdf-viewer: 1.0.0-0", "ros-indigo-urdf2inventor: 1.0.0-0", "ros-indigo-amcl: 1.12.8-0 -> 1.12.11-0", "ros-indigo-arm-navigation-msgs: 0.0.2-0 -> 0.0.3-0", "ros-indigo-base-local-planner: 1.12.8-0 -> 1.12.11-0", "ros-indigo-baxtereus: 1.0.5-1 -> 1.0.6-2", "ros-indigo-bwi-common: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-gazebo-entities: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-interruptable-action-server: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-kr-execution: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-logging: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-mapper: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-msgs: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-planning-common: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-rqt-plugins: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-scavenger: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-tasks: 0.3.7-0 -> 0.3.8-0", "ros-indigo-bwi-tools: 0.3.7-0 -> 0.3.8-0", "ros-indigo-camera-pose-calibration: 0.1.2-0 -> 0.1.5-0", "ros-indigo-carrot-planner: 1.12.8-0 -> 1.12.11-0", "ros-indigo-catkin-pure-python: 0.0.8-0 -> 0.1.2-0", "ros-indigo-clear-costmap-recovery: 1.12.8-0 -> 1.12.11-0", "ros-indigo-collada-parser: 1.11.10-0 -> 1.11.11-0", "ros-indigo-collada-robots: 0.0.2-0 -> 0.0.3-0", "ros-indigo-collada-urdf: 1.11.10-0 -> 1.11.11-0", "ros-indigo-costmap-2d: 1.12.8-0 -> 1.12.11-0", "ros-indigo-diagnostic-aggregator: 1.8.9-0 -> 1.8.10-0", "ros-indigo-diagnostic-analysis: 1.8.9-0 -> 1.8.10-0", "ros-indigo-diagnostic-common-diagnostics: 1.8.9-0 -> 1.8.10-0", "ros-indigo-diagnostic-updater: 1.8.9-0 -> 1.8.10-0", "ros-indigo-diagnostics: 1.8.9-0 -> 1.8.10-0", "ros-indigo-dji-sdk: 0.1.7-0 -> 0.1.9-0", "ros-indigo-dji-sdk-demo: 0.1.7-0 -> 0.1.9-0", "ros-indigo-dji-sdk-dji2mav: 0.1.7-0 -> 0.1.9-0", "ros-indigo-dji-sdk-lib: 0.1.7-0 -> 0.1.9-0", "ros-indigo-dji-sdk-web-groundstation: 0.1.7-0 -> 0.1.9-0", "ros-indigo-dwa-local-planner: 1.12.8-0 -> 1.12.11-0", "ros-indigo-dynamic-tf-publisher: 2.0.14-0 -> 2.0.16-1", "ros-indigo-dynpick-driver: 0.0.9-0 -> 0.0.11-0", "ros-indigo-ecl-command-line: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-concepts: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-config: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-console: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-containers: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-converters: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-converters-lite: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-core: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-core-apps: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-devices: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-eigen: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-errors: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-exceptions: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-filesystem: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-formatters: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-geometry: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-io: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-ipc: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-linear-algebra: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-lite: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-math: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-mpl: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-sigslots: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-sigslots-lite: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-statistics: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-streams: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-threads: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-time: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-time-lite: 0.61.4-0 -> 0.61.6-0", "ros-indigo-ecl-type-traits: 0.61.9-0 -> 0.61.14-0", "ros-indigo-ecl-utilities: 0.61.9-0 -> 0.61.14-0", "ros-indigo-fake-localization: 1.12.8-0 -> 1.12.11-0", "ros-indigo-fanuc: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-driver: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-lrmate200ic-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-lrmate200ic-moveit-plugins: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-lrmate200ic-support: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-lrmate200ic5h-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-lrmate200ic5l-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m10ia-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m10ia-moveit-plugins: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m10ia-support: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m16ib-moveit-plugins: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m16ib-support: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m16ib20-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m20ia-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m20ia-moveit-plugins: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m20ia-support: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m20ia10l-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m430ia-moveit-plugins: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m430ia-support: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m430ia2f-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-m430ia2p-moveit-config: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fanuc-resources: 0.4.0-1 -> 0.4.1-0", "ros-indigo-fetch-calibration: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetch-depth-layer: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetch-description: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetch-maps: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetch-moveit-config: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetch-navigation: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetch-teleop: 0.7.5-0 -> 0.7.7-0", "ros-indigo-fetcheus: 1.0.5-1 -> 1.0.6-2", "ros-indigo-global-planner: 1.12.8-0 -> 1.12.11-0", "ros-indigo-grasp-planning-graspit: 0.1.5-0 -> 1.0.0-0", "ros-indigo-grasp-planning-graspit-msgs: 0.1.5-0 -> 1.0.0-0", "ros-indigo-grasp-planning-graspit-ros: 0.1.5-0 -> 1.0.0-0", "ros-indigo-graspit-tools: 0.1.5-0 -> 1.0.0-0", "ros-indigo-hironx-calibration: 1.1.14-0 -> 1.1.15-0", "ros-indigo-hironx-moveit-config: 1.1.14-0 -> 1.1.15-0", "ros-indigo-hironx-ros-bridge: 1.1.14-0 -> 1.1.15-0", "ros-indigo-image-view2: 2.0.14-0 -> 2.0.16-1", "ros-indigo-jackal-control: 0.5.2-1 -> 0.5.3-0", "ros-indigo-jackal-description: 0.5.2-1 -> 0.5.3-0", "ros-indigo-jackal-desktop: 0.3.1-0 -> 0.3.2-0", "ros-indigo-jackal-msgs: 0.5.2-1 -> 0.5.3-0", "ros-indigo-jackal-navigation: 0.5.2-1 -> 0.5.3-0", "ros-indigo-jackal-tutorials: 0.5.2-1 -> 0.5.3-0", "ros-indigo-jackal-viz: 0.3.1-0 -> 0.3.2-0", "ros-indigo-jaco-graspit-sample: 0.1.5-0 -> 1.0.0-0", "ros-indigo-joint-state-publisher: 1.11.10-0 -> 1.11.11-0", "ros-indigo-jsk-2015-05-baxter-apc: 0.2.4-0 -> 0.8.0-2", "ros-indigo-jsk-201504-miraikan: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-apc2015-common: 0.2.4-0 -> 0.8.0-2", "ros-indigo-jsk-apc2016-common: 0.2.4-0 -> 0.8.0-2", "ros-indigo-jsk-baxter-desktop: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-baxter-startup: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-baxter-web: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-common: 2.0.14-0 -> 2.0.16-1", "ros-indigo-jsk-common-msgs: 2.0.1-0 -> 3.0.0-0", "ros-indigo-jsk-data: 2.0.14-0 -> 2.0.16-1", "ros-indigo-jsk-footstep-msgs: 2.0.1-0 -> 3.0.0-0", "ros-indigo-jsk-gui-msgs: 2.0.1-0 -> 3.0.0-0", "ros-indigo-jsk-hark-msgs: 2.0.1-0 -> 3.0.0-0", "ros-indigo-jsk-nao-startup: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-network-tools: 2.0.14-0 -> 2.0.16-1", "ros-indigo-jsk-pepper-startup: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-planning: 0.1.6-0 -> 0.1.7-0", "ros-indigo-jsk-pr2-calibration: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-pr2eus: 0.3.2-0 -> 0.3.4-0", "ros-indigo-jsk-robot-startup: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-robot-utils: 1.0.5-1 -> 1.0.6-2", "ros-indigo-jsk-roseus: 1.5.1-0 -> 1.5.3-0", "ros-indigo-jsk-tilt-laser: 2.0.14-0 -> 2.0.16-1", "ros-indigo-jsk-tools: 2.0.14-0 -> 2.0.16-1", "ros-indigo-jsk-topic-tools: 2.0.14-0 -> 2.0.16-1", "ros-indigo-kdl-parser: 1.11.10-0 -> 1.11.11-0", "ros-indigo-kdl-parser-py: 1.11.10-0 -> 1.11.11-0", "ros-indigo-kobuki-core: 0.6.3-0 -> 0.6.4-0", "ros-indigo-kobuki-dock-drive: 0.6.3-0 -> 0.6.4-0", "ros-indigo-kobuki-driver: 0.6.3-0 -> 0.6.4-0", "ros-indigo-kobuki-ftdi: 0.6.3-0 -> 0.6.4-0", "ros-indigo-leap-motion: 0.0.9-0 -> 0.0.10-0", "ros-indigo-map-server: 1.12.8-0 -> 1.12.11-0", "ros-indigo-message-multiplexing: 0.2.3-0 -> 0.2.4-0", "ros-indigo-micros-swarm-framework: 0.0.6-2 -> 0.0.8-0", "ros-indigo-mm-core-msgs: 0.2.3-0 -> 0.2.4-0", "ros-indigo-mm-eigen-msgs: 0.2.3-0 -> 0.2.4-0", "ros-indigo-mm-messages: 0.2.3-0 -> 0.2.4-0", "ros-indigo-mm-mux-demux: 0.2.3-0 -> 0.2.4-0", "ros-indigo-mm-radio: 0.2.3-0 -> 0.2.4-0", "ros-indigo-move-base: 1.12.8-0 -> 1.12.11-0", "ros-indigo-move-base-msgs: 1.12.8-0 -> 1.12.11-0", "ros-indigo-move-slow-and-clear: 1.12.8-0 -> 1.12.11-0", "ros-indigo-moveit-core: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-msgs: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-benchmarks: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-benchmarks-gui: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-manipulation: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-move-group: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-perception: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-planning: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-planning-interface: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-robot-interaction: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-visualization: 0.7.1-0 -> 0.7.2-0", "ros-indigo-moveit-ros-warehouse: 0.7.1-0 -> 0.7.2-0", "ros-indigo-multi-map-server: 2.0.14-0 -> 2.0.16-1", "ros-indigo-naoeus: 1.0.5-1 -> 1.0.6-2", "ros-indigo-naoqieus: 1.0.5-1 -> 1.0.6-2", "ros-indigo-nav-core: 1.12.8-0 -> 1.12.11-0", "ros-indigo-navfn: 1.12.8-0 -> 1.12.11-0", "ros-indigo-navigation: 1.12.8-0 -> 1.12.11-0", "ros-indigo-octomap-msgs: 0.3.2-0 -> 0.3.3-0", "ros-indigo-ompl: 1.0.0003094-0 -> 1.0.3094-0", "ros-indigo-opencv-apps: 1.11.12-0 -> 1.11.13-0", "ros-indigo-openni2-camera: 0.2.6-0 -> 0.2.7-0", "ros-indigo-openrave: 0.0.2-0 -> 0.0.3-0", "ros-indigo-openrave-planning: 0.0.2-0 -> 0.0.3-0", "ros-indigo-pddl-msgs: 0.1.6-0 -> 0.1.7-0", "ros-indigo-pddl-planner: 0.1.6-0 -> 0.1.7-0", "ros-indigo-pddl-planner-viewer: 0.1.6-0 -> 0.1.7-0", "ros-indigo-peppereus: 1.0.5-1 -> 1.0.6-2", "ros-indigo-posedetection-msgs: 2.0.1-0 -> 3.0.0-0", "ros-indigo-pr2-base-trajectory-action: 1.0.5-1 -> 1.0.6-2", "ros-indigo-pr2eus: 0.3.2-0 -> 0.3.4-0", "ros-indigo-pr2eus-moveit: 0.3.2-0 -> 0.3.4-0", "ros-indigo-pr2eus-tutorials: 0.3.2-0 -> 0.3.4-0", "ros-indigo-pyros-test: 0.0.3-0 -> 0.0.4-0", "ros-indigo-qglv-extras: 0.1.6-0 -> 0.1.7-0", "ros-indigo-qglv-gallery: 0.1.6-0 -> 0.1.7-0", "ros-indigo-qglv-opencv: 0.1.6-0 -> 0.1.7-0", "ros-indigo-qglv-opengl: 0.1.6-0 -> 0.1.7-0", "ros-indigo-qglv-pcl: 0.1.6-0 -> 0.1.7-0", "ros-indigo-qglv-toolkit: 0.1.6-0 -> 0.1.7-0", "ros-indigo-qwt-dependency: 1.0.0-0 -> 1.0.1-0", "ros-indigo-robot-model: 1.11.10-0 -> 1.11.11-0", "ros-indigo-robot-pose-ekf: 1.12.8-0 -> 1.12.11-0", "ros-indigo-roseus: 1.5.1-0 -> 1.5.3-0", "ros-indigo-roseus-mongo: 1.5.1-0 -> 1.5.3-0", "ros-indigo-roseus-remote: 1.0.5-1 -> 1.0.6-2", "ros-indigo-roseus-smach: 1.5.1-0 -> 1.5.3-0", "ros-indigo-roseus-tutorials: 1.5.1-0 -> 1.5.3-0", "ros-indigo-rotate-recovery: 1.12.8-0 -> 1.12.11-0", "ros-indigo-rplidar-ros: 1.5.2-0 -> 1.5.4-0", "ros-indigo-rtabmap: 0.11.5-0 -> 0.11.7-0", "ros-indigo-rtabmap-ros: 0.11.5-0 -> 0.11.7-1", "ros-indigo-rtmros-hironx: 1.1.14-0 -> 1.1.15-0", "ros-indigo-self-test: 1.8.9-0 -> 1.8.10-0", "ros-indigo-sophus: 0.9.0-2 -> 0.9.1-0", "ros-indigo-speech-recognition-msgs: 2.0.1-0 -> 3.0.0-0", "ros-indigo-stop-base: 0.3.7-0 -> 0.3.8-0", "ros-indigo-task-compiler: 0.1.6-0 -> 0.1.7-0", "ros-indigo-teb-local-planner: 0.4.1-0 -> 0.4.2-0", "ros-indigo-test-diagnostic-aggregator: 1.8.9-0 -> 1.8.10-0", "ros-indigo-ueye-cam: 1.0.13-0 -> 1.0.14-1", "ros-indigo-urdf: 1.11.10-0 -> 1.11.11-0", "ros-indigo-urdf-parser-plugin: 1.11.10-0 -> 1.11.11-0", "ros-indigo-urdf2graspit: 0.1.5-0 -> 1.0.0-0", "ros-indigo-utexas-gdc: 0.3.7-0 -> 0.3.8-0", "ros-indigo-virtual-force-publisher: 2.0.14-0 -> 2.0.16-1", "ros-indigo-voxel-grid: 1.12.8-0 -> 1.12.11-0", "ros-indigo-jsk-2016-01-baxter-apc", "ros-indigo-jsk-apc", "Alex Gonzales", "AlexV", "Anqi Xu", "Armin Hornung", "Austin Hendrix", "Brice Rebsamen", "Chris Liu", "Christoph R\u00f6smann", "Daniel Stonier", "David V. Lu!!", "Devon Ash", "Dirk Thomas", "Elliot Johnson", "Erwan Le Huitouze", "Florian Lier", "G.A. vd. Hoorn (TU Delft Robotics Institute)", "Ioan Sucan", "Jack O\u2019Quin", "Jackie Kay", "Jennifer Buehler", "KazutoMurase", "Kei Okada", "Kentaro Wada", "Mani Monajjemi", "Mathieu Labbe", "Michael Ferguson", "Mickael Gaillard", "Micka\u00ebl Gaillard", "Mike Purvis", "Minglong Li", "Norman Li", "Patrick Beeson", "Paul Mathieu", "Piyush Khandelwal", "Ronald Ensing", "Ryohei Ueda", "Sachin Chitta", "Shohei Fujii", "Slamtec ROS Maintainer", "TORK", "Tim Liu", "William Wu", "Xuefeng Chang", "YoheiKakiuchi", "Younghun Ju", "Yuki Furuta", "Yuto Inagaki", "inagaki", "saito"], "url": "https://discourse.ros.org/t/new-packages-for-indigo/291"}
,{"title": "New Packages for Indigo 2017-10-27", "thread_contents": ["We\u2019re happy to announce another batch of packages available for ROS Indigo. We have 16 new packages and 73 updated packages. The indigo rosdistro is now well past 2500 packages released and available to the community.", "As always thank you to all the maintainers and contributors. And full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-criutils: 0.1.1-1", "ros-indigo-fanuc-m6ib-moveit-config: 0.4.3-0", "ros-indigo-fanuc-m6ib-moveit-plugins: 0.4.3-0", "ros-indigo-fanuc-m6ib-support: 0.4.3-0", "ros-indigo-gcloud-speech: 0.0.4-0", "ros-indigo-gcloud-speech-msgs: 0.0.4-0", "ros-indigo-gcloud-speech-utils: 0.0.4-0", "ros-indigo-multikey-teleop: 1.0.0-0", "ros-indigo-xpp: 1.0.1-1", "ros-indigo-xpp-examples: 1.0.1-1", "ros-indigo-xpp-hyq: 1.0.1-1", "ros-indigo-xpp-msgs: 1.0.1-1", "ros-indigo-xpp-quadrotor: 1.0.1-1", "ros-indigo-xpp-ros-conversions: 1.0.1-1", "ros-indigo-xpp-states: 1.0.1-1", "ros-indigo-xpp-vis: 1.0.1-1", "ros-indigo-abseil-cpp: 0.1.2-0 -> 0.2.0-0", "ros-indigo-default-cfg-fkie: 0.7.6-0 -> 0.7.7-0", "ros-indigo-executive-smach-visualization: 2.0.1-0 -> 2.0.2-0", "ros-indigo-fanuc: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-driver: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-lrmate200ic-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-lrmate200ic-moveit-plugins: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-lrmate200ic-support: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-lrmate200ic5h-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-lrmate200ic5l-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m10ia-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m10ia-moveit-plugins: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m10ia-support: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m16ib-moveit-plugins: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m16ib-support: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m16ib20-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m20ia-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m20ia-moveit-plugins: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m20ia-support: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m20ia10l-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m430ia-moveit-plugins: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m430ia-support: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m430ia2f-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-m430ia2p-moveit-config: 0.4.2-0 -> 0.4.3-0", "ros-indigo-fanuc-resources: 0.4.2-0 -> 0.4.3-0", "ros-indigo-gmapping: 1.3.8-0 -> 1.3.9-0", "ros-indigo-grpc: 0.0.4-0 -> 0.0.5-0", "ros-indigo-jsk-2015-05-baxter-apc: 4.0.0-0 -> 4.1.8-0", "ros-indigo-jsk-2016-01-baxter-apc: 4.0.0-0 -> 4.1.8-0", "ros-indigo-jsk-apc2015-common: 4.0.0-0 -> 4.1.8-0", "ros-indigo-jsk-apc2016-common: 4.0.0-0 -> 4.1.8-0", "ros-indigo-jsk-arc2017-common: 4.0.0-0 -> 4.1.8-0", "ros-indigo-jsk-interactive: 2.1.2-0 -> 2.1.3-0", "ros-indigo-jsk-interactive-marker: 2.1.2-0 -> 2.1.3-0", "ros-indigo-jsk-interactive-test: 2.1.2-0 -> 2.1.3-0", "ros-indigo-jsk-rqt-plugins: 2.1.2-0 -> 2.1.3-0", "ros-indigo-jsk-rviz-plugins: 2.1.2-0 -> 2.1.3-0", "ros-indigo-jsk-visualization: 2.1.2-0 -> 2.1.3-0", "ros-indigo-marti-data-structures: 1.1.0-0 -> 1.2.0-0", "ros-indigo-master-discovery-fkie: 0.7.6-0 -> 0.7.7-0", "ros-indigo-master-sync-fkie: 0.7.6-0 -> 0.7.7-0", "ros-indigo-multimaster-fkie: 0.7.6-0 -> 0.7.7-0", "ros-indigo-multimaster-msgs-fkie: 0.7.6-0 -> 0.7.7-0", "ros-indigo-nerian-sp1: 1.6.2-0 -> 1.6.3-0", "ros-indigo-node-manager-fkie: 0.7.6-0 -> 0.7.7-0", "ros-indigo-novatel-gps-driver: 3.3.0-0 -> 3.4.0-0", "ros-indigo-novatel-gps-msgs: 3.3.0-0 -> 3.4.0-0", "ros-indigo-openni2-camera: 0.2.8-1 -> 0.2.9-0", "ros-indigo-plotjuggler: 1.2.1-0 -> 1.3.0-0", "ros-indigo-ros-type-introspection: 0.9.0-0 -> 1.0.0-0", "ros-indigo-rqt-pose-view: 0.5.7-0 -> 0.5.8-0", "ros-indigo-slam-gmapping: 1.3.8-0 -> 1.3.9-0", "ros-indigo-smach-viewer: 2.0.1-0 -> 2.0.2-0", "ros-indigo-swri-console-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-geometry-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-image-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-math-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-nodelet: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-opencv-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-prefix-tools: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-roscpp: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-rospy: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-route-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-serial-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-string-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-system-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-transform-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-swri-yaml-util: 1.1.0-0 -> 1.2.0-0", "ros-indigo-urg-node: 0.1.10-0 -> 0.1.11-1", "ros-indigo-velodyne-description: 0.0.3-0 -> 0.0.4-1", "ros-indigo-velodyne-gazebo-plugins: 0.0.3-0 -> 0.0.4-1", "ros-indigo-velodyne-simulator: 0.0.3-0 -> 0.0.4-1", "ros-indigo-youbot-driver-ros-interface: 1.1.1-0 -> 1.1.2-0", "Alexander Tiderko", "Alexander W. Winkler", "Davide Faconti", "Dirk Thomas", "Easymov Robotics", "Ed Venator", "Elliot Johnson", "Francisco Suarez-Ruiz", "G.A. vd. Hoorn (TU Delft Robotics Institute)", "Hasegawa Shun", "Jonathan Bohren", "Kei Okada", "Kentaro Wada", "Kevin Hallenbeck", "Konstantin Schauwecker", "Kris Kozak", "Marc Alban", "Michael Ferguson", "P. J. Reed", "Ryohei Ueda", "Shengye Wang", "Tony Baltovski", "Vincent Rabaud", "Walter Nowak", "Yusuke Furuta", "dfaconti", "furuta"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2017-10-27/3030"}
,{"title": "New Packages for Kinetic 2017-10-31", "thread_contents": ["We\u2019re happy to announce 38 new packages for Kinetic with 84 updated packages as well.", "There were 4 regressions as well, though the maintainer is actively working to resolve the issues and I expect them to be back in the next sync.", "Thank you to everyone who has contributed! Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-baldor: 0.1.0-0", "ros-kinetic-brics-actuator: 0.7.0-0", "ros-kinetic-cm-740-module: 0.1.1-0", "ros-kinetic-ez-interactive-marker: 0.0.2-0", "ros-kinetic-fcl-catkin: 0.5.90-6", "ros-kinetic-franka-control: 0.1.2-0", "ros-kinetic-franka-description: 0.1.2-0", "ros-kinetic-franka-example-controllers: 0.1.2-0", "ros-kinetic-franka-gripper: 0.1.2-0", "ros-kinetic-franka-hw: 0.1.2-0", "ros-kinetic-franka-msgs: 0.1.2-0", "ros-kinetic-franka-ros: 0.1.2-0", "ros-kinetic-franka-visualization: 0.1.2-0", "ros-kinetic-gmplot: 1.0.1-0", "ros-kinetic-gmplot-msgs: 1.0.1-0", "ros-kinetic-gmplot-ros: 1.0.1-0", "ros-kinetic-multikey-teleop: 1.0.0-0", "ros-kinetic-op3-action-module: 0.1.1-0", "ros-kinetic-op3-action-module-msgs: 0.1.0-0", "ros-kinetic-op3-head-control-module: 0.1.1-0", "ros-kinetic-op3-kinematics-dynamics: 0.1.1-0", "ros-kinetic-op3-offset-tuner-msgs: 0.1.0-0", "ros-kinetic-op3-walking-module-msgs: 0.1.0-0", "ros-kinetic-open-cr-module: 0.1.1-0", "ros-kinetic-panda-moveit-config: 0.1.2-0", "ros-kinetic-robotis-op3-msgs: 0.1.0-0", "ros-kinetic-sns-ik-lib: 0.2.3-0", "ros-kinetic-twistimu: 1.0.0-0", "ros-kinetic-xpp: 1.0.1-0", "ros-kinetic-xpp-examples: 1.0.1-0", "ros-kinetic-xpp-hyq: 1.0.1-0", "ros-kinetic-xpp-msgs: 1.0.1-0", "ros-kinetic-xpp-quadrotor: 1.0.1-0", "ros-kinetic-xpp-ros-conversions: 1.0.1-0", "ros-kinetic-xpp-states: 1.0.1-0", "ros-kinetic-xpp-vis: 1.0.1-0", "ros-kinetic-youbot-description: 0.8.1-0", "ros-kinetic-youbot-driver: 1.1.0-0", "ros-kinetic-abseil-cpp: 0.1.2-0 -> 0.2.0-0", "ros-kinetic-criutils: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-default-cfg-fkie: 0.7.6-0 -> 0.7.7-0", "ros-kinetic-dynamixel-sdk: 3.4.7-0 -> 3.5.3-0", "ros-kinetic-dynamixel-workbench-msgs: 0.1.5-2 -> 0.1.6-0", "ros-kinetic-executive-smach-visualization: 2.0.1-0 -> 2.0.2-0", "ros-kinetic-explore-lite: 2.0.0-1 -> 2.1.0-0", "ros-kinetic-fzi-icl-can: 1.0.10-0 -> 1.0.11-0", "ros-kinetic-gcloud-speech: 0.0.3-1 -> 0.0.4-0", "ros-kinetic-gcloud-speech-msgs: 0.0.3-1 -> 0.0.4-0", "ros-kinetic-gcloud-speech-utils: 0.0.3-1 -> 0.0.4-0", "ros-kinetic-gmapping: 1.3.8-0 -> 1.3.9-0", "ros-kinetic-image-exposure-msgs: 0.13.2-0 -> 0.13.4-0", "ros-kinetic-libfranka: 0.1.0-1 -> 0.1.0-2", "ros-kinetic-libmavconn: 0.21.2-0 -> 0.21.3-0", "ros-kinetic-magni-bringup: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-magni-demos: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-magni-description: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-magni-nav: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-magni-robot: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-magni-teleop: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-marti-data-structures: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-master-discovery-fkie: 0.7.6-0 -> 0.7.7-0", "ros-kinetic-master-sync-fkie: 0.7.6-0 -> 0.7.7-0", "ros-kinetic-mavlink: 2017.9.9-0 -> 2017.10.10-0", "ros-kinetic-mavros: 0.21.2-0 -> 0.21.3-0", "ros-kinetic-mavros-extras: 0.21.2-0 -> 0.21.3-0", "ros-kinetic-mavros-msgs: 0.21.2-0 -> 0.21.3-0", "ros-kinetic-move-basic: 0.2.1-0 -> 0.2.2-1", "ros-kinetic-msp: 2.0.2-0 -> 2.1.0-0", "ros-kinetic-multimaster-fkie: 0.7.6-0 -> 0.7.7-0", "ros-kinetic-multimaster-msgs-fkie: 0.7.6-0 -> 0.7.7-0", "ros-kinetic-multirobot-map-merge: 2.0.0-1 -> 2.1.0-0", "ros-kinetic-multiwii: 2.0.1-0 -> 2.1.0-0", "ros-kinetic-nerian-sp1: 1.6.2-0 -> 1.6.3-0", "ros-kinetic-nerian-stereo: 2.0.1-0 -> 2.0.3-0", "ros-kinetic-node-manager-fkie: 0.7.6-0 -> 0.7.7-0", "ros-kinetic-openni2-camera: 0.2.8-0 -> 0.2.9-0", "ros-kinetic-plotjuggler: 1.2.1-0 -> 1.3.0-0", "ros-kinetic-pointgrey-camera-description: 0.13.2-0 -> 0.13.4-0", "ros-kinetic-pointgrey-camera-driver: 0.13.2-0 -> 0.13.4-0", "ros-kinetic-python-qt-binding: 0.3.2-0 -> 0.3.3-0", "ros-kinetic-qt-dotgraph: 0.3.4-0 -> 0.3.7-0", "ros-kinetic-qt-gui: 0.3.4-0 -> 0.3.7-0", "ros-kinetic-qt-gui-app: 0.3.4-0 -> 0.3.7-0", "ros-kinetic-qt-gui-core: 0.3.4-0 -> 0.3.7-0", "ros-kinetic-qt-gui-cpp: 0.3.4-0 -> 0.3.7-0", "ros-kinetic-qt-gui-py-common: 0.3.4-0 -> 0.3.7-0", "ros-kinetic-ros-type-introspection: 0.9.0-1 -> 1.0.0-0", "ros-kinetic-rosapi: 0.8.3-0 -> 0.8.4-0", "ros-kinetic-rosbridge-library: 0.8.3-0 -> 0.8.4-0", "ros-kinetic-rosbridge-server: 0.8.3-0 -> 0.8.4-0", "ros-kinetic-rosbridge-suite: 0.8.3-0 -> 0.8.4-0", "ros-kinetic-rqt-bag: 0.4.8-0 -> 0.4.10-0", "ros-kinetic-rqt-bag-plugins: 0.4.8-0 -> 0.4.10-0", "ros-kinetic-rqt-graph: 0.4.8-0 -> 0.4.9-0", "ros-kinetic-rqt-image-view: 0.4.9-0 -> 0.4.11-0", "ros-kinetic-rqt-pose-view: 0.5.7-0 -> 0.5.8-0", "ros-kinetic-rqt-robot-steering: 0.5.7-0 -> 0.5.8-0", "ros-kinetic-schunk-canopen-driver: 1.0.6-0 -> 1.0.7-0", "ros-kinetic-slam-gmapping: 1.3.8-0 -> 1.3.9-0", "ros-kinetic-smach-viewer: 2.0.1-0 -> 2.0.2-0", "ros-kinetic-statistics-msgs: 0.13.2-0 -> 0.13.4-0", "ros-kinetic-swri-console-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-geometry-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-image-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-math-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-nodelet: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-opencv-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-prefix-tools: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-roscpp: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-rospy: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-route-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-serial-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-string-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-system-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-transform-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-swri-yaml-util: 1.1.0-0 -> 1.2.0-0", "ros-kinetic-test-mavros: 0.21.2-0 -> 0.21.3-0", "ros-kinetic-urg-node: 0.1.10-0 -> 0.1.11-0", "ros-kinetic-velodyne-description: 1.0.5-0 -> 1.0.6-0", "ros-kinetic-velodyne-gazebo-plugins: 1.0.5-0 -> 1.0.6-0", "ros-kinetic-velodyne-simulator: 1.0.5-0 -> 1.0.6-0", "ros-kinetic-wfov-camera-msgs: 0.13.2-0 -> 0.13.4-0", "ros-kinetic-dynamixel-workbench", "ros-kinetic-dynamixel-workbench-controllers", "ros-kinetic-dynamixel-workbench-single-manager", "ros-kinetic-dynamixel-workbench-single-manager-gui", "ros-kinetic-dynamixel-workbench-tutorials", "Alexander Tiderko", "Alexander W. Winkler", "Christian Rauch", "D. Hood", "Davide Faconti", "Dirk Thomas", "Easymov Robotics", "Ed Venator", "Elliot Johnson", "Francisco Suarez-Ruiz", "Franka Emika GmbH", "Georg Heppner", "Jim Vaughan", "Jiri Horner", "Jonathan Bohren", "Kevin Hallenbeck", "Konstantin Schauwecker", "Kris Kozak", "Marc Alban", "Michael Ferguson", "Micho Radovnikovich", "Mike Purvis", "Pyo", "Rethink Robotics Inc.", "Rohan Agrawal", "Russell Toris", "Shengye Wang", "Tony Baltovski", "Vincent Rabaud", "Vladimir Ermakov", "Walter Nowak", "Wolfgang Merkt", "dfaconti", "heppner", "neka-nat"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2017-10-31/3059"}
,{"title": "New packages for Melodic 2019-03-29", "thread_contents": ["We\u2019re happy to announce the next update for ROS Melodic. We have 100 new packages as well as 109 updated packages.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 1.2.2-1", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "\n", ": 2.0.0-2", "\n", ": 2.0.0-0", "ros-melodic-behaviortree-cpp-v3: 3.0.6-0", "\n", ": 0.7.0-0", "\n", ": 0.10.14-0", "\n", ": 0.10.14-0", "\n", ": 0.10.14-0", "\n", ": 0.10.14-0", "\n", ": 0.10.14-0", "\n", ": 2.0.0-1", "ros-melodic-cloudwatch-logs-common: 1.0.1-0", "\n", ": 2.0.0-0", "ros-melodic-cloudwatch-metrics-common: 1.0.1-0", "\n", ": 0.6.10-0", "\n", ": 0.6.10-0", "\n", ": 0.6.10-0", "ros-melodic-cob-msgs: 0.6.10-0", "\n", ": 0.6.10-0", "\n", ": 1.0.13-2", "ros-melodic-dynamic-robot-state-publisher: 1.1.1-0", "ros-melodic-eml: 1.8.15-2", "\n", ": 1.8.18-0", "\n", ": 1.8.18-0", "\n", ": 2.0.0-0", "\n", ": 1.1.1-0", "\n", ": 2.0.0-0", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "\n", ": 0.10.14-0", "\n", ": 0.10.14-0", "\n", ": 0.10.14-0", "\n", ": 2.0.0-0", "\n", ": 2.0.0-0", "\n", ": 2.0.0-0", "\n", ": 0.10.14-0", "\n", ": 2.0.0-0", "\n", ": 2.0.0-0", "\n", ": 1.0.13-2", "ros-melodic-linux-networking: 1.0.13-2", "\n", ": 0.1.4-1", "\n", ": 0.1.4-1", "\n", ": 0.1.4-1", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 1.0.3-0", "\n", ": 0.10.14-0", "ros-melodic-movie-publisher: 1.3.0-1", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "\n", ": 1.0.13-2", "ros-melodic-ouster-driver: 0.1.6-0", "\n", ": 1.8.18-0", "\n", ": 0.6.10-0", "\n", ": 1.0.0-0", "ros-melodic-rosmon-core: 2.0.2-0", "ros-melodic-rosmon-msgs: 2.0.2-0", "\n", ": 0.1.12-0", "ros-melodic-rqt-rosmon: 2.0.2-0", "ros-melodic-rslidar: 1.0.2-0", "ros-melodic-rslidar-driver: 1.0.2-0", "ros-melodic-rslidar-msgs: 1.0.2-0", "ros-melodic-rslidar-pointcloud: 1.0.2-0", "\n", ": 0.10.14-0", "ros-melodic-static-transform-mux: 1.1.0-0", "ros-melodic-tf-remapper-cpp: 1.1.1-0", "\n", ": 1.0.1-0", "ros-melodic-usb-cam-controllers: 0.0.4-0", "ros-melodic-usb-cam-hardware: 0.0.4-0", "ros-melodic-usb-cam-hardware-interface: 0.0.4-0", "ros-melodic-uuv-assistants: 0.6.10-0", "ros-melodic-uuv-auv-control-allocator: 0.6.10-0", "ros-melodic-uuv-control-cascaded-pid: 0.6.10-0", "ros-melodic-uuv-control-msgs: 0.6.10-0", "ros-melodic-uuv-control-utils: 0.6.10-0", "ros-melodic-uuv-descriptions: 0.6.10-0", "ros-melodic-uuv-gazebo: 0.6.10-0", "ros-melodic-uuv-gazebo-plugins: 0.6.10-0", "ros-melodic-uuv-gazebo-ros-plugins: 0.6.10-0", "ros-melodic-uuv-gazebo-ros-plugins-msgs: 0.6.10-0", "ros-melodic-uuv-gazebo-worlds: 0.6.10-0", "ros-melodic-uuv-sensor-ros-plugins: 0.6.10-0", "ros-melodic-uuv-sensor-ros-plugins-msgs: 0.6.10-0", "ros-melodic-uuv-simulator: 0.6.10-0", "ros-melodic-uuv-teleop: 0.6.10-0", "ros-melodic-uuv-thruster-manager: 0.6.10-0", "ros-melodic-uuv-trajectory-control: 0.6.10-0", "ros-melodic-uuv-world-plugins: 0.6.10-0", "ros-melodic-uuv-world-ros-plugins: 0.6.10-0", "ros-melodic-uuv-world-ros-plugins-msgs: 0.6.10-0", "\n", ": 0.7.14-0 -> 0.7.17-0", "ros-melodic-catkin-virtualenv: 0.2.2-0 -> 0.4.0-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "ros-melodic-cmake-modules: 0.4.1-0 -> 0.4.2-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 9.26.0-0 -> 9.26.0-1", "ros-melodic-gencpp: 0.6.0-0 -> 0.6.2-0", "\n", ": 0.5.11-0 -> 0.5.12-0", "ros-melodic-genpy: 0.6.7-0 -> 0.6.8-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.2.1-0 -> 1.2.1-1", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 2019.2.2-0 -> 2019.3.3-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 0.4.0-0 -> 0.4.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "ros-melodic-moveit-chomp-optimizer-adapter: 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "ros-melodic-moveit-planners-chomp: 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "ros-melodic-moveit-ros-control-interface: 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "\n", ": 0.10.8-0 -> 1.0.1-0", "ros-melodic-novatel-gps-driver: 3.6.0-0 -> 3.7.0-0", "ros-melodic-novatel-gps-msgs: 3.6.0-0 -> 3.7.0-0", "\n", ": 0.4.1-1 -> 0.4.2-0", "\n", ": 0.4.1-1 -> 0.4.2-0", "\n", ": 0.4.1-1 -> 0.4.2-0", "\n", ": 0.4.1-1 -> 0.4.2-0", "ros-melodic-pilz-robot-programming: 0.4.1-1 -> 0.4.2-0", "\n", ": 0.4.1-1 -> 0.4.2-0", "\n", ": 2.1.5-0 -> 2.1.9-0", "\n", ": 0.5.16-0 -> 0.5.18-0", "\n", ": 0.3.4-0 -> 0.3.5-0", "\n", ": 1.2.9-0 -> 1.2.10-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 0.9.0-0 -> 0.9.1-0", "\n", ": 1.3.0-0 -> 1.3.1-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 1.13.9-0 -> 1.13.10-0", "\n", ": 0.5.2-0 -> 0.5.3-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.9.0-0 -> 0.9.1-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "ros-melodic-rosmon: 1.0.10-0 -> 2.0.2-0", "\n", ": 2.5.2-0 -> 2.5.3-0", "\n", ": 0.9.0-0 -> 0.9.1-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 1.14.4-0 -> 1.14.6-0", "\n", ": 0.4.8-0 -> 0.4.9-0", "\n", ": 1.13.1-0 -> 1.13.3-0", "\n", ": 0.0.14-0 -> 0.0.15-0", "\n", ": 0.5.11-0 -> 0.5.12-0", "ros-melodic-test-mavros: 0.29.0-0 -> 0.29.2-0", "\n", ": 0.9.0-0 -> 0.9.1-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "AWS RoboMaker", "Abraham Monrroy", "Alexander Carballo", "Alexander Gutenkunst", "AutonomouStuff Software Development Team", "Chittaranjan Srinivas Swaminathan", "Daniel Stonier", "Dave Coleman", "David Feil-Seifer", "Davide Faconti", "Devon Ash", "Dirk Thomas", "Fabian Prinzing", "Felix Messmer", "Isaac I. Y. Saito", "Isaac I.Y. Saito", "Jannik Abbenseth", "Jon Binney", "Kei Okada", "Kevin Hallenbeck", "Luiz Ricardo Douat", "Martin G\u00fcnther", "Martin Pecka", "Mathias L\u00fcdtke", "Max Schwarz", "Michael Ferguson", "Michael G\u00f6rner", "Micho Radovnikovich", "MoveIt! Release Team", "Musa Morena Marcusso Manhaes", "P. J. Reed", "Paul Bovbel", "Pilz GmbH and Co. KG", "ROS Orphaned Package Maintainers", "Raghavender Sahdev", "Ronald Ensing", "Russell Toris", "Ryohei Ueda", "Tully Foote", "Vincent Rabaud", "Vladimir Ermakov", "Walter Nowak", "William Woodall", "Yohei Kakiuchi", "Youhei Kakiuchi", "yoshito"], "url": "https://discourse.ros.org/t/new-packages-for-melodic-2019-03-29/8524"}
,{"title": "## CALL FOR CONTRIBUTIONS: ROS Developers Conference 2019 (ROSDevCon2019) ##", "thread_contents": ["ROS Developers Conference 2019: An online conference for ROS developers worldwide", "\nDate: June 15-16, 2019", "\nConference website: ", "Dear colleagues,", "This is a reminder of our call for contributions.", "[ OVERVIEW ]", "The ROSDevCon 2019 is a hands-on online conference for ROS developers. The conference aims to connect ROS developers around the world without geographical restrictions and to advance ROS levels through real-time practice.", "During the conference, all participants will practice in real-time on any type of computer while the speakers are presenting. With a ready-made ROSject, participants will be able to launch the robot simulation, access the project\u2019s code, start developing control algorithms\u2026without any previous setup. All the participants will have:", "[ IMPORTANT DATES ]", "[ CALL FOR TUTORIALS ]", "If you are interested to submit a proposal, the instructions to do so are as follows:", "If your tutorial is accepted, you will have to prepare the following material (with our support):", "[ KEYNOTE SPEAKERS ]", "[ ORGANIZER ]", "\nThe Construct", "\nYou can contact us with questions and doubts here: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Access to the conference LIVE streaming", "Full access to the ROS Development Studio (ROSDS) for programming", "Full access to the online chat tool with all participants and ROS experts", "Access to all packages with pre-defined code", "An e-book of ROS2 Basics.", "Notification of acceptance: May 15, 2019", "Final tutorial submission: June 7, 2019", "Tutorials can target any field of ROS or ROS2 and should focus on practice, in order to provide a real practical conference for participants and to exchange results and ideas. (if you want to know more about how it works, please check the previous ", ")", "The tutorial should last 30 min + 10 min for Q&A.", "The submission of a tutorial must contain the following information:\n", "Title", "A brief introduction about yourself", "Statement of objectives: what is your speech about and what are you going to make attendants practice", "\n", "Submit your proposals to: ", "\n", "A notebook with the tutorial", "Package with the pre-defined code for participants", "Dominik Nowak (CEO at Husarion)"], "url": "https://discourse.ros.org/t/call-for-contributions-ros-developers-conference-2019-rosdevcon2019/8946"}
,{"title": "SingleThreadedExecutor creates a high CPU overhead in ROS 2", "thread_contents": ["Hello,", "We are looking into the performance of ROS 2 on Embedded boards and we find out that ROS 2 consumes high CPU because of the overhead introduced by SingleThreadedExecutor. We did some tests to profile the CPU usage and we observed that if we run 20 publishers and 200 subscribers in one ROS node, 70% of the CPU is consumed by SingleThreadedExecutor and 20% of the CPU is consumed by DDS implementation.", "By running the same example in Fast RTPS directly, it consumes 3.5 times less CPU as compared to ROS 2. The tests that we have performed along with their results can be found in this link: ", ".", "Is anyone else is also looking into measuring the CPU usage of ROS 2? Please share your findings here and let us know if we are doing something wrong.", "Our current analysis suggests that the SingleThreadedExecutor needs to be optimized otherwise normal ROS 2 cannot work properly on \u2018ARM A-class\u2019 embedded boards. We are willing to look more into this problem and can help by performing more tests and providing feedback to improvements. Please let us know if there is any other way to contribute to this.", "Thank you,", "\nIshu Goel", " and myself are also currently looking at this, but I\u2019m going to give him some time to work more on it by replying in his stead ", " He has looked into this based on his tracing work, see ", " It uses LTTng to directly instrument rclcpp and rcl.", "First of all, many thanks for describing your results so openly and so early, particularly for providing the initial benchmark programs. This makes it much easier to compare and combine results.", "In general, what I\u2019ve heard from the OSRF and others is that people are somewhat aware of the inefficiencies in the executor, but nobody had exact numbers so far, and therefore this problem was so far not prioritized. I think this has now changed ", "Regarding your analysis, one thing I would caution is that \u201cperf record\u201d is a sampling approach. This means it can miss executions which are too short. I don\u2019t think this compromises your results, but since you were asking, I wanted to mention it.", "Therefore, in our work, we use LTTng, which integrates both perf event and userspace tracepoints. We have tried both instrumenting every function automatically (which has noticeable overhead), and manual instrumentation of just the most relevant functions. The latter is a bit more work, but also gives more precise results.", "About the single-threaded executor, one thing that I noticed is that it operates in the following way:", "Could the overhead of the executor be due to the fact that after every message it checks again all the entities? this can potentially be very expensive in cases like the one that you tested (200 subscriptions in the same executor)", "Moreover, the ", " function in ", " is marked as ", ".", "\n", " do you already have any idea on how it should be improved?", " Yes, that is a big part of the overhead.", "Since the wait_set only really needs to be update whenever there is a change to the entity list, it is likely that some of this effort could be avoided, or made less expensive. However, without having had a more serious look at the design, I cannot currently say what the best option would be. Maybe ", " or ", " have some ideas.", "I noticed it operates in the follow way (although I could have missed something):", "There is a list of nodes, a node has multiple callbackgroups, a group has multiple executables (eg timer, subscription, client, service, any). So basically a tree: node -> group -> executable", "The tree can be quite large, and is walked often. There seems to be room for improvement by walking/copying/searching the list of executables less (mainly step 5 & 6).", "It\u2019s all weak_ptr by design, but it must keep the memory valid (shared_ptr) as long as it\u2019s in rcl_wait, which is a bit conflicting. It\u2019s also the reason for lots of lookups in the original tree: the only way to see if something disappeared is to rebuild it.", "Is there some information on the design somewhere? Typically a executor works by just submitting executables/callables/callback to a thread(pool), where the executor just maintains a queue of work to do. This is a more complicated design that is implemented in three different layers (rclcpp/rcl/rmw).", "I also saw something on the roadmap about changing the relation between nodes/groups en refactoring the executor. Are there already more concrete ideas about this?", "Could the overhead of the executor be due to the fact that after every message it checks again all the entities?", "i had the same concern with quick code scan and tried the following patch if it affects cpu consumption,", "so far it DOES NOT reduce cpu consumption.", "my environment is", "\ndocker: 18.09.8 ros:dashing", "\nHost: Ubuntu 16.04.6 LTS / Intel\u00ae Core\u2122 i7-4790 CPU @ 3.60GHz", "Hi all,", "what do you think about this?", "already checked if i can reduce cpu consumption, it does some but not a big deal\u2026", "will dig deeper.", "Hi ", ",", "Thanks a lot for your efforts. We are also working on creating a static scheduler to see how much performance gain can be achieved. We will share our result as soon as we complete our work. Please keep sharing the results of your work.", "got it, thanks!", "\nwe will do the same!", "tomoya", "Sorry for the delay! Here are the results of the investigation done by ", " and me.", "We could replicate the earlier results, showing that the Executor consumes a lot of CPU. In that, we could distinguish two cases:", "Compared to earlier work with similar results, we took care to minimize overhead and only count time spent actually on the CPU. Therefore, we consider not just the qualitative result, but also the absolute numbers to be trustworthy.", "This has been non-trivial, because the executor calls very many, very short functions (mainly to do with ", "s). This causes problems both for traditional profiling (which adds lots of overhead) and for sampling-based profiling (which may not notice these). Just to give an idea, initialising the nodes took at least a good 10 seconds when using ", "! Without profiling, it takes ~100 ms.", "To achieve this, we use 1) explicit instrumentation of only the relevant high-level functions and 2) we capture scheduling events. This allows us to sum CPU time only when the thread is actually executing on the CPU.", "Specifically, we only looked at the main ", " functions:", "See our executor instrumentation ", ".", "As mentioned before, based on scheduling information, we only count CPU time when the thread is running on the CPU, not when it is blocked.", "We chose the ", " test case, since it has the highest CPU usage. We traced it for a few seconds. The thread itself has a CPU usage of 55.87% (this is less than the 70% overall CPU usage reported earlier, because it does not include time spent in the dedicated middleware threads).", "In our first analysis, we looked at ", " in some detail, because of the high overhead numbers reported earlier.", "As you can see, from the \u201cfunction\u201d bar, the core ", " function indeed only takes ~32% CPU, the rest is Executor overhead. However, as you can also see from the \u201cthread\u201d bar, the whole method only makes up ~18% of the CPU usage of overall thread. This means that other parts of the Executor are more important.", "Therefore, we took at step back and looked at the two high-level functions in ", ": ", " and ", ".", "The ON CPU time for each function is compared to the whole thread and to the parent function. In this case, 79.21% of the CPU time for the whole thread is spent in ", " vs. 8.22% for ", ". These numbers are similar to ", ".", "Since ", " is likely dominated by running user code, we took a closer look at the functions inside ", ": ", " and ", ".", "Here, ", " represents 67.02% of ", "'s CPU time, and 53.09% of ", " the actual CPU time for the thread!", "Looking at the code, ", " checks its lists of timers/subscriptions/services/clients/waitables and returns once it has found one that is ready to execute. As a side note, having to loop over all the lists would explain the large CPU usage difference between the ", " test case and the ", " test case, since the latter has only one node.", "If we look at the CPU usage for each function individually, we can see that ", " is indeed the most CPU-intensive function.", "The full data is below.", "In conclusion, the executor should be optimized. Figuring out if \u2013 and which \u2013 executable is ready seems to take ", " of CPU time.", "We used LTTng and the ", " & ", " packages. The Jupyter notebook which was used to get the results above can be found ", ". This post can also be found ", ", which also shows how profiling overhead can really mess with the results.", "Thank you Christophe, very nice results. Good to see that we came to the same conclusions, this makes our case even stronger. I\u2019m currently working on posting an issue on the rclcpp github where I will reference this discussion. I think your findings will be very helpful!", "Edit: The issue is now available here: ", "btw, for reference with respect to the changes ", " did: No single call is to blame. The main issue is that, for every single timer-invocation or message, the whole internal representation is traversed. In contrast, if you use the middleware directly, you can attach a listener directly to each communication objects. This avoids traversal ", ".", "However, the listener approach has the problem that we have very little control over when which message is being executed. That\u2019s precisely why ROS 2 adds executors, and can even have different ones.", "IMHO, it would help to look at the interface between rmw and the executor, to pass more information across and thus avoid traversal.", "The main issue is that, for every single timer-invocation or message, the whole internal representation is traversed. In contrast, if you use the middleware directly, you can attach a listener directly to each communication objects. This avoids traversal ", " .", "i need more time to dig deeper but i do agree on this.", "besides, since this is optimization, we might as well define reasonable goal to achieve.", "tomoya", "Just FYI,", "\ncreate \u201cexecute_any_executable_list\u201d and \u201cget_next_ready_executable_list\u201d to reap the executable event as much as possible in single iteration. (that is said if the multiple executables are ready to fire, number of iteration to reap the executables will be much less.)", "so far, we do not see much improvement.", ", and all", "could you take a look at the following PR?", "\n", "\n", "thanks,", "\nTomoya", "Hello everyone,", "Our first POC for a Static version of the Executor can be found here ", " . This version works with the latest stable release of dashing giving the following results:", "\n", "Our StaticExecutor has been added to rclcpp in such a way that the old functionality remains intact. To use our executor please follow the README. The package also contains dockerfiles to quickly inspect the CPU usage on your PC for different executors (the LET executor created by Bosch for micro-ROS is also included in this comparison).", "If you try out the docker example please share your results. It would be even better if you could use our executor with your own source code. This way it can be tested for more use-cases. If you run into bugs, please let us know! We did make some assumptions with respect to the source code, given that this is a POC (assumptions are mentioned in the README).", "We think this POC is a good first step to highlight possible performance gains. The final goal is to get an optimized Executor with proper scheduling mechanics in the core ros2 stack. We are currently working on a fork from ros2 master to create a proper PR for this version. We will keep you updated on the PR progress here.", "Rather than a fork, you could probably provide your new executor as a separate library.", "Great!!! we will look into that.", "Rather than a fork, you could probably provide your new executor as a separate library.", "+1 on this.", "thanks", "Hello everyone,", "For now we created this PR for rclcpp ", " . We are considering making the code a separate library. Having the static executor as an optional package would prevent the bloating of ROS2. However, the package would also require a maintainer. Since we are a relatively small team that plans on doing more work (creating more packages in the future), we have to consider if and what packages we want to maintain. The static executor is a relatively small package, so we could consider picking it up (this is an internal discussion we are yet to have).", "Please leave your comments and thoughts on the code under the PR. Even if the PR does not get approved, we hope to at least draw attention to the CPU overhead of the current implementation.", "Small update: We updated the dashing version of our static executor to be semi-dynamic. The node guard_conditions are used as event trigger to rebuild the wait-set and executable list. This means that when a subscriber, timer etc. is added during spin(), the executor will notice (by checking the guard_condition) and rebuild, making the use of the static executor less restrictive.", "This updated version can (still) be found here ", ".", "We will create a master (eloquent) version of this, but we first want to fix some Jenkins linter errors and do some clean up on our PR.", "If you try out our code please share your results here. Please report any bugs you find. Possible optimizations are best posted on the PR when we apply the changes there.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a node starts spinning", "receives a message (awakening from spin)", "the executor calls ", " to retrieve the entity that has to handle the message", "the message is handled by the subscription", "the executor checks again all the registered entities", "if no entities have work to do, the executor goes to sleep again", "It populates a list of all executables by walking the mentioned tree into a memorystrategy. (promote weak to shared_ptr)", "The memory strategy is then converted into a wait-set (to call the rcl)", "The wait set is waited upon. Implementation is all the way down into the RMW layer. It differs per RMW implementation.", "After the wait, only ready executables are left in the waitset (not null).", "The memory strategy is updated with this list (remove all that are not ready, to allow weak_ptr to cease)", "For a ready executable, the group is retrieved from the original tree by searching the entire tree.", "Execute", "Go back to step 6, if more executables where ready, otherwise go to step 1", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "when there are few or no messages (e.g., for a timer-driven node), then the ", " method causes the majority of the overhead, with 70% of its time spent in ", " and only 30% (excluding waiting) spent in the RMW layer and below. We determined this using the \u201cnopub\u201d, pure timer benchmark.", "when there are many messages, the majority of the CPU usage \u2013 up to about ", " in our tests \u2013 is caused by the ", " function. This is pure ", ". We determined this using scg\u2019s \u201cros\u201d benchmark, which sends 10000 small messages per second.", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/singlethreadedexecutor-creates-a-high-cpu-overhead-in-ros-2/10077"}
,{"title": "IPC in ros2", "thread_contents": ["Does ros2 use inter-process communication when we use it in  linux OS ? What about in case of bare metal micro-controllers where there is no OS?", "In the future, please set the category to \u201cNext Generation ROS\u201d when discussing ROS 2.", "Does ros2 use inter-process communication when we use it in  linux OS ?", "First, I\u2019ll assume you mean \u201cdoes ROS 2 use any operating system IPC mechanisms between nodes on the same machine, rather than sending data over UDP?\u201d, because the term \u201cinter-process communication\u201d only means communication between two processes, but does not imply what the mechanism is (i.e. UDP or TCP can be IPC).", "It depends on the middleware implementation you are using. Currently the default of Fast-RTPS does not use anything other than UDP single cast or multicast. It could, RTPS allows for this in the protocol. I believe some of the proprietary implementations provide a shared-memory based IPC mechanism when on the same machine. Fast-RTPS will hopefully support this in the future.", "What about in case of bare metal micro-controllers where there is no OS?", "There have been some experiments which use RTPS on a OS-less microcontroller to communicate with ROS 2 on a desktop and some other experiments to get our ROS 2 API compiling for microcontrollers, but there is no out of the box support for OS-less ROS 2 at this time. We did enough work to convince ourselves that is possible, but haven\u2019t followed through with sustained support because we didn\u2019t have time, though we would love to do that in the future.", "In that case though, with no OS, you probably would not be using processes so the idea of \u201cInter-", " communication\u201d doesn\u2019t really make sense. You would be using something more like our \u201c", "-process communication\u201d, which can avoid sending the message data over the network (UDP in most cases).", "RTI\u2019s DDS does use shared memory internally when possible, I think.", "Hi sagniknitr,", "Fast RTPS will support shared memory in future releases, is in our roadmap. Regarding microcontrollers we are developing a lightweight version of Fast RTPS based on an OMG future standard for constrained resources devices. Of course it is not a full DDS/RTPS implementation, but a reduced API and different protocol.", "You can see the first prototypes in the ", " use case, and we are working in an release now. This will let you to use ROS2 in micro-controlllers.", "If you want more details, please contact me.", "Jamie,", "Thanks for the update about how Fast RTPS will be evolving.  A few questions, is the prototype with Dronecode you\u2019re referring to the work you did on the ", "?  Additionally, can you point to any public links regarding this new OMG standard for constrained resources? I know that the ", " is publicly available, is a draft of the document you\u2019re referring to available?", "Thanks", "\n- Eddy", "The DDS XRCE (\u201ceXtremely Resource Constrained Environments\u201d because all OMG specifications need a cool acronym) specification isn\u2019t even close to finished yet. The ", " but you have to be an OMG member to get the working documents, unless someone involved is willing to provide them. When the specification is adopted, that becomes publicly available.", "As seems to happen a lot with the DDS specs these days, the RTI/eProsima/Twin Oaks camp and the PrismTech camp have produced their own versions of the XRCE specification and are struggling to reconcile them into a single specification that the OMG can adopt. I won\u2019t wade into the debate over which is better, but I\u2019ll let you draw your own conclusions from the fact that one of the two camps only has one company it it. ", " I hope it\u2019s not going to be the RPC for DDS spec all over again; the arguments over that could be heard in other meeting rooms. But it seems likely that it will be a year and a half or more before the final specification is released.", "Thanks for the informative reply ", ", I had no idea the work was under way.", "I attended the OMG meeting last week, so I took the opportunity to hear what PrismTech and RTI/TwinOaks/eProsima had to say about their competing submissions. The following are the notes I took during their presentations. Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "Prismtech presentation:", "RTI/eProsima/TwinOaks presentation:", "My summary:", "Thanks for the summary ", "!  I\u2019m very interested in how the work progresses", "Thanks Geoff, great stuff!", "Hello ", ",", "First of all I\u2019d like to point out that I am one of the author of PrismTech\u2019s/ADLINK XRCE proposal, thus you may think I am giving you my perspective, in that case I urge you to read original documents to see how what I am providing here are facts. That said, let me make a few rectifications to some of the points above.", "You say:", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure ", " You can find the result of our analysts here ", " but you are welcome to derive them by reading both specs.", "This is also partially correct. We had asked to present what would have been our final proposal in Bruxelles to give a chance to the task-force to digest and one more opportunity to the competing team to join. This has nothing to do with the Vote-to-Vote and the fact that the task-force did not decide to allow the Vote-to-Vote procedure. The OMG has very complex rules\u2026 I know that can seam strange, and they still surprise me after having spent more than 10 years dealing with it.", "The RFP (which I wrote) asks for a protocol not for an API.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "\nAdditionally, you may argue that this may add some complexity in the message parsing \u2013 but again \u2013 I think that checking a flag and deciding wether some field is going to be present or not does not belong to the realm of hard. It is also worth  pointing out that some flags are just informative and don\u2019t require any kind of branching in the decoding. Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes ", " And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple ", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples. Our proposal currently has 4 compared to that of RTI/TwinOaks/eProsima which has 16 bytes. We have had our implementation run on a Makeblock robot using BLE with an MTU of 20 bytes.  You can check the comparison on this deck ", " and should wonder why the competing proposal did not provide a proper analysis of the wire overhead. Additionally when leveraging batching we have a wire overhead of (3+n)/n where n is the number of samples being batched.", "Some other thing worth point out is that the protocol allows for data to be pushed, pulled or periodically pushed or pulled. The write protocol also allow to pace the streaming of data that results from a remote query. This is extremely important when dealing with resource constrained nodes that need to consume data little by little.", "I did not hear RTI saying:", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those. The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP. The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "I\u2019d like to understand why you say this:", "What do you think is our submission cutting out? We are wire efficient yes, extremely wire efficient but at the same tine we support:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions. I\u2019ll also be more than happy to answer any question you may have about our protocol.", "A+", "Dear ", ",", "You are being to nice to PrismTech as everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises. I think it is best for people to look with their eyes at what the two submission can do and make their own decisions.", "On my side I like debates, I like inquisitive minds and I like hard questions. Thus I\u2019ll be more than happy to answer any question on the XRCE protocol proposed by the PrismTech/ADLINK team explain why it is better than RTI proposal\u2026 And actually it is better than RTPS.", "Thus, please let\u2019s start the open debate to dissect the reasons why our proposal is the one which should be voted and by far the better one.", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being ", "A+", "Thanks ", " for jumping into the community in such an energetic manner. I think we all appreciate having one of the authors of the proposals providing feedback. That said:", "Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "which answers:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I think I\u2019ll stop here\u2026 for the time being ", " .", "Hi Angelo (", "),", "Coordinate different companies to get a common view on a complex matter is always hard, and in this case, there are multiple design options leading to different tradeoffs.", "Our submission (RTI, Twin Oaks & eProsima) already aligns the views of three different DDS vendors, and sure we will try to incorporate ideas of your submission.", "Our submission tries to accomplish the following:", "We coded a PoC of our submission, and during the presentation, you asked about numbers. At that point, with no optimizations at all, and in debug mode we answered 43Kb. I asked my team to optimize a little bit the code, and here are the numbers we have now for the client:", "Total Memory Use: 8 Kb", "But we could squeeze that even a little more. We are releasing this as Open Source (Apache 2) so anyone can review the results.", "But again, we are not aiming to be as small as possible. We are covering all the requisites of the DDS-XRCE RFP, and testing our solution in what we consider typical microcontrollers today.", "Regarding the process at the OMG meeting, we (RTI, Twin Oaks & eProsima) didn\u2019t want to confront both specifications and choose one of them, but have the time to incorporate ideas from your submission, and that is why now we have an extended deadline.", "Let me join in the fray. I\u2019m the other author of the PrismTech submission and the one who built our tiny prototype. I wasn\u2019t present at the OMG meetings, and I won\u2019t waste any words on what may or may not have happened there.", "Firstly, I don\u2019t think a contest of bytes of RAM adds real value to the discussion, although of course it is an honourable contest in itself ", " I am surprised that you, ", ", had never even had a proper look at the memory use of your implementation given the purpose of the exercise in the first place, but if it is 8kB now then it is much better already \u2014 if still 7kB overweight ", " In any case, memory use is determined more by the implementation than by the protocol messages.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already. Yet even that is not of such great interest to me in this discussion.", "What really matters in my opinion is a difference in philosophy. The two proposals suggest very different views of what one would ideally want to accomplish.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that. In a sense, it is just a hand-crafted alternative to CORBA with a lower overhead. (Simply using CORBA actually \u201cjust works\u201d if the DDS implementation is faithful to the IDL interface mappings, even if it is ugly.) To me, this route is a pragmatic way of going about satisfying the RFP, but at the same time, an uninteresting one. (Sorry ", " and others \u2026)", "We chose to design a compact protocol that can support what amounts to performing DDS operations remotely, but doesn\u2019t limit itself to it. Instead, it also supports a DDS-like peer-to-peer network with vastly lower overhead, and, in many ways a level of flexibility in specifying what data is of interest (through URIs and selections) that DDS doesn\u2019t natively support.", "All of that would be of little value if it doesn\u2019t perform well or doesn\u2019t scale well. Just like the code size and memory use are mostly determined by the implementation, so is maximum sustainable performance more determined by implementation than by the details of the protocol headers. Size-wise, my prototype can run as a client on an Arduino Uno (8-bit CPU, 2kB RAM). A small test application using the same implementation configured as a peer easily sends ~700k 8-byte msgs/s from one RPi3 to 3 others (CPU is << 100%, network load ~75% of Fast Ethernet, so I really should investigate why it isn\u2019t faster), and goes another order of magnitude faster when run over local loopback on my MBP. That\u2019s better than your typically DDSI implementation. Is this relevant? That depends on whether you have high rate, tiny messages \u2026", "Now my test application doesn\u2019t implement all of DDS \u2014 not even close \u2014 and this is another significant reason why it can do this with only a few kB of code and RAM. At the same time, this is, I believe, where it gets interesting for ROS2.", "As ROS2 has its own middleware abstraction layer that uses only a fraction of the DDS feature set, putting ROS2 directly on our protocol would get you a smaller and a faster system. Smaller and faster usually allows doing more interesting things, even if I can\u2019t say what exactly those interesting things will be.", "Disclaimer: I can\u2019t do run ROS2 over it today, there\u2019s more work to be done on my prototype before it supports all that is required. And I wish none of you would have to take my word for the data I mentioned, but that is not something that is in my power to solve today.", "It looks like I started something of a minor storm moments before starting a holiday\u2026", "I\u2019m thankful that the DDS vendors, PrismTech, RTI, Twin Oaks and eProsima, are all engaged enough in the ROS community to be present on the Discourse board. It is encouraging to future adopters of ROS 2.", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure", "I wasn\u2019t implying religious believe. It\u2019s an simply expression to describe someone making an assertion. ", " I fully agree that PrismTech\u2019s submission has much smaller messages than the RTI/Twin Oaks/eProsima submission based on the two presentations alone.", "The RFP (which I wrote) asks for a protocol not for an API.", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "It is quite common to use flags. TCP is full of them. My concern is that the protocol is, in my opinion, undoubtedly complex and, based solely on the presentations, more complex than the other submission. Complexity and size are often a balance and in this situation we appear to have one submission at each end of the balance.", "Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes", "With the sorts of overhead you are achieving, I\u2019m not surprised performance is amazing. I\u2019d still like to see numbers, though. ", "And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple", "As was stated elsewhere in this thread, eProsima\u2019s implementation wasn\u2019t optimised. Since you said you are trying to bring yours to market already and eProsima claimed theirs was a tech demo, I\u2019m not surprised yours is more optimised and thus smaller. Of course, the numbers are definitely in your favour for RAM usage. But I\u2019m curious how much program memory each implementation requires, too. This is often the limiting factor on embedded microprocessors rather than the RAM usage.", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP.", "In that case I am very interested in seeing what these AB members wrote.", "The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "What do you think is our submission cutting out?", "\u201cCutting down\u201d, not \u201ccutting out\u201d. I meant that you are trying to reduce the size of the messages on the wire as much as possible, at the expense of possibly needing more complex parsing code. I didn\u2019t mean to say that you are cutting out features. It was clear from the presentation that PrismTech supports more features and has more flexibility than the other submission. But there are trade-offs involved.", "Dynamic Discovery (RTI does not, please read their spec!)", "\nPeer to Peer Communication (RTI does not)", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Non IP Transports (RTI does not)", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "Generalised Queries (RTI only supports DDS-like queries)", "Well, it is ", "-XRCE, is it not?", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions.", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises", "While this is true, we are operating at a standardisation organisation, not a rubber stamp provider. There are interested parties beyond just the implementers. We would prefer not to just hold a vote on which submission to go forward with. We would prefer the submitters to actually work together and produce a single submission that combines the best of both without any technological compromises (yes, I\u2019m aware how hard that is).", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being", "Or, you could provide the answers to those questions, along with the equivalent answers for your submission, so we can see and compare the data to back up your claims.", "Our submission tries to accomplish the following:", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "\nNeither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use.", "This is the strongest impression I got from the presentation, as I said in my own notes. The data model is similar to DDS, which makes adoption by existing DDS users easier, and the ability to implement the protocol in a relatively simple piece of code (which makes it easier to verify and certify) was considered as important as saving bytes on the wire. I\u2019m not sure where the correct balance is between these two requirements, but the PrismTech implementation really gave the impression of being at one extreme.", "testing our solution in what we consider typical microcontrollers today", "This is something that I think is really relevant but that PrismTech have not addressed at all, and RTI/Twin Oaks/eProsima have not addressed enough. What are the typical microcontrollers in use today? What are the target environments for this protocol to be used in?", "The RFP explicitly says this:", "From a high level perspective, the key requirements that a DDS-XRCE implementation has to satisfy are (1) extremely low footprint \u2013 addressing targets with less than 100 Kbytes of RAM, (2) extremely efficient wire protocol \u2013 inducing a protocol overhead of just a few tens of byte over the user data, and (3) support devices that undergo aggressive sleep cycles.", "Both submissions fit within both the RAM usage (with much room to spare) and the protocol overhead.", "More specifically, the actual mandatory requirement is:", "6.5.3. DDS-XRCE protocol overhead, when sending or receiving user data, shall not exceed 24 bytes per packet.", "Let M be the size of the DDS-XRCE message sent out, and U the size of the user data, then M-U <= 24 bytes.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already.", "Again, this should have been in the RFP if it is so important. That would have saved a lot of trouble. All we got was an evaluation criteria:", "Protocol overhead shall be considered when evaluating submissions. Submissions with smaller overhead are preferable.", "This is a miserably small set of criteria for a complex design space. Even you, ", ", say that protocol overhead is not as important as the design philosophy.", "Which I agree is fundamentally different between the two proposals, and that this is where the root cause lies in the failure to reconcile them.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that.", "The RFP not-so-subtly pushes submitters in this direction. You cannot fault them for taking it at face value.", "I will read both submissions and when I do, I will report back with more technically-informed comments.", "Hello ", ",", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "And you see this as a positive aspect? Our model is simpler and more user friendly. For instance, how many people can digest DDS partitions? That said, we have a well defined mapping between XRCE resources and DDS topics.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "Are you an OMG member? If so I\u2019ll forward you the reviews. Both submissions went to the AB and the reviews were posted both on ", " and ", ". If you have access to those mailing list you\u2019ll be able to see them. I also recommend you take a look at those.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "It was impossible as the other vendors did not want to agree on such a low bound. The 24 bytes was the least we could agree on. This is why there is an evaluation on wire-efficiency. This matters were discussed at length, but again I don\u2019t think you attended those meetings, thus you are missing part of the history and the context. In any case, all of those documents are on the OMG archives, thus if of interest you to reconstruct it. Just search for presentation I did on XRCE for almost a year. starting from 2015!", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "Yes, that is correct as it is since the very beginning that we are trying to do a joint submission. They\u2019ve refused with futile arguments \u2013 if you ask me. We have put lots of effort to trying to join but that has not been corresponded. A pity that you were not in the Bruxelles meeting, otherwise you would have had a taste of it.", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Again, you did not attend the end-less arguments we had during the RFP drafting. RTI does not want peer-to-peer in XRCE because they fear it could become as substitute for DDSI-RTPS. Again, this is not something I am inferring, but something that was openly debated during the RFP drafting. We don\u2019t have any issue with that as we think that having a more efficient protocol than DDSI-RTPS for some use cases would be extremely useful.", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "For me that disqualifies completely the submission as in LowPAN environments nobody can afford to use TCP/IP\u2026", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "I am glad that this helped clarifying the situation, please don\u2019t hesitate to ask any other question. Concerning the code availability we are working on that. I\u2019ll keep you posted.", "A+", "Dear All,", "I wanted to let you know that we have just released under Apache 2 a peer-to-peer implementation of our zenoh protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating zenoh as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", "N.B. For those of you that are familiar with XRCE, zenoh is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as zenoh.", "A+", "Kydos,", "Thank you for publishing the Zeno-He library so we can all begin to interact with it.  It is especially useful for the ROS 2 user community to be aware of the effort since it implements the ATLab XRCE proposal.", "I know I would be extremely interested in someone benchmarking Zeno and the proposed epromisa XRCE implementation, and perhaps can find time to do that.", "This is really appreciated ", ", thanks for making this available.", "At present it\u2019s unlicensed code though (", ").", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They believe that their proposal is more efficient in the wire protocol (trying to save every single byte possible), and supports brokered as well as peer-to-peer communication.", "They have invited the competing submitters to join the PrismTech submission.", "PrismTech believes that they have presented their final submission in Brussels (June) and so were expecting to vote for acceptance in New Orleans (September), but because there are still two competing submissions, and no final submission document was received (only a presentation), this is not possible.", "PrismTech\u2019s reasons for going forward with their own submission:\n", "XRCE tries to target the most wire/power/memory efficient protocol, targeting not just IP infrastructures but a whole range of infrastructures.", "Their submission provides reliability and fragmentation.", "Their current prototype runs on an 8-bit microprocessor with 1 KB of RAM and has a wire overhead of 4 bytes for data samples.", "They got a review from the AB which was favourable (only editorial comments), and they believe that the AB review shows that they satisfy all the mandatory requirements.", "\n", "XRCE applications can be brokered into a DDS data space, or they can discover one another and communicate P2P.", "Their submission is only about the protocol, it does not say anything about the API.", "A DDS-XRCE Agent running on permanently connected hardware provides the access to the rest of the (non-XRCE) DDS domain.", "XRCE provides a data space abstraction in which applictions can read and write data autonomously and asynchronously.", "Data read and written by XRCE applications is associated with one or more resources identified by a URI.\n", "An XRCE resource is a closed description for a set of named values.", "Resource URIs allow for wildcards, which means that more than one resource can be targeted in one definition, which is useful for capturing collections of sub-namespaces.", "A resource with a cardinality of one is called a Trivial Resource.", "Resources also have properties, which are used to attach QoS settings to them, such as \u201cthis resource is transient\u201d or \u201cthis resource is reliable\u201d.", "An XRCE selection is the conjunction of a resource and a predicate over the resource content and properties, used to filter a selection. For example, finding all lights with a luminosity greater than zero. (It seems to provide functionality similar to a very basic subset of SQL.)", "\n", "There is a mapping from XRCE resources to DDS topics.", "Resource serialisation uses the same format as DDS.", "All XRCE messages are a single-byte header and a body.\n", "There are flags in the header for reliability and synchronicity.", "Messages can be \u201cdecorated\u201d by prefixing them with a one-byte pre-header, which determines some of the properties for the following header byte, such as fragmentation.", "The protocol is little-endian.", "Variable-length encoding is used to save space.", "\n", "For addressing, the source address for each message is assumed to be a unique address of the sender.", "The protocol is modular, with a core profile (required for any communication), an optional query profile, and an optional discovery profile.\n", "e.g. If using a communications system like Bluetooth, which already has discovery, the XRCE discovery profile can be removed, saving some space.", "Discovery happens by scouting (sending a scout message to ask for types of nodes to reply, i.e. broker nodes or durability services or peers or clients).", "Other nodes reply to a scout with a HELLO message.", "\n", "After discovery, two nodes need to open a session, which enables publishing and subscribing between those two nodes.\n", "Authentication data is included in the session open message.", "Locator information, telling the other node how it can be reached, is included in the open message.", "The receiving node replies with an accept message or a reject message.", "There is an FSM describing the states a session goes through.", "\n", "Resources and selections are uniquely identified by numerical IDs to save space on the wire.", "A declare message is sent to declare what resources, selections, etc. a node has or will publish or subscribe to.", "Subscriptions can be push, pull, periodic pull or periodic push.", "All data messages and declarations are transported over a conduit (for the session), which is a pair of a reliable and a best-effort channel. Multiple conduits may be used in parallel to avoid head-of-line blocking and allow multicasting.", "One decorator is used to select the conduit for the next message.\n", "It is possible to make this decorator one byte instead of two if the number of conduits is less than five.", "\n", "There is a sync message available to set the next sequence number to expect (e.g. for when not starting at zero).", "The ACKNACK message can acknowledge multiple messages at once, usually up to the given sequence number. It has the option to optionally request retransmission of one or more messages after that point.", "There is a one-shot write function in the protocol, which can do a write of a resource without needing to do any prior registration or discovery.", "This proposal appears to be a very complex protocol with a lot of options in message header structures. An implementation would have a lot of choice points during the decoding of a stream of messages. The \u201cdecorator\u201d idea especially may save bytes on the wire (and in message construction buffers) in some cases, but it complicates the protocol implementation.", "PrismTech are already trying to bring their submission to market as a product.", "This proposal also uses an XRCE agent to provide access from DDS-XRCE nodes and the DDS domain.", "An important use case for them is that devices will tend to mostly sleep, and only wake up occassionally to do some processing, and send and receive data. This means that two devices may never be awake at the same time. This is why they have the agent, which is permanently present and provides a way for XRCE nodes to communicate with each other.", "They say that their proposal focuses not just on the XRCE protocol, but also on the interaction between the XRCE protocol and the DDS domain.", "It is possible for an XRCE agent to behave as an XRCE client to another agent, allowing for a hierarchical structure of agents and clients.", "Their proposal is based on the web-enabled DDS specification, with a DDS-XRCE object model in the agent that has a one-to-one mapping to the DDS data model.", "eProsima have put up a demo on Youtube: ", "\n", "Their demo uses 43 KB of RAM (much bigger than the PrismTech proposal, which can fit in 1 KB).", "The XRCE Agent object model is very similar to the DDS object model, making the mapping very simple.", "XRCE objects are modeled as resources that are addressable by their name and have CRUD operations.\n", "Each resource has a name to address it within the agent and a context; a representation describing the resource; and an ID.", "Resources can be represented as a name, an XML description, or a binary representation.", "\n", "Authentication capability is built into the proposal.", "Types are typically pre-defined profiles in the XRCE Agent.\n", "It is possible to transmit types as binary or XML representations.", "\n", "Message structure:\n", "A message header is either 4 or 8 bytes, depending on if a clientKey is used.", "There is a sub-message header, which is another 8 bytes.", "It is possible to send data in a sequence, meaning the header only needs to be sent once for a bunch of samples.", "\n", "The transport can be message-oriented or packet-oriented.", "CDR and DDS-XML representations are reused.", "Message overhead is typically 12 bytes.\n", "They consider that further reduction in overhead would increase complexity and reduce robustness. e.g. They do not need different code paths for multiple sessions, re-connections, handling variable-length encoding.", "They say it compares favourably to TCP/IP overhead (40 bytes).", "They think they could save 6 bytes from their message header, but the reduction is only 6% in the context of total overhead (when considering the use case of using TCP/IP as the underlying transport protocol) and so is not worth it in the face of increased complexity.", "\n", "They also received 3 AB reviews, which they claim were supportive and did not find any non-editorial problems.", "The submissions are very different. PrismTech is aiming for cutting down the bytes used by the protocol to the absolute minimum at the expense of all else. RTI/eProsima/TwinOaks are favouring some overhead in order to achieve a simpler and more robust protocol and implementation.", "PrismTech\u2019s protocol is overly complex with too many choices during the decoding of a message.", "The extra overhead of the RTI/eProsima/TwinOaks submission is not likely to be a problem in the majority of embedded micro-processors used these days (although I\u2019m not sure how many 8-byte, 1KB-of-RAM micros there are in use in new products). However, their consideration that TCP/IP is going to be the most common transport may not be accurate.", "Dynamic Discovery (RTI does not, please read their spec!)", "Peer to Peer Communication (RTI does not)", "Client to Broker Communication", "Non IP Transports (RTI does not)", "Generalised Queries (RTI only supports DDS-like queries)", "Push/Pull/Periodic-Pull and Periodic-Pull Readers", "Unicast and Multicast communication \u2013 for both client to broker and peer-to-peer", "Durability", "The view ", " provides is a) unbiased, b) the view of a roboticist (that\u2019s what this community is about ", " ! )  and c) based on the information someone got from hearing \u201cyour presentation\u201d. Note the following:", "I believe we all appreciate technical argumentation and slides. I love slides. But what I love even more is code and things that I can reproduce. How can I verify your arguments through experimental results? Is there any open code that supports your arguments? More than bashing around, i think it will do a lot of good to facilitate implementations that others can reproduce in common platforms. Even early stages will do. You might find that you could even get some support (and feedback!) before launching it officially and furthermore, that\u2019ll definitely convince a lot of people on why your approach is better.", "Last but not least, I really hope that the passion you\u2019ve shown answering this thread is shown by supporting and making OpenSplice better.", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "Neither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use."], "url": "https://discourse.ros.org/t/ipc-in-ros2/2619"}
,{"title": "Waffle - HDMI no signal", "thread_contents": ["Hello,", "I need help installing Ubuntu on TurtleBot3 Waffle.", "\nI flashed Joule BIOS with version 193 (since 1J2 didn\u2019t worked and seemed a lot of people had trouble with it).", "\nLooked good :", "However after that I can\u2019t get any screen with micro HDMI-HDMI cable, still no signal.", "\nI plugged power supply one openCR, keyboard, mouse and USB with Ubuntu on USB hub (which is connected to the Joule). All leds are green, plus the white/blue one.", "Any suggestion ?", "\nThanks", "Hello r00t,", "\nThe below link is the method that worked for us to recover the joule.", "\nIt seems like the sequence of connecting and disconnecting cables do matter.", "\n", "Thank you for your reply.", "Tried everything, still can\u2019t get it working.", "\nFirst connecting type C USB, first connecting power supply, waiting 10min first boot before connecting HDMI, really everything. With both bios version 193 and 1J2. Even re-done exactly like in the video tutorial. Did it with two different Joule board (we got three of them), same thing.", "Flashing is always fine, but can\u2019t get that output working.", "Anyone with an other idea ? (Hardware is fine, test moves worked)", "According to McCool, sometimes specific monitors would not connect to the Joule.", "\n", "\nWere you able to connect them with your current monitor before updating firmware?", "Yep, it was working before updating\u2026", "I\u2019m trying to reproduce the problem with my Joule and if I figure out what the problem is, I\u2019ll share the solution for it.", "Well, I took my chance installing directly the \u201cAlternative Ubuntu for Joule\u201d on a second Joule not flashed (since this way I did have acces to BIOS to boot on USB), and it works properly\u2026 (ROS install and turtlebot teleop working with no problem)", "\nSeems weird 'cause there must be a reason the guide tells us to flash before doing it, but i am not complaining, it works.", "I\u2019ll keep trying make the first one work, and come back to you if I find a solution.", "Thanks ", "Hello r00t,", "As far as I understand, BIOS 193 is required in order to install Ubuntu with a USB flash drive. Since it was one of requirements from Ubuntu Developer(", "), I didn\u2019t doubt about updating the BIOS firmware.", "\nAt least you figured out your solution with the second Joule which is definitely a good news ", "\nPlease let me know if your second Joule has different BIOS version other than 193(", ") so I can also try with the same version.", "\nCheers!", "I was able to use joule-firmware-2017-06-26-1J2-public, specifically the debug bin file.  My initial flash with the release file would not boot, nor display anything on screen.  With the debug bin the board does run, although bootup times are very slow.", "That said, the Joule board has proven to be unstable.  I\u2019m unsure on the root cause, be it BIOS, Ubuntu, or hardware, but since Intel has discontinued the board I don\u2019t expect much in the way of software improvements.  At present, I\u2019ve swapped over to a Raspberry Pi, and have an Odroid XU4 which will eventually end up in the Turtlebot.  I\u2019ll go back to the Joule at some point, but it\u2019s not a long term solution for development.", "Hello,", "\nI got the same problem. The board was displaying the bios using hdmi but when I flashed bios version 193 I could not get hdmi output anymore. The flashing process didn\u2019t give any errors. I also tried newer bios versions with same negative results.", "\nI can connect to the board using serial connection. It seems to recognize the display when I execute xrandr -q. If I reboot the board it displays the whole boot sequence. These are the last commands before stopping:", "[   12.378641] Bluetooth: RFCOMM ver 1.11", "\n[   12.820056] HDMI HDA Codec ehdaudio0D1: HDMI: failed to get afg sub nodes", "\n[   12.827761] HDMI HDA Codec ehdaudio0D1: Failed in parse and map nid with err: -22", "\n[   12.836166] HDMI HDA Codec: probe of ehdaudio0D1 failed with error -22", "It seems to be some sort of hdmi error. Any idea how to correct the issue?", "\nThanks", "Just a data point.", "\nMy Joule running 193 works with my Dell and Samsung monitors, with a generic type A to type A HDMI cable, and generic type A to type D adapter.", "\nIt won\u2019t work with my Elecrow monitor, its included type A to C cable, and the above generic type A to type D adapter", "\nIt will work with my Elecrow monitor, its included type A to C cable, and a Rocketfish type A to D adapter.", "Ed", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Downloading BIOS", "Clearing NvStorage\u2026"], "url": "https://discourse.ros.org/t/waffle-hdmi-no-signal/2687"}
,{"title": "Turtlebot3 - Joule stalled", "thread_contents": ["Hi", "Turtlebot 3 - Waffle \u2026 Joule - after installing Joule following the official documentation (Ubuntu 16.04 LTS) : everything works fine\u2026problem: Joule stops working/running after approx 10min up - independently if operated on 12V/3A external power supply or with the battery provided with the Turtlebo3 kit. Logs don\u2019t show any weird symptoms\u2026", "Any ideas? pls help\u2026", "Tnx in advance", "\nml", "First thing to check, what is the temperature doing? Is the module overheating?", "Also, does anyone know what the replacement for the Joule is, now that they are EOL?", "\nThanks ", "Tnx Ilia for the hint\u2026checking continously temp via /sys/class/thermal zones 0-7 - max. 45C \u2026 assuming that isn\u00b4t the root cause. Keep on digging\u2026Best ml", "\u2026quick update: if you push the Power button (SW2) the \u201cstalled\u201d system recovers - even the Unix sessions\u2026", "\n\u2026did I miss something here - kind of \u201csleep mode\u201d /   hibernation mode - some BIOS settings?", "Interesting.  Our bot was doing the same thing, with symptoms just like a software crash, so that\u2019s what I assumed.  Maybe it is going into a sleep mode of some sort.", "I experience the same stalling that you describe.  Have you managed to figure out a workaround?", "Hey ashoed,", "unfortunately not - tried all 3 BIOS versions, including the recommended ", " \u2026 same negative results.", "Any luck on your side?", "Best ml", "Hi Michael,", "Have you checked to see if there are settings within Ubuntu 16.04 for screensaver or power savings?", "You should also check if you can update to the latest Linux kernel.", "If you can type  ", " in the terminal on your Intel Joule and post the output, I may be able to load up my Intel Joule with your versions and check to see if I encounter the same issue.", "Please keep us updated on any other efforts you may try.", "Hi MyNameIsCosmo,", "tnx for your support\u2026some progress but not 100% done yet:", "all updates based on \u201capt-get update\u201d installed", "HDMI not working - need to work with cli", "uname -a:", "\nLinux turtle 4.4.0-1000-joule #0+joule21-Ubuntu SMP PREEMPT Thu Mar 16 14:46:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux", "following your hint \u201cpower saving\u201d -> focussing on Ubuntu not Joule I did the following:", "sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target", "-> unix processes will not get stopped/suspended anymore -> seems ok !", "\n-> issue: every 30min the network manager gets a request to \u201csleep\u201d (syslog) -> wifi connection goes down", "\n-> can\u2019t restart it automatically (service network-manager restart -  nmcli network on - ifup -a)", "No clue what causes the request for network manager to sleep\u2026given that it is reproducible it seems a \u201cfeature\u201d. Any idea how to tackle it? Seems I didn\u2019t disable all \u201cpower saving\u201d options (or what soever causes the nm shutdown\u2026) - can\u2019t find anything on the net that fixed it\u2026", "Every hint/comment welcome\u2026", "SOLVED: guess the \u201cGUI\u201d environment triggered the request to sleep after 30min - given that I run the Joule headless: disabling \u201cGnome\u201d and \u201clightdm\u201d did the trick - system up and running for hours (didn\u2019t reverse the masking of sleep, suspend, hibernate, hybrid\u2026)", "\nThank to all of you for the hints - Best Michael", "Had the same issue, disabling gnome and lightdm did the trick! Thanks for sharing!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/turtlebot3-joule-stalled/2739"}
,{"title": "TB3 Launch details", "thread_contents": ["Hi all,", "Does anyone have any information about when the TB3 will be officially available, how we will be able to purchase it, costs, etc?", "Curious minds want to know!", "//Mike", "Thanks, Mike", "The official launch date of TurtleBot3 is not yet known, but it is seen from the end of April to the beginning of May. Also, we will post a notice for the beta tester this month. The number of people is about 30 to 40.", "In addition, the public information is published every week through this ", " forum, and the wiki site is also in preparation, and it will be open to the public next week so that all questions can be resolved. ", "Good to know, thanks for the info, Pyo!", "We release a wiki page which contains specifications and various information so that we can solve your question about the TurtleBot3. Please visit TurtleBot 3 Wiki page and let us know what you want more ", "Will we be able to get spare parts, e.g. if we like to have more wheels?", "Hi ", "You can find the parts list of TurtleBot 3 from the link below.", "The bolts and nuts include extra quantities, but do not include spare parts for other parts.", "\nBut we will make it possible for users to buy the parts they need.", "Perfect, thanks for the info.", "Hi ", ",", "there is some documentation about various \u201cturtlebot friends\u201d. To build them additional parts to extend Burger/Waffle are necessary. This there any info about the availability of these extra parts? Or will a friend be an complete different package?", "Thx for the info", "Hi ", "We released the TurtleBot3 Friends hardware design file at the link below.", "So, you can download the STL files directly from each Onshape address as shown in the following figures.", "Hi ", ",", "\nyes, thank you for providing all these designs on Onshape - I think it really helps boosting creativity.", "\nHowever, for some \u201cfriends\u201d very special parts are needed you can\u2019t probably 3D print, e.g. meccanum wheels and strong brackets for the servos.", "So my question is: is Robotis thinking about providing these \u201cspecial\u201d not-really-3-d-printable parts. Like: first you buy the Waffle, then there is a box with additional parts to build the Converyor?", "Thx", "Hi ", "There are so many kinds of mecanum wheels and omni wheels. Making it for all users\u2019 needs is hard.", "\nSo, we provide 3D design files that can be edited for each user\u2019s desired purpose on the web.", "\nAlso, we have plans to sell the \u201cspecial\u201d parts for the user, if many users want it.  ", "Thanks!", "Will there be an option to purchase the burger or waffle without any sensors or the raspberry pi/joule? I don\u2019t think I am the only person who already has the sensors and computer, and just wants a platform to put them on.", "Hi ", "Thank you for your comments.", "We are also considering selling parts of TurtleBot3 separately.", "\nWe can\u2019t sell all parts individually. But if many users think so, I think that it is not impossible. ", "Thanks!", "Selling parts individually is definitely a great idea. I think you\u2019d get a lot of people buying extra parts to modify their robots. But what I most want to see is the burger and waffle kits just without the sensors or computer boards.", "I would like to see the injection-molded waffle plates for sale if they are stronger than 3d printed ones.", "Besides, home 3d-printers are both slow and they can stink. I live in a motorhome which means that any place I am at a place where I have time to run my printer I have to balance it very carefully.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Sign in. (If you do not have an ID, you have to create one.)", "Click the \u201ctoggle tab manager\u201d (A menu will appear on the left side of the browser.)", "Click the \u201cParts folder\u201d", "Right-click on the icon of the file you want to download.", "Click the \u201cExport\u2026\u201d", "Finally, you can download the output file type you want."], "url": "https://discourse.ros.org/t/tb3-launch-details/1420"}
,{"title": "Looks like intel has an eol document for joule module", "thread_contents": ["Sometimes the end of a product\u2019s production run is surrounded by publicity, a mix of a party atmosphere celebrating its impact either good or bad, and perhaps a tinge of regret at its passing\u2026", "With the Turtlebot 3 still listed as pre-order, what does this mean for people who ordered a Waffle, which comes with the joule?", "ROBOTIS and Open Robotics will ensure that pre-order users do not inconvenience the delivery and use of Intel Joule. Currently, we have a substantial number of Intel Joule Boards for TurtleBot3 Waffle, so we have no problem with customer support.", "I\u2019d like to ask about what you will use after Joule, but I\u2019m guessing that this news is as much of a surprise to Robotis as it is to the rest of us and you haven\u2019t had a chance to evaluate alternatives yet.", "I would like to take this chance, however, to ask for an option to purchase either Turtlebot 3 model without sensors and a Rasberry Pi or Joule board. Some of us have those, or suitable replacements, already.", "Thank you for your comments. ", "As you said, we are also looking for Joule\u2019s next board. We are discussing this with Open Robotics and partners, and we plan to announce related plans in the near future.", "Also, shipment of preordered TurtleBot3 (Burger, Waffle) will start next week. In addition, the TurtleBot3 package, excluding the SBC and sensors you mentioned, is scheduled to be sold from the second half of 2017.", "Thanks. ", "That\u2019s excellent news. ", " I\u2019m looking forward to it!", "Intel will support it through the ", " until 2020.", "Intel will archive its online resources and maintain availability to the Intel Joule community until June 15, 2020. ", "TurtleBot team will also support it, and we are going to find another SBC which replacement to Joule.", "Another interesting SBC is the UDOO x86. It has an Intel quad-core 64-bit processor, plus an onboard Arduino-like processor. I believe that it will take Arduino shields and is programmed with a similar IDE. I haven\u2019t had time to set mine up yet, but it\u2019s supposed to run Ubuntu out of the box.", "What about Odroid XU4 with lib.realsense?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looks-like-intel-has-an-eol-document-for-joule-module/2017"}
,{"title": "Turtlebot 3: Successfully upload Alternative Ubuntu Desktop 16.04 to Joule?", "thread_contents": ["When I set Turtlebot 3, I met some problems:", "\nIt is very hard to upload the Ubuntu image to Intel Joule board. I am not able to get my PC to connect to Joule board through USB-C successfully. And After that I am not able to upload Ubuntu image to Joule board successfully. It is very hard to find any useful info online regarding these issues.", "After some study, I finally got it work. Here I would like to share with you some critical info.", "1, Connect your PC with Intel Joule board via USB-C connector.", "\nBefore doing anything, we need to make our PC recognize and connect to Joule board. We should see \u201cIntel DnX device\u201d under \u201cUniversal Serial Devices\u201d in Device manager. If not, you have a problem.", "\nIf you get it work by following Intel\u2019s description. It is great. If not, it is normal. Here is what you need to do:", "\nAt first, follow the link ", ".  User \u201cvraoresearch\u201d provides a solution. (Thank you, vraoresearch.)", "\nIf you still cannot get it work, here is my suggestion:", "\nStep#1, disconnect 12VDC power supply.", "\nStep#2, Connect PC to USB-B of the Intel Joule board, if you want to see how Joule runs through UART terminal.", "\nStep#3, In Device manager in your PC, select \u201cShow Hidden Devices\u201d. And choose \u201cUninstall device\u201d on all failed devices.", "\nStep#4, Connect PC to USB-C of the Intel Joule board, at the same time press \u201cDnX\u201d button on the Joule board. Wait for PC to detect the DnX device and then release the button. If it does not work, try it multiple times.", "2, Upgrade the BIOS image to Intel Joule board.", "\nThere is a bug in the file DNX/Flash.bat in the downloaded and unzipped Joule BIOS image. \u201cclearRpmbFlag\u201d needs to changed to \u201cClearRpmbFlag\u201d. And its value should be defined as \u201cTRUE\u201d", "When running Flash.bat, you should see the following output:", "\nClearing NvStorage\u2026", "After this, you can disconnect PC from USB-C of Joule Board, and connect the 12VDC power supply, the board should be able to boot up properly, Then you go ahead to follow the description to use a USB flash drive to upload the Ubuntu image to Joule board.", "By the way, the bug I mentioned is found in the version 1J2 of Joule BIOS image. Since Intel discontinued Joule. I believe this is the last release of Joule BIOS image.", "Hi rknlhrqy,", "Thank you so much for sharing your method for solving Joule\u2019s USB-C issue when updating BIOS!", "\nI\u2019ll create a suggested solution link to your post from TurtleBot3 wiki FAQ page if you don\u2019t mind.", "Sincerely,", "\nWill", "Hi Will,", "\nNot at all. It is my great pleasure.", "\nThank you.", "\n\u2013Kening", "I\u2019m sooooooooooooooooooooooooooo happy \u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c  you are SOOOOOOO genius!!! I have suffered for a long long long!!! time. Because I couldn\u2019t enter the BIOS even after installing the BIOS successfully\u2026 At first I thought that the connection method of my HDMI wire would be a problem, but when I tried to connect the serial port with putty, I could not enter the BIOS too!!! I searched all the questions related to the bios of the joule board to find the answer. \u201cjoule Bios not boot\u201d, \u201cjoule board putty serial bios\u201d, \u201cjoule hdmi not output\u201d etc\u2026 For a very very!!! long~~~~~~ time \u2026 and I was able to solve the problem through \u201c\u201d\"\"\u201cyour answers\u201d\"\"!!!. My BIOS was the latest version, and Flash.bat was False. After I changed this to True, I was finally!!! free from all the problems. I can sleep well now. It\u2019s all for you. I really love you!!!", "Hi kingbob,", "\nI am thrilled by your email.    Thank you very much for the nice words.", "\nI am so glad that it did help people.", "-Kening", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/turtlebot-3-successfully-upload-alternative-ubuntu-desktop-16-04-to-joule/2224"}
,{"title": "ROSIN at ERF2018", "thread_contents": ["Have questions about the ROSIN FTP funding scheme? Not sure whether to proceed with your FTP application or just want to get in touch with the ROSIN consortium?", "Come talk to us at ERF2018 in Tampere, Finland on both the 14th and the 15th of March.", "Learn more about ROS-Industrial and our FTP cascade funding and education activities.", ": March 14, ", " \u201cEU Projects offering services: don\u2019t miss it!\u201d", "\n", ": KUKA Room", "\n", ": Carlos Hernandez will pitch on ROSIN FTP grant program.", "You can find us by looking for the ROSIN poster in the poster session afterwards.", ": March 15, ", " \u201cTeaching Robotics with ROS\u201d - ROSIN workshop", "\n", ": Sandvik room", "\n", ": Alexander Ferrein and Carlos Hernandez", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosin-at-erf2018/4160"}
,{"title": "SROS2 - Securing certs and keys", "thread_contents": ["Hey guys,", "\nI was wondering if there was a technical document for SROS2. I\u2019m very much interested in what it offers and want to get to understand its inner workings. I also understand that the project currently is able to create signed certificates. How are the certificates and private keys getting saved? I was planning on using secure keystores to do that. Would something like that be valuable?", "Hello ", ",", "The current way the certificates are being constructed is quite basic: via subprocess commands to openssl\u2019s CLI. Once you exicute the ", " command via the SROS CLI, an ", " is triggered to bootstrap the openssl configuration files, then calls upon openssl command to generate the private keys, and singe the necessary public certificates.", "Later, should you decide to invoke access control, the ", " command will again use the keystore to sing governance and permission files consumed by the vendor specific middle ware to enable access control enforcement as defined in the ", " spec.", "I\u2019m working on refactoring this to use something like a modern python library such as ", " to more pragmatically control the key generation and signing of certificate authorities, like I did for SROS1. Last year I did spend a brief amount of time exploring more rigorous keystore solutions, such as open source projects like Vault:", "Centrally secure, store, and tightly control access to secrets across distributed infrastructure and applications.", "However, in the end I figured most end users would not enjoy installing and learning a host of other dependencies and frameworks, and would more likely impeded the ease of use and adoption of SROS. So I went with the simple method of optionally ciphering the private keys to disk. Most PKI frameworks support loading ciphered keys via secrets that can be supplied at runtime, as used in SROS1 ", ". End users can easily take additional steps from there to guard private keys via additional custom solutions if need be.", "What secure keystores methods where you looking at? It would still be nice if we could design SROS2 to interoperate easily with other 3rd party keystores methods.", ",", "In all honesty, I\u2019m still in the process of figuring it out. My idea was creating a package that would help users make use of a TPM to secure the certificates and keys. This work is meant to go towards my Master\u2019s research.", "\nOnce again, this is just an idea and hasn\u2019t fully taken shape yet.", " Good point, can you share more details regarding your enabling to TPM ? thank you . ", "I have another question for the SROS2, you know, the SROS supports AppArmor, why is it dropped from the SROS2 ? is it not necessary to protect from the system level now from your perspective ? thank you !", " Thanks for your interest in SROS and SROS2.", "SROS supports AppArmor, why is it dropped from the SROS2 ?", "The development of SROS2 has been focused on the communication security rather than the system security so far. I wouldn\u2019t say that system-security is \u201cdropped\u201d but rather \u201cnot implemented\u201d yet.", "\nAs you may know ROS2 targets a wider variety of platforms (non-Linux or non-posix) and applications than ROS1. Currently SROS2 has been geared towards providing secure communication at the lowest level of the ros client library (rcl) to allow users to leverage it regardless of the programming language, platform or implementation of the communication protocol. Currently we test on Ubuntu, MacOS and Windows, in C++ and Python, and with eProsima\u2019s FastRTPS and RTI\u2019s Connext.", "We haven\u2019t yet looked into the best way to interface system security tools to the current SROS2 implementation but we definitely want to do it in the future. Given that it is pretty orthogonal to the encryption of the communication it can be addressed separately. While Apparmor is awesome, it is Linux only and thus will target only one of our supported platforms. Ideally SROS2 will provide a way to define the permissions of your application in a platforms-agnostic manner and be extensible to implement generators that will provide \u201cconfiguration files\u201d or \u201cprofiles\u201d according to the tool or platform you want to use. And the tool on Linux can very well be AppArmor.", "As ", " well put it, AppArmor support has not necessarily been dropped from SROS2, but rather not yet implemented. My present focus has been integrating ROS2 with DDS Security, and have not yet had time to expand upon the apparmor profile to support ROS2. Another reason is that I haven\u2019t yet surveyed ROS2\u2019s installation structure yet, as the majority of the apparmor profile library for ROS1 helps users to author profiles of there own by abstracting the ROS1 file directory layout.", "However, as mentioned before, apparmor support is somewhat orthogonal to SROS2 interrogation with DDS Security, and so could be developed in parallel. I would certainly invite contributions and pull requests from the rest of the community to add support for ROS2, or help review what ", ".", "Contribute to apparmor_profiles development by creating an account on GitHub.", "Hmm\u2026 What literature have you found on the subject of Trusted Platform Module and robotics? I do like the idea of having the private credentials used for SROS2 isolated from the host OS, but that may require coding 3\u2019rd party plugin, as the default DDS crypto plugin must load the private key from a path on disk, or serialized PEM string.", "Funny you should mention TPM though, as last week or so when I was at the RoboCub 2017 Symposium, I met a PhD student, Sarah Haas from Graz University of Technology, who was presenting work on a similar topic:", "Secure Authentication for Industrial Mobile Robots using Biometric Data", "You may want to investigate the lab Sarah is from. I recall some of Sara\u2019s peers working on using TPM for the Diffie-Hellman key exchange and establish a symmetric session key without revealing the private key to the network host.", " ", " thanks for your infomation, let me have an basic gap/work evaluation to implement the AppArmor to SROS2 and I may try to enable that if no conflicts after evaluation ", "It seems that this solution focus more on external authentication to an IMR(Industrial Mobile Robot), you know, now the ROS nodes can employ the key/certificate produced using SROS2 to authenticate/encrypt/access control for the nodes/topics etc, but do you think it\u2019s necessary to provide the security to the key/certificate itself ? now they\u2019re placed on the disk without any protection, that means it\u2019s easy to be accessed or tampered.", "\nBTW, you know more details regarding the details of SROS2 implementation, so could u please help double confirm/clarify the following questions:", "Many thanks", "\nBRs", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Keystore: does it support key exchange between publisher / subscriber, etc? Can keystore be changed?", "Encryption of data AES-GCM-GMAC: used to have license issue on GCM, has its license model been changed?", "Tools to create keystore, certificates: are they out-of-band tool that user needs to use this tool to create keystore manually first then pass it to DDS? How does the whole solution work?", "Is there any access control for SROS2: tools, encryption, etc?"], "url": "https://discourse.ros.org/t/sros2-securing-certs-and-keys/2400"}
,{"title": "New packages and patch release for ROS 2 Dashing Diademata 2019-09-10", "thread_contents": ["The new release of ROS 2 Dashing Diademata is ", ".", "This release brings a number of new packages as well as some updates to ROS 2 core packages.", "The default opensplice configuration was changed from ", " to ", " . This means that Opensplice will now use multicast for discovery but unicast for data delivery, which brings it inline with how the other RTPS/DDS implementations operate. This should improve data delivery on wireless networks when using Opensplice, at the cost of increasing network bandwidth on wired networks when there are many subscribers to the same topic. This is a backport of ", ". For full details see the ", ".", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "This release brings a number of new packages as well as some updates to ROS 2 core packages.", "Are meta packages ever listed in \u201cUpdated Packages\u201d release notes, e.g. ", ", ", ", ", "?", "Updated packages on this list are packages whose version number has been bumped. Packages that have only been rebuilt due to upstream changes are not listed. So the meta packages will be listed if they are modified and re-released but they don\u2019t show up in this report when they are rebuilt.", " thanks.", "To see what was a new feature, fixed bug or an improvement for this patch release, we should refer to  ", "?", "Does every Dashing and Eloquent patch release have a corresponding github project?", "Does every Dashing and Eloquent patch release have a corresponding github project?", "Yes that\u2019s the current workflow. Starting with Dashing Patch Release 1 and continuing on with subsequent Dashing, Crystal, and eventually Eloquent patch releases each patch release has a GitHub project linked in its issue description. The tracking issue is now somewhat vestigial but is still kept as an entry point, status tracker, and discussion hub for in-progress patch releases. Items in the \u201cReleased\u201d column of the project were released in that patch release or an earlier sync. Items in other columns at the end of a release cycle are either bumped to later releases or removed from the release schedule.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-dashing-ackermann-msgs: 2.0.2-1", "ros-dashing-aws-common: 2.1.0-1", "ros-dashing-aws-ros2-common: 1.0.0-1", "ros-dashing-cartographer-ros: 1.0.9000-1", "ros-dashing-cartographer-ros-msgs: 1.0.9000-1", "ros-dashing-cloudwatch-logger: 3.0.0-2", "ros-dashing-cloudwatch-logs-common: 1.1.0-1", "ros-dashing-cloudwatch-metrics-collector: 3.0.0-2", "ros-dashing-cloudwatch-metrics-common: 1.1.0-1", "ros-dashing-control-msgs: 2.2.0-1", "ros-dashing-cyclonedds: 0.1.0-1", "ros-dashing-cyclonedds-cmake-module: 0.4.0-1", "ros-dashing-dataflow-lite: 1.1.0-1", "ros-dashing-diagnostic-updater: 2.0.0-1", "ros-dashing-file-management: 1.1.0-1", "ros-dashing-h264-encoder-core: 2.0.1-1", "ros-dashing-h264-video-encoder: 2.0.0-1", "ros-dashing-health-metric-collector: 3.0.0-1", "ros-dashing-kinesis-manager: 2.0.1-1", "ros-dashing-kinesis-video-msgs: 3.0.0-2", "ros-dashing-kinesis-video-streamer: 3.0.0-2", "ros-dashing-lex-common: 1.0.0-1", "ros-dashing-lex-common-msgs: 3.0.0-1", "ros-dashing-lex-node: 3.0.0-1", "ros-dashing-people-msgs: 1.3.0-1", "ros-dashing-realtime-tools: 2.0.0-1", "ros-dashing-rmw-cyclonedds-cpp: 0.4.0-1", "ros-dashing-ros-monitoring-msgs: 2.0.0-1", "ros-dashing-self-test: 2.0.0-1", "ros-dashing-tts: 2.0.0-1", "ros-dashing-tts-interfaces: 2.0.0-1", "ros-dashing-turtlebot3: 2.0.1-1", "ros-dashing-turtlebot3-bringup: 2.0.1-1", "ros-dashing-turtlebot3-cartographer: 2.0.1-1", "ros-dashing-turtlebot3-description: 2.0.1-1", "ros-dashing-turtlebot3-gazebo: 2.0.1-1", "ros-dashing-turtlebot3-navigation2: 2.0.1-1", "ros-dashing-turtlebot3-node: 2.0.1-1", "ros-dashing-turtlebot3-simulations: 2.0.1-1", "ros-dashing-turtlebot3-teleop: 2.0.1-1", "ros-dashing-ament-clang-format: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-clang-tidy: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-clang-format: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-clang-tidy: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-copyright: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-cppcheck: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-cpplint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-flake8: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-lint-cmake: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-pclint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-pep257: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-pep8: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-pyflakes: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-uncrustify: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cmake-xmllint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-copyright: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cppcheck: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-cpplint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-flake8: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-lint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-lint-auto: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-lint-cmake: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-lint-common: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-pclint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-pep257: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-pep8: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-pyflakes: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-uncrustify: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-xmllint: 0.7.8-1 -> 0.7.9-1", "ros-dashing-dynamixel-sdk: 3.7.10-1 -> 3.7.20-1", "ros-dashing-ecl-command-line: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-concepts: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-config: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-console: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-containers: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-converters: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-converters-lite: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-core: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-core-apps: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-devices: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-eigen: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-errors: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-exceptions: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-filesystem: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-formatters: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-geometry: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-io: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-ipc: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-linear-algebra: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-lite: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-manipulators: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-math: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-mobile-robot: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-mpl: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-sigslots: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-sigslots-lite: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-statistics: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-streams: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-threads: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-time: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-time-lite: 1.0.1-1 -> 1.0.3-2", "ros-dashing-ecl-type-traits: 1.0.1-1 -> 1.0.4-1", "ros-dashing-ecl-utilities: 1.0.1-1 -> 1.0.4-1", "ros-dashing-launch: 0.8.5-3 -> 0.8.6-1", "ros-dashing-launch-ros: 0.8.5-1 -> 0.8.6-1", "ros-dashing-launch-testing: 0.8.5-3 -> 0.8.6-1", "ros-dashing-launch-testing-ament-cmake: 0.8.5-3 -> 0.8.6-1", "ros-dashing-launch-testing-ros: 0.8.5-1 -> 0.8.6-1", "ros-dashing-opensplice-cmake-module: 0.7.2-1 -> 0.7.3-1", "ros-dashing-py-trees-js: 0.4.0-1 -> 0.5.0-1", "ros-dashing-py-trees-ros-viewer: 0.1.2-1 -> 0.1.3-1", "ros-dashing-python-cmake-module: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rclcpp: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rclcpp-action: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rclcpp-components: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rclcpp-lifecycle: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rclpy: 0.7.5-1 -> 0.7.7-1", "ros-dashing-robot-state-publisher: 2.2.3-1 -> 2.2.4-1", "ros-dashing-ros2bag: 0.1.4-1 -> 0.1.5-1", "ros-dashing-ros2launch: 0.8.5-1 -> 0.8.6-1", "ros-dashing-rosbag2: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosbag2-converter-default-plugins: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosbag2-storage: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosbag2-storage-default-plugins: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosbag2-test-common: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosbag2-tests: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosbag2-transport: 0.1.4-1 -> 0.1.5-1", "ros-dashing-rosidl-adapter: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-cmake: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-generator-c: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-generator-cpp: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-generator-py: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rosidl-parser: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-runtime-py: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rosidl-typesupport-interface: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-typesupport-introspection-c: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-typesupport-introspection-cpp: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rosidl-typesupport-opensplice-c: 0.7.2-1 -> 0.7.3-1", "ros-dashing-rosidl-typesupport-opensplice-cpp: 0.7.2-1 -> 0.7.3-1", "ros-dashing-rviz-assimp-vendor: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz-common: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz-default-plugins: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz-ogre-vendor: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz-rendering: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz-rendering-tests: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz-visual-testing-framework: 6.1.3-1 -> 6.1.4-1", "ros-dashing-rviz2: 6.1.3-1 -> 6.1.4-1", "ros-dashing-shared-queues-vendor: 0.1.4-1 -> 0.1.5-1", "ros-dashing-sqlite3-vendor: 0.1.4-1 -> 0.1.5-1", "ros-dashing-tf2: 0.11.4-1 -> 0.11.5-1", "ros-dashing-tf2-eigen: 0.11.4-1 -> 0.11.5-1", "ros-dashing-tf2-geometry-msgs: 0.11.4-1 -> 0.11.5-1", "ros-dashing-tf2-kdl: 0.11.4-1 -> 0.11.5-1", "ros-dashing-tf2-msgs: 0.11.4-1 -> 0.11.5-1", "ros-dashing-tf2-ros: 0.11.4-1 -> 0.11.5-1", "ros-dashing-tf2-sensor-msgs: 0.11.4-1 -> 0.11.5-1", "AWS B9 Team", "AWS RoboMaker", "Austin Hendrix", "Bence Magyar", "Brice Rebsamen", "Chris Lalancette", "Dan Lazewatsky", "Daniel Stonier", "Dirk Thomas", "Erik Boasson", "Jack O\u2019Quin", "Jacob Perron", "John Shepherd", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Pete Baughman", "Pyo", "Scott K Logan", "Steven! Ragnarok", "Tully Foote", "Vincent Rabaud", "William Woodall"], "url": "https://discourse.ros.org/t/new-packages-and-patch-release-for-ros-2-dashing-diademata-2019-09-10/10627"}
,{"title": "New packages and patch release for ROS 2 Dashing Diademata", "thread_contents": ["The new release of ROS 2 Dashing Diademata is ", "!", "This release brings many new packages and an updated version of the OpenSplice RMW provider.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-dashing-ament-clang-tidy: 0.7.6-1", "ros-dashing-ament-cmake-clang-tidy: 0.7.6-1", "ros-dashing-ament-lint: 0.7.6-1", "ros-dashing-apriltag-msgs: 2.0.0-2", "ros-dashing-apriltag-ros: 2.1.0-1", "ros-dashing-costmap-queue: 0.2.3-1", "ros-dashing-dwb-controller: 0.2.3-1", "ros-dashing-dwb-core: 0.2.3-1", "ros-dashing-dwb-critics: 0.2.3-1", "ros-dashing-dwb-msgs: 0.2.3-1", "ros-dashing-dwb-plugins: 0.2.3-1", "ros-dashing-ml-classifiers: 1.0.1-1", "ros-dashing-nav-2d-msgs: 0.2.3-1", "ros-dashing-nav-2d-utils: 0.2.3-1", "ros-dashing-nav2-amcl: 0.2.3-1", "ros-dashing-nav2-behavior-tree: 0.2.3-1", "ros-dashing-nav2-bringup: 0.2.3-1", "ros-dashing-nav2-bt-navigator: 0.2.3-1", "ros-dashing-nav2-common: 0.2.3-1", "ros-dashing-nav2-costmap-2d: 0.2.3-1", "ros-dashing-nav2-dwb-controller: 0.2.3-1", "ros-dashing-nav2-dynamic-params: 0.2.3-1", "ros-dashing-nav2-lifecycle-manager: 0.2.3-1", "ros-dashing-nav2-map-server: 0.2.3-1", "ros-dashing-nav2-msgs: 0.2.3-1", "ros-dashing-nav2-navfn-planner: 0.2.3-1", "ros-dashing-nav2-recoveries: 0.2.3-1", "ros-dashing-nav2-rviz-plugins: 0.2.3-1", "ros-dashing-nav2-util: 0.2.3-1", "ros-dashing-nav2-voxel-grid: 0.2.3-1", "ros-dashing-nav2-world-model: 0.2.3-1", "ros-dashing-navigation2: 0.2.3-1", "ros-dashing-py-trees-ros: 1.1.1-1", "ros-dashing-py-trees-ros-interfaces: 1.1.2-1", "ros-dashing-py-trees-ros-tutorials: 1.0.2-1", "ros-dashing-ros2trace: 0.2.0-1", "ros-dashing-tracetools: 0.2.0-1", "ros-dashing-tracetools-analysis: 0.1.1-1", "ros-dashing-tracetools-launch: 0.2.0-1", "ros-dashing-tracetools-read: 0.2.0-1", "ros-dashing-tracetools-test: 0.2.0-1", "ros-dashing-tracetools-trace: 0.2.0-1", "ros-dashing-ament-clang-format: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-clang-format: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-copyright: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-cppcheck: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-cpplint: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-flake8: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-lint-cmake: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-pclint: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-pep257: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-pep8: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-pyflakes: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-uncrustify: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cmake-xmllint: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-copyright: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cppcheck: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-cpplint: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-flake8: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-lint-auto: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-lint-cmake: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-lint-common: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-pclint: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-pep257: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-pep8: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-pyflakes: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-uncrustify: 0.7.3-1 -> 0.7.6-1", "ros-dashing-ament-xmllint: 0.7.3-1 -> 0.7.6-1", "ros-dashing-apriltag: 3.1.0-2 -> 3.1.1-1", "ros-dashing-class-loader: 1.3.1-1 -> 1.3.2-1", "ros-dashing-examples-rclcpp-minimal-action-client: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-action-server: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-client: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-composition: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-publisher: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-service: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-subscriber: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclcpp-minimal-timer: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-executors: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-minimal-action-client: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-minimal-action-server: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-minimal-client: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-minimal-publisher: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-minimal-service: 0.7.3-1 -> 0.7.4-1", "ros-dashing-examples-rclpy-minimal-subscriber: 0.7.3-1 -> 0.7.4-1", "ros-dashing-gazebo-dev: 3.3.1-1 -> 3.3.2-1", "ros-dashing-gazebo-msgs: 3.3.1-1 -> 3.3.2-1", "ros-dashing-gazebo-plugins: 3.3.1-1 -> 3.3.2-1", "ros-dashing-gazebo-ros: 3.3.1-1 -> 3.3.2-1", "ros-dashing-gazebo-ros-pkgs: 3.3.1-1 -> 3.3.2-1", "ros-dashing-launch-ros: 0.8.4-1 -> 0.8.5-1", "ros-dashing-launch-testing-ros: 0.8.4-1 -> 0.8.5-1", "ros-dashing-ompl: 1.4.2-1 -> 1.4.2-2", "ros-dashing-opensplice-cmake-module: 0.7.1-1 -> 0.7.2-1", "ros-dashing-pluginlib: 2.3.1-1 -> 2.3.2-1", "ros-dashing-rcl: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rcl-action: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rcl-lifecycle: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rcl-yaml-param-parser: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rclcpp: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rclcpp-action: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rclcpp-components: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rclcpp-lifecycle: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rclpy: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rcpputils: 0.1.0-1 -> 0.1.1-1", "ros-dashing-rmw-fastrtps-cpp: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rmw-fastrtps-dynamic-cpp: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rmw-fastrtps-shared-cpp: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rmw-opensplice-cpp: 0.7.2-1 -> 0.7.3-1", "ros-dashing-ros1-bridge: 0.7.2-4 -> 0.7.3-1", "ros-dashing-ros2bag: 0.1.3-1 -> 0.1.4-1", "ros-dashing-ros2launch: 0.8.4-1 -> 0.8.5-1", "ros-dashing-rosbag2: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosbag2-converter-default-plugins: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosbag2-storage: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosbag2-storage-default-plugins: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosbag2-test-common: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosbag2-tests: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosbag2-transport: 0.1.3-1 -> 0.1.4-1", "ros-dashing-rosidl-adapter: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-cmake: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-generator-c: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-generator-cpp: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-parser: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-typesupport-interface: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-typesupport-introspection-c: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-typesupport-introspection-cpp: 0.7.4-1 -> 0.7.5-1", "ros-dashing-rosidl-typesupport-opensplice-c: 0.7.1-1 -> 0.7.2-1", "ros-dashing-rosidl-typesupport-opensplice-cpp: 0.7.1-1 -> 0.7.2-1", "ros-dashing-rviz-assimp-vendor: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz-common: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz-default-plugins: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz-ogre-vendor: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz-rendering: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz-rendering-tests: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz-visual-testing-framework: 6.1.2-1 -> 6.1.3-1", "ros-dashing-rviz2: 6.1.2-1 -> 6.1.3-1", "ros-dashing-shared-queues-vendor: 0.1.3-1 -> 0.1.4-1", "ros-dashing-sqlite3-vendor: 0.1.3-1 -> 0.1.4-1", "ros-dashing-tf2: 0.11.3-2 -> 0.11.4-1", "ros-dashing-tf2-eigen: 0.11.3-2 -> 0.11.4-1", "ros-dashing-tf2-geometry-msgs: 0.11.3-2 -> 0.11.4-1", "ros-dashing-tf2-kdl: 0.11.3-2 -> 0.11.4-1", "ros-dashing-tf2-msgs: 0.11.3-2 -> 0.11.4-1", "ros-dashing-tf2-ros: 0.11.3-2 -> 0.11.4-1", "ros-dashing-tf2-sensor-msgs: 0.11.3-2 -> 0.11.4-1", "Anup Pemmaiah", "AutonomouStuff Software Development Team", "Brian Wilcox", "Carl Delsey", "Carlos Orduno", "Christian Rauch", "Christophe Bedard", "Daniel Stonier", "David V. Lu!!", "Dirk Thomas", "Emerson Knapp", "Jacob Perron", "John Shepherd", "Jose Luis Rivero", "Juan Pablo Samper", "Karsten Knese", "Mark Moll", "Max Krogius", "Michael Carroll", "Michael Jeronimo", "Mohammad Haghighipanah", "Pete Baughman", "Scott K Logan", "Shane Loretz", "Steve Macenski", "Steven! Ragnar\u00f6k", "Tully Foote", "Vincent Rabaud", "William Woodall"], "url": "https://discourse.ros.org/t/new-packages-and-patch-release-for-ros-2-dashing-diademata/10200"}
,{"title": "New packages for ROS 2 Dashing Diademata", "thread_contents": ["We\u2019re happy to announce a new sync of packages into Dashing. Thanks to all the maintainers and contributors who made this and all releases possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-dashing-autoware-auto-cmake: 0.0.1-1", "ros-dashing-autoware-auto-create-pkg: 0.0.1-1", "ros-dashing-autoware-auto-examples: 0.0.1-1", "ros-dashing-autoware-auto-geometry: 0.0.1-1", "ros-dashing-autoware-auto-msgs: 0.0.1-1", "ros-dashing-compressed-depth-image-transport: 2.1.0-1", "ros-dashing-compressed-image-transport: 2.1.0-1", "ros-dashing-dynamixel-sdk: 3.7.10-1", "ros-dashing-eigen3-cmake-module: 0.1.1-1", "ros-dashing-fmilibrary-vendor: 0.1.0-1", "ros-dashing-hls-lfcd-lds-driver: 2.0.0-1", "ros-dashing-image-transport-plugins: 2.1.0-1", "ros-dashing-py-trees-js: 0.4.0-1", "ros-dashing-py-trees-ros-viewer: 0.1.2-1", "ros-dashing-rqt-reconfigure: 1.0.3-1", "ros-dashing-theora-image-transport: 2.1.0-1", "ros-dashing-turtlebot3-msgs: 2.1.0-1", "ros-dashing-udp-driver: 0.0.3-1", "ros-dashing-v4l2-camera: 0.1.1-1", "ros-dashing-ament-clang-format: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-clang-tidy: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-clang-format: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-clang-tidy: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-copyright: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-cppcheck: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-cpplint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-flake8: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-lint-cmake: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-pclint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-pep257: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-pep8: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-pyflakes: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-uncrustify: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cmake-xmllint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-copyright: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cppcheck: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-cpplint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-flake8: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-lint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-lint-auto: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-lint-cmake: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-lint-common: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-package: 0.7.0-0 -> 0.7.1-1", "ros-dashing-ament-pclint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-pep257: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-pep8: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-pyflakes: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-uncrustify: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ament-xmllint: 0.7.6-1 -> 0.7.8-1", "ros-dashing-ecl-build: 1.0.1-2 -> 1.0.2-1", "ros-dashing-ecl-license: 1.0.1-2 -> 1.0.2-1", "ros-dashing-ecl-tools: 1.0.1-2 -> 1.0.2-1", "ros-dashing-gazebo-dev: 3.3.2-1 -> 3.3.3-1", "ros-dashing-gazebo-msgs: 3.3.2-1 -> 3.3.3-1", "ros-dashing-gazebo-plugins: 3.3.2-1 -> 3.3.3-1", "ros-dashing-gazebo-ros: 3.3.2-1 -> 3.3.3-1", "ros-dashing-gazebo-ros-pkgs: 3.3.2-1 -> 3.3.3-1", "ros-dashing-launch: 0.8.4-1 -> 0.8.5-3", "ros-dashing-launch-testing: 0.8.4-1 -> 0.8.5-3", "ros-dashing-launch-testing-ament-cmake: 0.8.4-1 -> 0.8.5-3", "ros-dashing-map-msgs: 2.0.1-0 -> 2.0.1-1", "ros-dashing-py-trees: 1.2.1-1 -> 1.2.2-1", "ros-dashing-py-trees-ros: 1.1.1-1 -> 1.1.2-1", "ros-dashing-py-trees-ros-tutorials: 1.0.2-1 -> 1.0.3-1", "Apex.AI, Inc.", "Daniel Stonier", "David Gossow", "David V. Lu!!", "Dirk Thomas", "John Shepherd", "Jose Luis Rivero", "Juan Pablo Samper", "Julius Kammerl", "Michael Carroll", "Pete Baughman", "Pyo", "Ralph Lange", "Sander G. van Dijk", "Scott K Logan", "Shane Loretz", "Steven! Ragnar\u00f6k"], "url": "https://discourse.ros.org/t/new-packages-for-ros-2-dashing-diademata/10455"}
,{"title": "New packages for ROS 2 Dashing Diademata 2019-09-24", "thread_contents": ["We\u2019re happy to announce 25 new packages and 135 updated packages for dashing.", "This is only a sync of the debian packages ahead of the next patch release coming. You can see the status towards the patch release on the ", ".", "Thank you to everyone who helped contribute to these updates. Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 1.0.0-1", "ros-dashing-async-web-server-cpp-dbgsym: 1.0.0-1", "\n", ": 1.0.1-1", "\n", ": 1.0.1-1", "\n", ": 1.0.1-1", "ros-dashing-geographic-msgs-dbgsym: 1.0.1-1", "ros-dashing-joy-teleop: 1.0.1-0", "ros-dashing-key-teleop: 1.0.1-0", "ros-dashing-mouse-teleop: 1.0.1-0", "\n", ": 1.0.1-1", "ros-dashing-rosapi-dbgsym: 1.0.1-1", "\n", ": 2.0.1-1", "ros-dashing-rosauth-dbgsym: 2.0.1-1", "\n", ": 1.0.1-1", "ros-dashing-rosbridge-msgs: 1.0.1-1", "ros-dashing-rosbridge-msgs-dbgsym: 1.0.1-1", "\n", ": 1.0.1-1", "\n", ": 1.0.1-1", "ros-dashing-teleop-tools: 1.0.1-0", "ros-dashing-teleop-tools-msgs: 1.0.1-0", "ros-dashing-teleop-tools-msgs-dbgsym: 1.0.1-0", "\n", ": 1.0.0-1", "ros-dashing-turtlesim-dbgsym: 1.0.0-1", "\n", ": 1.0.0-1", "ros-dashing-web-video-server-dbgsym: 1.0.0-1", "ros-dashing-ament-clang-format: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-clang-tidy: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-clang-format: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-clang-tidy: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-copyright: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-cppcheck: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-cpplint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-flake8: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-lint-cmake: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-pclint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-pep257: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-pep8: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-pyflakes: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-uncrustify: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cmake-xmllint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-copyright: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cppcheck: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-cpplint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-flake8: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-index-cpp: 0.7.0-1 -> 0.7.1-1", "ros-dashing-ament-index-cpp-dbgsym: 0.7.0-1 -> 0.7.1-1", "ros-dashing-ament-index-python: 0.7.0-1 -> 0.7.1-1", "ros-dashing-ament-lint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-lint-auto: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-lint-cmake: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-lint-common: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-package: 0.7.1-1 -> 0.7.2-1", "ros-dashing-ament-pclint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-pep257: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-pep8: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-pyflakes: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-uncrustify: 0.7.9-1 -> 0.7.10-1", "ros-dashing-ament-xmllint: 0.7.9-1 -> 0.7.10-1", "ros-dashing-cloudwatch-logs-common: 1.1.0-1 -> 1.1.1-0", "ros-dashing-cloudwatch-logs-common-dbgsym: 1.1.0-1 -> 1.1.1-0", "ros-dashing-cloudwatch-metrics-common: 1.1.0-1 -> 1.1.1-0", "ros-dashing-costmap-queue: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dataflow-lite: 1.1.0-1 -> 1.1.1-0", "ros-dashing-dataflow-lite-dbgsym: 1.1.0-1 -> 1.1.1-0", "ros-dashing-dwb-controller: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-controller-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-core: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-core-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-critics: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-critics-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-msgs: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-msgs-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-plugins: 0.2.3-1 -> 0.2.4-1", "ros-dashing-dwb-plugins-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-file-management: 1.1.0-1 -> 1.1.1-0", "ros-dashing-file-management-dbgsym: 1.1.0-1 -> 1.1.1-0", "\n", ": 3.3.3-1 -> 3.3.4-1", "\n", ": 3.3.3-1 -> 3.3.4-1", "ros-dashing-gazebo-msgs-dbgsym: 3.3.3-1 -> 3.3.4-1", "\n", ": 3.3.3-1 -> 3.3.4-1", "ros-dashing-gazebo-plugins-dbgsym: 3.3.3-1 -> 3.3.4-1", "\n", ": 3.3.3-1 -> 3.3.4-1", "ros-dashing-gazebo-ros-dbgsym: 3.3.3-1 -> 3.3.4-1", "\n", ": 3.3.3-1 -> 3.3.4-1", "\n", ": 2.0.1-1 -> 2.0.2-1", "ros-dashing-h264-encoder-core-dbgsym: 2.0.1-1 -> 2.0.2-1", "ros-dashing-health-metric-collector: 3.0.0-1 -> 3.0.1-1", "ros-dashing-health-metric-collector-dbgsym: 3.0.0-1 -> 3.0.1-1", "ros-dashing-nav-2d-msgs: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav-2d-msgs-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav-2d-utils: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav-2d-utils-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-amcl: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-amcl-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-behavior-tree: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-behavior-tree-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-bringup: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-bt-navigator: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-bt-navigator-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-common: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-costmap-2d: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-costmap-2d-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-dwb-controller: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-dynamic-params: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-dynamic-params-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-lifecycle-manager: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-lifecycle-manager-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-map-server: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-map-server-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-msgs: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-msgs-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-navfn-planner: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-navfn-planner-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-recoveries: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-recoveries-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-rviz-plugins: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-rviz-plugins-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-util: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-util-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-voxel-grid: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-voxel-grid-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-world-model: 0.2.3-1 -> 0.2.4-1", "ros-dashing-nav2-world-model-dbgsym: 0.2.3-1 -> 0.2.4-1", "ros-dashing-navigation2: 0.2.3-1 -> 0.2.4-1", "ros-dashing-rcl: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-action: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-action-dbgsym: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-dbgsym: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-lifecycle: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-lifecycle-dbgsym: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-yaml-param-parser: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rcl-yaml-param-parser-dbgsym: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rclcpp: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-action: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-action-dbgsym: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-components: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-components-dbgsym: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-dbgsym: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-lifecycle: 0.7.8-1 -> 0.7.10-1", "ros-dashing-rclcpp-lifecycle-dbgsym: 0.7.8-1 -> 0.7.10-1", "ros-dashing-ros2bag: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-converter-default-plugins: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-converter-default-plugins-dbgsym: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-dbgsym: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-storage: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-storage-dbgsym: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-storage-default-plugins: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-storage-default-plugins-dbgsym: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-test-common: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-tests: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-transport: 0.1.5-1 -> 0.1.6-1", "ros-dashing-rosbag2-transport-dbgsym: 0.1.5-1 -> 0.1.6-1", "ros-dashing-shared-queues-vendor: 0.1.5-1 -> 0.1.6-1", "ros-dashing-sqlite3-vendor: 0.1.5-1 -> 0.1.6-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-dashing-teleop-twist-joy-dbgsym: 2.2.0-1 -> 2.2.1-1", "ros-dashing-tts: 2.0.0-1 -> 2.0.1-1", "ros-dashing-tts-interfaces: 2.0.0-1 -> 2.0.1-1", "ros-dashing-tts-interfaces-dbgsym: 2.0.0-1 -> 2.0.1-1", "AWS RoboMaker", "Anup Pemmaiah", "Bence Magyar", "Brian Wilcox", "Carl Delsey", "Carlos Orduno", "Chris Lalancette", "David V. Lu!!", "Dirk Thomas", "Enrique Fernandez", "Hans-Joachim Krauch", "Jack O\u2019Quin", "Jacob Perron", "John Shepherd", "Jose Luis Rivero", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Michael Jeronimo", "Mohammad Haghighipanah", "Rein Appeldoorn", "Russell Toris", "Steve Macenski", "Steven! Ragnarok", "William Woodall"], "url": "https://discourse.ros.org/t/new-packages-for-ros-2-dashing-diademata-2019-09-24/10794"}
,{"title": "New packages and patch release for ROS 2 Dashing 2019-10-18", "thread_contents": ["The new sync and patch release is ", "!", "This release includes a number of core changes including an updated version of Fast-RTPS. For full details of this patch release refer to the ", " column of the ", ".", "Due to dropped support from Homebrew for macOS 10.12 Sierra, this will be the last Dashing release to include macOS binaries as there is no other supported macOS platform for this release. ", "Thanks, as always, to those who made this release possible. I want to express specific gratitude to ", " for managing a very challenging span of platform upgrades for this release.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-dashing-automotive-navigation-msgs: 3.0.0-1", "ros-dashing-automotive-platform-msgs: 3.0.0-1", "ros-dashing-autoware-auto-algorithm: 0.0.2-1", "ros-dashing-autoware-auto-helper-functions: 0.0.2-1", "ros-dashing-delphi-esr-msgs: 3.0.0-2", "ros-dashing-delphi-mrr-msgs: 3.0.0-2", "ros-dashing-delphi-srr-msgs: 3.0.0-2", "ros-dashing-derived-object-msgs: 3.0.0-2", "ros-dashing-eigen-stl-containers: 1.0.0-1", "ros-dashing-euclidean-cluster: 0.0.2-1", "ros-dashing-euclidean-cluster-nodes: 0.0.2-1", "ros-dashing-gps-msgs: 1.0.0-1", "ros-dashing-gps-tools: 1.0.0-1", "ros-dashing-gps-umd: 1.0.0-1", "ros-dashing-gpsd-client: 1.0.0-1", "ros-dashing-hungarian-assigner: 0.0.2-1", "ros-dashing-ibeo-msgs: 3.0.0-2", "ros-dashing-kalman-filter: 0.0.2-1", "ros-dashing-kartech-linear-actuator-msgs: 3.0.0-2", "ros-dashing-launch-ros-sandbox: 0.0.2-4", "ros-dashing-lidar-utils: 0.0.2-1", "ros-dashing-localization-common: 0.0.2-1", "ros-dashing-localization-nodes: 0.0.2-1", "ros-dashing-marti-can-msgs: 1.0.0-1", "ros-dashing-marti-common-msgs: 1.0.0-1", "ros-dashing-marti-nav-msgs: 1.0.0-1", "ros-dashing-marti-perception-msgs: 1.0.0-1", "ros-dashing-marti-sensor-msgs: 1.0.0-1", "ros-dashing-marti-status-msgs: 1.0.0-1", "ros-dashing-marti-visualization-msgs: 1.0.0-1", "ros-dashing-mobileye-560-660-msgs: 3.0.0-2", "ros-dashing-motion-model: 0.0.2-1", "ros-dashing-move-base-msgs: 2.0.2-1", "ros-dashing-nav2-system-tests: 0.2.6-1", "ros-dashing-ndt: 0.0.2-1", "ros-dashing-neobotix-usboard-msgs: 3.0.0-2", "ros-dashing-optimization: 0.0.2-1", "ros-dashing-pacmod-msgs: 3.0.0-2", "ros-dashing-pcl-msgs: 1.0.0-1", "ros-dashing-point-cloud-fusion: 0.0.2-1", "ros-dashing-radar-msgs: 3.0.0-2", "ros-dashing-ray-ground-classifier: 0.0.2-1", "ros-dashing-ray-ground-classifier-nodes: 0.0.2-1", "ros-dashing-ros2trace-analysis: 0.2.1-1", "ros-dashing-serial-driver: 0.0.2-1", "ros-dashing-slam-toolbox: 2.0.2-1", "ros-dashing-velodyne-driver: 0.0.2-1", "ros-dashing-velodyne-node: 0.0.2-1", "ros-dashing-voxel-grid: 0.0.2-1", "ros-dashing-voxel-grid-nodes: 0.0.2-1", "ros-dashing-xacro: 2.0.1-2", "ros-dashing-action-tutorials: 0.7.8-1 -> 0.7.9-1", "ros-dashing-ament-clang-format: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-clang-tidy: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-clang-format: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-clang-tidy: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-copyright: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-cppcheck: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-cpplint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-flake8: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-lint-cmake: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-pclint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-pep257: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-pep8: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-pyflakes: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-uncrustify: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cmake-xmllint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-copyright: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cppcheck: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-cpplint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-flake8: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-index-cpp: 0.7.1-1 -> 0.7.2-1", "ros-dashing-ament-index-python: 0.7.1-1 -> 0.7.2-1", "ros-dashing-ament-lint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-lint-auto: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-lint-cmake: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-lint-common: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-package: 0.7.2-1 -> 0.7.3-1", "ros-dashing-ament-pclint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-pep257: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-pep8: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-pyflakes: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-uncrustify: 0.7.10-1 -> 0.7.11-1", "ros-dashing-ament-xmllint: 0.7.10-1 -> 0.7.11-1", "ros-dashing-autoware-auto-cmake: 0.0.1-1 -> 0.0.2-1", "ros-dashing-autoware-auto-create-pkg: 0.0.1-1 -> 0.0.2-1", "ros-dashing-autoware-auto-examples: 0.0.1-1 -> 0.0.2-1", "ros-dashing-autoware-auto-geometry: 0.0.1-1 -> 0.0.2-1", "ros-dashing-autoware-auto-msgs: 0.0.1-1 -> 0.0.2-1", "ros-dashing-composition: 0.7.8-1 -> 0.7.9-1", "ros-dashing-costmap-queue: 0.2.4-1 -> 0.2.6-1", "ros-dashing-demo-nodes-cpp: 0.7.8-1 -> 0.7.9-1", "ros-dashing-demo-nodes-cpp-native: 0.7.8-1 -> 0.7.9-1", "ros-dashing-demo-nodes-py: 0.7.8-1 -> 0.7.9-1", "ros-dashing-depthimage-to-laserscan: 2.2.1-1 -> 2.2.2-1", "ros-dashing-dummy-map-server: 0.7.8-1 -> 0.7.9-1", "ros-dashing-dummy-robot-bringup: 0.7.8-1 -> 0.7.9-1", "ros-dashing-dummy-sensors: 0.7.8-1 -> 0.7.9-1", "ros-dashing-dwb-controller: 0.2.4-1 -> 0.2.6-1", "ros-dashing-dwb-core: 0.2.4-1 -> 0.2.6-1", "ros-dashing-dwb-critics: 0.2.4-1 -> 0.2.6-1", "ros-dashing-dwb-msgs: 0.2.4-1 -> 0.2.6-1", "ros-dashing-dwb-plugins: 0.2.4-1 -> 0.2.6-1", "ros-dashing-fastcdr: 1.0.9-2 -> 1.0.11-1", "ros-dashing-fastrtps: 1.8.0-2 -> 1.8.2-1", "ros-dashing-image-tools: 0.7.8-1 -> 0.7.9-1", "ros-dashing-intra-process-demo: 0.7.8-1 -> 0.7.9-1", "ros-dashing-launch: 0.8.6-1 -> 0.8.7-1", "ros-dashing-launch-ros: 0.8.6-1 -> 0.8.7-1", "ros-dashing-launch-testing: 0.8.6-1 -> 0.8.7-1", "ros-dashing-launch-testing-ament-cmake: 0.8.6-1 -> 0.8.7-1", "ros-dashing-launch-testing-ros: 0.8.6-1 -> 0.8.7-1", "ros-dashing-lifecycle: 0.7.8-1 -> 0.7.9-1", "ros-dashing-logging-demo: 0.7.8-1 -> 0.7.9-1", "ros-dashing-map-msgs: 2.0.1-1 -> 2.0.2-1", "ros-dashing-nav-2d-msgs: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav-2d-utils: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-amcl: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-behavior-tree: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-bringup: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-bt-navigator: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-common: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-costmap-2d: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-dwb-controller: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-dynamic-params: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-lifecycle-manager: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-map-server: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-msgs: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-navfn-planner: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-recoveries: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-rviz-plugins: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-util: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-voxel-grid: 0.2.4-1 -> 0.2.6-1", "ros-dashing-nav2-world-model: 0.2.4-1 -> 0.2.6-1", "ros-dashing-navigation2: 0.2.4-1 -> 0.2.6-1", "ros-dashing-osrf-pycommon: 0.1.7-1 -> 0.1.9-1", "ros-dashing-pendulum-control: 0.7.8-1 -> 0.7.9-1", "ros-dashing-pendulum-msgs: 0.7.8-1 -> 0.7.9-1", "ros-dashing-pluginlib: 2.3.2-1 -> 2.3.3-1", "ros-dashing-py-trees: 1.2.2-1 -> 1.3.0-1", "ros-dashing-py-trees-ros: 1.1.2-1 -> 1.2.1-2", "ros-dashing-py-trees-ros-interfaces: 1.1.2-1 -> 1.2.0-1", "ros-dashing-py-trees-ros-tutorials: 1.0.3-1 -> 1.0.5-1", "ros-dashing-python-cmake-module: 0.7.8-1 -> 0.7.9-1", "ros-dashing-python-qt-binding: 1.0.1-1 -> 1.0.2-1", "ros-dashing-qt-dotgraph: 1.0.6-1 -> 1.0.7-1", "ros-dashing-qt-gui: 1.0.6-1 -> 1.0.7-1", "ros-dashing-qt-gui-app: 1.0.6-1 -> 1.0.7-1", "ros-dashing-qt-gui-core: 1.0.6-1 -> 1.0.7-1", "ros-dashing-qt-gui-cpp: 1.0.6-1 -> 1.0.7-1", "ros-dashing-qt-gui-py-common: 1.0.6-1 -> 1.0.7-1", "ros-dashing-quality-of-service-demo-cpp: 0.7.8-1 -> 0.7.9-1", "ros-dashing-quality-of-service-demo-py: 0.7.8-1 -> 0.7.9-1", "ros-dashing-rclcpp: 0.7.10-1 -> 0.7.11-1", "ros-dashing-rclcpp-action: 0.7.10-1 -> 0.7.11-1", "ros-dashing-rclcpp-components: 0.7.10-1 -> 0.7.11-1", "ros-dashing-rclcpp-lifecycle: 0.7.10-1 -> 0.7.11-1", "ros-dashing-rclpy: 0.7.7-1 -> 0.7.8-1", "ros-dashing-rmw-fastrtps-cpp: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rmw-fastrtps-dynamic-cpp: 0.7.5-1 -> 0.7.6-1", "ros-dashing-rmw-fastrtps-shared-cpp: 0.7.5-1 -> 0.7.6-1", "ros-dashing-ros-testing: 0.1.0-1 -> 0.1.1-1", "ros-dashing-ros-workspace: 0.7.1-1 -> 0.7.2-1", "ros-dashing-ros2action: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2bag: 0.1.6-1 -> 0.1.8-1", "ros-dashing-ros2cli: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2component: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2launch: 0.8.6-1 -> 0.8.7-1", "ros-dashing-ros2lifecycle: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2msg: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2multicast: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2node: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2param: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2pkg: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2run: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2service: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2srv: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2test: 0.1.0-1 -> 0.1.1-1", "ros-dashing-ros2topic: 0.7.4-1 -> 0.7.6-1", "ros-dashing-ros2trace: 0.2.0-1 -> 0.2.8-1", "ros-dashing-rosapi: 1.0.1-1 -> 1.0.2-1", "ros-dashing-rosbag2: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbag2-converter-default-plugins: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbag2-storage: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbag2-storage-default-plugins: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbag2-test-common: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbag2-tests: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbag2-transport: 0.1.6-1 -> 0.1.8-1", "ros-dashing-rosbridge-library: 1.0.1-1 -> 1.0.2-1", "ros-dashing-rosbridge-msgs: 1.0.1-1 -> 1.0.2-1", "ros-dashing-rosbridge-server: 1.0.1-1 -> 1.0.2-1", "ros-dashing-rosbridge-suite: 1.0.1-1 -> 1.0.2-1", "ros-dashing-rosidl-adapter: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-cmake: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-generator-c: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-generator-cpp: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-generator-py: 0.7.8-1 -> 0.7.9-1", "ros-dashing-rosidl-parser: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-runtime-py: 0.7.8-1 -> 0.7.9-1", "ros-dashing-rosidl-typesupport-interface: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-typesupport-introspection-c: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rosidl-typesupport-introspection-cpp: 0.7.6-1 -> 0.7.7-1", "ros-dashing-rqt: 1.0.4-1 -> 1.0.5-1", "ros-dashing-rqt-console: 1.0.1-1 -> 1.1.0-1", "ros-dashing-rqt-graph: 1.0.1-1 -> 1.0.2-1", "ros-dashing-rqt-gui: 1.0.4-1 -> 1.0.5-1", "ros-dashing-rqt-gui-cpp: 1.0.4-1 -> 1.0.5-1", "ros-dashing-rqt-gui-py: 1.0.4-1 -> 1.0.5-1", "ros-dashing-rqt-plot: 1.0.6-1 -> 1.0.7-1", "ros-dashing-rqt-publisher: 1.0.5-1 -> 1.1.0-1", "ros-dashing-rqt-py-common: 1.0.4-1 -> 1.0.5-1", "ros-dashing-rqt-reconfigure: 1.0.3-1 -> 1.0.4-1", "ros-dashing-shared-queues-vendor: 0.1.6-1 -> 0.1.8-1", "ros-dashing-sqlite3-vendor: 0.1.6-1 -> 0.1.8-1", "ros-dashing-teleop-twist-keyboard: 2.3.0-1 -> 2.3.1-1", "ros-dashing-topic-monitor: 0.7.8-1 -> 0.7.9-1", "ros-dashing-tracetools: 0.2.0-1 -> 0.2.8-1", "ros-dashing-tracetools-analysis: 0.1.1-1 -> 0.2.1-1", "ros-dashing-tracetools-launch: 0.2.0-1 -> 0.2.8-1", "ros-dashing-tracetools-read: 0.2.0-1 -> 0.2.8-1", "ros-dashing-tracetools-test: 0.2.0-1 -> 0.2.8-1", "ros-dashing-tracetools-trace: 0.2.0-1 -> 0.2.8-1", "ros-dashing-turtlebot3-msgs: 2.1.0-1 -> 2.2.0-1", "ros-dashing-turtlesim: 1.0.0-1 -> 1.0.1-1", "AWS RoboMaker", "Amazon ROS Contributions", "Apex.AI", "Apex.AI, Inc.", "AutonomouStuff Software Development Team", "AutonomouStuff Software Team", "Brian Wilcox", "Carl Delsey", "Carlos Orduno", "Chris Lalancette", "Christophe Bedard", "Daniel Stonier", "David V. Lu!!", "Dirk Thomas", "Dorian Scholz", "Hans-Joachim Krauch", "Jacob Perron", "John Shepherd", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Michael Jeronimo", "Michel Hidalgo", "Mohammad Haghighipanah", "P. J. Reed", "Paul Bovbel", "Pete Baughman", "Pyo", "Robert Haschke", "Russell Toris", "Scott K Logan", "Steve Macenski", "Steven! Ragnarok", "Steven! Ragnar\u00f6k", "William Woodall"], "url": "https://discourse.ros.org/t/new-packages-and-patch-release-for-ros-2-dashing-2019-10-18/11086"}
,{"title": "Patch release and new packages for ROS 2 Crystal Clemmys 2019-01-17", "thread_contents": ["The first patch release for ROS 2 Crystal Clemmys is ", "!", "There are some fixes that I\u2019d like to highlight:", "For everything included in the patch release check out the ", ".", "Patch releases will not happen every sync. We\u2019ll select an upcoming sync as a patch release as needed to include new features and fixes in core ROS 2 packages. To see what is scheduled for the next patch release you can follow ", ".", "This was also the first sync for Crystal Clemmys", "Note that the counts are somewhat inflated since they include the debug symbol packages which I have otherwise filtered from the lists below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Add a guard against bad allocation in rcl. ", "\n", "Fix for a segfault when string parameters contain non-ASCII characters. ", "\n", "Add a generate_permissions verb to sros2. ", "\n", "Upgrade to support OpenCV 4. ", " & ", ".", "\n", ": 1.0.1-1", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-1", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-1", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "ros-crystal-ros1-rosbag-storage-vendor: 0.0.5-0", "ros-crystal-rosbag2-bag-v2-plugins: 0.0.5-0", "ros-crystal-sophus: 1.0.2-0", "ros-crystal-action-msgs: 0.6.1-0 -> 0.6.2-0", "ros-crystal-ament-clang-format: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-clang-format: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-copyright: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-cppcheck: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-cpplint: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-flake8: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-lint-cmake: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-pclint: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-pep257: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-pep8: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-pyflakes: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-uncrustify: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cmake-xmllint: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-copyright: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cppcheck: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-cpplint: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-flake8: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-lint-auto: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-lint-cmake: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-lint-common: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-pclint: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-pep257: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-pep8: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-pyflakes: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-uncrustify: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-xmllint: 0.6.2-0 -> 0.6.3-0", "ros-crystal-builtin-interfaces: 0.6.1-0 -> 0.6.2-0", "ros-crystal-composition: 0.6.1-0 -> 0.6.2-0", "ros-crystal-connext-cmake-module: 0.6.3-0 -> 0.6.4-0", "ros-crystal-demo-nodes-cpp: 0.6.1-0 -> 0.6.2-0", "ros-crystal-demo-nodes-cpp-native: 0.6.1-0 -> 0.6.2-0", "ros-crystal-demo-nodes-py: 0.6.1-0 -> 0.6.2-0", "ros-crystal-dummy-map-server: 0.6.1-0 -> 0.6.2-0", "ros-crystal-dummy-robot-bringup: 0.6.1-0 -> 0.6.2-0", "ros-crystal-dummy-sensors: 0.6.1-0 -> 0.6.2-0", "ros-crystal-fastrtps-cmake-module: 0.6.0-0 -> 0.6.1-0", "ros-crystal-image-tools: 0.6.1-0 -> 0.6.2-0", "ros-crystal-intra-process-demo: 0.6.1-0 -> 0.6.2-0", "ros-crystal-lifecycle: 0.6.1-0 -> 0.6.2-0", "ros-crystal-lifecycle-msgs: 0.6.1-0 -> 0.6.2-0", "ros-crystal-logging-demo: 0.6.1-0 -> 0.6.2-0", "ros-crystal-opensplice-cmake-module: 0.6.1-0 -> 0.6.2-0", "ros-crystal-pendulum-control: 0.6.1-0 -> 0.6.2-0", "ros-crystal-pendulum-msgs: 0.6.1-0 -> 0.6.2-0", "ros-crystal-python-cmake-module: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rcl: 0.6.3-0 -> 0.6.4-0", "ros-crystal-rcl-action: 0.6.3-0 -> 0.6.4-0", "ros-crystal-rcl-interfaces: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rcl-lifecycle: 0.6.3-0 -> 0.6.4-0", "ros-crystal-rcl-yaml-param-parser: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ros1-bridge: 0.6.1-0 -> 0.6.1-1", "ros-crystal-ros2bag: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2-converter-default-plugins: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2-storage: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2-storage-default-plugins: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2-test-common: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2-tests: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosbag2-transport: 0.0.2-0 -> 0.0.5-0", "ros-crystal-rosgraph-msgs: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rosidl-generator-py: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rosidl-typesupport-c: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rosidl-typesupport-connext-c: 0.6.3-0 -> 0.6.4-0", "ros-crystal-rosidl-typesupport-connext-cpp: 0.6.3-0 -> 0.6.4-0", "ros-crystal-rosidl-typesupport-cpp: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rosidl-typesupport-fastrtps-c: 0.6.0-0 -> 0.6.1-0", "ros-crystal-rosidl-typesupport-fastrtps-cpp: 0.6.0-0 -> 0.6.1-0", "ros-crystal-rosidl-typesupport-opensplice-c: 0.6.1-0 -> 0.6.2-0", "ros-crystal-rosidl-typesupport-opensplice-cpp: 0.6.1-0 -> 0.6.2-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "ros-crystal-rviz-rendering-tests: 5.0.0-2 -> 5.1.0-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "\n", ": 5.0.0-2 -> 5.1.0-0", "ros-crystal-shared-queues-vendor: 0.0.2-0 -> 0.0.5-0", "ros-crystal-sqlite3-vendor: 0.0.2-0 -> 0.0.5-0", "ros-crystal-sros2: 0.6.0-0 -> 0.6.1-0", "ros-crystal-test-msgs: 0.6.1-0 -> 0.6.2-0", "ros-crystal-tinyxml2-vendor: 0.6.0-0 -> 0.6.1-0", "ros-crystal-topic-monitor: 0.6.1-0 -> 0.6.2-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "Anup Pemmaiah", "Daniel Stonier", "Dirk Thomas", "Jacob Perron", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Mikael Arguedas", "Scott K Logan", "Steven! Ragnarok", "Tully Foote", "William Woodall"], "url": "https://discourse.ros.org/t/patch-release-and-new-packages-for-ros-2-crystal-clemmys-2019-01-17/7505"}
,{"title": "Patch release and new packages for ROS 2 Crystal Clemmys 2019-03-14", "thread_contents": ["Crystal\u2019s third patch release is ", "!", "The changes I\u2019m most excited about:", "The changes in ", " introduce a new dependency on the rosdep key: ", "\nThe installation documentation has been updated but I\u2019ll highlight the changes here:", "Depending on the platform, users may need to get the additional dependency:", "You can check out full list of changes in the patch release ", ". We\u2019ve also discovered some issues in Fast-RTPS that are being added to the ", " section of the ", ". I\u2019m linking the section now even though they won\u2019t appear until the next documentation rebuild. ", " is the change for you first viewers.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "As was initially ", " by ", " the binary artifacts uploaded to GitHub for this release were incomplete. I\u2019ve re-uploaded the artifacts and published their checksums to the release page. If you had any trouble with them previously, please give them another try.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Actions are now available in rclpy. You can get the details from ", ".", "If you\u2019re interested in using log4cxx for your logging needs, the rcl_logging_log4cxx package has now been released. Using this package naturally relies on log4cxx, so refer to the installation instructions for your platform to get it.", "With Ubuntu debs: apt will install suitable the suitable dependency", "With Ubuntu binary releases: The ", " tool should resolve the dependency (installs ", ")", "With macOS: The dependency should be resolved by installing ", " via pip until using rosdep is an option (see ", ")", "With Windows: The dependency should be resolved by installing ", " via pip", "With Windows (debug): The dependency should be resolved by installing a custom-built ", " via pip: ", "\n", "ros-crystal-behaviortree-cpp-v3: 3.0.4-0", "ros-crystal-behaviortree-cpp-v3-dbgsym: 3.0.4-0", "ros-crystal-examples-rclpy-minimal-action-client: 0.6.3-0", "ros-crystal-examples-rclpy-minimal-action-server: 0.6.3-0", "ros-crystal-nav2-common: 0.1.7-0", "ros-crystal-py-trees-ros-interfaces: 1.0.0-0", "ros-crystal-py-trees-ros-interfaces-dbgsym: 1.0.0-0", "ros-crystal-rcl-logging-log4cxx: 0.2.0-0", "ros-crystal-rcl-logging-log4cxx-dbgsym: 0.2.0-0", "ros-crystal-rqt-action: 1.0.1-0", "ros-crystal-rqt-topic: 1.0.0-0", "ros-crystal-action-msgs: 0.6.2-0 -> 0.6.3-0", "ros-crystal-action-msgs-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-ament-clang-format: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-auto: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-clang-format: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-copyright: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-core: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-cppcheck: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-cpplint: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-export-definitions: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-export-dependencies: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-export-include-directories: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-export-interfaces: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-export-libraries: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-export-link-flags: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-flake8: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-gmock: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-gtest: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-include-directories: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-libraries: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-lint-cmake: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-nose: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-pclint: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-pep257: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-pep8: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-pyflakes: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-pytest: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-python: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-target-dependencies: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-test: 0.6.0-4 -> 0.6.1-0", "ros-crystal-ament-cmake-uncrustify: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cmake-xmllint: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-copyright: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cppcheck: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-cpplint: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-flake8: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-lint-auto: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-lint-cmake: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-lint-common: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-pclint: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-pep257: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-pep8: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-pyflakes: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-uncrustify: 0.6.3-0 -> 0.6.4-0", "ros-crystal-ament-xmllint: 0.6.3-0 -> 0.6.4-0", "ros-crystal-builtin-interfaces: 0.6.2-0 -> 0.6.3-0", "ros-crystal-builtin-interfaces-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-costmap-queue: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-controller: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-controller-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-core: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-core-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-critics: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-critics-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-msgs: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-msgs-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-plugins: 0.1.5-0 -> 0.1.7-0", "ros-crystal-dwb-plugins-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-examples-rclcpp-minimal-action-client: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-action-client-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-action-server: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-action-server-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-client: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-client-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-composition: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-composition-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-publisher: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-publisher-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-service: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-service-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-subscriber: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-subscriber-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-timer: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclcpp-minimal-timer-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclpy-executors: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclpy-minimal-client: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclpy-minimal-publisher: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclpy-minimal-service: 0.6.2-0 -> 0.6.3-0", "ros-crystal-examples-rclpy-minimal-subscriber: 0.6.2-0 -> 0.6.3-0", "ros-crystal-gazebo-dev: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-msgs: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-msgs-dbgsym: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-plugins: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-plugins-dbgsym: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-ros: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-ros-dbgsym: 3.1.0-0 -> 3.2.0-0", "ros-crystal-gazebo-ros-pkgs: 3.1.0-0 -> 3.2.0-0", "ros-crystal-lifecycle-msgs: 0.6.2-0 -> 0.6.3-0", "ros-crystal-lifecycle-msgs-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-nav-2d-msgs: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav-2d-msgs-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav-2d-utils: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav-2d-utils-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-amcl: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-amcl-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-bringup: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-bt-navigator: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-bt-navigator-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-costmap-2d: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-costmap-2d-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-dwb-controller: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-dynamic-params: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-dynamic-params-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-map-server: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-map-server-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-mission-executor: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-mission-executor-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-motion-primitives: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-motion-primitives-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-msgs: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-msgs-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-navfn-planner: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-navfn-planner-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-robot: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-robot-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-simple-navigator: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-simple-navigator-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-tasks: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-tasks-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-util: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-util-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-voxel-grid: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-voxel-grid-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-world-model: 0.1.5-0 -> 0.1.7-0", "ros-crystal-nav2-world-model-dbgsym: 0.1.5-0 -> 0.1.7-0", "ros-crystal-navigation2: 0.1.5-0 -> 0.1.7-0", "ros-crystal-python-cmake-module: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rcl-interfaces: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rcl-interfaces-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rcl-logging-noop: 0.1.0-0 -> 0.2.0-0", "ros-crystal-rcl-logging-noop-dbgsym: 0.1.0-0 -> 0.2.0-0", "ros-crystal-rclpy: 0.6.2-0 -> 0.6.3-1", "ros-crystal-rclpy-dbgsym: 0.6.2-0 -> 0.6.3-1", "ros-crystal-ros1-bridge: 0.6.1-1 -> 0.6.2-0", "ros-crystal-ros1-bridge-dbgsym: 0.6.1-1 -> 0.6.2-0", "ros-crystal-ros1-rosbag-storage-vendor: 0.0.5-0 -> 0.0.6-0", "ros-crystal-ros2bag: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-bag-v2-plugins: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-converter-default-plugins: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-converter-default-plugins-dbgsym: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-dbgsym: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-storage: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-storage-dbgsym: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-storage-default-plugins: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-storage-default-plugins-dbgsym: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-test-common: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-tests: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-transport: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosbag2-transport-dbgsym: 0.0.5-0 -> 0.0.6-0", "ros-crystal-rosgraph-msgs: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rosgraph-msgs-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rosidl-generator-py: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rosidl-typesupport-c: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rosidl-typesupport-c-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rosidl-typesupport-cpp: 0.6.2-0 -> 0.6.3-0", "ros-crystal-rosidl-typesupport-cpp-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-shared-queues-vendor: 0.0.5-0 -> 0.0.6-0", "ros-crystal-sqlite3-vendor: 0.0.5-0 -> 0.0.6-0", "ros-crystal-sros2: 0.6.2-0 -> 0.6.3-0", "ros-crystal-sros2-cmake: 0.6.2-0 -> 0.6.3-0", "ros-crystal-teleop-twist-keyboard: 2.1.1-1 -> 2.2.0-0", "ros-crystal-test-msgs: 0.6.2-0 -> 0.6.3-0", "ros-crystal-test-msgs-dbgsym: 0.6.2-0 -> 0.6.3-0", "ros-crystal-nav2-system-tests", "AWS B9 Team", "Amazon B9", "Brian Wilcox", "Carl Delsey", "Carlos Orduno", "Chris Lalancette", "Daniel Stonier", "David V. Lu!!", "Davide Faconti", "Dirk Thomas", "Jacob Perron", "Jose Luis Rivero", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Michael Ferguson", "Michael Jeronimo", "Mikael Arguedas", "Scott K Logan", "Shane Loretz", "Steve Macenski", "Steven! Ragnarok", "Tully Foote", "William Woodall"], "url": "https://discourse.ros.org/t/patch-release-and-new-packages-for-ros-2-crystal-clemmys-2019-03-14/8311"}
,{"title": "Autoware meeting at RosCon 19?", "thread_contents": ["Hi Autoware users, who is going to be at RosCon in Macau?", "It would be great to meet for 1h and start planning for the hackathon in Palo Alto in April 2020.", "Please reply to this message and we will post the date/place accordingly.", "D.", " from Arm will be there.", "I propose to hold a hackathon in Moscow. I will be at ROSCON.", "I\u2019ll be there and also join the real-time workshop tomorrow.", "Yo.", "(This post was less than 20 characters.)", "Yo , there, camping near the coffee machine.", " let\u2019s meet on Friday at 8 am at the ROSCon registration desk. We will discuss the work packages needed to make an autonomous valet parking demo possible in April 2020: ", " ", " ", " ", " ", " ", " ", " ", " would anyone from osrf want to participate too?", "Friday at 8 am is OK for me.", "Sorry just saw this. I can try to answer any follow questions if you guys point them out to me.", "I wil try to join this meeting maybe with some other guys from StreetScooter", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Hackathon in April, 2020 in Palo Alto at the Apex.AI office.", "Goal is to achieve full autonomous valet parking in the simulator before the hackathon, with the hackathon purpose being vehicle integration and polishing", "Hackathon will be 5 days\n", "4 days work on preparing the demo", "1 day for the demo", "\n", "We will use the existing Apex.AI Lexus with its current hardware and sensor configuration\n", "Nouvo board", "Two velodynes", "\n", "Will build on Eloquent, which in turn means Ubuntu for the OS.", "Tasks\n", "Someone needs to take charge of the simulation and recreate the Apex.Autonomy demo in Autoware.auto", "Someone needs to to take charge of the map", "ROS 2 interface to the vehicle", "Someone needs to be in charge of the architecture", "\n", "Fail-safe operation will be to just perform emergency stop", "Apex.AI can put out an engineer to lead the architecture work for two weeks full time. This person will design the architecture; Tier IV will help to write it up.", "Scenario-based design", "Map:\n", "Want to go to OpenDrive so we can use the same map in the simulator", "Run-time access API is lanelet2 or something customised from it", "We need a ros2 launch command that will just work", "Brian/Parkopedia can probably push this because they have an immediate need; the Autoware WG will provide need to the Maps WG, which will coordinate the work", "\n", "Simulator:\n", "LGE needs to make sure the simulator runs and provides the correct APIs", "\n", " , ", " and ", "  to talk about what is.", "\n", "Localisation:\n", "NDT is just about in Autoware.Auto", "\n", "Perception:\n", "Autoware.Auto has object detection sufficient for the AVP demo", "\n", "Planner:\n", "Tier IV will provide a new planning architecture", "Fallback solution is just choose and follow way points", "Will make a decision on whether to go with the fallback on 1 December.", "Embotech has a planning architecture for parking which may be useful. Apex.AI is talking to them.", "\n", "Controller:\n", "Chris/Apex.AI has an MPC controller for the Lexus", "\n", "Vehicle interface and low level controller\n", "AutonomouStuff? Dejan to talk to them.", "\n", "Other\n", "Logger functionality is very important, but secondary to the demo", "We need something comprehensive and tamper-proof", "\n", "Set meetings with: (", " ", ")\n", "AutonomouStuff (include ", ")", "Embotech (include ", ")", "LGE (include ", " and ", ")", "\n", "Run a poll to find the most-preferred week in April for the hackathon and demo ", "\n", "Set some intermediate deadlines ", " ", "\n"], "url": "https://discourse.ros.org/t/autoware-meeting-at-roscon-19/11263"}
,{"title": "ROS 2 Eloquent Elusor call for testing and package releases", "thread_contents": ["Hello all,", "The ROS 2 team is preparing for the release of Eloquent Elusor and we\u2019d like to invite you to help us by testing out the alpha or releasing your own packages to Eloquent.  For more information on the schedule for the release of Eloquent Elusor, consult the ", ".", "ROS 2 is available with multiple RMW implementations on multiple platforms and architectures. Depending on your system configuration, you can help us out by doing any of the following:", "After testing, reply back here to let us know what worked and what didn\u2019t work, along with what platform you\u2019re on and how you installed ROS 2.", "If you run into problems, all the usual recommendations apply. You can ask questions on ", " with the  ", "  tag or report issues on ", " or the relevant package repository. Be sure to follow the contributing guidelines and issue templates.", "If you maintain a package you would like to release in ROS 2 Eloquent we encourage you to do so by following the ", ".", ", ", ", and ", ",", "\nThe ROS 2 team", "And the packages included in the first sync:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Thanks for sharing this ", "! I will test this out and share this post. ", "Hi ", ", on my box there is no ", " package yet. The build farm status page is all green though, indicating the sync is completed. Is this expected? I can get ", " packages alright from shadow-fixed.", "The last stage of the pipeline for deploying to the mirrors was not yet setup for the main repository. I\u2019ve added it and the packages should now be accessible from the main repository.", " ", " Is it possible to configure the testing mirror deployment as well ?", "\nThe nightly docker images are using the testing repository and would need a sync to start building again.", "Thanks!", "It\u2019s done. I didn\u2019t realize it hadn\u2019t been done already.", "With many packages being updated for the Beta, I have done another sync to main:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " on your preferred platform. Be sure to use the  ", "  branch version of the repos file (described ", ") as Eloquent is still in pre-release.", "Install and use the ", " available for MacOS Sierra, Windows 10, or Ubuntu 18.04 Bionic.", "\n", " as they become available. (", ", ", ")", "Follow the ROS 2 ", " with one or more different RMW implementations.", "Run some ", " and ", ".", "ros-eloquent-action-msgs: 0.8.0-1", "ros-eloquent-action-msgs-dbgsym: 0.8.0-1", "ros-eloquent-action-tutorials-cpp: 0.8.0-1", "ros-eloquent-action-tutorials-cpp-dbgsym: 0.8.0-1", "ros-eloquent-action-tutorials-interfaces: 0.8.0-1", "ros-eloquent-action-tutorials-interfaces-dbgsym: 0.8.0-1", "ros-eloquent-action-tutorials-py: 0.8.0-1", "ros-eloquent-actionlib-msgs: 0.8.0-1", "ros-eloquent-actionlib-msgs-dbgsym: 0.8.0-1", "ros-eloquent-ament-clang-format: 0.8.0-1", "ros-eloquent-ament-clang-tidy: 0.8.0-1", "ros-eloquent-ament-cmake: 0.8.0-1", "ros-eloquent-ament-cmake-auto: 0.8.0-1", "ros-eloquent-ament-cmake-clang-format: 0.8.0-1", "ros-eloquent-ament-cmake-clang-tidy: 0.8.0-1", "ros-eloquent-ament-cmake-copyright: 0.8.0-1", "ros-eloquent-ament-cmake-core: 0.8.0-1", "ros-eloquent-ament-cmake-cppcheck: 0.8.0-1", "ros-eloquent-ament-cmake-cpplint: 0.8.0-1", "ros-eloquent-ament-cmake-export-definitions: 0.8.0-1", "ros-eloquent-ament-cmake-export-dependencies: 0.8.0-1", "ros-eloquent-ament-cmake-export-include-directories: 0.8.0-1", "ros-eloquent-ament-cmake-export-interfaces: 0.8.0-1", "ros-eloquent-ament-cmake-export-libraries: 0.8.0-1", "ros-eloquent-ament-cmake-export-link-flags: 0.8.0-1", "ros-eloquent-ament-cmake-flake8: 0.8.0-1", "ros-eloquent-ament-cmake-gmock: 0.8.0-1", "ros-eloquent-ament-cmake-gtest: 0.8.0-1", "ros-eloquent-ament-cmake-include-directories: 0.8.0-1", "ros-eloquent-ament-cmake-libraries: 0.8.0-1", "ros-eloquent-ament-cmake-lint-cmake: 0.8.0-1", "ros-eloquent-ament-cmake-mypy: 0.8.0-1", "ros-eloquent-ament-cmake-nose: 0.8.0-1", "ros-eloquent-ament-cmake-pclint: 0.8.0-1", "ros-eloquent-ament-cmake-pep257: 0.8.0-1", "ros-eloquent-ament-cmake-pep8: 0.8.0-1", "ros-eloquent-ament-cmake-pyflakes: 0.8.0-1", "ros-eloquent-ament-cmake-pytest: 0.8.0-1", "ros-eloquent-ament-cmake-python: 0.8.0-1", "ros-eloquent-ament-cmake-ros: 0.8.0-1", "ros-eloquent-ament-cmake-target-dependencies: 0.8.0-1", "ros-eloquent-ament-cmake-test: 0.8.0-1", "ros-eloquent-ament-cmake-uncrustify: 0.8.0-1", "ros-eloquent-ament-cmake-xmllint: 0.8.0-1", "ros-eloquent-ament-copyright: 0.8.0-1", "ros-eloquent-ament-cppcheck: 0.8.0-1", "ros-eloquent-ament-cpplint: 0.8.0-1", "ros-eloquent-ament-flake8: 0.8.0-1", "ros-eloquent-ament-index-cpp: 0.7.1-1", "ros-eloquent-ament-index-cpp-dbgsym: 0.7.1-1", "ros-eloquent-ament-index-python: 0.7.1-1", "ros-eloquent-ament-lint: 0.8.0-1", "ros-eloquent-ament-lint-auto: 0.8.0-1", "ros-eloquent-ament-lint-cmake: 0.8.0-1", "ros-eloquent-ament-lint-common: 0.8.0-1", "ros-eloquent-ament-mypy: 0.8.0-1", "ros-eloquent-ament-package: 0.8.2-1", "ros-eloquent-ament-pclint: 0.8.0-1", "ros-eloquent-ament-pep257: 0.8.0-1", "ros-eloquent-ament-pep8: 0.8.0-1", "ros-eloquent-ament-pyflakes: 0.8.0-1", "ros-eloquent-ament-uncrustify: 0.8.0-1", "ros-eloquent-ament-xmllint: 0.8.0-1", "\n", ": 1.12.1-1", "ros-eloquent-builtin-interfaces: 0.8.0-1", "ros-eloquent-builtin-interfaces-dbgsym: 0.8.0-1", "\n", ": 2.2.0-1", "ros-eloquent-camera-calibration-parsers-dbgsym: 2.2.0-1", "\n", ": 2.2.0-1", "ros-eloquent-camera-info-manager-dbgsym: 2.2.0-1", "\n", ": 1.0.0-1", "ros-eloquent-cartographer-dbgsym: 1.0.0-1", "\n", ": 1.4.0-1", "ros-eloquent-class-loader-dbgsym: 1.4.0-1", "ros-eloquent-common-interfaces: 0.8.0-1", "ros-eloquent-composition: 0.8.0-1", "ros-eloquent-composition-dbgsym: 0.8.0-1", "ros-eloquent-composition-interfaces: 0.8.0-1", "ros-eloquent-composition-interfaces-dbgsym: 0.8.0-1", "\n", ": 2.2.0-1", "ros-eloquent-compressed-depth-image-transport-dbgsym: 2.2.0-1", "\n", ": 2.2.0-1", "ros-eloquent-compressed-image-transport-dbgsym: 2.2.0-1", "ros-eloquent-connext-cmake-module: 0.8.1-1", "\n", ": 1.2.0-1", "ros-eloquent-console-bridge-vendor-dbgsym: 1.2.0-1", "\n", ": 2.1.2-1", "ros-eloquent-cv-bridge-dbgsym: 2.1.2-1", "\n", ": 0.1.0-1", "ros-eloquent-cyclonedds-dbgsym: 0.1.0-1", "ros-eloquent-demo-nodes-cpp: 0.8.0-1", "ros-eloquent-demo-nodes-cpp-dbgsym: 0.8.0-1", "ros-eloquent-demo-nodes-cpp-native: 0.8.0-1", "ros-eloquent-demo-nodes-cpp-native-dbgsym: 0.8.0-1", "ros-eloquent-demo-nodes-py: 0.8.0-1", "\n", ": 2.2.2-1", "ros-eloquent-depthimage-to-laserscan-dbgsym: 2.2.2-1", "ros-eloquent-diagnostic-msgs: 0.8.0-1", "ros-eloquent-diagnostic-msgs-dbgsym: 0.8.0-1", "ros-eloquent-domain-coordinator: 0.8.0-1", "ros-eloquent-dummy-map-server: 0.8.0-1", "ros-eloquent-dummy-map-server-dbgsym: 0.8.0-1", "ros-eloquent-dummy-robot-bringup: 0.8.0-1", "ros-eloquent-dummy-sensors: 0.8.0-1", "ros-eloquent-dummy-sensors-dbgsym: 0.8.0-1", "\n", ": 1.0.0-1", "ros-eloquent-eigen3-cmake-module: 0.1.1-1", "ros-eloquent-example-interfaces: 0.7.1-1", "ros-eloquent-example-interfaces-dbgsym: 0.7.1-1", "ros-eloquent-examples-rclcpp-minimal-action-client: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-action-client-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-action-server: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-action-server-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-client: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-client-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-composition: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-composition-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-publisher: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-publisher-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-service: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-service-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-subscriber: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-subscriber-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-timer: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-timer-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclpy-executors: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-action-client: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-action-server: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-client: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-publisher: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-service: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-subscriber: 0.8.0-1", "ros-eloquent-fastcdr: 1.0.11-1", "ros-eloquent-fastcdr-dbgsym: 1.0.11-1", "ros-eloquent-fastrtps: 1.9.0-3", "ros-eloquent-fastrtps-cmake-module: 0.8.0-1", "ros-eloquent-fastrtps-dbgsym: 1.9.0-3", "ros-eloquent-foonathan-memory-vendor: 0.1.0-1", "\n", ": 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-gazebo-msgs-dbgsym: 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-gazebo-plugins-dbgsym: 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-gazebo-ros-dbgsym: 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-geometry-msgs: 0.8.0-1", "ros-eloquent-geometry-msgs-dbgsym: 0.8.0-1", "ros-eloquent-gmock-vendor: 1.8.9000-1", "ros-eloquent-gtest-vendor: 1.8.9000-1", "\n", ": 2.1.2-1", "ros-eloquent-image-geometry-dbgsym: 2.1.2-1", "ros-eloquent-image-tools: 0.8.0-1", "ros-eloquent-image-tools-dbgsym: 0.8.0-1", "\n", ": 2.2.0-1", "ros-eloquent-image-transport-dbgsym: 2.2.0-1", "\n", ": 2.0.0-1", "ros-eloquent-interactive-markers-dbgsym: 2.0.0-1", "ros-eloquent-intra-process-demo: 0.8.0-1", "ros-eloquent-intra-process-demo-dbgsym: 0.8.0-1", "ros-eloquent-joy: 2.3.2-3", "ros-eloquent-joy-dbgsym: 2.3.2-3", "ros-eloquent-kdl-parser: 2.2.0-1", "ros-eloquent-kdl-parser-dbgsym: 2.2.0-1", "\n", ": 2.1.0-1", "ros-eloquent-laser-geometry-dbgsym: 2.1.0-1", "ros-eloquent-launch: 0.9.1-1", "ros-eloquent-launch-ros: 0.9.1-1", "ros-eloquent-launch-testing: 0.9.1-1", "ros-eloquent-launch-testing-ament-cmake: 0.9.1-1", "ros-eloquent-launch-testing-ros: 0.9.1-1", "ros-eloquent-launch-xml: 0.9.1-1", "ros-eloquent-launch-yaml: 0.9.1-1", "\n", ": 2.2.0-1", "\n", ": 1.0.0-1", "ros-eloquent-libyaml-vendor-dbgsym: 1.0.0-1", "ros-eloquent-lifecycle: 0.8.0-1", "ros-eloquent-lifecycle-dbgsym: 0.8.0-1", "ros-eloquent-lifecycle-msgs: 0.8.0-1", "ros-eloquent-lifecycle-msgs-dbgsym: 0.8.0-1", "ros-eloquent-logging-demo: 0.8.0-1", "ros-eloquent-logging-demo-dbgsym: 0.8.0-1", "\n", ": 2.0.2-1", "ros-eloquent-map-msgs-dbgsym: 2.0.2-1", "\n", ": 3.2.0-1", "ros-eloquent-message-filters-dbgsym: 3.2.0-1", "\n", ": 2.0.2-1", "ros-eloquent-move-base-msgs-dbgsym: 2.0.2-1", "ros-eloquent-nav-msgs: 0.8.0-1", "ros-eloquent-nav-msgs-dbgsym: 0.8.0-1", "ros-eloquent-opensplice-cmake-module: 0.8.1-1", "\n", ": 3.2.0-1", "ros-eloquent-orocos-kdl-dbgsym: 3.2.0-1", "ros-eloquent-osrf-pycommon: 0.1.8-2", "ros-eloquent-osrf-testing-tools-cpp: 1.2.1-1", "ros-eloquent-osrf-testing-tools-cpp-dbgsym: 1.2.1-1", "ros-eloquent-pendulum-control: 0.8.0-1", "ros-eloquent-pendulum-control-dbgsym: 0.8.0-1", "ros-eloquent-pendulum-msgs: 0.8.0-1", "ros-eloquent-pendulum-msgs-dbgsym: 0.8.0-1", "\n", ": 2.4.0-1", "ros-eloquent-poco-vendor: 1.2.0-1", "ros-eloquent-python-cmake-module: 0.8.0-2", "\n", ": 1.0.2-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "ros-eloquent-qt-gui-cpp-dbgsym: 1.0.7-1", "\n", ": 1.0.7-1", "ros-eloquent-quality-of-service-demo-cpp: 0.8.0-1", "ros-eloquent-quality-of-service-demo-cpp-dbgsym: 0.8.0-1", "ros-eloquent-quality-of-service-demo-py: 0.8.0-1", "ros-eloquent-rcl: 0.8.0-1", "ros-eloquent-rcl-action: 0.8.0-1", "ros-eloquent-rcl-action-dbgsym: 0.8.0-1", "ros-eloquent-rcl-dbgsym: 0.8.0-1", "ros-eloquent-rcl-interfaces: 0.8.0-1", "ros-eloquent-rcl-interfaces-dbgsym: 0.8.0-1", "ros-eloquent-rcl-lifecycle: 0.8.0-1", "ros-eloquent-rcl-lifecycle-dbgsym: 0.8.0-1", "ros-eloquent-rcl-logging-log4cxx: 0.3.2-1", "ros-eloquent-rcl-logging-log4cxx-dbgsym: 0.3.2-1", "ros-eloquent-rcl-logging-noop: 0.3.2-1", "ros-eloquent-rcl-logging-noop-dbgsym: 0.3.2-1", "ros-eloquent-rcl-logging-spdlog: 0.3.2-1", "ros-eloquent-rcl-logging-spdlog-dbgsym: 0.3.2-1", "ros-eloquent-rcl-yaml-param-parser: 0.8.0-1", "ros-eloquent-rcl-yaml-param-parser-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp: 0.8.0-1", "ros-eloquent-rclcpp-action: 0.8.0-1", "ros-eloquent-rclcpp-action-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp-components: 0.8.0-1", "ros-eloquent-rclcpp-components-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp-lifecycle: 0.8.0-1", "ros-eloquent-rclcpp-lifecycle-dbgsym: 0.8.0-1", "ros-eloquent-rclpy: 0.8.0-1", "ros-eloquent-rclpy-dbgsym: 0.8.0-1", "ros-eloquent-rcpputils: 0.2.0-1", "ros-eloquent-rcpputils-dbgsym: 0.2.0-1", "ros-eloquent-rcutils: 0.8.1-1", "ros-eloquent-rcutils-dbgsym: 0.8.1-1", "\n", ": 2.2.0-1", "ros-eloquent-resource-retriever-dbgsym: 2.2.0-1", "ros-eloquent-rmw: 0.8.0-1", "ros-eloquent-rmw-connext-cpp: 0.8.0-1", "ros-eloquent-rmw-connext-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-connext-shared-cpp: 0.8.0-1", "ros-eloquent-rmw-connext-shared-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-dbgsym: 0.8.0-1", "ros-eloquent-rmw-fastrtps-cpp: 0.8.0-1", "ros-eloquent-rmw-fastrtps-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp: 0.8.0-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-fastrtps-shared-cpp: 0.8.0-1", "ros-eloquent-rmw-fastrtps-shared-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-implementation: 0.8.0-4", "ros-eloquent-rmw-implementation-cmake: 0.8.0-1", "ros-eloquent-rmw-implementation-dbgsym: 0.8.0-4", "ros-eloquent-rmw-opensplice-cpp: 0.8.0-1", "ros-eloquent-rmw-opensplice-cpp-dbgsym: 0.8.0-1", "ros-eloquent-robot-state-publisher: 2.3.0-1", "ros-eloquent-robot-state-publisher-dbgsym: 2.3.0-1", "ros-eloquent-ros-environment: 2.4.0-1", "ros-eloquent-ros-testing: 0.2.0-1", "ros-eloquent-ros-workspace: 0.7.1-2", "ros-eloquent-ros1-bridge: 0.8.0-2", "ros-eloquent-ros1-bridge-dbgsym: 0.8.0-2", "ros-eloquent-ros2action: 0.8.1-1", "ros-eloquent-ros2bag: 0.2.0-1", "ros-eloquent-ros2cli: 0.8.1-1", "ros-eloquent-ros2component: 0.8.1-1", "ros-eloquent-ros2doctor: 0.8.1-1", "ros-eloquent-ros2interface: 0.8.1-1", "ros-eloquent-ros2launch: 0.9.1-1", "ros-eloquent-ros2lifecycle: 0.8.1-1", "ros-eloquent-ros2msg: 0.8.1-1", "ros-eloquent-ros2multicast: 0.8.1-1", "ros-eloquent-ros2node: 0.8.1-1", "ros-eloquent-ros2param: 0.8.1-1", "ros-eloquent-ros2pkg: 0.8.1-1", "ros-eloquent-ros2run: 0.8.1-1", "ros-eloquent-ros2service: 0.8.1-1", "ros-eloquent-ros2srv: 0.8.1-1", "ros-eloquent-ros2test: 0.2.0-1", "ros-eloquent-ros2topic: 0.8.1-1", "ros-eloquent-rosbag2: 0.2.0-1", "ros-eloquent-rosbag2-converter-default-plugins: 0.2.0-1", "ros-eloquent-rosbag2-converter-default-plugins-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-storage: 0.2.0-1", "ros-eloquent-rosbag2-storage-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-storage-default-plugins: 0.2.0-1", "ros-eloquent-rosbag2-storage-default-plugins-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-test-common: 0.2.0-1", "ros-eloquent-rosbag2-tests: 0.2.0-1", "ros-eloquent-rosbag2-transport: 0.2.0-1", "ros-eloquent-rosbag2-transport-dbgsym: 0.2.0-1", "ros-eloquent-rosgraph-msgs: 0.8.0-1", "ros-eloquent-rosgraph-msgs-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-adapter: 0.8.0-1", "ros-eloquent-rosidl-cmake: 0.8.0-1", "ros-eloquent-rosidl-default-generators: 0.8.0-1", "ros-eloquent-rosidl-default-runtime: 0.8.0-1", "ros-eloquent-rosidl-generator-c: 0.8.0-1", "ros-eloquent-rosidl-generator-c-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-generator-cpp: 0.8.0-1", "ros-eloquent-rosidl-generator-dds-idl: 0.7.1-1", "ros-eloquent-rosidl-generator-py: 0.8.0-1", "ros-eloquent-rosidl-parser: 0.8.0-1", "ros-eloquent-rosidl-runtime-py: 0.8.0-1", "ros-eloquent-rosidl-typesupport-c: 0.8.0-2", "ros-eloquent-rosidl-typesupport-c-dbgsym: 0.8.0-2", "ros-eloquent-rosidl-typesupport-connext-c: 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-c-dbgsym: 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-cpp: 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-cpp-dbgsym: 0.8.1-1", "ros-eloquent-rosidl-typesupport-cpp: 0.8.0-2", "ros-eloquent-rosidl-typesupport-cpp-dbgsym: 0.8.0-2", "ros-eloquent-rosidl-typesupport-fastrtps-c: 0.8.0-1", "ros-eloquent-rosidl-typesupport-fastrtps-c-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-fastrtps-cpp: 0.8.0-1", "ros-eloquent-rosidl-typesupport-fastrtps-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-interface: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-c: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-c-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-cpp: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-opensplice-c: 0.8.1-1", "ros-eloquent-rosidl-typesupport-opensplice-c-dbgsym: 0.8.1-1", "ros-eloquent-rosidl-typesupport-opensplice-cpp: 0.8.1-1", "ros-eloquent-rosidl-typesupport-opensplice-cpp-dbgsym: 0.8.1-1", "\n", ": 1.0.5-1", "\n", ": 1.0.1-1", "\n", ": 1.1.0-1", "\n", ": 1.0.2-1", "\n", ": 1.0.5-1", "\n", ": 1.0.5-1", "ros-eloquent-rqt-gui-cpp-dbgsym: 1.0.5-1", "\n", ": 1.0.5-1", "\n", ": 1.0.2-1", "ros-eloquent-rqt-image-view-dbgsym: 1.0.2-1", "\n", ": 1.0.2-1", "\n", ": 1.0.7-1", "\n", ": 1.1.0-1", "\n", ": 1.0.5-1", "ros-eloquent-rqt-py-common-dbgsym: 1.0.5-1", "\n", ": 1.0.0-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.0-1", "\n", ": 1.0.1-1", "\n", ": 1.0.0-1", "\n", ": 1.0.0-1", "ros-eloquent-rttest: 0.8.0-1", "ros-eloquent-rttest-dbgsym: 0.8.0-1", "\n", ": 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-common-dbgsym: 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-default-plugins-dbgsym: 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-ogre-vendor-dbgsym: 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-rendering-dbgsym: 7.0.1-1", "ros-eloquent-rviz-rendering-tests: 7.0.1-1", "\n", ": 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz2-dbgsym: 7.0.1-1", "ros-eloquent-sensor-msgs: 0.8.0-1", "ros-eloquent-sensor-msgs-dbgsym: 0.8.0-1", "ros-eloquent-shape-msgs: 0.8.0-1", "ros-eloquent-shape-msgs-dbgsym: 0.8.0-1", "ros-eloquent-shared-queues-vendor: 0.2.0-1", "\n", ": 1.0.1-1", "ros-eloquent-sqlite3-vendor: 0.2.0-1", "ros-eloquent-sros2: 0.8.0-1", "ros-eloquent-sros2-cmake: 0.8.0-1", "ros-eloquent-std-msgs: 0.8.0-1", "ros-eloquent-std-msgs-dbgsym: 0.8.0-1", "ros-eloquent-std-srvs: 0.8.0-1", "ros-eloquent-std-srvs-dbgsym: 0.8.0-1", "ros-eloquent-stereo-msgs: 0.8.0-1", "ros-eloquent-stereo-msgs-dbgsym: 0.8.0-1", "\n", ": 2.2.1-1", "ros-eloquent-teleop-twist-joy-dbgsym: 2.2.1-1", "\n", ": 2.3.1-1", "ros-eloquent-test-interface-files: 0.8.0-1", "ros-eloquent-test-msgs: 0.8.0-1", "ros-eloquent-test-msgs-dbgsym: 0.8.0-1", "ros-eloquent-test-osrf-testing-tools-cpp: 1.2.1-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-dbgsym: 0.12.0-1", "ros-eloquent-tf2-eigen: 0.12.0-1", "\n", ": 0.12.0-1", "\n", ": 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-msgs-dbgsym: 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-py-dbgsym: 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-ros-dbgsym: 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tinydir-vendor: 1.1.0-1", "ros-eloquent-tinyxml-vendor: 0.7.0-1", "ros-eloquent-tinyxml2-vendor: 0.6.1-1", "ros-eloquent-tlsf: 0.5.0-1", "ros-eloquent-tlsf-cpp: 0.8.0-1", "ros-eloquent-tlsf-cpp-dbgsym: 0.8.0-1", "ros-eloquent-topic-monitor: 0.8.0-1", "ros-eloquent-trajectory-msgs: 0.8.0-1", "ros-eloquent-trajectory-msgs-dbgsym: 0.8.0-1", "\n", ": 1.0.1-1", "ros-eloquent-turtlesim-dbgsym: 1.0.1-1", "\n", ": 1.3.0-1", "ros-eloquent-uncrustify-vendor-dbgsym: 1.3.0-1", "\n", ": 2.1.0-1", "ros-eloquent-unique-identifier-msgs-dbgsym: 2.1.0-1", "ros-eloquent-urdf: 2.2.0-1", "ros-eloquent-urdf-dbgsym: 2.2.0-1", "ros-eloquent-urdfdom: 2.2.0-1", "ros-eloquent-urdfdom-dbgsym: 2.2.0-1", "\n", ": 1.0.4-1", "\n", ": 2.1.2-1", "ros-eloquent-visualization-msgs: 0.8.0-1", "ros-eloquent-visualization-msgs-dbgsym: 0.8.0-1", "\n", ": 7.0.0-1", "ros-eloquent-yaml-cpp-vendor-dbgsym: 7.0.0-1", "AWS RoboMaker", "Amazon B9", "Amazon ROS Contributions", "Anup Pemmaiah", "Chris Lalancette", "Claire Wang", "Dan Lazewatsky", "David Gossow", "David V. Lu!!", "Dirk Thomas", "Dorian Scholz", "Eclipse Foundation, Inc.", "Emerson Knapp", "Ethan Gao", "Ivan Paunovic", "Jacob Perron", "John Shepherd", "Jose Luis Rivero", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Michel Hidalgo", "Miguel Company", "Mikael Arguedas", "Pete Baughman", "Scott K Logan", "Shane Loretz", "Steve Macenski", "Steven! Ragnarok", "Steven! Ragnar\u00f6k", "Ted Kern", "Tully Foote", "Vincent Rabaud", "William Woodall", "ros-eloquent-ament-cmake-version: 0.8.1-1", "\n", ": 1.0.9000-1", "ros-eloquent-cartographer-ros-dbgsym: 1.0.9000-1", "\n", ": 1.0.9000-1", "ros-eloquent-cartographer-ros-msgs-dbgsym: 1.0.9000-1", "ros-eloquent-cyclonedds-cmake-module: 0.4.1-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-config-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-errors-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-io-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-sigslots-lite-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-time-lite-dbgsym: 1.0.3-1", "ros-eloquent-examples-rclcpp-multithreaded-executor: 0.8.1-1", "ros-eloquent-examples-rclcpp-multithreaded-executor-dbgsym: 0.8.1-1", "\n", ": 0.12.1-2", "\n", ": 2.2.1-1", "\n", ": 2.2.1-1", "\n", ": 1.0.0-1", "ros-eloquent-pcl-msgs-dbgsym: 1.0.0-1", "\n", ": 1.2.1-1", "ros-eloquent-rmw-cyclonedds-cpp: 0.4.1-1", "ros-eloquent-rmw-cyclonedds-cpp-dbgsym: 0.4.1-1", "ros-eloquent-ros2trace: 0.2.9-1", "ros-eloquent-ros2trace-analysis: 0.2.1-1", "ros-eloquent-sophus: 1.1.0-1", "ros-eloquent-system-modes: 0.1.5-1", "ros-eloquent-system-modes-dbgsym: 0.1.5-1", "ros-eloquent-system-modes-examples: 0.1.5-1", "ros-eloquent-system-modes-examples-dbgsym: 0.1.5-1", "\n", ": 2.2.1-1", "ros-eloquent-theora-image-transport-dbgsym: 2.2.1-1", "ros-eloquent-tracetools: 0.2.9-1", "ros-eloquent-tracetools-analysis: 0.2.1-1", "ros-eloquent-tracetools-dbgsym: 0.2.9-1", "ros-eloquent-tracetools-launch: 0.2.9-1", "ros-eloquent-tracetools-read: 0.2.9-1", "ros-eloquent-tracetools-test: 0.2.9-1", "ros-eloquent-tracetools-test-dbgsym: 0.2.9-1", "ros-eloquent-tracetools-trace: 0.2.9-1", "ros-eloquent-action-tutorials-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-interfaces: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-interfaces-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-actionlib-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-actionlib-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-clang-format: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-clang-tidy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-auto: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-clang-format: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-clang-tidy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-copyright: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-core: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-cppcheck: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-cpplint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-definitions: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-dependencies: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-include-directories: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-interfaces: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-libraries: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-link-flags: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-flake8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-gmock: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-gtest: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-include-directories: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-libraries: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-lint-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-mypy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-nose: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pclint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pep257: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pep8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pyflakes: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pytest: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-python: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-target-dependencies: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-test: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-uncrustify: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-xmllint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-copyright: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cppcheck: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cpplint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-flake8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-index-cpp: 0.7.1-1 -> 0.7.2-1", "ros-eloquent-ament-index-cpp-dbgsym: 0.7.1-1 -> 0.7.2-1", "ros-eloquent-ament-index-python: 0.7.1-1 -> 0.7.2-1", "ros-eloquent-ament-lint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-lint-auto: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-lint-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-lint-common: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-mypy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-package: 0.8.3-1 -> 0.8.4-1", "ros-eloquent-ament-pclint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-pep257: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-pep8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-pyflakes: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-uncrustify: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-xmllint: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-camera-calibration-parsers-dbgsym: 2.2.0-1 -> 2.2.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-camera-info-manager-dbgsym: 2.2.0-1 -> 2.2.1-1", "ros-eloquent-common-interfaces: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-composition: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-composition-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-compressed-depth-image-transport-dbgsym: 2.2.0-1 -> 2.2.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-compressed-image-transport-dbgsym: 2.2.0-1 -> 2.2.1-1", "ros-eloquent-connext-cmake-module: 0.8.1-1 -> 0.8.2-1", "\n", ": 2.1.2-1 -> 2.1.3-1", "ros-eloquent-cv-bridge-dbgsym: 2.1.2-1 -> 2.1.3-1", "\n", ": 0.1.0-1 -> 0.1.0-3", "ros-eloquent-cyclonedds-dbgsym: 0.1.0-1 -> 0.1.0-3", "ros-eloquent-demo-nodes-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-cpp-native: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-cpp-native-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-py: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.2-1 -> 2.2.4-1", "ros-eloquent-depthimage-to-laserscan-dbgsym: 2.2.2-1 -> 2.2.4-1", "ros-eloquent-diagnostic-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-diagnostic-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-map-server: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-map-server-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-robot-bringup: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-sensors: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-sensors-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-client-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-server: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-server-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-client-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-composition: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-composition-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-publisher: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-publisher-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-service: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-service-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-subscriber: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-subscriber-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-timer: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-timer-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-executors: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-action-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-action-server: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-publisher: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-service: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-subscriber: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-fastrtps: 1.9.0-3 -> 1.9.2-1", "ros-eloquent-fastrtps-dbgsym: 1.9.0-3 -> 1.9.2-1", "ros-eloquent-foonathan-memory-vendor: 0.1.0-1 -> 0.3.0-1", "ros-eloquent-geometry-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-geometry-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.1.2-1 -> 2.1.3-1", "ros-eloquent-image-geometry-dbgsym: 2.1.2-1 -> 2.1.3-1", "ros-eloquent-image-tools: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-image-tools-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-image-transport-dbgsym: 2.2.0-1 -> 2.2.1-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "ros-eloquent-interactive-markers-dbgsym: 2.0.0-1 -> 2.0.1-1", "ros-eloquent-intra-process-demo: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-intra-process-demo-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-launch: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-ros: 0.9.1-1 -> 0.9.2-1", "ros-eloquent-launch-testing: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-testing-ament-cmake: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-testing-ros: 0.9.1-1 -> 0.9.2-1", "ros-eloquent-launch-xml: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-yaml: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-lifecycle: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-lifecycle-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-logging-demo: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-logging-demo-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 3.2.0-1 -> 3.2.1-1", "ros-eloquent-message-filters-dbgsym: 3.2.0-1 -> 3.2.1-1", "ros-eloquent-nav-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-nav-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-control: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-control-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.4.0-1 -> 2.4.1-1", "ros-eloquent-quality-of-service-demo-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-quality-of-service-demo-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-quality-of-service-demo-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rcl: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-action: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-action-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-lifecycle: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-lifecycle-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-logging-log4cxx: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-log4cxx-dbgsym: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-noop: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-noop-dbgsym: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-spdlog: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-spdlog-dbgsym: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-yaml-param-parser: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-yaml-param-parser-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rclcpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-action: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-action-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-components: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-components-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-lifecycle: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-lifecycle-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclpy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclpy-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rcutils: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcutils-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rmw: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-shared-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-shared-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-shared-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-shared-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-implementation: 0.8.0-4 -> 0.8.1-1", "ros-eloquent-rmw-implementation-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-implementation-dbgsym: 0.8.0-4 -> 0.8.1-1", "ros-eloquent-rmw-opensplice-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-opensplice-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-robot-state-publisher: 2.3.0-1 -> 2.3.1-1", "ros-eloquent-robot-state-publisher-dbgsym: 2.3.0-1 -> 2.3.1-1", "ros-eloquent-ros-environment: 2.4.0-1 -> 2.4.1-1", "ros-eloquent-ros1-bridge: 0.8.0-2 -> 0.8.1-3", "ros-eloquent-ros1-bridge-dbgsym: 0.8.0-2 -> 0.8.1-3", "ros-eloquent-ros2action: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2bag: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-ros2cli: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2component: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2doctor: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2interface: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2launch: 0.9.1-1 -> 0.9.2-1", "ros-eloquent-ros2lifecycle: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2msg: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2multicast: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2node: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2param: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2pkg: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2run: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2service: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2srv: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2topic: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-rosbag2: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-converter-default-plugins: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-converter-default-plugins-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage-default-plugins: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage-default-plugins-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-test-common: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-tests: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-transport: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-transport-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosidl-adapter: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-c: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-c-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-parser: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-runtime-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-c: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-connext-c-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-connext-cpp: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-connext-cpp-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-interface: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-c: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-c-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 1.0.2-1 -> 1.0.3-1", "\n", ": 1.0.2-1 -> 1.0.3-1", "ros-eloquent-rqt-image-view-dbgsym: 1.0.2-1 -> 1.0.3-1", "ros-eloquent-rttest: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rttest-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-common-dbgsym: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-default-plugins-dbgsym: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-ogre-vendor-dbgsym: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-rendering-dbgsym: 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-rendering-tests: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz2-dbgsym: 7.0.1-1 -> 7.0.2-1", "ros-eloquent-sensor-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-sensor-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-shape-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-shape-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-shared-queues-vendor: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-sqlite3-vendor: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-std-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-std-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-std-srvs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-std-srvs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-stereo-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-stereo-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.1-1 -> 2.2.2-1", "ros-eloquent-teleop-twist-joy-dbgsym: 2.2.1-1 -> 2.2.2-1", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-dbgsym: 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-eigen: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-msgs-dbgsym: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-py-dbgsym: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-ros-dbgsym: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tlsf-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-tlsf-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-topic-monitor: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-trajectory-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-trajectory-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 1.0.1-1 -> 1.0.2-1", "ros-eloquent-turtlesim-dbgsym: 1.0.1-1 -> 1.0.2-1", "\n", ": 2.1.2-1 -> 2.1.3-1", "ros-eloquent-visualization-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-visualization-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "Amazon B9", "Amazon ROS Contributions", "Anup Pemmaiah", "Arne Nordmann", "Chris Lalancette", "Christophe Bedard", "Claire Wang", "Daniel Stonier", "David Gossow", "Dirk Thomas", "Eclipse Foundation, Inc.", "Erik Boasson", "Ethan Gao", "Ivan Paunovic", "Jack O\u2019Quin", "Jacob Hassold", "Jacob Perron", "John Shepherd", "Juan Pablo Samper", "Julius Kammerl", "Karsten Knese", "Michael Carroll", "Michel Hidalgo", "Miguel Company", "Mikael Arguedas", "Paul Bovbel", "Pete Baughman", "Scott K Logan", "Shane Loretz", "Steven! Ragnarok", "Steven! Ragnar\u00f6k", "Ted Kern", "Tully Foote", "Vincent Rabaud", "William Woodall"], "url": "https://discourse.ros.org/t/ros-2-eloquent-elusor-call-for-testing-and-package-releases/10923"}
]
